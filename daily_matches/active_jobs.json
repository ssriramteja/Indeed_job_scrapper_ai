[
    {
        "title": "Identity AI / ML Engineer",
        "company": "Openkyber",
        "location": "AK, US USA",
        "posted_at": "2026-02-24",
        "score": 28.9,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Hugging Face",
            "FAISS",
            "Pinecone",
            "ChromaDB",
            "Prompt Engineering",
            "TensorFlow"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4fff7807f26ccfa6",
        "description": "**job title:** gen ai engineer with python work location: dallas, tx or charlotte, nc (onsite\\-hybrid. will consider candidates willing to relocate to client s location) contract duration: 12 months contract\\_w2 must have skills: gen ai agentic ai ml ops python ml data science rag llm nice to have skills: google cloud platform prompt engineering detailed job description: we are seeking a highly skilled generative ai engineer with a strong python background to design, develop, and deploy cutting\\-edge ai solutions. the ideal candidate will have hands\\-on experience with large language models (llms), prompt engineering, and gen ai frameworks, along with expertise in building scalable ai applications. experience in developing agentic ai solutions. key responsibilities: design and implement generative ai models for text, image, or multimodal applications. develop prompt engineering strategies and embedding\\-based retrieval systems. integrate gen ai capabilities into web applications and enterprise workflows. build agentic ai applications with context engineering and mcp tools. required skills \\& qualifications: 10\\+ years of hands\\-on experience in ai, data science, ml, gen ai. strong hands on experience designing and deploying retrieval\\-augmented generation (rag) pipelines strong mlops/llmops experience with ci/cd automation, extensive experience with langchain, langgraph, and agentic ai patterns including routing, memory, multi\\-agent orchestration, guardrails, and failure recovery. experience in cloud\\-native engineering across aws (sagemaker, lambda, ecs/fargate, s3, api gateway, step functions) and google cloud platform (vertex ai) for scalable ai delivery experience in developing microservices and api development using fastapi, rest apis, pydantic/json schemas, docker, and kubernetes for low\\-latency serving. strong hands\\-on experience with vector databases and semantic search technologies including pinecone, faiss, chromadb, and embedding lifecycle management strong proficiency in python and ai/ml frameworks (pytorch, tensorflow). hands on experience using session and memory for building multi\\-agent systems along with using mcp tools. hands\\-on experience with llms, transformers, and hugging face ecosystem. knowledge and experience with vector databases and rag technique for semantic search. familiarity with cloud ai services (aws sagemaker, azure openai, google cloud platform vertex ai). understanding of mlops practices for scalable ai deployment. strong experience in working with llm fine\\-tuning with lora, qlora, peft, strong experience in architected advanced rag systems using pinecone, faiss, weaviate, chroma, hybrid retrieval, and custom embeddings, strong experience in designing end\\-to\\-end llmops/mlops pipelines using mlflow, dvc, sagemaker pipelines, vertex ai pipelines, and github actions experience in using cloud\\-native ai systems on aws (sagemaker, lambda, eks, ec2, step functions, s3, glue) and google cloud platform vertex ai, supporting high\\-volume inference and secure enterprise operations experience in developing multi\\-agent orchestration workflows using langgraph and crewai for tool\\-calling, validation agents, automated reasoning, and workflow supervision minimum years of experience: 10\\+ years certifications needed: yes top 3 responsibilities you would expect the subcon to shoulder and execute: strong experience in gen ai, llm, rag,ml, dl,ml ops, llmops, cloud platform,model servicing optimization, python strong communication skills\n**for applications and inquiries, contact:** hirings@openkyber.com",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Co-Op",
        "company": "ArtiFlex Manufacturing",
        "location": "Wooster, OH, US USA",
        "posted_at": "2026-02-24",
        "score": 24.4,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "Azure ML",
            "S3",
            "Glue",
            "Athena",
            "Redshift",
            "BigQuery"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9bd105687c5aa984",
        "description": "**ai co\\-op**\n**general description:**\n------------------------\n\n\nartiflex manufacturing is seeking a highly motivated **technology intern** with a strong foundation in ai, data engineering, and modern software development to support real\\-world manufacturing, quality, and operations use cases. this role offers hands\\-on exposure to building, deploying, and scaling ai\\-driven applications using enterprise data.  \n\n  \n\nthe intern will work closely with engineering and business stakeholders to design, develop, and deploy data\\- and ai\\-powered solutions that improve operational efficiency, quality analytics, and decision\\-making across the organization.\n**responsibilities:**\n---------------------\n\n\n* design, develop, and maintain ai and data\\-driven applications for manufacturing, quality, and operational analytics\n* work with large\\-scale datasets stored in sql server and cloud data platforms\n* build and optimize machine learning and generative ai solutions, including rag (retrieval\\-augmented generation) systems\n* develop and deploy ai models and ai\\-powered applications into production environments\n* collaborate on data pipelines, etl/elt workflows, and analytics solutions (databricks preferred)\n* contribute innovative ideas to solve complex, real\\-world manufacturing problems\n* follow artiflex code of behaviors, engineering standards, and best practices\n* participate in code reviews, debugging, testing, and performance optimization\n* communicate clearly with technical and non\\-technical stakeholders\n\n**required qualifications**\n---------------------------\n\n\n* prior internship experience or minimum 1 year of relevant work experience\n* currently pursuing or recently completed a master\u2019s degree in computer science, data science, information technology, or artificial intelligence / machine learning (preferred)\n* strong programming skills in python, sql, and c / c\\+\\+\n* experience working with large datasets and relational databases (sql server preferred)\n* strong understanding of modern ai trends, especially generative ai\n* proven ability to think critically, solve problems quickly, and work independently\n* excellent communication and collaboration skills\n* familiarity with modern cloud\\-based data and ai platforms is preferred but not mandatory, such as:\n* lakehouse platforms (databricks or snowflake)\n* aws (s3, glue, athena, redshift, sagemaker)\n* google cloud platform (bigquery, vertex ai, cloud storage)\n* azure (preferred) (azure data lake, synapse, azure ml)\n\n**preferred qualifications**\n----------------------------\n\n\n* hands\\-on experience developing ai/ml models, applications, and deploying them to production\n* strong experience with rag systems, vector databases, and embedding\\-based search\n* experience building, fine\\-tuning, or evaluating llms\n* hands\\-on experience with generative ai frameworks (langchain, llamaindex, openai, etc.)\n* experience using cli tools, git workflows, and ci/cd pipelines\n* experience working on large, collaborative development teams\n* exposure to cutting\\-edge web technologies and ai\\-enabled applications\n* experience with ux/ui principles, design\\-centric approaches, and building engaging user interfaces\n\n**what you\u2019ll gain**\n--------------------\n\n\n* real\\-world experience applying ai and data engineering in manufacturing and enterprise environments\n* hands\\-on exposure to production\\-grade ai systems\n* opportunity to contribute directly to ai initiatives that impact operations and decision\\-making",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Systems Engineer, UDS Data Management",
        "company": "Dell Technologies",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 23.3,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "LLaMA",
            "TensorFlow",
            "PyTorch",
            "Redshift",
            "BigQuery",
            "Synapse",
            "Data Lake"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8b555f123481de91",
        "description": "**germany, remote**\n\n\nour field sales professionals rely on proactive technical support during the sales process \u2013 and our expert systems engineering team always steps up to the mark. we lead the development and implementation of complex and specialized products, applications, services and solutions. from delivering sales presentations and product demonstrations, to developing detailed installation or system integration plans, we ensure customers get the innovative, relevant, interoperable solutions they need.\n\n\njoin us to do the best work of your career and make a profound social impact as a **senior systems engineer** on our systems engineering team in **germany.**\n\n**what you\u2019ll achieve**\n\n\nin this role, you will operate at the intersection of technology, customer success, and business impact. you will design modern data and ai architectures, translate complex requirements into clear use cases, and shape compelling solutions that drive our customers\u2019 data and ai transformation.\n\n**you will:**\n\n* build trusted relationships with customers and lead discovery to understand their goals, challenges, and technical needs.\n* design and validate modern cloud architectures for data, analytics, and ai/genai/rag solutions.\n* create clear solution designs, proposals, and technical materials that guide customers through their data \\& ai transformation.\n* deliver impactful demos and pocs that highlight value, feasibility, and business outcomes.\n* communicate complex data and ai concepts in a simple, customer\\-friendly way to both technical and non\\-technical stakeholders.\n* evaluate modern data and ai platforms and explain how our solutions stand out in the competitive landscape.\n\n**take the first step towards your dream career**\n\n\nevery dell technologies team member brings something unique to the table. here\u2019s what we are looking for with this role:\n\n**essential requirements**\n\n\ntechnical skills\n\n* hands\\-on experience with leading cloud \\& data platforms  \n\n(snowflake, databricks, bigquery, redshift, synapse, cloudera) plus at least one major cloud (aws, azure or gcp).\n* strong data architecture \\& engineering foundation  \n\ndata warehousing, data lakes/lakehouse, sql, etl/elt, data modeling (star/snowflake), performance tuning, governance, and compliance (gdpr/ccpa).\n* experience with data pipelines \\& unstructured data  \n\nintegration, orchestration (airflow, dbt, spark, kafka), batch/streaming, and processing of pdfs, logs, images, text into structured/vectorized formats.\n* familiarity with ai/genai \\& rag concepts  \n\nvector databases (pgvector, elasticsearch, milvus), embeddings, document processing, semantic search, chunking, and rag pipeline fundamentals.\n* broad analytics \\& bi knowledge  \n\nbi tools (power bi, tableau, looker, qlik), self\\-service analytics enablement, and end\u2011to\u2011end solution design from raw data to dashboards and models.\n\n\nconsulting skills\n\n* strong customer\\-facing communication \\& storytelling  \n\nability to explain complex topics clearly to technical/non\u2011technical audiences; delivering presentations to senior stakeholders (cio/cdo).\n* discovery, requirements analysis \\& poc excellence  \n\nskilled at uncovering business drivers, structuring pocs, defining success criteria, and demonstrating solutions that showcase customer value.\n\n\nproven experience\n\n* 5\\+ years in a customer\u2011facing technical role such as sales engineer, solutions architect, data engineer, analytics consultant, or data scientist with strong commercial exposure.\n* track record delivering modern data \\& ai solutions  \n\nexperience in cloud dwh/lakehouse migrations, bi modernization, genai/rag implementations, streaming analytics, or advanced analytics programs.\n\n**desirable requirements**\n\n* hands\\-on experience with ml/ai workflows \\& tools  \n\nfeature engineering, model development, notebooks, python, spark, mlflow, tensorflow/pytorch, langchain, llamaindex.\n* ability to translate ambiguous business problems  \n\ninto clear data, analytics, or ai use cases with measurable value.\n* understanding of competitive data \\& ai landscapes  \n\nability to articulate differentiation across platforms (data warehouses, lakehouses, analytics tools, ai ecosystems).\n\n**who we are**\n\n\nwe believe that each of us has the power to make an impact. that\u2019s why we put our team members at the center of everything we do. if you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.  \n\n  \n\ndell technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. join us to build a future that works for everyone because progress takes all of us.\n\n\ndell technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "(USA) Senior, Data Engineer - Senior Software Engineer",
        "company": "Sam's Club",
        "location": "Sunnyvale, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 23.3,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "LLaMA",
            "BigQuery",
            "Dataflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "BigQuery"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6bedd59c754ec609",
        "description": "**position summary...**\n-----------------------\n\n**what you'll do...**\n---------------------\n\n\nyou have a deep interest and passion for technology. you have a passion to drive critical business initiatives with data. you love writing and owning codes and enjoy working with people who will keep challenging you at every stage. you have strong problem solving, analytic, decision\\- making and excellent communication with interpersonal skills. you are self\\-driven and motivated with the desire to work in a fast\\-paced, results\\-driven agile environment with varied responsibilities.  \n\n  \n\n**about team:**  \n\nsam's club is our membership warehouse club, a business model that provides our members with high\\-quality products at prices that are unrivaled by traditional retail. sam's club provides a carefully curated assortment of items, as well as developing and leading technologies and services such as scan \\& go, club pickup, and home delivery service in select markets. sam's club also provides travel, auto purchasing, pharmacy, optical, hearing aid centers, tire and battery centers, and a portfolio of business operations support services.  \n\n**what you'll do:**\n* design, build, test, and deploy **scalable, intelligent data solutions** that support millions of sam\u2019s club customers\u2014while laying the groundwork for **agentic ai systems** that consume and act on this data.\n* partner with engineering, ai/ml, and product teams to ensure **data services are discoverable, trustworthy, and consumable** by next\\-gen ai agents and llm\\-based systems.\n* collaborate across sam\u2019s club engineering teams to contribute to a **data\\-first, agent\\-aware architecture** and foster a culture of innovation around intelligent automation.\n* engage with product management and business teams to prioritize data products that will drive **autonomous decision\\-making** and **context\\-rich recommendations**.\n* build features that **enable seamless integration between structured/unstructured data and downstream ai agents**, enabling smarter responses, faster insights, and automation.\n* deploy and monitor products on **cloud platforms with agent observability**, telemetry, and auditability in mind.\n* develop and implement best\\-in\\-class **data health monitoring, traceability, and context enrichment** processes to ensure data used by agents is reliable and governed.\n* lead technical solutioning for full\\-lifecycle projects, with an eye on how data will power **autonomous workflows, co\\-pilots, and multi\\-agent systems**.\n\n  \n\n**what you'll bring:**\n* **4\u20136 years of experience** in big data development with a focus on **scalable, fault\\-tolerant** architectures.\n* **2\u20133 years of hands\\-on experience** with cloud platforms such as **gcp or azure**, including services like bigquery, dataflow, pub/sub, or equivalent.\n* strong foundation in **data engineering best practices** and experience building complex data pipelines optimized for **agent consumption**.\n* experience designing and implementing **semantic layers or knowledge graphs** that could power **data\\-aware ai agents**.\n* proven experience in **data modeling and architecture**, with awareness of how data structures affect **retrieval quality and contextual relevance** for agents.\n* exposure to **llm\\-driven workflows**, prompt templating, or orchestration tools (e.g., langchain, llamaindex, crewai) is beneficial but not required.\n* understanding of **data governance**, including quality, access control, and lineage\u2014especially in the context of **agent auditability and trust**.\n* experience writing clean, testable code in **python, java, or scala**; experience with **pyspark/spark** for distributed data processing.\n* demonstrated ability to write optimized, scalable sql and to work with large datasets across cloud\\-native and open\\-source platforms.\n* familiarity with tools like **kafka, spark streaming, druid, and presto**, and how they interact in real\\-time or hybrid data systems.\n* solid software engineering experience across backend and frontend development.\n* advanced experience with java and spring boot, along with strong experience using modern frontend frameworks such as react, angular, or vue.js.\n* experience working with sql (azure sql) and nosql (cosmos, cassandra, mongodb) databases in both backend services and full stack systems.\n* hands\\-on experience with kafka, docker/kubernetes, and cloud platforms such as azure, and gcp.\n* a strong understanding of devops principles, ci/cd pipelines, system observability, and deployment patterns for applications with both frontend and backend components.\n* a demonstrated ability to design and maintain scalable, reliable, high\\-performing full stack applications.\n* success building enterprise\\-level systems that include both user\\-facing interfaces and backend services.\n* strong experience with api design, performance optimization, and scalable solution development across the full stack.\n\nat sam's club, we offer competitive pay as well as performance\\-based bonus awards and other great benefits for a happier mind, body, and wallet!  \n\n  \n\n* **health benefits** include medical, vision and dental coverage\n\n  \n\n* **financial benefits** include 401(k), stock purchase and company\\-paid life insurance\n\n  \n\n* **paid time off benefits** include pto, parental leave, family care leave, bereavement, jury duty, and voting. you will also receive pto and/or ppto that can be used for vacation, sick leave, holidays, or other purposes. the amount you receive depends on your job classification and length of employment. it will meet or exceed the requirements of paid sick leave laws, where applicable.\n\n  \n\nfor information about pto, see https://one.walmart.com/notices.  \n\n  \n\n* **other benefits** include short\\-term and long\\-term disability, company discounts, military leave pay, adoption and surrogacy expense reimbursement, and more.\n\n  \n\nlive better u is a company paid education benefit program for full\\-time and part\\-time associates in walmart and sam's club facilities. programs range from high school completion to bachelor's degrees, including english language learning and short\\-form certificates. tuition, books, and fees are completely paid for by walmart.  \n\n  \n\neligibility requirements apply to some benefits and may depend on your job classification and length of employment. benefits are subject to change and may be subject to a specific plan or program terms.  \n\n  \n\nfor information about benefits and eligibility, see one.walmart.  \n\n  \n\nthe annual salary range for this position is $117,000\\.00 \\- $234,000\\.00  \n\n  \n\nadditional compensation includes annual or quarterly performance bonuses.  \n\n  \n\nadditional compensation for certain positions may also include regional pay zone (rpz) (based on location). at sam's club, we offer competitive pay as well as performance\\-based bonus awards and other great benefits for a happier mind, body, and wallet!\n  \n\n**\\-health benefits** include medical, vision and dental coverage  \n\n  \n\n**\\-financial benefits** include 401(k), stock purchase and company\\-paid life insurance  \n\n  \n\n**\\-paid time off benefits** include pto, parental leave, family care leave, bereavement, jury duty, and voting. you will also receive pto and/or ppto that can be used for vacation, sick leave, holidays, or other purposes. the amount you receive depends on your job classification and length of employment. it will meet or exceed the requirements of paid sick leave laws, where applicable.\n  \n\n  \n\nfor information about pto, see https://one.walmart.com/notices.  \n\n  \n\n**\\- other benefits** include short\\-term and long\\-term disability, company discounts, military leave pay, adoption and surrogacy expense reimbursement, and more.\n  \n\nlive better u is a company paid education benefit program for full\\-time and part\\-time associates in walmart and sam's club facilities. programs range from high school completion to bachelor's degrees, including english language learning and short\\-form certificates. tuition, books, and fees are completely paid for by walmart.\n  \n\neligibility requirements apply to some benefits and may depend on your job classification and length of employment. benefits are subject to change and may be subject to a specific plan or program terms.\n  \n\n  \n\nfor information about benefits and eligibility, see one.walmart.\n  \n\nthe annual salary range for this position is $117,000\\.00 \\- $234,000\\.00 additional compensation includes annual or quarterly performance bonuses. additional compensation for certain positions may also include regional pay zone (rpz) (based on location).\n\u3164\n\n\n\u3164\n\n\n\u3164\n\n\n\u3164\n\n\n\u200e\n\n**minimum qualifications...**\n-----------------------------\n\n*outlined below are the required minimum qualifications for this position. if none are listed, there are no minimum qualifications.*\n\noption 1: bachelor\u2019s degree in computer science and 3 years' experience in software engineering or related field. option 2: 5 years\u2019 experience in  \n\nsoftware engineering or related field. option 3: master's degree in computer science and 1 year\u2019s experience in software engineering or related  \n\nfield.  \n\n2 years' experience in data engineering, database engineering, business intelligence, or business analytics.**preferred qualifications...**\n-------------------------------\n\n*outlined below are the optional preferred qualifications for this position. if none are listed, there are no preferred qualifications.*\n\n\ndata engineering, database engineering, business intelligence, or business analytics, etl tools and working with large data sets in the cloud, master\u2019s degree in computer science or related field and 3 years' experience in software engineering, we value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing web content accessibility guidelines (wcag) 2\\.2 aa standards, assistive technologies, and integrating digital accessibility seamlessly. the ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following walmart\u2019s accessibility standards and guidelines for supporting an inclusive culture.**primary location...**\n-----------------------\n\n\n1395 crossman ave, sunnyvale, ca 94089\\-1114, united states of america\nwalmart and its subsidiaries are committed to maintaining a drug\\-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. this policy applies to all employees and aims to create a safe and productive work environment.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "ML Ops Engineer",
        "company": "Hitachi Digital Services",
        "location": "Reading, PA, US USA",
        "posted_at": "2026-02-24",
        "score": 21.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "S3",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "GitHub Actions"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5df2e993fb881155",
        "description": "**our company**\n\n\n\nwe're hitachi digital services, a global digital solutions and transformation business with a bold vision of our world's potential. we're people\\-centric and here to power good. every day, we future\\-proof urban spaces, conserve natural resources, protect rainforests, and save lives. this is a world where innovation, technology, and deep expertise come together to take our company and customers from what's now to what's next. we make it happen through the power of acceleration.\n\n\n\nimagine the sheer breadth of talent it takes to bring a better tomorrow closer to today. we don't expect you to 'fit' every requirement \u2013 your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\n\n**the team**\n\n\n**mlops l2 support engineer** to provide **24/7 production support** for machine learning (ml) and data pipelines. the role requires **on\\-call support, including weekends**, to ensure high availability and reliability of ml workflows. the candidate will work with **dataiku, aws, ci/cd pipelines, and containerized deployments** to maintain and troubleshoot ml models in production.\n\n\n**the role**\n\n\n**key responsibilities:**\n\n\n**incident management \\& support:**\n\n\n* provide **l2 support** for mlops production environments, ensuring **uptime and reliability**.\n* troubleshoot **ml pipelines, data processing jobs, and api issues**.\n* monitor logs, alerts, and performance metrics using **dataiku, prometheus, grafana, or aws tools such cloudwatch**.\n* perform **root cause analysis (rca)** and resolve incidents within slas.\n* escalate unresolved issues to **l3 engineering teams** when needed.\n\n\n**dataiku platform management:**\n\n\n* manage **dataiku dss workflows**, troubleshoot job failures, and optimize performance.\n* monitor and support **dataiku plugins, apis, and automation scenarios**.\n* collaborate with data scientists and data engineers to **debug ml model deployments**.\n* perform **version control** and ci/cd integration for dataiku projects.\n\n\n**deployment \\& automation:**\n\n\n* support **ci/cd pipelines** for ml model deployment (bamboo, bitbucket etc).\n* deploy ml models and data pipelines using **docker, kubernetes, or dataiku flow**.\n* automate monitoring and alerting for **ml model drift, data quality, and performance**.\n\n\n**cloud \\& infrastructure support:**\n\n\n* monitor **aws\\-based ml workloads** (sagemaker, lambda, ecs, s3, rds).\n* manage storage and compute resources for ml workflows.\n* support **database connections, data ingestion, and etl pipelines** (sql, spark, kafka).\n\n\n**security \\& compliance:**\n\n\n* ensure **secure access control** for ml models and data pipelines.\n* support **audit, compliance, and governance** for dataiku and mlops workflows.\n* respond to **security incidents** related to ml models and data access.\n\n\n**what you'll bring**\n\n\n**experience:** 5\\+ years in mlops, data engineering, or production support.  \n\n\u2705 **dataiku dss:** strong experience in **dataiku workflows, scenarios, plugins, and apis**.  \n\n\u2705 **cloud platforms:** hands\\-on experience with **aws ml services (sagemaker, lambda, s3, rds, ecs, iam)**.  \n\n\u2705 **ci/cd \\& automation:** familiarity with **github actions, jenkins, or terraform**.  \n\n\u2705 **scripting \\& debugging:** proficiency in **python, bash, sql** for automation \\& debugging.  \n\n\u2705 **monitoring \\& logging:** experience with **prometheus, grafana, cloudwatch, or elk stack**.  \n\n\u2705 **incident response:** ability to handle **on\\-call support, weekend shifts, and sla\\-based issue resolution**.\n\n\n**preferred qualifications:**\n\n\n\n\u2795 **containerization:** experience with **docker, kubernetes, or openshift**.  \n\n\u2795 **ml model deployment:** familiarity with **tensorflow serving, mlflow, or dataiku model api**.  \n\n\u2795 **data engineering:** experience with **spark, databricks, kafka, or snowflake**.  \n\n\u2795 **itil/devops certifications:** itil foundation, aws ml certifications; dataiku certification\n\n\n**work schedule \\& on\\-call requirements:**\n\n\n* **rotational on\\-call support (including weekends and nights)**.\n* shift\\-based monitoring for **ml workflows and dataiku jobs**.\n* flexible work schedule to **handle production incidents** and critical ml model failures.\n\n\n**about us**\n\n\n\nwe're a global, team of innovators. together, we harness engineering excellence and passion to co\\-create meaningful solutions to complex challenges. we turn organizations into data\\-driven leaders that can make a positive impact on their industries and society. if you believe that innovation can bring a better tomorrow closer to today, this is the place for you.\n\n\n\n\\#li\\-rs2\n\n  \n\n**fostering innovation through diverse perspectives**\n\n\n\nhitachi is a global company operating across a wide range of industries and regions. one of the things that sets hitachi apart is the diversity of our business and people, which drives our innovation and growth.\n\n\n\nwe are committed to building an inclusive culture based on mutual respect and merit\\-based systems. we believe that when people feel valued, heard, and safe to express themselves, they do their best work.\n\n\n**how we look after you**\n\n\n\nwe help take care of your today and tomorrow with industry\\-leading benefits, support, and services that look after your holistic health and wellbeing. we're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). we're always looking for new ways of working that bring out our best, which leads to unexpected ideas. so here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\n\n*we're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status* *or any other protected characteristic.* should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. MLOps Engineer",
        "company": "Hudson Manpower",
        "location": "Plano, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 21.1,
        "matched_keywords": [
            "TensorFlow",
            "PyTorch",
            "AWS SageMaker",
            "GCP Vertex AI",
            "Azure ML",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=df841e2c966e93a8",
        "description": "**job description**\n\n\nwe are looking for a **senior mlops engineer** with strong experience in building and maintaining machine learning infrastructure and deployment pipelines. the role requires expertise in cloud ml services, automation, and model lifecycle management.\n\n**location:** plano, tx \u2013 onsite  \n\n**duration:** long term  \n\n**work authorization:** w2 only\n\n**technical skills**\n\n* strong programming skills in **python and sql**\n* experience with ml frameworks:\n\t+ tensorflow\n\t+ pytorch\n\t+ scikit\\-learn\n* experience with cloud ml platforms:\n\t+ azure ml\n\t+ aws sagemaker\n\t+ gcp vertex ai\n* containerization and orchestration:\n\t+ docker\n\t+ kubernetes\n* ci/cd pipeline tools:\n\t+ github actions\n\t+ azure devops\n\t+ jenkins\n\t+ gitlab ci\n* observability and monitoring tools:\n\t+ prometheus\n\t+ grafana\n\t+ mlflow\n\t+ evidently ai\n\n **preferred background**\n\n* candidates from tier 1 universities preferred\n* experience working with deloitte or kpmg preferred\n\n **required skills**\n\n* **java full stack development**\n* **spring boot**\n* **rest apis / microservices**\n* **frontend:** angular or react\n* **aws cloud services**\n* **database:** sql / nosql (like mysql, oracle, mongodb)\n* **version control:** git\n* **ci/cd tools** (jenkins or similar)\n* **strong problem\\-solving skills**\n\n **years of experience**\n\n* **8\\+ years** of experience in **java full stack development**\n* strong hands\\-on experience with **java \\+ aws**\n\n **educational qualification**\n\n* **bachelor\u2019s degree** in: computer science, or information technology, or related field",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - ML Ops",
        "company": "Pindrop",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 20.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7db54c6b588d8f5c",
        "description": "**who we are**\n--------------\n\n\n\npindrop is redefining trust in the digital age. our patented voice, and video authentication, fraud detection, and deepfake detection technologies protect some of the world's largest banks, insurers, retailers, and healthcare leaders. as ai\\-driven threats evolve in the form of synthetic voices, deepfakes, face swapping and more, our solutions stay ahead, helping ensure that the real human and the right human are recognized.\n\n\n\npindrop is trusted by fortune 500 enterprises to secure voice interactions, and with $100m arr we're entering our next phase of innovation and growth, backed by world\\-class investors including andreessen horowitz, ivp, and capitalg.\n\n**what you'll do**\n==================\n\n\n\nas a senior software engineer, you will play a critical role in the development and maintenance of software applications and systems. you will be responsible for leading and contributing to complex software projects, providing technical expertise, and mentoring junior engineers. you will expand capabilities and bring new solutions to market. as a member of the mlops team, you will be responsible for systems that train models and produce predictions.\n\n\n\nmore specifically, you will:\n\n\n* software development: design, develop, test, and maintain our complex software applications, ensuring high\\-quality code and adherence to best practices. play a critical role in the development and maintenance of our software products by designing, building, evolving, and scaling state\\-of\\-the\\-art solutions for our pindrop platform.\n* technical leadership: provide technical leadership and guidance to junior engineers and the development team, including code reviews, architecture decisions, and mentoring.\n* architecture and design: contribute to the design and architecture of software systems, ensuring scalability, maintainability, and performance\n* problem solving: analyze and solve complex technical problems, and make recommendations for improvements and optimizations.\n* quality assurance: implement and advocate for best practices in testing and quality assurance, including unit testing, integration testing, and automated testing.\n* code review: participate in code reviews and provide constructive feedback to ensure code quality and consistency.\n* research and innovation: stay current with emerging technologies, tools, and programming languages and apply them where relevant to improve software development processes.\n* security and compliance: ensure software adheres to security standards and compliance requirements, addressing vulnerabilities and potential risks.\n* design and implement cloud solutions, build mlops on cloud (aws, azure, or gcp)\n* build ci/cd pipelines orchestration by gitlab ci, github actions, circle ci, airflow or similar tools\n* data science model review: run code and refactor, optimize, containerize, deploy, version, and monitor its quality\n* validate and add automated tests for data science models\n* work closely with a team of researchers and data scientists to productionize and document research innovations\n\n**who you are**\n===============\n\n\n* you are resilient in the face of challenges, change, and ambiguity\n* you are optimistic and believe that you can make a problem into a solution\n* you are resourceful, excited to uncover innovative solutions, and teach yourself something new when needed\n* you take accountability, do the things you say you'll do, under\\-promise and over\\-deliver\n* you are a strong problem\\-solver with excellent analytical skills.\n* you are an owner and enjoy taking on project leadership as well as mentorship\n* you are a strong verbal and written communicator\n\n**your skill\\-set**\n===================\n\n\n* must have\n\t+ 5\\-7 years of software engineering experience\n\t+ experience with cloud computing environments, especially aws and container\\-based deployment using docker and kubernetes\n\t+ experience working with python, 2\\-3 years minimum\n\t+ experience operating services in production environments\n\t+ a strong understanding of software design principles, software architecture, design patterns, as well as software development best practices, including testing, version control, and continuous integration\n\t+ experience with infrastructure as code tools like terraform or aws cdk\n\t+ experience in monitoring and performance of production platforms using tech stacks and tools such as datadog, elk, grafana, prometheus\n\t+ participation in the on\\-call rotation is required\n* nice to have\n\t+ experience with machine learning frameworks and libraries such as xgboost, scikit\\-learn, h2o, tensorflow, pytorch, keras, spark mllib\n\t+ experience with leading industry machine learning tools and operation frameworks such as mlflow, kubeflow, airflow, seldon core, tfserving\n\t+ experience building microservices and restful apis\n\t+ ci/cd pipelines using tools such as git, jenkins.\n\n**what's in it for you**\n========================\n\n\n\nas a pindropper, you join a rapidly growing company that is making technology more human with the power of voice. you will work alongside some of the best and brightest. we're a passionate group committed to excellence \\- but that doesn't stop us from enjoying the journey as a team with chess and poker tournaments, catered lunches and happy hours, wellness programming, and more. because we take our jobs seriously, we add time for rest with unlimited pto.\n\n\n**0\\-30 (acclimating)**\n\n\n* + complete onboarding and attend new employee orientation sessions with other new pindroppers\n\t+ onboard in the mlops team\n\t+ 1:1s with all the team members\n\t+ get started with your first project, first pr merged\n\n**30\\-60 (learning)**\n\n\n* + be a part of planning and contribute to the smaller tasks to fix existing issues\n\t+ be a part of triaging only the most important issues for the team to be focusing on\n\t+ add small features/resolve tech debt for the mlops team\n\n**60\\-90 (assimilating)**\n\n\n* + fully acclimated with the team\n\t+ be able to pick up any task that comes out of sprint planning\n\t+ be able to design enhancements to the ml platform\n\t+ teach us something new\n\n**what we offer**\n=================\n\n\n\nas a part of pindrop, you'll have a direct impact on our growing list of products and the future of security in the voice\\-driven economy. we hire great people and take care of them. here's a snapshot of the benefits we offer:\n\n\n* competitive compensation, including equity for all employees\n* unlimited paid time off (pto)\n* generous health and welfare plans to choose from \\- including one employer\\-paid \"employee\\-only\" plan!\n* best\\-in\\-class health savings account (hsa) employer contribution\n* affordable vision and dental plans for you and your family\n* employer\\-provided life and disability coverage with additional supplemental options\n* paid parental leave \\- equal for all parents, including birth, adoptive \\& foster parents\n* + give your newest addition to the family one year of diaper delivery! it's our way of welcoming new pindroplets to the family!\n* identity protection through norton lifelock\n* remote\\-first culture with opportunities for in\\-person team events\n* recurring monthly home office allowance\n* when we need a break, we keep it fun with happy hours, ping pong and foosball, drinks and snacks, and monthly massages!\n* remote and in\\-person team activities (think cheese tastings, chess tournaments, talent shows, murder mysteries, and more!)\n* company holidays\n* annual professional development and learning benefit\n* pick your own apple macbook pro\n* retirement plan with competitive 401(k) match\n* wellness program including employee assistance program, 24/7 telemedicine\n\n**what we live by**\n-------------------\n\n\n\nat pindrop, our core values are fundamental beliefs at the center of all we do. they are our guiding principles that dictate our actions and behaviors. our values are deeply embedded into our culture in big and small ways and even help us decide right from wrong when the path forward is unclear. at pindrop, we believe in taking accountability to make decisions and act in a way that reflects who we are. we truly believe making decisions and acting with our core values in mind will help us to achieve our goals and keep pindrop a great place to work:\n\n\n* **audaciously innovate** \\- we continue to change the world, and the way people safely engage and interact with technology. as first principle thinkers, we challenge standards, take risks and learn from our mistakes in order to make positive change and continuous improvement. we believe nothing is impossible.\n* **evangelical customers for life** \\- we delight, inspire and empower customers from day one and for life. we create a partnership and experience that results in a shared passion. we are champions for our customers, and our customers become our champions, creating a universal commitment to one another.\n* **execution excellence** \\- we do what we say and say what we do. we are accountable for making the tough decisions and necessary tradeoffs to deliver quality and effective solutions on time.\n* **win as a company** \\- every time we win, we win as a company. every time we lose, we lose as a company. we break down silos, support one another, embrace diversity and celebrate our successes. we are better together.\n* **make a difference** \\- every day we have the opportunity to make a positive impact. we operate with dedication, passion, and uncompromising integrity, creating a safer, more secure world.\n\n**not sure if this is you?**\n----------------------------\n\n\n\nwe want a diverse, global team, with a broad range of experience and perspectives. if this job sounds great, but you're not sure if you qualify, apply anyway! we carefully consider every application and will either move forward with you, find another team that might be a better fit, keep in touch for future opportunities, or thank you for your time.\n\n\n### **pindrop is an equal opportunity employer**\n\n\n\nhere at pindrop, it is our mission to create and maintain a diverse and inclusive work environment. as an equal opportunity employer, all qualified applicants receive consideration for employment without regard to race, color, age, religion, sex, gender, gender identity or expression, sexual orientation, national origin, genetic information, disability, marital and/or veteran status.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Data Analyst",
        "company": "Maximus",
        "location": "Tysons, VA, US USA",
        "posted_at": "2026-02-24",
        "score": 20.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Copilot",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "AWS SageMaker",
            "Snowflake",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5908ca8b0f7aacbd",
        "description": "essential duties and responsibilities:* perform hands\\-on data analysis and modeling with huge data sets.\n* apply data mining, nlp, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms.\n* work side\\-by\\-side with product managers, software engineers, and designers in designing experiments and minimum viable products.\n* discover data sources, get access to them, import them, clean them up, and make them \"model\\-ready.\"\n* create and refine features from the underlying data.\n* run regular a/b tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders.\n* explore new design or technology shifts in order to determine how they might connect with the customer benefits we wish to deliver.\n\n**ai\u2011enhanced analytics \\& insight generation**\n  \n\n* develop, deploy, and operationalize advanced analytical and ai/ml models to uncover trends, identify anomalies, and produce predictive and prescriptive insights.\n* apply machine learning techniques (regression, classification, clustering, nlp, time\\-series forecasting) to support strategic decision\\-making and operational efficiency.\n* serve as a subject matter expert on ai\u2011supported analytics, guiding internal stakeholders on appropriate methodology, data quality considerations, model limitations, and responsible ai use.\n\n**data analysis, kpi development \\& reporting*** analyze large volumes of structured and unstructured data and translate findings into clear, actionable insights for leaders, program teams, and business development partners.\n* design, implement, and maintain new kpis, intelligent metrics, and automated analytical scripts used across dashboards and operational reporting frameworks.\n* perform feature engineering, statistical testing, and exploratory analysis to support model development and performance measurement.\n\n**power bi, dashboarding \\& data products*** design, build, and maintain interactive dashboards in power bi, leveraging ai\u2011assisted capabilities such as automated insights, anomaly detection, and forecast visualizations.\n* enhance reporting workflows through semantic modeling, dax optimization, and integration with\n* microsoft fabric or other enterprise analytics platforms.\n\n**automation, low\u2011code solutions \\& ai tooling*** use power automate, power apps, and ai builder to automate workflows, streamline data processes, and develop intelligent low\u2011code applications.\n* build ai\u2011enabled tools such as semantic search features, automated classification engines, natural\u2011language query interfaces, or document\u2011processing models.\n\n**technology enablement \\& collaboration*** stay current with emerging technologies, including microsoft fabric, azure ai/cognitive services, azure machine learning, and other platforms used across maximus.\n* promote responsible ai principles, documenting processes and advocating best practices across teams.\n* support team communication and strategy through documentation, presentations, training materials, and collaborative planning activities.\n* participate in project management tasks including planning, execution, retrospectives, and performance tracking.\n\nminimum requirements\n\n\n* bachelor's degree in relevant field of study and 3\\+ years of relevant professional experience required, or equivalent combination of education and experience.\n* advanced degree in computer science, information systems, business analytics, mathematics, statistics, engineering, business administration or a related field preferred.\n* 1\\-3\\+ years of professional experience with applying quantitative research in optimizing human decisions using technologies like machine learning and/or deep learning.\n* 1\\+ years using major machine learning/deep learning frameworks (e.g., scikit\\-learn, pytorch, tensorflow and keras) and algorithms (e.g., cnn, gan, lstm, rnn, xgboost).\n* 1\\+ years of data engineering experience with modern big data analytics architectures (hadoop, sql, hive, spark, snowflake, etc.) on major cloud platforms (e.g., aws, azure, google cloud).\n* 1\\+ years programming skill in python, scala, or julia.\n* working knowledge with modern cloud\\-based data storage and compute environments (e.g., aws sagemaker, databricks in azure, ai\\-platform in gcp, etc.).\n* experience deploying ml models into discovery/production environment to drive insights using mlops a plus.\n* experience working in an agile delivery model a plus.\n* demonstrated leadership and self\\-direction. willingness to both teach others and learn new techniques.\n* ability to communicate complex ideas in a clear, precise, and actionable manner.\n* excellent communication and presentation skills, with the ability to articulate new ideas and concepts to technical and non\\-technical partners.\n* powerbi experience a plus.\n\n**program specific requirements:**\n\nminimum requirements\n\n* experience developing analytics or ml solution\n* proficiency in power bi, sql, dax, data modeling, and visualization best practice\n* experience analyzing large datasets and presenting insights to technical and non\u2011technical audiences\n* understanding of responsible ai governance, model evaluation, and mlops practices\n\npreferred qualifications\n\n\n\\- experience with azure machine learning, cognitive services, databricks, microsoft \\- fabric, or similar cloud\u2011based ai/analytics tools\n\n\n* experience deploying machine learning models in production environments\n* familiarity with power platform ai builder, copilot capabilities, or generative ai tooling\n* experience working with governance on new tools\n\n**home office requirements:**\n\n* reliable high\\-speed internet service\n* minimum 20 mpbs download speeds/50 mpbs for shared internet connectivity\n* minimum 5 mpbs upload speeds\n\neeo statement\n  \n\n  \n\nmaximus is an equal opportunity employer. we evaluate qualified applicants without regard to race, color, religion, sex, age, national origin, disability, veteran status, genetic information and other legally protected characteristics.\n\n\npay transparency\n  \n\n  \n\nmaximus compensation is based on various factors including but not limited to job location, a candidate's education, training, experience, expected quality and quantity of work, required travel (if any), external market and internal value analysis including seniority and merit systems, as well as internal pay alignment. annual salary is just one component of maximus's total compensation package. other rewards may include short\\- and long\\-term incentives as well as program\\-specific awards. additionally, maximus provides a variety of benefits to employees, including health insurance coverage, life and disability insurance, a retirement savings plan, paid holidays and paid time off. compensation ranges may differ based on contract value but will be commensurate with job duties and relevant work experience. an applicant's salary history will not be used in determining compensation. maximus will comply with regulatory minimum wage rates and exempt salary thresholds in all instances.\n\n\n**accommodations**\n  \n\nmaximus provides reasonable accommodations to individuals requiring assistance during any phase of the employment process due to a disability, medical condition, or physical or mental impairment. if you require assistance at any stage of the employment process\\-including accessing job postings, completing assessments, or participating in interviews,\\-please contact people operations at **applicantaccom@maximus.com** .\n\n\nminimum salary  \n\n$80,000\\.00\n\n\nmaximum salary  \n\n$100,000\\.00",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist- Payments Optimization",
        "company": "Worldpay",
        "location": "Cincinnati, OH, US USA",
        "posted_at": "2026-02-24",
        "score": 20.0,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "CI/CD",
            "Git",
            "Snowflake",
            "Databricks",
            "PySpark",
            "Hadoop",
            "Tableau",
            "Power BI"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=68e605c94d2c096e",
        "description": "**job description**\n\n\nare you ready to unleash your full potential? we\u2019re looking for people who are passionate about payments to chart worldpay\u2019s path to being the largest and most\\-loved payments company in the world.\n\n**about the team**\n\n\nworldpay, llc seeks data scientist\\- payments optimization in cincinnati, oh to work within the payment strategic routing team to build, validate, and deploy machine learning (ml) models and statistical models that drive smarter debit routing decisions and real time transaction optimization.\n\n**what you will be doing**\n\n\nthe data scientist\\- payments optimization will develop automated, end to end pipelines for data ingestion, feature engineering, and model deployment using databricks, snowflake, apache spark, and other big data frameworks. additionally, the role will:\n\n\n* perform exploratory analysis on large structured \\& unstructured datasets and present findings through dashboards and interactive visualizations for both technical and non\\-technical stakeholders.\n* design and evaluate a/b tests to measure uplift, optimize routing tables, and inform product strategy.\n* audit existing analytical workflows and introduce ai powered automation to improve efficiency, accuracy, and scalability.\n* translate business challenges into technical solutions, working closely with product, engineering, risk, and compliance teams.\n* ensure data quality, security, and regulatory compliance; document models and establish monitoring to track performance over time.\n* balance sophistication with practicality, choosing the right level of complexity to meet performance, latency, and cost goals.\n\n**requirements**\n\n\nbachelor\u2019s degree or foreign equivalent in information systems or related field and five (5\\) years of progressively responsible experience in the job offered or a related occupation: utilizing python , r, sql, and at least one jvm language (scala/java) for spark; working with ml/data science libraries (scikit learn, pyspark ml, tensorflow, etc.); working with cloud data platforms (aws, gcp, or azure), databricks, and snowflake; operating with data modeling, etl/elt pipelines, and distributed computing frameworks (spark, hadoop); implementing version control \\& devops fundamentals (git, ci/cd); working with a/b testing, experiment design, and optimization techniques; employing tableau, power bi, matplotlib, or r shiny. telecommuting and/or working from home may be permissible pursuant to company policies.\n\n**what we offer you**\n\n* a competitive salary and benefits\n* a variety of career development tools, resources and opportunities\n* the chance to work on some of the most challenging, relevant issues in the payment industry\n* time to support charities and give back in your community\n\n  \n\n\n**eeoc statement**\n\n\nworldpay is an equal opportunity employer. we evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics. the eeo is the law poster is available here.\n\n\nif you are made a conditional offer of employment and will be working in the united states, you will be required to undergo a drug test. in developing this job description care was taken to include all competencies and requirements needed to successfully perform the position. reasonable accommodations will be provided for individuals with qualified disabilities both during the hiring process, as well as to allow the individual to perform the essential functions of the job, if hired.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer II - Backend - AI Search",
        "company": "Seismic",
        "location": "US USA",
        "posted_at": "2026-02-23",
        "score": 18.9,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Terraform",
            "Snowflake"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9ee26f17ffea2401",
        "description": "overview:\n\nai is one of the fastest growing product areas in seismic. we believe that ai, particularly generative ai, will empower and transform how enterprise sales and marketing organizations operate and interact with customers. seismic aura, our leading ai engine, is powering this change in the sales enablement space and is being infused across the seismic enablement cloud. our focus is to leverage ai across the seismic platform to make our customers more productive and efficient in their day\\-to\\-day tasks, and to drive more successful sales outcomes. **as a senior software engineer ii \\- search**, you will play a crucial role in developing and optimizing backend systems that power our web application, including search, content discovery, and ai capabilities. you will collaborate with cross\\-functional teams to design, build, and maintain scalable, high\\-performance systems that deliver exceptional value to our customers. this position offers a unique opportunity to make a significant impact on our company's growth and success by contributing to the technical excellence and innovation of our search solutions.  \n\nwho you are::\n* **experience**: 7\\+ years of experience in software engineering and a proven track record of building and scaling microservices and working with data retrieval systems.\n\n* **technical expertise:**\n\t+ 5\\+ experience with c\\# and .net, unit testing, object\\-oriented programming, and web services.\n\t+ 3\\+ experience with python, with the ability to work concurrently on python and .net repositories.\n\t+ 3\\+ experience with redis, including expertise in managing large\\-scale redis clusters\n\t+ 2\\+ experience with postgresql, including maintaining and performing tuning\n\t+ proficient in test driven development (tdd) with hands\\-on experience using xunit and postman to develop automation test scripts.\"\n\t+ experience with infrastructure as code (terraform, pulumi, etc.),\n\t+ experience with event driven architectures with tools like kafka,\n\t+ experienced in container technologies such as docker and proficient in microservice frameworks like kubernetes (k8s)\n\t+ experienced in continuous integration and continuous deployment (ci/cd) with expertise in developing jenkins pipelines using scala.\n\t+ experience with ddd (domain driven development) or feature toggle (launch darkly) is good to have.\n\t+ newrelic, snowflake, ansible, ninjia2 experience is a plus\n\t+ front\\-end/full stack experience a plus.\n\n* **cloud expertise**: experience with cloud platforms like aws, google cloud platform (gcp), or microsoft azure. knowledge of cloud\\-native services for ai/ml, data storage, and processing. experience deploying containerized applications into kubernetes is a plus.\n\t+ good to have: **search/retrieval/ai:** prefer expertise in search platforms like elasticsearch, apache solr, or similar. experience with natural language processing (nlp), semantic search and understanding of text processing techniques is a plus.\n\t+ **saas knowledge:** extensive experience in saas application development and cloud technologies, with a deep understanding of modern distributed system and cloud operational infrastructure.\n\t+ **product development:** experience in collaborating with product management and design, with the ability to translate business requirements into technical solutions that drive successful delivery. proven record of driving feature development from concept to launch.\n* proven ability to collaborate effectively with teams across different regions.\n* ability to collaborate effectively with the china team and adapt to the china timezone is essential\n* scrum and jira experience a plus\n\n  \n\n\n\n* **education:** bachelor's or master's degree in computer science, engineering, or a related field.\n\n* **fast\\-paced environment:** experience working in a fast\\-paced, dynamic environment, preferably in a saas or technology\\-driven company.\n\n\nwhat you'll be doing::\n* **distributed systems development:** design, develop, and maintain backend systems and services for search functionality, ensuring high performance, scalability, and reliability.\n* **search \\& algorithm optimization:** implement and optimize search and ai\\-driven semantic algorithms, indexing, and information retrieval techniques to enhance search accuracy and efficiency.\n* **integration:** collaborate with data scientists, ai engineers, and product teams to integrate ai\\-driven search capabilities across the seismic platform.\n* **performance tuning:** monitor and optimize search performance, addressing bottlenecks and ensuring low\\-latency query responses.\n* **technical leadership:** provide technical guidance and mentorship to junior engineers, promoting best practices in search backend development.\n* **collaboration:** work closely with cross\\-functional and geographically distributed teams, including product managers, frontend engineers, and ux designers, to deliver seamless and intuitive search experiences.\n* **continuous improvement:** stay updated with the latest trends and advancements in search technologies, conducting research and experimentation to drive innovation.\n\n\nwhat we have for you::\n\nat seismic, we\u2019re committed to providing benefits and perks for the whole self. to explore our benefits available in each country, please visit the global benefits page.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Software Engineer - Content Platform Engineering",
        "company": "nan",
        "location": "Glendale, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 18.9,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "S3",
            "EC2",
            "Docker",
            "Kubernetes",
            "Jenkins"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2dc4184d266d0070",
        "description": "**department/group overview:**\n\n**disney entertainment and espn product \\& technology**\n\n\ntechnology is at the heart of disney\u2019s past, present, and future. disney entertainment and espn product \\& technology is a global organization of engineers, product developers, designers, technologists, data scientists, and more \u2013 all working to build and advance the technological backbone for disney\u2019s media business globally.\n\n\nthe team marries technology with creativity to build world\\-class products, enhance storytelling, and drive velocity, innovation, and scalability for our businesses. we are storytellers and innovators. creators and builders. entertainers and engineers. we work with every part of the walt disney company\u2019s media portfolio to advance the technological foundation and consumer media touch points serving millions of people around the world.\n\n **here are a few reasons why we think you\u2019d love working here:**\n\n\n1\\.**building the future of disney\u2019s media:** our technologists are designing and building the products and platforms that will power our media, advertising, and distribution businesses for years to come.\n\n\n2\\. **reach, scale \\& impact:** more than ever, disney\u2019s technology and products serve as a signature doorway for fans' connections with the company\u2019s brands and stories. disney\\+. hulu. espn. abc. abc news\u2026and many more. these products and brands \u2013 and the unmatched stories, storytellers, and events they carry \u2013 matter to millions of people globally.\n\n\n3\\. **innovation:** we develop and implement groundbreaking products and techniques that shape industry norms, and solve complex and distinctive technical problems.\n\n\ncontent platforms \\& operations is responsible for the development and ongoing advancement of the technical and operational functions driving the worldwide distribution and monetization of disney\u2019s linear networks and theatrical content. this includes disney\u2019s media supply chain and storage, network and theatrical distribution operations, media localization, network transmission and origination, and more. the global team also plays a critical role as the primary collaboration point for product \\& technology with disney\u2019s emea, apac, and latam regional business teams.\n\n**job summary:**\n\n\nwe are seeking a highly skilled senior software engineer to join our content platform engineering team, focusing on building and enhancing platform capabilities with a strong emphasis on computer vision and machine learning (ml) workflows. our team develops and manages a diverse suite of products and services that power disney\u2019s media supply chain, enabling content delivery for iconic brands like disney\\+, espn\\+, hulu, abc, marvel, and star wars.\n\n\nin this role, you will design and implement scalable systems, data pipelines, and platform services that deliver automated inspection, validation, and enrichment of media assets. you will be responsible for building end\\-to\\-end media inspection pipelines that leverage computer vision to streamline content analysis and inspection. leveraging modern software engineering practices and cloud infrastructure, you will build robust, extensible platforms that enable intelligent content operations at scale.\n\n**responsibilities and duties of the role:**\n\n* **write and maintain software solutions**: design, write, test, and deploy high\\-quality code to develop reliable, maintainable software solutions.\n* **collaborate with cross\\-functional teams**: work closely with platform engineers, product teams, and data scientists to ensure technical solutions effectively support business goals and content strategies.\n* **production support**: monitor and optimize production systems, ensuring platform stability, performance, and uptime.\n* **mentorship:** provide technical guidance and mentorship to junior engineers, fostering a culture of collaboration and continuous learning.\n\n**required education, experience/skills/training:**\n\n\nbasic qualifications:\n\n* 5\\+ years relevant industry experience\n* proficiency in at least one core programming language (e.g., java, python, or javascript)\n* experience working with rest apis or graphql for building and consuming web services.\n* experience with cloud platforms like aws and their core services (e.g., s3, lambda, ec2\\).\n* solid understanding of software design principles, algorithms, and data structures.\n* experience with build and deployment technologies such as docker, gitlab\\-ci, terraform/cloud formation, and jenkins\n* experience with containerization and orchestration technologies (e.g., docker, kubernetes).\n* expertise with full lifecycle of application development, including best practices of unit testing, code reviews, documentation, etc\n* experience with a wide range of data store technologies such as mongodb, redis, elasticsearch, postgres, and dynamodb\n* experience in agile/scrum methodology\n* team player with strong oral and written communications skills\n\n\npreferred qualifications:\n\n* exposure to machine learning frameworks (e.g., tensorflow, pytorch, scikit\\-learn) or building ai/ml solutions.\n* familiar with machine learning model training\n* familiar with prompt engineering and interacting with llms\n* experience integrating machine learning models into production systems, including working with data pipelines and ai\\-driven solutions.\n* previous work experience in media or entertainment technology, particularly in content distribution and media supply chain.\n\n\nrequired education:\n\n* bachelor\u2019s degree in computer science or comparable field of study, and/or equivalent work experience.\n\n  \n\n\nthe hiring range for this position in glendale, ca and bristol, ct is $138,900 to $186,200 per year. the base pay actually offered will take into account internal equity and also may vary depending on the candidate\u2019s geographic region, job\\-related knowledge, skills, and experience among other factors. a bonus and/or long\\-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer II - AI/ML",
        "company": "Seismic",
        "location": "US USA",
        "posted_at": "2026-02-23",
        "score": 17.8,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Terraform",
            "Snowflake"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1b9be3c418bc5646",
        "description": "overview:\n\nai is one of the fastest growing product areas in seismic. we believe that ai, particularly generative ai, will empower and transform how enterprise sales and marketing organizations operate and interact with customers. seismic aura, our leading ai engine, is powering this change in the sales enablement space and is being infused across the seismic enablement cloud. our focus is to leverage ai across the seismic platform to make our customers more productive and efficient in their day\\-to\\-day tasks, and to drive more successful sales outcomes.\n\n **as a senior software engineer \u2013 ai/ml**, you will play a crucial role in developing and optimizing backend systems that power our ai agents and ai capabilities. you will collaborate with cross\\-functional teams to design, build, and maintain scalable, high\\-performance systems that deliver exceptional value to our customers. this position offers a unique opportunity to make a significant impact on our company's growth and success by contributing to the technical excellence and innovation of our ai solutions.\n\n\nwho you are::\n* **experience**: 6\\+ years of experience in software engineering and a proven track record of building and scaling microservices and working with data retrieval systems.\n* **technical expertise:**\n\t+ 5\\+ experience with c\\# and .net, unit testing, object\\-oriented programming, and web services.\n\t+ 3\\+ experience with python, with the ability to work concurrently on python and .net repositories.\n\t+ 2\\+ experience with postgresql, including maintaining and performing tuning\n\t+ proficient in test driven development (tdd) with hands\\-on experience using xunit and postman to develop automation test scripts.\"\n\t+ experience with infrastructure as code (terraform, pulumi, etc.),\n\t+ experience with event driven architectures with tools like kafka,\n\t+ experienced in container technologies such as docker and proficient in microservice frameworks like kubernetes (k8s)\n\t+ experienced in continuous integration and continuous deployment (ci/cd) with expertise in developing jenkins pipelines using scala.\n\t+ experience with ddd (domain driven development) or feature toggle (launch darkly) is good to have.\n\t+ newrelic, snowflake, ansible, ninjia2 experience is a plus\n\t+ front\\-end/full stack experience a plus.\n* **cloud expertise**: experience with cloud platforms like aws, google cloud platform (gcp), or microsoft azure. knowledge of cloud\\-native services for ai/ml, data storage, and processing. experience deploying containerized applications into kubernetes is a plus.\n* **saas knowledge:** extensive experience in saas application development and cloud technologies, with a deep understanding of modern distributed system and cloud operational infrastructure.\n* **product development:** experience in collaborating with product management and design, with the ability to translate business requirements into technical solutions that drive successful delivery. proven record of driving feature development from concept to launch.\n\t+ proven ability to collaborate effectively with teams across different regions.\n\t+ scrum and jira experience a plus\n* **education:** bachelor's or master's degree in computer science, engineering, or a related field.\n* **fast\\-paced environment:** experience working in a fast\\-paced, dynamic environment, preferably in a saas or technology\\-driven company.\n\n\nwhat you'll be doing::\n* **distributed systems development:** design, develop, and maintain backend systems and services for generative ai and agentic workflows, ensuring high performance, scalability, and reliability.\n* **integration:** collaborate with data scientists, ai engineers, and product teams to integrate ai\\-driven capabilities across the seismic platform.\n* **performance tuning:** monitor and optimize agentic workflows\u2019 performance, addressing bottlenecks and ensuring low\\-latency query responses.\n* **technical leadership:** provide technical guidance and mentorship to junior engineers, promoting best practices in backend development.\n* **collaboration:** work closely with cross\\-functional and geographically distributed teams, including product managers, frontend engineers, and ux designers, to deliver seamless and intuitive ai experiences.\n* **continuous improvement:** stay updated with the latest trends and advancements in ai technologies, conducting research and experimentation to drive innovation.\n\n\nwhat we have for you::\n\nat seismic, we\u2019re committed to providing benefits and perks for the whole self. to explore our benefits available in each country, please visit the global benefits page.\n: \\#li\\-dl1",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Machine Learning Engineer",
        "company": "nan",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-24",
        "score": 17.8,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "RAG",
            "LLaMA",
            "Gemini",
            "TensorFlow",
            "PyTorch",
            "AWS SageMaker",
            "Git",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=03889315af72efca",
        "description": "technology is at the heart of disney\u2019s past, present, and future. disney entertainment and espn product \\& technology is a global organization of engineers, product developers, designers, technologists, data scientists, and more \u2013 all working to build and advance the technological backbone for disney\u2019s media business globally.\n\n\nthe team marries technology with creativity to build world\\-class products, enhance storytelling, and drive velocity, innovation, and scalability for our businesses. we are storytellers and innovators. creators and builders. entertainers and engineers. we work with every part of the walt disney company\u2019s media portfolio to advance the technological foundation and consumer media touch points serving millions of people around the world.\n\n\nhere are a few reasons why we think you\u2019d love working here:\n\n* **building the future of disney\u2019s media:** our technologists are designing and building the products and platforms that will power our media, advertising, and distribution businesses for years to come.\n* **reach, scale \\& impact:** more than ever, disney\u2019s technology and products serve as a signature doorway for fans' connections with the company\u2019s brands and stories. disney\\+. hulu. espn. abc. abc news\u2026and many more. these products and brands \u2013 and the unmatched stories, storytellers, and events they carry \u2013 matter to millions of people globally.\n* **innovation:** we develop and implement groundbreaking products and techniques that shape industry norms, and solve complex and distinctive technical problems.\n\n\nad platforms is responsible for disney\u2019s industry\\-leading ad technology and products \u2013 driving advertising performance, innovation, and value in disney\u2019s sports, news, and entertainment content, across all media platforms.\n\n\nour mission is to advance ai and machine learning capabilities across ad platform by delivering scalable, high impact ai/ml and data science solutions that enhance ad decisioning, forecasting, experimentation and ad experience. we are seeking a senior machine learning engineer to join this innovative team. this role offers a unique leadership opportunity for an experienced ml engineer who thrives at the intersection of technical excellence, strategic impact, and cross\\-functional collaboration.\n\n**what you\u2019ll do**\n\n* drive innovation and apply state of the art ai and machine learning across advertising domains, including inventory forecasting, ad experience, ad pacing, pricing, targeting, and efficient ad delivery.\n* invent and iterate on novel solutions to complex advertising challenges with rapid prototyping and deployment cycles.\n* design, build, and scale robust ml systems that power core ad platform capabilities\n* champion engineering excellence through best practices in code quality, system design, and operational reliability.\n* mentor and support junior engineers, fostering a culture of continuous learning and technical growth.\n\n**what to bring**\n\n* bachelor's in computer science or equivalent experience.\n* minimum 5 years of hands\\-on experience developing and deploying large\\-scale machine learning systems.\n* strong knowledge of ai/ml technologies, mathematics and statistics.\n* excellent communication, collaboration skills, and a strong teamwork ethic with both technical and non\\-technical audiences\n* strong foundations in algorithms, data structures, and numerical optimization with experience in programming languages such as python (primary), java and sql\n* familiarity with tools and frameworks such as tensorflow, pytorch, hugging libraries etc\n* proven proficiency in deep learning methodologies, including recurrent and sequence\\-based models.\n* hands\\-on experience with transformer architectures (e.g., bert, gpt, vit) for natural language and vision tasks.\n* strong understanding of multimodal embedding techniques for integrating text, image, audio, and structured data.\n* experience with llm models such as gpt models, claude, gemini, llama, etc.\n* strong grasp of llm evaluation methodologies, experience with rag architectures.\n* a proven track record of thriving in a fast\\-paced, data\\-driven, and collaborative work environment is required.\n\n**nice\\-to\\-haves**\n\n* ms or phd(preferred) in computer science or equivalent experience.\n* experience in digital video advertising or digital marketing domain.\n* experience with databricks and aws sagemaker.\n\n**location**: seattle / santa monica\n\n  \n\nthe hiring range for this position in los angeles, ca area is $141,900 to $190,300 per year and in seattle, wa is $148,700 \\- $199,400 per year. the base pay actually offered will take into account internal equity and also may vary depending on the candidate\u2019s geographic region, job\\-related knowledge, skills, and experience among other factors. a bonus and/or long\\-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Dir, AI Platform Engineering",
        "company": "NYC Health + Hospitals",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 16.7,
        "matched_keywords": [
            "AI Engineer",
            "MLflow",
            "Docker",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=34c920d00f0a63af",
        "description": "**about nyc health \\+ hospitals**\n---------------------------------\n\n  \n\n\nnyc health \\+ hospitals is the largest public health care system in the united states. we provide essential outpatient, inpatient and home\\-based services to more than one million new yorkers every year across the city\u2019s five boroughs. our large health system consists of ambulatory centers, acute care centers, post\\-acute care/long\\-term care, rehabilitation programs, home care, and correctional health services. our diverse workforce is uniquely focused on empowering new yorkers.\n\n\nat nyc health \\+ hospitals, our mission is to deliver high quality care health services, without exception. every employee takes a person\\-centered approach that exemplifies the icare values (integrity, compassion, accountability, respect, and excellence) through empathic communication and partnerships between all persons.\n\n\n**work shifts**\n---------------\n\n  \n\n\n**9:00 a.m \u2013 5:00 p.m**\n\n\n**duties \\& responsibilities**\n------------------------------\n\n  \n\n\npurpose of functional assignment:  \n\nthe director of ai platform engineering provides strategic leadership for the cloud, platform, and deployment infrastructure supporting artificial intelligence (ai) across the system. this role ensures ai systems used in clinical workflows operate safely, reliably, securely, and are in compliance with applicable laws and nyc health \\+ hospitals rules and regulations. the director leads platform engineering, cloud architecture, continuous integration and continuous delivery/deployment (ci/cd) modernization, and reliability functions ensuring that ai tools enhance clinical excellence and protect patient safety.\n\n  \n\nessential duties and responsibilities:  \n\n1\\. provides strategic leadership for cloud, platform, and infrastructure engineering, developing and leading multi\\-year roadmaps, standards, and strategies for the secure and scalable deployment of ai products.\n\n\n2\\. oversees architecture and governance of ci/cd pipelines, infrastructure\u2011as\u2011code (terraform), and kubernetes/azure kubernetes services (aks) orchestration to support reliable ai deployment.\n\n\n3\\. defines and oversee the infrastructure for the high\\-volume, low\\-latency data pipelines, feature stores, and data access layers required for training and real\\-time serving of ai models.\n\n\n4\\. establishes enterprise\\-wide reliability, and monitoring frameworks to ensure stable, and safe operation of ai systems used by clinicians and care teams.\n\n\n5\\. implements platform controls and audit trails to monitor and ensure responsible ai practices, model explainability (xai), and checks for model drift and bias on an ongoing basis.\n\n\n6\\. partners with product management, product development, cybersecurity, machine learning operations (mlops) engineering, and interoperability teams to ensure ai platform readiness, safe integrations.\n\n\n7\\. leads incident management and root\u2011cause analysis, to minimize disruptions to clinical workflows and drive reliability improvements.\n\n\n8\\. ensures the ai platform and infrastructure provide the necessary controls, logging, and audit capabilities to meet compliance requirements and support ai safety frameworks.\n\n\n9\\. develops long\u2011term platform resilience, disaster recovery, and cost optimization strategies to support system\u2011wide ai expansion.\n\n\n10\\. defines and standardizes the platform's toolchain and application programming interface (api) for the machine learning (ml) lifecycle, including model experimentation tracking (e.g., mlflow, clearml), model registry, and automated testing/validation frameworks.\n\n\n11\\. manages a team of platform and infrastructure engineers.\n\n\n12\\. performs other duties as assigned.\n\n  \n\n**minimum qualifications**\n--------------------------\n\n  \n\n\n1\\. master's degree from an accredited college or university in computer science, information systems or technology, cybersecurity, hospital administration, health care planning, business administration, mathematics, engineering or public administration; and three (3\\) years of progressively responsible experience in health care information security, multifaced information technology, health and medical service administration, public administration, or a related discipline with an emphasis on systems programming, systems engineering, software developing, or providing technical support as a specialist; two (2\\) years of which must have been in a related administrative, managerial or supervisory capacity; or,  \n\n2\\. bachelor\u2019s degree from an accredited college or university in disciplines, as listed in \u201c1\u201d above; and five (5\\) years of progressively responsible experience in health care information security, multifaced information technology, health and medical service administration, public administration, or a related discipline with an emphasis on systems programming, systems engineering, software developing, or providing technical support as a specialist; two (2\\) years of which must have been in a related administrative, managerial or supervisory capacity.  \n\n  \n\nassignment qualification preferences:\n  \n\n1\\. master\u2019s degree from an accredited college or university in computer science, engineering, information systems, or related discipline; and,  \n\n2\\. five (5\\) years of experience in machine learning operations (mlops), machine learning (ml) engineering, artificial intelligence (ai) platform engineering, or operating production machine learning (ml) / large language model (llm) system; or ten (10\\) years of experience in software and data engineering.\n  \n\n  \n\ncertifications preferred:  \n\n1\\. professional certifications in cloud architecture, ml/ai engineering, or devops from leading cloud platforms.  \n\n  \n\npreferred knowledge areas, skills, abilities, and other qualifications:\n  \n\n1\\. figma, sketch, adobe xd, or similar design and prototyping tools.  \n\n2\\. expertise in azure architecture, kubernetes/ azure kubernetes services (aks), terraform, continuous integration and continuous deployment (ci/cd), and automation frameworks.  \n\n3\\. experience supporting production ai/ml systems or mission\u2011critical workloads.  \n\n4\\. knowledge of observability tools, monitoring frameworks, and reliability engineering practices.  \n\n5\\. understanding of security and compliance standards including health insurance portability and accountability act of 1996 (hipaa) and national institute of standards and technology (nist).  \n\n6\\. demonstrated leadership, cross\\-functional collaboration, and technical communication skills.  \n\n7\\. strong stakeholder engagement and change\u2011management skill.  \n\n8\\. experience in healthcare, public sector, or other regulated environments.  \n\n9\\. experience deploying or supporting ai/ml, llm, or agentic ai systems in production.  \n\n10\\. familiarity with site reliability engineering (sre) or platform engineering frameworks.  \n\n11\\. experience using the following software and/or platforms:\n  \n\n* python, bash/shell, hashicorp configuration language (hcl terraform), and yaml/ javascript object notation (json), with familiarity in go preferred for kubernetes\\-based platform tooling.\n* azure cloud services, docker, kubernetes/aks, terraform, ci/cd platforms (azure devops, github actions, jenkins), monitoring/observability tools (grafana, azure monitor), secrets/iam security tooling.\n\n**benefits**\n------------\n\n  \n\n\nnyc health and hospitals offers a competitive benefits package that includes:\n\n* comprehensive health benefits for employees hired to work 20\\+ hrs. per week\n* retirement savings and pension plans\n* paid holidays and vacation in accordance with employees' collectively bargained contracts\n* loan forgiveness programs for eligible employees\n* college tuition discounts and professional development opportunities\n* college savings program\n* union benefits for eligible titles\n* multiple employee discounts programs\n* commuter benefits programs\n\n\n**how to apply**\n----------------\n\n  \n\n\nif you wish to apply for this position, please apply online by clicking the \"apply for job\" button.\n\n***note: candidates selected for a position are required to come to nyc as part of their onboarding.***",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineering Intern",
        "company": "Global Partners LP",
        "location": "Waltham, MA, US USA",
        "posted_at": "2026-02-24",
        "score": 16.7,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Glue",
            "Redshift",
            "BigQuery",
            "Docker",
            "CI/CD",
            "Git",
            "Snowflake"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b60d9899ce76c629",
        "description": "**job summary:**\n\n\nwe are seeking a motivated and talented data engineering intern to join our team. in this role you will get work on building scalable data pipelines and deploying them on our modern data stack. with this opportunity you will gain hands on experience in building required infrastructure for data integrations for optimizing data delivery and automating processes.\nat global partners, business starts with people. since 1933, we\u2019ve believed in taking care of our customers, our guests, our communities, and each other\u2014and that belief continues to guide us.\n\n\nthe global spirit is how we work to fuel that long term commitment to success. as a fortune 500 company with 90\\+ years of experience, we\u2019re proud to fuel communities\u2014responsibly and sustainably. we show up every day with grit, passion, and purpose\u2014anticipating needs, building lasting relationships, and creating shared value.\n\n**job description:**\n\n* work with the data engineering team to design, develop, and maintain data pipelines in cloud \u2013 native platforms using technologies like python, aws, snowflake, docker\n* collaborate with analytics engineer and data scientists to build data ingestion pipelines coming from variety of sources.\n* assist in automating workflows, monitoring and enhancing existing workflows to support data integration efforts.\n* build ci/cd pipelines to automate testing, security scans and deployment processes.\n* build solutions for data governance focusing on data quality, data security and data access controls.\n\n**additional job description:**\n\n* currently pursuing a bachelor\u2019s, master\u2019s in computer science, data engineering, machine learning, or a related field.\n* familiarity with aws technologies like s3, aws glue, lambda, iam roles and permissions\n* basic understanding of cloud\\-based data platforms like redshift, snowflake, bigquery\n* strong programming skills in python, object\\-oriented programming, modular design, and api development\n* experience with software engineering best practices, including version control (git), ci/cd pipelines, and unit testing.\n* familiarity with cloud platforms (aws, gcp, or azure) and containerization tools like docker.\n\n**we encourage you to learn more about our** **emerging leaders programs** **here.**\n\n**pay range:**\n\n\n$16\\.03 \\- $20\\.46\nthe pay range for this position is outlined above. the final amount offered at the start of employment is determined based on factors including, but not limited to, experience level, knowledge, skills, abilities and geographic location, and the company reserves the right to modify base salary at any time, including for reasons related to individual performance, company or individual department/team performance and market factors.\n\n**our commitments to you**\n\n* coins! we offer competitive salaries and opportunities for growth. we have an amazing talent development team who create trainings for growth and job development.\n* health \\& wellness \\- medical, dental, visions and life insurance. along with additional wellness support.\n* the road ahead \u2013 we offer 401k and a match component!\n* professional development \\- we provide tuition reimbursement; this benefit is offered after 6 months of service.\n\n**what to expect from the hiring process**\n\n\nwe value passion and potential. please apply if you\u2019re qualified and interested\u2014we\u2019d love to hear from you.\n\n\na member of our talent acquisition team will review your application and may connect you with the hiring manager if your experience is a strong match.\n\n\ninterviews are conducted virtually and in person, depending on the role. we\u2019ll provide more details about next steps if selected to move forward.\n\n\nglobal partners lp is an equal opportunity employer. we foster a company culture where ideas from all people help us grow, move and thrive. we embrace the diversity of all applicants and do not discriminate against race, color, religion, sex, age, national origin, sexual orientation, gender identity, disability, protected veteran status or any other basis prohibited by federal, state or local law. if you have a disability and need an accommodation to apply, please contact our recruiting department at 781\\-747\\-9675 or 781\\-7gp\\-work.\n\n* disclaimer: at global partners, we don't use lie detector tests for any employment decisions. we follow all the rules and regulations, so we need to let you know: in massachusetts, it's illegal to require or administer a lie detector test as a condition of employment of continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer Sys 3",
        "company": "Lam Research",
        "location": "Fremont, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 16.7,
        "matched_keywords": [
            "RAG",
            "Data Lake",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Git",
            "Kafka",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=0b87979ce36653f6",
        "description": "requisition id**195745**\ndate posted**02/24/2026**\nwork location model**on\\-site flex**\nwork location**fremont\\-ca**\nwork country**united states****the group you\u2019ll be a part of**\n---------------------------------\n\n\n**software \\& controls**\n\n**the impact you\u2019ll make**\n--------------------------\n\n\n\nas a software engineer at lam, you will play a critical role across the full software development lifecycle, applying your depth of technical expertise to design, develop, and sustain high quality software solutions. you will help shape and deliver cloud\\-based big data and enterprise analytics platforms that support lam\u2019s products, infrastructure, and technology roadmap. you will help shape and deliver cloud\\-based big data and enterprise analytics platforms that support lam\u2019s products, infrastructure, and technology roadmap.\n\n\nin this role, you will collaborate closely with product and technical leads defining software requirements and scope, translating business needs into robust, scalable solutions. you will design, develop, and maintain cloud\\-based data pipelines and support infrastructure, delivering high performance, reliable, and maintainable software. you will do this while working within lam\u2019s centralized software engineering team, collaborating with some of the brightest minds in the industry.\n\n**what you\u2019ll do**\n------------------\n\n\n* design, build, and maintain cloud\\-native software services and data pipelines that support big data and enterprise analytics solutions.\n* partner with product and technical leads to translate requirements into well\\-scoped designs, implementation plans, and deliverables.\n* implement new features and enhancements in existing applications and services with a focus on reliability, maintainability, and operational excellence.\n* troubleshoot, debug, and resolve issues across the stack, performing root\\-cause analysis and driving fixes to completion.\n* improve system performance through profiling, optimization, and effective use of concurrency and data\\-access patterns.\n* produce clear technical documentation and artifacts (design notes, flow diagrams, runbooks) to support development and ongoing operations.\n* collaborate with engineering teams through code reviews, testing, and continuous improvement to deliver high\\-quality software.\n**who we\u2019re looking for**\n-------------------------\n\n\n* bachelor\u2019s degree in computer engineering or related field with 5\\+ years of experience; or master\u2019s degree with 3\\+ years\u2019 experience; or a phd with no previous professional experience; or equivalent experience.\n* experience with python and java.\n* excellent knowledge of object\\-oriented software design and implementation.\n* experience developing cloud\\-native applications and data pipelines on azure, including event\\-driven, batch, and serverless architectures (e.g., event grid,\n* event hub, azure functions, azure batch).\n* experience building and optimizing big data pipelines, including ingestion, transformation, and persistence using azure data lake storage (adls) and related analytics services.\n* experience developing enterprise analytics software on cloud infrastructure, including data processing, indexing, and query enablement (e.g., hdinsight, azure ai search, azure sql / cosmos db).\n* experience implementing distributed data processing systems, leveraging caching, orchestration, and background execution (e.g., fastapi, celery, redis) to improve performance and scalability.\n* strong analytical, problem\\-solving, and troubleshooting skills.\n* excellent verbal and written communication skills.\n* ability to work and thrive in a fast\\-paced environment, learn rapidly, and master diverse technologies and techniques.\n**preferred qualifications**\n----------------------------\n\n\n* familiarity with lakehouse table formats (e.g., iceberg) and columnar storage (e.g., parquet).\n* working knowledge of containerization and orchestration technologies (e.g., docker, kubernetes).\n* experience with stream processing technologies (e.g., apache kafka, spark streaming) in enterprise analytics environments.\n* familiarity with ci/cd and software development lifecycle tooling (e.g., azure devops, git, jira, confluence, maven, jenkins).\n* experience working in agile scrum teams, contributing to sprint planning, execution, and continuous improvement.\n* solid understanding of computer system architecture fundamentals (processes, memory, storage, networking) and their impact on large\\-scale data processing.\n* experience in the semiconductor equipment manufacturing industry is a plus.\n**our commitment**\n------------------\n\n  \n\nwe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. by bringing unique individuals and viewpoints together, we achieve extraordinary results.\n\n\nlam research (\"lam\" or the \"company\") is an equal opportunity employer. lam is committed to and reaffirms support of equal opportunity in employment and non\\-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. it is the company's intention to comply with all applicable laws and regulations. company policy prohibits unlawful discrimination against applicants or employees.\n\n*lam offers a variety of work location models based on the needs of each role. our hybrid roles combine the benefits of on\\-site collaboration with colleagues and the flexibility to work remotely and fall into two categories \u2013 on\\-site flex and virtual flex. \u2018on\\-site flex\u2019 you\u2019ll work 3\\+ days per week on\\-site at a lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. \u2018virtual flex\u2019 you\u2019ll work 1\\-2 days per week on\\-site at a lam or customer/supplier location, and remotely the rest of the time.*\n\n**salary**\n\n  \n\nca san francisco bay area salary range for this position: $99,000\\.00 \\- $220,000\\.00\\.\n\n  \n\nthe above salary range for this position is relevant to applicants that reside or work onsite in the california, san francisco bay area only. salary offers will depend on factors that include the location you work from, your level, education, training, specific skills, years of experience and comparison to other employees already in this role. actual salary may vary from salary offered due to numerous factors including but not limited to unpaid time off, unpaid leave, company mandated shutdown, and other relevant factors.\n\n **our perks and benefits**\n\n  \n\nat lam, our people make amazing things possible. that\u2019s why we invest in you throughout the phases of your life with a comprehensive set of outstanding benefits.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data & ML Engineer",
        "company": "EssilorLuxottica",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 16.7,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Synapse",
            "Data Lake",
            "MLflow",
            "Docker",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=84539e9d32b8f8db",
        "description": "**requisition i****d:** 918774  \n\n**store \\#:** a00152 tech srvcs wholesale dal\\-t  \n\n**position**: full\\-time  \n\n**total rewards:** **benefits/incentive information**  \n\n\n\nif you\u2019ve worn a pair of glasses, we\u2019ve already met.\n\nwe are a global leader in the design, manufacture, and distribution of ophthalmic lenses, frames, and sunglasses. we offer our industry stakeholders in over 150 countries access to a global platform of high\\-quality vision care products such as the essilor brand, with varilux, crizal, eyezen, stellest and transitions, iconic brands that consumers love such as ray\\-ban, oakley, persol and oliver peoples, as well as a network that offers consumers high\\-quality vision care and best\\-in\\-class shopping experiences such as sunglass hut, lenscrafters, and target optical, and leading e\\-commerce platforms.\n\nour portfolio of more than 150 renowned brands span various categories, from frames, lenses and instruments to brick and mortar and digital distribution as well as mid\\-range to premium segments. our shared services team, accompany and enable others within the essilorluxottica collective to achieve their targets. they keep people and projects running smoothly, ensuring every part of our business is provided for and well taken care of.\n\njoin our global community of over 200,000 dedicated employees around the world in driving the transformation of the eyewear and eyecare industry. discover more by following us on linkedin!**general function**  \n\nthe data \\& ml engineer designs, builds, and operates scalable, secure, and reliable data and machine learning platforms primarily on azure, with exposure to aws and gcp when applicable. this role requires expertise in python or scala, data processing frameworks (e.g., spark), and container orchestration tools like kubernetes. strong proficiency in ci/cd, devops, and mlops is essential to support the deployment and operationalization of analytics and ai solutions. the role partners closely with applied data scientists to enable production\u2011ready models and robust engineering foundations.\n**major duties and responsibilities**\n* design, develop, and operate large\\-scale data ingestion, transformation, and storage pipelines\n* manage ml infrastructure and ci/cd, devops, and mlops pipelines for model training and deployment\n* optimize platform performance, reliability, cost, and availability\n* ensure data security, governance, and regulatory compliance\n* collaborate with applied data scientists to productionize models\n* design etl/elt workflows using azure data factory and orchestration tools\n* structure lakehouse, data lake, and synapse environments for scalable analytics\n* organize data formats, schemas, and versioning (delta, parquet, json, csv)\n* build reusable pipelines and ml components to accelerate delivery\n* implement monitoring, logging, and alerting for data and ml pipelines\n* champion best practices for scalable data and ml platforms\n* drive automation and infrastructure\\-as\\-code approaches\n* guide solution design for performance, resilience, and cost efficiency\n* lead troubleshooting and root\\-cause analysis for pipeline issues\n* mentor engineers in cloud\\-native, big data, and mlops practices\n\n\n **basic qualifications**\n* bachelor\u2019s degree in computer science, engineering, or related field\n* experience as a data engineer, ml engineer, or platform engineer\n* strong hands on experience with azure and big data platforms\n* proficiency in python, sql, scala, and scripting languages\n* experience building production grade data pipelines\n* ability to independently deliver complex data and ml engineering solutions\n\n\n**preferred qualifications**\n* master\u2019s degree in related discipline\n* experience with azure databricks, spark, synapse, mlflow\n* experience with docker, aks, apis, and containerized ml workloads\n* experience with azure data factory or airflow\n* exposure to sap cdc and enterprise data integration\n* experience in agile, fast paced, cross functional environments\n* strong ownership and independence\n* ability to translate analytical needs into scalable engineering solutions\n* strong collaboration skills with data scientists and business teams\n* excellent problem solving and troubleshooting capabilities\n* focus on reliability, scalability, and operational excellence\n\n\nthis posting is for an existing vacancy within our business. employee pay is determined by multiple factors, including geography, experience, qualifications, skills and local minimum wage requirements. in addition, you may also be offered a competitive bonus and/or commission plan, which complements a first\\-class total rewards package. benefits may include health care, retirement savings, paid time off/vacation, and various employee discounts.  \n\n\n\nessilorluxottica complies with all applicable laws related to the application and hiring process. if you would like to provide feedback regarding an active job posting, or if you are an individual with a disability who would like to request a reasonable accommodation, please call the essilorluxottica speakup hotline at 844\\-303\\-0229 (be sure to provide your name, job id number, and contact information so that we may follow up in a timely manner) or email hrcompliance@luxotticaretail.com.  \n\n\n\nwe are an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, gender, national origin, social origin, social condition, being perceived as a victim of domestic violence, sexual aggression or stalking, religion, age, disability, sexual orientation, gender identity or expression, citizenship, ancestry, veteran or military status, marital status, pregnancy (including unlawful discrimination on the basis of a legally protected pregnancy or maternity leave), genetic information or any other characteristics protected by law. native americans in the us receive preference in accordance with tribal law.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Analyst",
        "company": "Konrad Group",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-24",
        "score": 16.7,
        "matched_keywords": [
            "RAG",
            "Redshift",
            "BigQuery",
            "Git",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Tableau",
            "Power BI"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=85e5ffc39cf3d166",
        "description": "**who we are**\n\n\n\nkonrad is a next\\-generation digital consultancy. we are dedicated to solving complex business problems for our global clients with creative and forward\\-thinking solutions. our employees enjoy a culture built on innovation and a commitment to creating best\\-in\\-class digital products in use by hundreds of millions of consumers around the world. we hire exceptionally smart, analytical, and hard\\-working people who are lifelong learners.\n\n\n**about the role**\n\n\n\nas a senior data analyst, you are an experienced analytics professional who leads analytical initiatives and partners directly with stakeholders to shape data\\-informed strategy. you drive the design and delivery of analytics and reporting solutions that enable clients to understand performance, identify opportunities, and make confident business decisions.\n\n\n\nyou work closely with consultants, data engineers, developers, designers and clients across multiple engagements. you own analytical problem\\-solving from discovery to delivery, mentor junior analysts, and help establish best practices across data modeling, experimentation, and reporting.\n\n\n\nyou are curious, pragmatic, and comfortable navigating ambiguity. you translate complex business problems into structured analysis, proactively surface insights, and influence decisions through clear communication and thoughtful storytelling.\n\n\n**what you'll do**\n\n\n* partner with client and internal stakeholders to define business questions, success metrics, and measurement strategies\n* lead analytical projects end\\-to\\-end including discovery, data validation, analysis, and presentation of recommendations\n* design and implement scalable dashboards and reporting frameworks for executive and operational decision\\-making\n* develop and maintain metric definitions, documentation, and analytics standards across engagements\n* perform deep\\-dive analyses to identify drivers, opportunities, and risks across product and business performance\n* design and evaluate a/b tests and experimentation strategies, including statistical interpretation and recommendations\n* collaborate with data engineers to design data models and ensure reliable, high\\-quality reporting layers\n* mentor junior analysts and review analytical work to ensure quality and rigor\n* communicate insights through clear narratives, visualizations, and executive\\-level presentations\n* identify and implement improvements to analytics workflows, governance, and data quality processes\n\n\n**qualifications**\n\n\n* minimum 4\\+ years of experience in a data analyst, business intelligence, product analytics, or consulting analytics role\n* strong sql expertise and experience querying cloud data warehouses (e.g., snowflake, databricks, redshift, bigquery)\n* proficiency in python, r, or similar language for data analysis and automation\n* advanced experience with business intelligence and visualization tools (e.g., looker, tableau, power bi)\n* strong understanding of data modeling, dimensional modeling, and analytics architecture concepts\n* experience defining kpis, measurement frameworks, and decision metrics\n* hands\\-on experience designing and analyzing experiments (a/b testing, causal inference, statistical interpretation)\n* proven ability to independently structure ambiguous problems and deliver actionable insights\n* experience working directly with stakeholders and presenting to leadership audiences\n* strong written and verbal communication skills and ability to influence decisions  \n\nexperience mentoring analysts or reviewing analytical work\n* familiarity with data governance, privacy, and security best practices\n\n\n**perks and benefits**\n\n\n* comprehensive health \\& wellness benefits package\n* retirement planning\n* parental leave program\n* flexible working hours\n* work from home flexibility\n* service recognition programs\n* socials, outings \\& retreats\n* culture of learning \\& development\n\n\n**bonus points**\n\n\n\nhave you taken any courses at brainstation? a lot of our design and development best practices and processes are taught during our courses, make sure to highlight this experience in your cover letter if you have!\n\n\n*konrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. all qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. if you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.*\n\n\n*the estimated compensation for this position is $100,000 to $125,000\\. this is an estimate and a compensation offer will vary based on applicant's education, experience, skills, abilities and alignment with market data.*\n\n\n\nwhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.\n\n\n\n\\#li\\-hybrid",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer",
        "company": "Cargomatic",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 15.6,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "LLaMA",
            "Data Lake",
            "FastAPI",
            "Git",
            "Snowflake",
            "Databricks",
            "Kafka",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e4eebde6b6cf3dae",
        "description": "senior data architect \u2013 data engineering\n\n\n\nlocation: san francisco, ca  \n\nreports to: vp of engineering  \n\nflsa status: exempt  \n\nemployment type: full\\-time  \n\ncompensation: $140,000 \u2013 $160,000 annually (based on experience)\n\n\n\nabout cargomatic\n\n\n\ncargomatic is transforming the local trucking industry with cutting\\-edge technology that connects shippers and carriers in real time. every product that humans build, grow, or sell has spent time on a truck. local trucking is the lifeblood of every regional economy, yet this $82 billion industry still relies heavily on outdated systems. cargomatic is bringing transparency, efficiency, and intelligence to local freight through modern technology and data\\-driven solutions.\n\n\n\nwe are solving complex, real\\-world logistics problems every day. if you thrive in a fast\\-paced environment, enjoy building scalable systems, and want to help shape the future of ai\\-powered logistics, we'd love to meet you.\n\n\n\nposition summary\n\n\n\ncargomatic is seeking a senior data architect \u2013 data engineering to design and build scalable, cloud\\-native data infrastructure that powers analytics, machine learning, and ai\\-driven applications. this role combines deep data architecture expertise with hands\\-on experience in modern data platforms and llm\\-enabled application development.\n\n\n\nyou will lead the design of enterprise\\-grade data models, architect rag systems, implement agentic workflows, and integrate secure, production\\-ready llm capabilities into our ecosystem. this is a high\\-impact role with significant ownership, visibility, and opportunity to shape the future of intelligent logistics technology.\n\n\n\nkey responsibilities\n\n\n\ndata architecture \\& engineering\n\n\n* design and build scalable, cloud\\-native data pipelines (batch and streaming) supporting analytics, ml, and ai\\-powered applications\n* architect enterprise\\-grade data models across data lakes, warehouses, and real\\-time systems (snowflake, databricks, kafka, dbt)\n* define standards for data governance, reliability, performance, and cost optimization\n* optimize storage formats and distributed data systems (parquet, delta lake, iceberg)\n\nai \\& llm\\-enabled systems\n\n\n* develop retrieval\\-augmented generation (rag) systems integrating structured and unstructured enterprise data\n* design and implement agentic workflows using frameworks such as langchain, langgraph, llamaindex, n8n, or similar\n* integrate llm apis (openai, anthropic, or similar) into secure, production\\-ready applications\n* implement guardrails, validation layers, monitoring, and evaluation frameworks to mitigate hallucination, prompt injection, and data security risks\n\nbackend \\& api development\n\n\n* build secure backend apis (python/fastapi) to expose ai\\-powered capabilities\n* ensure observability, monitoring, and cost controls across ai and data services\n* contribute to microservices architecture and distributed system design\n\ncollaboration \\& leadership\n\n\n* partner cross\\-functionally with product, engineering, and operations to translate business requirements into scalable technical solutions\n* mentor junior engineers and contribute to architectural standards and best practices\n* drive innovation in data engineering and ai\\-powered logistics systems\n\nqualifications\n\n\n* bachelor's degree in computer science or equivalent practical experience\n* 8\\+ years of software or data engineering experience in production environments\n* strong expertise in data modeling, distributed systems, and scalable cloud architectures\n* hands\\-on experience with etl/elt frameworks and streaming technologies (kafka, spark, hevo, snowflake, dbt, etc.)\n* advanced sql skills and deep understanding of modern storage formats\n* proficiency in python and restful api development\n* experience integrating llm apis into production applications\n* strong understanding of system reliability, observability, and cost management in cloud environments\n\ndesired experience\n\n\n* experience building rag pipelines including embeddings, vector search, chunking strategies, and hybrid retrieval\n* experience designing multi\\-agent or agentic ai workflows with orchestration frameworks\n* knowledge of llm evaluation, monitoring, and tracing tools (langsmith or similar)\n* experience with microservices architecture and distributed system design\n* exposure to transportation, logistics, or supply chain domains\n* active github contributions or demonstrated passion for emerging ai and data technologies\n\nwhy join cargomatic?\n\n\n\nwe offer competitive compensation and a comprehensive benefits package, including:\n\n\n* medical, dental, and vision insurance\n* 401(k) with company match\n* flexible spending accounts (fsa)\n* company\\-paid life and disability insurance\n* flexible paid time off (pto) and company holidays\n* paid parental leave\n* employee assistance program (eap)\n* opportunity to build cutting\\-edge ai solutions in a high\\-growth logistics technology company\n* collaborative, high\\-impact team environment\n\ncargomatic is proud to be an equal opportunity employer. we are committed to creating a diverse and inclusive workplace where all employees feel valued and empowered to succeed.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer (Hybrid)",
        "company": "Match Made Tech",
        "location": "Irvine, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Glue",
            "Redshift",
            "Snowflake",
            "Redshift",
            "PostgreSQL",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7496d31cfec6658e",
        "description": "**job title:** data engineer\n\n\n**location:** irvine, ca (onsite). monday through thursday onsite, fridays remote.\n\n\n**compensation:** $50\\-95 an hour. this is a 2\\-year contract that will convert to full\\-time.\n\n\n**about us:** we are on a mission to develop innovative ai solutions that will revolutionize our workforce. as we embark on an exciting new greenfield ai project, our data team is forward\\-thinking and data\\-driven dedicated to transforming data into actionable insights. we are seeking a skilled data engineer to join our team, responsible for leveraging python, java, postgresql, snowflake and aws to design, develop, and maintain data pipelines and systems that support our analytics and decision\\-making processes.\n\n\n**job description:**\n\n**position overview:** as a data engineer, you will play a pivotal role in the development and management of our data infrastructure. you will work closely with cross\\-functional teams to ensure data availability, reliability, and quality, enabling data\\-driven decision\\-making across the organization.\n\n\n**responsibilities:**\n\n**data pipeline development:**\n\n* design, implement, and maintain data pipelines using python, postgresql, and aws technologies.\n* develop efficient and scalable etl processes to extract, transform, and load data from various sources into our snowflake data warehouses.\n\n**database management:**\n\n* design and maintain postgresql databases, including schema design, indexing, and performance optimization.\n* monitor database performance, troubleshoot issues, and implement improvements as needed.\n* aws integration:\n* utilize aws services, such as aws glue, s3, redshift, and emr, for data storage, processing, and analytics.\n* collaborate with aws solutions architects to optimize data\\-related aws infrastructure.\n\n**data quality and governance:**\n\n* implement data quality checks and validation processes to ensure data accuracy and consistency.\n* enforce data governance policies and security measures to protect sensitive data.\n\n**documentation and collaboration:**\n\n* maintain comprehensive documentation of data pipelines, database schemas, and etl processes.\n* collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver actionable insights.\n\n**performance optimization:**\n\n* continuously monitor and optimize data pipelines and database performance for scalability and efficiency.\n\n**qualifications:**\n\n* bachelor's or master's degree in computer science, data engineering, or a related field.\n* proven experience (3\\+ years) as a data engineer with expertise in python, java, postgresql, and aws.\n* strong sql skills and proficiency in data modeling and database design.\n* hands\\-on experience with aws services, particularly glue, s3, redshift, and emr.\n* knowledge of etl processes and data integration techniques.\n* excellent problem\\-solving skills and attention to detail.\n* effective communication and collaboration skills to work within a team.\n* a commitment to staying current with industry trends and best practices in data engineering.\n\n**what makes this exciting:**\n\n* be part of a dynamic team in a data\\-driven organization.\n* technical ownership on greenfield projects\n* contribute to projects that have a meaningful impact on our business and industry.\n* competitive compensation and opportunities for professional growth.\n* work in a collaborative and innovative environment.\n\nif you are a skilled data engineer with expertise in python, java, postgresql, and aws and are passionate about turning data into insights, we encourage you to apply. join us in our mission to harness the power of data!",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. Software Engineer- Keystone",
        "company": "NetApp",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "Data Lake",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Git",
            "Kafka",
            "PostgreSQL",
            "NoSQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a103161b01705039",
        "description": "**overview**\n------------\n\n\nat netapp, we have a history of helping customers turn challenges into business opportunities. that\u2019s because we bring new thinking to age\\-old problems, like how to use data most effectively in the most efficient possible way. as an engineer with netapp, you\u2019ll have the opportunity to work with modern cloud and container orchestration technologies in a production setting. you\u2019ll play an important role in scaling systems sustainably through automation and evolving them by pushing for changes to improve reliability and velocity.\n  \n**own every moment at netapp**\n\n\nat netapp, your ideas power innovation. we lead in intelligent data infrastructure\u2014delivering unified storage, integrated data services, and solutions that help organizations unlock the full potential of their data, from ai to multicloud.\n\n\nready to innovate and contribute to our path to $10b? here, you'll collaborate with passionate teams, tackle real\\-world challenges, and see your impact in how customers transform and grow. if you're ready to bring curiosity, creativity, and drive to every moment, netapp is where your journey begins.\n\n\n**job summary**\n---------------\n\n\nare you passionate about building the future of hybrid cloud data management? netapp is developing a portfolio of data\\-centric platforms and services to help organizations unlock the true power of their data.\n\n\nour keystone team is at the forefront of this transformation, delivering innovative, subscription\\-based, pay\\-as\\-you\\-go solutions that give customers a seamless data management experience, whether on\\-premises or in the cloud. we are building the engine that powers this flexible consumption model, and we're looking for a technical visionary to help us solve the next generation of challenges in distributed systems, cloud services, and data analytics.\n\n**job responsibilities**\n------------------------\n\nas a staff software engineer in the keystone organization, you will be a technical leader and a force multiplier for our engineering teams. you will go beyond leading projects; you will set the technical vision for critical components of our platform. you will be responsible for designing elegant, scalable, and resilient solutions to our most complex architectural challenges.  \n\nthis role requires a blend of deep technical expertise, a strategic mindset, and the ability to influence and mentor engineers across the organization. if you thrive on solving ambiguous problems, driving technical excellence, and building systems that operate at a massive scale, this is the role for you.\n\n\nwhat you\u2019ll do:  \n\n* architect and build the core business engine that automates the entire subscription lifecycle, from product catalog creation and dynamic pricing to managing customer renewals.\n* design, build, and operate robust data pipelines (etl/elt) to collect and process subscription, usage, and metering data from disparate sources.\n* architect the data ecosystem, including data models, data warehousing, and data lake strategies, to serve as the single source of truth for all analytics and reporting.\n* write high\\-quality, high\\-performance code, primarily in go and python, to build and evolve our restful services and data pipelines.\n* act as a technical leader and visionary, influencing the long\\-term strategy for keystone's commercial and data platforms.\n* mentor and cultivate the growth of senior and junior engineers, fostering a culture of technical excellence and innovation.\n* collaborate with product, business, and engineering leaders across netapp to deliver cohesive, industry\\-leading solutions.\n* drive operational excellence by ensuring our services are seamlessly integrated into ci/cd pipelines and are built for world\\-class reliability, performance, and security.\n**job requirements**\n--------------------\n\n* 10\\+ years of professional software development experience, with a proven track record of delivering high\\-quality software.\n* experience acting as a mentor, tech lead, or a key technical influencer on an engineering team.\n* demonstrated expertise in leading the architectural design (design patterns, reliability, scaling) of new and existing complex systems.\n* deep, hands\\-on proficiency in modern programming languages like go and/or python.\n* extensive experience designing, implementing, and consuming restful apis.\n* proven experience with relational (e.g., postgresql) and nosql databases, and a deep understanding of when to use each.\n* hands\\-on experience with containerization and orchestration technologies such as docker and kubernetes.\n* experience with modern frontend frameworks (e.g., react.js) and an understanding of full\\-stack architecture.\n* experience building and maintaining ci/cd pipelines using tools like git, jenkins, or gitlab ci.\n* deep knowledge of data warehousing concepts, streaming technologies (like kafka), and data modeling for analytics.\n* experience with major public cloud providers (aws, azure, gcp).\n* a bachelor of science degree in computer science, engineering, or a related field; or equivalent, relevant experience.\n\ncompensation:  \n\nthe target salary range for this position is 187,000 \\- 278,300 usd. the salary offered will be determined by the candidate's location, qualifications, experience, and education and may be outside of this range. the range is based on 'on target earnings\u2019 (ote) representing the total potential earnings, which is the sum of the base salary and potential commission earned when performance targets are achieved. final compensation packages are competitive and in line with industry standards, reflecting a variety of factors, and include a comprehensive benefits package. this may cover health insurance, life insurance, retirement or pension plans, paid time off, various leave options, employee stock purchase plan, and/or restricted stocks (rsu\u2019s). these offerings are subject to regional variations and governed by local laws, regulations, and company policies. we will provide detailed information about the specific benefits for your region during the recruitment process.\n\n\nat netapp, we embrace a hybrid working environment designed to strengthen connection, collaboration, and culture for all employees. this means that most roles will have some level of in\\-office and/or in\\-person expectations, which will be shared during the recruitment process.\n\n\n**equal opportunity employer:**\n\n\nnetapp is firmly committed to equal employment opportunity (eeo) and to compliance with all federal, state and local laws that prohibit employment discrimination based on age, race, color, gender, sexual orientation, gender identity, national origin, religion, disability or genetic information, pregnancy, protected veteran status, and any other protected classification.\n\n**why you'll thrive at netapp**\n\n\nat netapp, you won't wait for the perfect moment\u2014you'll make it. the early planning, the extra thought, the bold idea that turns good into great: that's how our people operate and how we continue to push the boundaries of data infrastructure.\n\n\nnetapp is the trusted partner for organizations transforming data into opportunity. as the only enterprise\\-grade storage service natively embedded in google cloud, aws, and microsoft azure, we empower customers to run everything from traditional workloads to enterprise ai with unmatched performance, resilience, and security.\n\n**our culture**\n\n\nwe celebrate mold breakers, bold thinkers, and problem solvers. we reward initiative, impact, and ownership. we provide flexibility so you can balance professional ambition with your personal life. here, differences are not just welcomed\u2014they drive everything we do.\n\n\nif you're ready to innovate, rise to the challenge, and own every moment \\- make your next move your best one. now.\n\n\n**submitting an application**  \n\n  \n\nto ensure a streamlined and fair hiring process for all candidates, our team only reviews applications submitted through our company website. this practice allows us to track, assess, and respond to applicants efficiently. emailing our employees, recruiters, or human resources personnel directly will not influence your application.\n\n\n  \n**our values**\n--------------\n\n\nput the customer at the center. care for each other and our communities. think and act like owners. build belonging every day. embrace a growth mindset.\n\n\n  \n**benefits**\n------------\n\n\n### **volunteer time off**\n\n\n40 hours of paid volunteer time each year.\n### **well\\-being**\n\n\nemployee assistance program, fitness, and mental health resources to help employees be their best.\n### **time away**\n\n\npaid time off for vacation and to recharge.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. Site Reliability Engineer",
        "company": "Teladoc Health",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Snowflake",
            "Kafka",
            "Tableau",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cb16a49ed3afacf1",
        "description": "**join the team leading the next evolution of virtual care.**\n\nat teladoc health, you are empowered to bring your true self to work while helping millions of people live their healthiest lives.  \n\n\n\nhere you will be part of a high\\-performance culture where colleagues embrace challenges, drive transformative solutions, and create opportunities for growth. together, we\u2019re transforming how better health happens.\n\n\n**summary of position**\n\nwe\u2019re looking for a senior site reliability engineer to own reliability, automation, and infrastructure\\-as\\-code for our modern data \\& ai platform. in this role, you\u2019ll ensure our azure\\-based data ecosystem is reliable, scalable, and efficient. you\u2019ll build terraform\\-first infrastructure, improve developer experience, and support a healthcare environment where uptime and data reliability directly impact patient care.\n\n\n**essential duties and responsibilities**\n\n**infrastructure as code**\n\n* build and maintain terraform modules for data platform services (snowflake, airbyte, astronomer, dbt, kafka).\n* develop iac standards, gitops workflows, and automated ci/cd pipelines using github actions.\n* migrate manual configurations to fully codified infrastructure and enable self\u2011service provisioning for engineer\n\n**platform reliability \\& operations**\n\n* implement monitoring, alerting, and slo/slis for data pipelines and platform components.\n* lead incident response, root cause analysis, and postmortems.\n* create automation, runbooks, and self\u2011healing capabilities to reduce mttr.\n\n**cross\\-cloud architecture**\n\n* design secure connectivity patterns between azure and aws vendor systems.\n* troubleshoot networking, vpn, private endpoints, dns, and mft integrations.\n\n**automation \\& developer experience**\n\n* build ci/cd pipelines using github actions for infrastructure changes with comprehensive testing (terraform plan, validate, compliance checks)\n* implement policy\\-as\\-code using tools like sentinel, opa, or azure policy integrated into github workflows\n* develop testing frameworks for infrastructure code (terratest, kitchen\\-terraform) with automated execution in github actions\n* improve abstractions and tooling to streamline development workflows.\n\n**performance \\& cost optimization**\n\n* optimize snowflake compute usage and airflow/dbt performance.\n* apply cloud cost management practices and tagging strategies.\n* support capacity planning and forecasting.\n\n**systems troubleshooting \\& problem resolution**\n\n* lead complex troubleshooting efforts across distributed systems spanning multiple cloud providers\n* debug integration issues with kafka streams, cdc patterns, and real\\-time data pipelines\n* resolve platform\\-wide incidents involving snowflake, astronomer, airbyte, and downstream bi tools (powerbi, tableau, cube cloud)\n* partner with vendors for escalated support cases and coordinate resolution across multiple teams\n\nthe time spent on each responsibility reflects an estimate and is subject to change dependent on business needs.  \n\n\n\n**supervisory responsibilities**\n\nno\n\n\n**qualifications expected for position**\n\n* 7\\+ years in site reliability engineering, devops, or platform engineering roles.\n* 5\\+ years production experience with terraform at scale.\n* strong azure expertise; aws experience beneficial.\n* experience operating cloud\\-based data platforms (snowflake, airflow, etc.).\n* expert github knowledge (pull requests, actions, branching strategies).\n* strong troubleshooting skills across distributed systems, networking, and data pipelines.\n* proficient in python, bash, powershell; able to read sql and yaml/json.\n* strong experience with containerization and orchestration (docker, kubernetes).\n\n**bonus qualifications**\n\n* healthcare data experience (fhir, hl7, claims data)\n* kafka experience, dbt administration, bi tools (powerbi/tableau).\n* experience with data quality frameworks and synthetic data generation\n* policy\\-as\\-code tools (sentinel, opa, checkov)\n\nthe base salary range for this position is $155,000 \\- $175,000\\. in addition to a base salary, this position is eligible for a performance bonus and benefits (subject to eligibility requirements) listed here: teladoc health benefits 2026 . total compensation is based on several factors including, but not limited to, type of position, location, education level, work experience, and certifications. this information is applicable for all full\\-time positions.\n\n\n\\#li\\-ss2 \\#li\\-remote\n\n\nwe follow a flexible vacation policy, intended for rest, relaxation, and personal time. all time off must be approved by your manager prior to use. you will also receive 80 hours of paid sick, safe, and caregiver leave annually. this applies to full\\-time positions only. if you are applying for a part\\-time role, your recruiter can provide additional details.\n\n\nas part of our hiring process, we verify identity and credentials, conduct interviews (live or video), and screen for fraud or misrepresentation. applicants who falsify information will be disqualified.  \n\n\n\nteladoc health will not sponsor or transfer employment work visas for this position. applicants must be currently authorized to work in the united states without the need for visa sponsorship now or in the future.\n\n\n**why join teladoc health?**\n\n* teladoc health is transforming how better health happens. learn how when you join us in pursuit of our impactful mission .\n* chart your career path with **meaningful opportunities** that empower you to grow, lead, and make a difference.\n* join a **multi\\-faceted community** that celebrates each colleague\u2019s unique perspective and is focused on continually improving, each and every day.\n* contribute to an **innovative culture** where fresh ideas are valued as we increase access to care in new ways.\n* enjoy an inclusive benefits program centered around you and your family, with tailored programs that address your unique needs.\n* explore candidate resources with tips and tricks from teladoc health recruiters and learn more about our company culture by exploring \\#teamteladochealth on linkedin .\n\n*as an equal opportunity employer, we never have and never will discriminate against any job candidate or employee due to age, race, religion, color, ethnicity, national origin, gender, gender identity/expression, sexual orientation, membership in an employee organization, medical condition, family history, genetic information, veteran status, marital status, parental status, or pregnancy). in our innovative and inclusive workplace, we prohibit discrimination and harassment of any kind.*\n\n*teladoc health respects your privacy and is committed to maintaining the confidentiality and security of your personal information. in furtherance of your employment relationship with teladoc health, we collect personal information responsibly and in accordance with applicable data privacy laws, including but not limited to, the california consumer privacy act (ccpa). personal information is defined as: any information or set of information relating to you, including (a) all information that identifies you or could reasonably be used to identify you, and (b) all information that any applicable law treats as personal information. teladoc health\u2019s notice of privacy practices for u.s. employees\u2019 personal information is available* *at this link* *.*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "R&T Software Engineer 3",
        "company": "Safran Cabin",
        "location": "Costa Mesa, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Redshift",
            "CI/CD",
            "Terraform",
            "Git",
            "Redshift",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d390689f78ce46c9",
        "description": "safran cabin is \\#1 worldwide for aircraft interiors.  \n\n  \n\nhere, you will build your skills and grow with a community of experts to enrich yourself every day.  \n\nhere, collaboration is embodied within the diversity of our teams all around the world.  \n\nhere, we're cutting our emissions, not your ambitions.  \n\n  \n\nhere, we craft excellence together. your mission? making the journey the most enjoyable part of the trip.  \n\n  \n\njoin our first\\-class team to reinvent in\\-flight experience. in the role of r\\&t software engineer 3 you'll play a pivotal part on our engineering team.  \n\n  \n\nthe r\\&t software engineer 3 acts as the technical lead in strategy and design of airworthiness software solutions for the business. this position requires full\\-stack engineering knowledge and skills to provide leadership to develop and integrate digital technologies such as iot devices, ai/ml models, cloud and mobile/web applications deployment. the position requires understanding of the business context and perform in\\-depth analysis to evaluate design, risk, technology, time and cost to architect a pragmatic solution that will satisfy business objectives. the position also requires the candidate to foster a culture of continuous improvement, coach and mentor more junior software engineers. the key responsibility of this position is to develop end\\-to\\-end solutions for aircraft oem / airlines customers and for internal use.  \n\n  \n\nthe r\\&t software engineer 3 will take ownership and/or support technical engineering teams and projects related to engineering and r\\&t products/systems and components. using sound engineering principles, this engineer will interface with cross functional groups (internal and external) to ensure robust, airworthy designs are attained which meet budget, schedule and internal requirements and goals. this leadership position will be responsible for participating in and leading improvement activities at a group and/or company level.  \n\n  \n\n**this position contributes to our vision by:*** design and develop full\\-stack applications for device management, control, and aircraft fleet coordination.\n* build and maintain mobile apps (react native/android) and web apps used by operators in the field.\n* develop web dashboards and apis for maintenance planning, status, and data analysis.\n* integrate aws services for data storage, monitoring, and deployment.\n* ensure reliable, low\\-latency communication between cloud systems and aircraft on\\-board equipment.\n* collaborate with cross\\-disciplinary teams to deliver seamless end\\-to\\-end functionality.\n* write documentation for apis, architecture, and software modules.\n* optimize software for scalability, reliability, and field performance.\n* lead product cybersecurity risk assessment, requirements, and architecture by collaborating with product development teams.\n* ensure software solutions comply with data privacy, governance, and security best practices on aws cloud.\n* work in an agile setup, collaborating with data scientists, cloud engineers, and business teams to prototype, test, and deploy solutions.\n\n  \n\nat safran cabin, we provide equal employment opportunity to all individuals regardless of race, color, religion, sex/gender, sexual orientation, gender identity/gender expression, marital status, pregnancy, age, national origin, ancestry, disability/medical condition, military or veteran status, citizenship status, genetic characteristics or information, or any other characteristic protected by applicable federal, state, and local laws.  \n\n  \n\n**your benefits:** our suite of comprehensive benefits include health care (medical, dental and vision), life insurance, 401(k) savings plans with company match, paid time off, and employee discounts \\& rewards for consumer products/services and more.  \n\n  \n\nthe expected salary range for this position is between $99,670 \\- $156,620 usd. actual compensation will be determined based on experience, education, and other factors permitted by law.  \n\n  \n\nthis job posting has been designed to indicate the general nature and level of work performed by an employee within this position. the actual duties, responsibilities and qualifications may vary based on assignment or group.  \n\n  \n\n**candidate skills \\& requirements \\| education:**  \n\nbachelor's or master's degree in computer science, electrical engineering or equivalent experience in a related field.  \n\n  \n\n**experience:**  \n\n9\\+ years of experience, 3\\+ years of experience in a leadership/management role, 3\\+ years of experience in aerospace / automotive / technology or similar industry.  \n\n  \n\n**has experience with the entire end\\-to\\-end software development life cycle implementation:*** design specifications, write documentation, define scope/limitations.\n* work in both agile and waterfall development models.\n* effectively troubleshoot and debug issues that are technical or behavioral.\n* provide continuous meaningful improvements.\n\n  \n\n**strong knowledge of the full technology stack:*** experience in data modeling, creating queries, stored procedures, triggers, and optimization for performance.\n* expert in object\\-oriented programming, parallel processing and design patterns including dependency injection.\n* experience in design and implementation of security, access control, and robust data validation.\n* experience in service\\-oriented architecture and integration of systems.\n* experience in implementation of fast and responsive user interface along with experience in integration of third\\-party framework and components.\n* experience in web application development.\n\n  \n\n**technical skills (programming languages, protocols, technologies, frameworks):*** knowledge of professional software engineering practices \\& best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.\n* web technologies: http, rest, json, soap, and xml.\n* programming languages: c/c\\+\\+, python, html5, css3, and javascript.\n* databases: any rdbms, no\\-sql databases, and vector db.\n* cloud technologies: aws iot, eks, s3, dynamodb, redshift, and other aws technologies.\n* iot technologies: wi\\-fi/ble5\\.1/802\\.15\\.4, 6lowpan, mqtt/coap.\n* computing technologies: nxp imx6/8, arm, stm32, hardware accelerators.\n* configuration management: git, github, gitlab or svn.\n* productivity tools: microsoft office (word, excel, powerpoint), jira.\n* devops \\& ci/cd: experience with builds, github/gitlab runners, aws devops, terraform for infrastructure automation.\n\n  \n\n**other skills:*** excellent collaboration and leadership skills.\n* able to use discretion and independent judgement when making decisions on behalf of the company.\n* effective oral and written communications skills.\n* demonstrated ability to handle multiple projects and assignments with attention to detail.\n* problem solving, well organized, detailed oriented and accurate.\n\n  \n\n**additional preferred skills (not required):*** product lifecycle management (plm) systems.\n* ui libraries: telerik ui libraries, jquery ui, or similar.\n* object relation mapping frameworks: entity, nhibernate or similar.\n* data distribution service (dds) concepts for middleware architecture/designs.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer (IT Data Management Entry In-Training to Journey) DOH8822",
        "company": "State of Washington",
        "location": "WA, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "Redshift",
            "BigQuery",
            "BigQuery",
            "Redshift",
            "Kafka",
            "Hadoop",
            "Cassandra",
            "Tableau",
            "Power BI"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2e85dc13d85ca930",
        "description": "**description**\n---------------\n\n  \n\n  \n\n**data engineer** **it data management entry in\\-training to journey**  \n\nthis recruitment is open to washington residents and those residing on the id/wa and or/wa borders. **the opportunity**\nthis position plays a key role in supporting statewide public health data systems that inform decision making across washington. the team manages enterprise data pipelines, reporting environments, and business intelligence platforms that support critical programs, federal reporting requirements, and public facing dashboards.  \n\nworking within a highly collaborative technical environment, this role supports the movement, transformation, automation, and governance of large data sets across on premises, cloud based, and vendor supported systems. the work directly enables reliable analytics, reporting, and visualization for internal teams, local health jurisdictions, healthcare partners, and federal agencies.  \n\nat the journey level, this role independently designs, builds, and maintains complex data pipelines and integration processes. at the entry in training level, the role develops foundational skills under supervision while contributing to database management, etl processes, and system implementation activities.  \n\nthe goal classification for this position is journey level. **in\\-training classification information:**\nthis position may be filled at either the data management entry in\\-training or journey level, depending on the selected candidate\u2019s qualifications. the journey level is the fully qualified, independent performing classification and is the goal level for this role. if hired at the entry in\\-training level, the employee will work under closer supervision while developing the knowledge and skills necessary to advance to the journey level. advancement is based on demonstrated competency and successful completion of training requirements. **key responsibilities include:*** data engineering and pipeline development\n\t+ design, build, and maintain data pipelines supporting bi and analytics platforms\n\t+ develop and manage etl and elt processes across oltp and olap environments\n\t+ map and transform operational data into analytical environments\n\t+ implement data schemas and structures to support high usage reporting engines\n\t+ automate data processing workflows to improve reliability and efficiency\n* business intelligence and reporting support\n\t+ support power bi, ssrs, and related reporting technologies\n\t+ ensure accurate, secure, and timely delivery of data to dashboards and reporting tools\n\t+ partner with analysts and subject matter experts to translate business needs into technical solutions\n* database and application management\n\t+ maintain and optimize database environments\n\t+ conduct capacity planning and performance monitoring\n\t+ participate in it risk assessment and security review processes\n\t+ support application lifecycle management and migration strategies\n* testing and quality assurance\n\t+ develop and execute unit, integration, and performance testing\n\t+ implement quality control processes to reduce defects and production issues\n\t+ troubleshoot complex data and pipeline failures\n* implementation and collaboration\n\t+ support system deployments and implementation planning\n\t+ coordinate with project teams, architects, vendors, and technical partners\n\t+ assist during public health emergencies as assigned\n **what changes between levels** **entry in training*** works under general supervision\n* assists with database development and etl processes\n* supports testing, documentation, and system implementation\n* builds foundational knowledge in data governance, integration, and analytics systems\n **journey level*** works under general direction\n* independently designs and implements complex data solutions\n* leads data pipeline troubleshooting and optimization\n* makes tactical decisions regarding production processes and data delivery\n* provides technical guidance to entry level staff\n **why you\u2019ll love this role:*** the data systems supported by this role inform statewide reporting, federal compliance, public transparency efforts, and mission critical public health operations. reliable data directly supports healthcare partners, laboratories, and local health jurisdictions across washington.\n* the environments include hundreds of databases and applications supporting a large statewide workforce and significant public funding. the systems are complex, visible, and relied upon daily.\n* the role builds and maintains pipelines, automates data movement, manages integrations across cloud and on premises systems, and solves foundational data challenges.\n* entry level hires build toward independent engineering work. journey level engineers operate with autonomy and influence technical direction. the path is clear and competency based.\n* when data flows correctly, programs function. when it doesn\u2019t, operations slow down. this role sits directly in that critical space and helps ensure continuity, reliability, and security of high value systems.\n **what you bring:**\nyou bring strong technical and professional competence in data engineering, database systems, and data pipelines, along with a clear achievement orientation that adds measurable value to team outcomes. you use sound analysis and problem\\-solving skills to address complex data challenges and communicate technical concepts clearly to diverse audiences. you demonstrate cultural competency, customer service orientation, and strong interpersonal skills when working across business and technical teams. at the journey level, you provide informal leadership and help align technical work with agency priorities. above all, you maintain professional conduct, accept feedback, collaborate effectively, and contribute to a positive and accountable work environment. **required qualifications***there are multiple pathways to qualify for this position. you must meet one of the options provided and any additional criteria listed. experience may have been gained through paid or unpaid activities. please ensure any relevant experience defined belo**w is outlined in your cover letter, resume, and/or applicant profile. experience may have been gained concurrently.* **entry level/in\\-training requirements****:*** **option 1:** four (4\\) years of professional experience in one or more of the following data disciplines: data pipeline, data processing, data migration, etl, database development, application development.\n* **option 2:** an associate\u2019s degree or higher in information technology program or a closely related field; and two (2\\) years of professional experience in one or more of the following data disciplines: data pipeline, data processing, data migration, etl, database development, application development.\n* **option 3:** a bachelor\u2019s degree in information technology program or a closely related field; and one (1\\) year of professional experience in one or more of the following data disciplines: data pipeline, data processing, data migration, etl, database development, application development.\n **preferred qualifications**\nwhile these aren\u2019t required, having them can help you stand out as a candidate.* one (1\\) or more years of using python or machine learning.\n* knowledge of etl processing\n* knowledge of replication, high availability, and clustering technologies\n* familiarity with version control and deployment processes\n* knowledge of salesforce or completed salesforce training\n* knowledge of hl7\n **journey level requirements****:*** **option1:** eight (8\\) years of full\\-time equivalent, professional experience in one or more of the following it disciplines: systems development, data administration, database management, data pipeline engineering.\n* **option 2:** an associate\u2019s degree or higher in information technology program or a closely related field; and six (6\\) years of full\\-time equivalent, professional experience in one or more of the following it disciplines: systems development, data administration, database management, data pipeline engineering.\n* **option 3:** a bachelor\u2019s degree or higher in information technology program or a closely related field; and four (4\\) years of full\\-time equivalent, professional experience in one or more of the following it disciplines: systems development, data administration, database management, data pipeline engineering.\n **additional required knowledge, skills, abilities, and experience*** two (2\\) or more years, of professional experience in the software development field.\n* experience in the sql data administration at an enterprise level demonstrating the ability to perform sql server 2012/2014/2016/2019 database administration.\n* building and maintaining reports in ssrs\n* developing ssis packages, building cubes and datamart in ssas.\n* experience in data architecture, data administration, data analysis, data modeling, reports development, data pipeline development or database administration\n **preferred qualifications**\nwhile these aren\u2019t required, having them can help you stand out as a candidate.* two (2\\) or more years of experience at the expert level in ms power bi.\n* demonstrated extensive advanced experience using and or managing sql server reporting services(ssrs).\n* experience developing web\\-based applications, databases, and visualizations.\n* demonstrated experience working with and implementing business intelligence tools such as tableau, developing and/or using business intelligence reports.\n* three (3\\) years\u2019 experience working with large\\-scale distributed systems such as hadoop/spark/storm, data warehousing systems such as redshift or bigquery, event brokers such as kafka or google cloud pub/sub, and/or databases such as hbase/cassandra.\n* experience with cloud computing platforms like aws, google cloud or microsoft azure\n* experience building data pipelines at internet scale (terabytes per day)\n **employee benefits**\nwe offer a solid benefits package that supports you and your family\u2019s health, financial security, and work\\-life balance. you\u2019ll have access to comprehensive medical, dental, and vision coverage, life and long\\-term disability insurance, flexible spending and health savings accounts, and retirement plans that help you plan for tomorrow while you\u2019re living today. paid holidays, vacation and sick leave help you recharge, and additional programs like dependent care assistance and professional development opportunities add value beyond basic coverage. join us and enjoy benefits designed to care for you as much as you care about public health. learn more about doh benefits and see how we support your life at work and beyond by visiting work@health. **oit: shaping a healthier future through innovation**\nthe office of innovation and technology works to harness technology and data to advance public health, supporting over 220 systems and 305 applications, including critical programs like the immunization registry. we drive interoperability, strengthen cybersecurity, and promote health equity, delivering impactful solutions that empower communities. our team fosters a culture of curiosity, critical thinking, and collaboration, where every member\u2019s contribution shapes a healthier future. **about the washington state department of health**\nwe're nearly 2,000 professionals across washington working together to protect and improve community health. guided by our values of equity, innovation, and engagement, we address health disparities, respond to emerging challenges, and strengthen systems that support resilience. at doh, we help reduce barriers, collaborate with diverse communities, and champion equitable health outcomes. we\u2019re passionate people who are driven to make a difference in public health. explore more about the department of health, our programs, and our impact by visiting our website. **working conditions:**\nthe following describes the working conditions of this position, with or without reasonable accommodation. **work setting:*** this position may work from home, a doh facility, or remotely from an alternate location. the official duty station for this position is home\\-based. if onsite work is requested by the supervisor, the request will be planned and communicated in advance.\n* work is performed almost exclusively indoors in an office environment. exposure to hazards is limited to those commonly found in government office environments.\n**schedule:*** standard business hours are monday \u2013 friday from 8 a.m. to 5 p.m., but the incumbent may be expected to adjust the work schedule to meet business needs and to meet deadlines for system implementations.\n* occasionally the you may be contacted outside of business hours to address emergency or urgent business needs and is expected to assist, if possible. maintenance windows vary from team to team but are scheduled in advance. this position may participate in stand\\-by coverage to support agency systems. an alternative flex schedule may be allowed.\n**travel requirements:*** travel is not required to perform the duties of this position; however some travel may be expected, and is typically local or regional, to meet with clients, conduct business, or attend or provide training. when driving for state business, the employee must be able to legally operate a state or privately\\-owned vehicle; or provide alternate transportation while on state business.\n**tools \\& equipment:*** duties require the use of standard office furniture and equipment (e.g., desk, filing cabinet, computer, printer, telephone, fax machine, copy machine, etc.) may be issued an agency mobile device or use a byod device. will use vpn connectivity for remote access.\n**customer interactions:*** frequent contact (likely daily) with clients, including some contact with those who may be angry, distraught, or frustrated\n**other:*** this position is covered by a bargaining unit for which the washington federation of state employees (wfse) is the exclusive representative.\n* the doh campus is a smoke\\-free, drug\\-free, alcohol\\-free, scent\\-neutral environment.\n* this position may be required to conduct and/or participate in public health emergency preparedness and response activities.\n **application directions**:\nwe\u2019re committed to a fair and equitable hiring process. only materials submitted through the official application will be considered. emailed resumes or documents won\u2019t be accepted or shared with the hiring manager.  \n\nclick \u201capply\u201d to complete your application. attach your resume, cover letter, and dd\\-214 (if applicable). list at least three professional references, directly in your applicant profile or as a separate attachment, including a supervisor, a peer, and someone you\u2019ve supervised or led (if applicable). ***do not include*** private details like your ssn or birth year, personal photos, transcripts, certifications, diplomas, projects, portfolios, or letters of recommendation. **veterans preference**: applicants wishing to claim veterans preference must attach a copy of their dd\\-214 (member 4 copy), ngb 22, or a signed verification of service letter from the united states department of veterans affairs to their application. please remove or cover any personally identifiable data such as social security numbers and birth year **equity, diversity, and inclusion**: we regard diversity as the foundation of our strength, recognizing that differing insights and abilities enable us to reflect the unique needs of the communities we serve.  \n\ndoh is an equal\\-opportunity employer. we prohibit discrimination based on race/ethnicity/color, creed, sex, pregnancy, age, religion, national origin, marital status, the presence or perception of a disability, veteran\u2019s status, military status, genetic information, sexual orientation, gender expression, or gender identity. **questions and accommodations:** if you have questions, need assistance with the application process, require an accommodation, or would like to request this posting in an alternative format, please contact **shawnelle goalder,** talent acquisition consultant/recruiter at employment@doh.wa.gov and reference dohxxxx in the subject. **technical support**: reach out to neogov directly at 1\\-855\\-524\\-5627 for technical support and login issues. **subscribe to doh job alerts**\n**supplemental information**\n----------------------------\n\n\nthis recruitment may be used to fill positions of the same job classification across the agency. once all the position(s) from the recruitment are filled, the candidate pool may be used to fill additional open positions for the next sixty (60\\) days.\n\n  \n\nonly applicants who follow the directions and complete the application process in full will have their responses reviewed for consideration.\n\n  \n\nexperience and education selected, listed, or detailed in the supplemental questions must be verifiable on the submitted applicant profile.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist - USA Remote",
        "company": "Danaher Diagnostics",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-24",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "LLaMA",
            "Mistral",
            "Hugging Face",
            "Prompt Engineering",
            "MLflow",
            "Git",
            "Snowflake",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=34dde1de84a8d35d",
        "description": "**bring more to life.**\n\n\nare you ready to accelerate your potential and make a real difference within life sciences, diagnostics and biotechnology?\n\n\nwithin danaher the work our diagnostic businesses do saves lives\u2014and we\u2019re all united by a shared commitment to innovate for tangible impact.\n\n\nyou\u2019ll thrive in a culture of belonging where you and your unique viewpoint matter. and by harnessing danaher\u2019s system of continuous improvement, you help turn ideas into impact \u2013 innovating at the speed of life.\n\n\nwe\u2019re accelerating the development of cutting\\-edge diagnostics to solve some of the world\u2019s most pressing health challenges. across our diagnostics operating companies we are driving innovation through partnerships with top academic institutions and leading players in biopharma and translational research. we\u2019re bringing the best minds together to accelerate innovation and unlock the full potential of the latest scientific advances. together, we\u2019re expanding access to precision diagnostics for millions of people worldwide \\- and we\u2019re using our unmatched global scale and proven playbook to make it happen, from hospital labs to mobile clinics. by helping providers, patients, and families get faster, more precise diagnostic results, we\u2019re improving treatment options and saving lives.\n\n\nlearn about the danaher business system which makes everything possible.\n\n\nthe **senior data scientist, diagnostics platform** supports the diagnostics platform by contributing to the strategy, development, and operationalization of large language models (llms), agentic ai, and image ai solutions. in this role, you will help build multimodal datasets, develop production\\-grade models, contribute to rag pipelines and agent workflows, and integrate insights into products, clinical, regulatory, and post\\-market evidence programs. the role works cross\\-functionally with clinical, r\\&d, regulatory/quality, medical affairs, it/security, and operating companies to deliver measurable patients and business impact.\n\n\nthis position reports to the director, data science and is part of the diagnostics platform and will be fully remote.\n\n**in this role, you will have the opportunity to:**\n\n* lead assay development optimization and r\\&d process acceleration using advanced statistical and ml methods (e.g. bayesian optimization).\n* conduct rwe analytics of ehr/claims/registries/device telemetry to drive biomarker discovery and development, and support dataset curation across text, tabular, device logs, lab results, and ehr/claims.\n* implement reliable and reproducible ai frameworks that improve diagnostic performance analytics, workflow efficiency, labeling/claims support, heor/rwe insights, image processing, and customer experience.\n* contribute to the development of solutions on azure, snowflake/databricks, vector databases, and enterprise content platforms (sharepoint, confluence).\n* translate analyses into clear, concise narratives, dashboards, and cross\u2011functional updates, and collaborate with clinical, regulatory, r\\&d, it, and academic partners as needed.\n\n**the essential requirements of the job include:**\n\n* master\u2019s or phd in biomedical engineering, biostatistics, bioinformatics, computer science, data science, or related fields.\n* 3\\+ years of combined industry and/or postdoctoral research experience in medtech, diagnostics, biopharma, or applied ai domains.\n* hands\\-on proficiency in python/r and modern ml frameworks; familiarity with doe (design of experiments), bayesian optimization, causal, survival, and longitudinal methods.\n* experience working with rwe/rwd datasets (ehr, claims, registries, device logs).\n\n**travel, motor vehicle record \\& physical/environment requirements:**\n\n* ability to travel \u2013 10%\n\n**it would be a plus if you also possess previous experience in:**\n\n* background in oncology, infectious diseases, acute care, or neurodegenerative disorders.\n* experience with imaging data (wsi/ihc and/or ct/pet/mri) and llm\\-based systems (agents, rag, finetuning, prompt engineering)\n* experience with azure openai, snowflake, coding agents (e.g claude code, vs code, codex), llama/mistral, mlflow, delta lake, hugging face, and largescale model deployment.\n\n\nwithin danaher diagnostics, we offer a broad array of comprehensive, competitive benefit programs that add value to our lives. whether it\u2019s a health care program or paid time off, our programs contribute to life beyond the job. check out our benefits at danaher benefits info.\n\n\nwithin danaher diagnostics we believe in designing a better, more sustainable workforce. we recognize the benefits of flexible, remote working arrangements for eligible roles and are committed to providing enriching careers, no matter the work arrangement. this position is eligible for a remote work arrangement in which you can work remotely from your home. additional information about this remote work arrangement will be provided by your interview team. explore the flexibility and challenge that working within danaher diagnostics can provide.\n\n\nthe annual salary range for this role is $150,000 to $170,000\\. this is the range that we in good faith believe is the range of possible compensation for this role at the time of this posting. this range may be modified in the future.\n\n\nthis job is also eligible for bonus/incentive pay.\n\n\n\\#li\\-remote\n\n\nwe offer comprehensive package of benefits including paid time off, medical/dental/vision insurance and 401(k) to eligible employees.\n\n\nnote: no amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. the amount and availability of any bonus, commission, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the company's sole discretion unless and until paid and may be modified at the company\u2019s sole discretion, consistent with the law.\n\n\njoin our winning team today. together, we\u2019ll accelerate the real\\-life impact of tomorrow\u2019s science and technology. we partner with customers across the globe to help them solve their most complex challenges, architecting solutions that bring the power of science to life.\n\n\nfor more information, visit www.danaher.com.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Analyst, Analytics Engineering",
        "company": "Pearl Health",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-24",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "Athena",
            "Redshift",
            "BigQuery",
            "CI/CD",
            "Git",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ab2e931bbf1e84bf",
        "description": "**the opportunity**\n\n\nas a senior data analyst at pearl, you will bridge the gap between raw data and business value by designing and maintaining the trusted data models that power critical decisions across clinical outcomes and p\\&l. you have spent time \"in the trenches\" as a data analyst or bi engineer\u2014you don't just build models to be \"correct\"; you build them to be usable, understanding how downstream users in bi tools will interact with your work. you will partner with data science, product, and engineering to architect a self\\-service data environment. rather than just answering ad\\-hoc queries, you will codify business logic into clean, documented datasets that empower the entire organization to explore practice engagement, patient spend, and network analytics with confidence. you will drive team best practices by managing pull requests, conducting rigorous code reviews, and maintaining documentation within a git\\-based workflow, applying software engineering principles\u2014like automated testing and ci/cd\u2014to ensure our pipelines are robust and scalable.\n\n**who we are. . .**\n\n\npearl health is powering the future of healthcare. we help primary care providers and organizations to deliver quality healthcare to the patients who need it most, when they need it most \u2014 and get rewarded for keeping patients healthy.\n\n\nour technology, services, and financial tools enable better, more proactive care, decrease total cost of care across patient panels, and optimize performance in value\\-based care models for traditional medicare and medicare advantage.\n\n\nwe are a team of physicians and public health experts (stanford, harvard, mount sinai), technologists (athenahealth, amazon, meta, flatiron), healthcare innovators (centivo, aledade, stellar, arcadia), and experienced risk management professionals (cvs/aetna, humana, oscar) who believe that primary care providers are the key to addressing our healthcare system\u2019s biggest challenges.\n\n\nsince its founding in 2020, pearl has expanded to partner with thousands of primary care providers in practices and organizations across 44 states. our investors include andreessen horowitz, viking global investors, alleycorp, and sv angel.\n\n**what senior data analyst means to us**\n\n* partner with our technical lead to design flexible and scalable data models that capture the ontological relationships of our healthcare data, balancing immediate business needs with long\\-term architectural health.\n* translate complex company goals into governed, reusable data models in dbt and snowflake, serving as the \"source of truth\" for downstream bi tools (hex).\n* execute the design and development of robust data transformation pipelines. you will focus on modularity, performance optimization, and maintainability, ensuring our data assets scale with our data volume.\n* move beyond simple schema design by implementing automated testing (data freshness, schema validation, null checks) and observability. you will ensure that stakeholders trust the data implicitly.\n* partner with data engineering to define ingestion requirements and with product/customer success to scope analytical needs, turning vague business problems into concrete technical requirements.\n* collaborate with technical lead to identify technical debt and architectural opportunities, proposing solutions that align our data infrastructure with our 6\\-month okrs.\n\n**who you are**\n\n* bachelor's or master's degree in a quantitative field (e.g., statistics, mathematics, computer science, economics, or related field).\n* 3\\-5 years of experience in delivering analytics across the data pipeline: data ingestion, data architecture/schema design (including dimensional modeling), data transformation, data analysis, and reporting data to business stakeholders (including data visualization)\n* proven track record building end\\-to\\-end data transformations for business stakeholders; direct experience using dbt and cloud\\-based warehousing (e.g., snowflake, bigquery, or redshift) is required.\n* demonstrated ability to design data schemas that maximize dataset utility by analyzing input data, understanding business and technical needs, and structuring staging tables, functional data marts, or conceptual data marts to capture ontological relationships.\n* advanced proficiency in sql (comfortable with window functions, cte organization for readability, and performance optimization) with a track record of working with legacy logic and migrating it to a standardized framework. python proficiency for data manipulation is a plus but not required.\n* excellent communication, presentation, and facilitation skills, with the ability to explain complex concepts clearly and concisely.\n* familiarity with aws cloud architectures.\n* healthcare or insurance experience, and/or experience working in a startup is preferred.\n* thoughtful collaborator with a passion for building analytical systems while balancing the tradeoffs between speed and thoroughness in development and scalability.\n\n**our values**\n--------------\n\n\ncollaborate to innovate: we believe the best solutions arise from intelligent teamwork. we trust the expertise of our teammates and pursue opportunities to learn and grow from each other. by embracing diverse perspectives and encouraging authenticity, we create and evangelize groundbreaking health solutions.\n\n\ntrust through transparency: we prioritize transparency in all our interactions, ensuring that employees, patients, clinicians, and partners have access to the information they need to make informed decisions. integrity is at the core of how we operate.\n\n\nserious impact, big heart: we go above and beyond to empower proactive, patient\\-centered care \u2014 and we celebrate every step forward. humor and positivity fuel our creativity and strengthen relationships.\n\n*we are an equal opportunity employer on a mission to improve lives. our strength comes from the diverse backgrounds, experiences, and perspectives of our team. we welcome all candidates and are committed to a fair, inclusive hiring process free from discrimination.*\n\n**what we offer**\n-----------------\n\n\nthe expected offer for this role includes the following components:\n\n* base salary range: $105,000\\-$160,000 per year\n* additional compensation: eligible for a discretionary performance bonus and equity options\n* benefits: we offer a competitive benefits package. more on our careers page.\n\n*final compensation will be determined by a variety of factors, including relevant skills, experience, labor market conditions, and location.*\n\n**agency submissions**\n----------------------\n\n\nwe are not currently working with contingency search firms. if a resume is submitted to any pearl health employee by a third party without a valid written and signed search agreement, it will become the property of pearl health and no fee will be paid, irrespective of whether the candidate is hired.\n\n**the interview process**\n-------------------------\n\n\nwhile steps may vary by role, you can typically expect:\n\n* recruiter screen: introductory call to discuss background and motivation\n* subject matter expert interview: deep\\-dive conversation with a core member of the team\n* panel interview: meetings with teammates and cross\\-functional partners\n* case assignment / presentation: practical exercise solving a real\\-world challenge\n* executive interview: final conversation(s) with 1\u20132 members of our leadership team",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Development Engineer",
        "company": "Expedia Group",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f6610ed807a1c6f6",
        "description": "expedia group brands power global travel for everyone, everywhere. we design cutting\\-edge tech to make travel smoother and more memorable, and we create groundbreaking solutions for our partners. our diverse, vibrant, and welcoming community is essential in driving our success.\n\n**why join us?**\n\n\nto shape the future of travel, people must come first. guided by our values and leadership agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\n\n\nwe provide a full benefits package, including exciting travel perks, generous time\\-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees' passion for travel and ensure a rewarding career journey. we\u2019re building a more open world. join us.\n\n**software development engineer iii**\n\n**introduction to team:**\n\nexpedia platform technology teams partner with our product teams to create innovative products, services, and tools to deliver high\\-quality experiences for travelers, partners, and our employees. a singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\n\n\nour team in eg is responsible for building a core platform that includes life cycle management of account, access and clusters; standardize governance by configuring network, security, observability, data and ai through core platform to make the infrastructure available for several hundred production services in eg.\n\n\nwe are hiring a hands\\-on software development engineer to lead the design, implementation and operational excellence of our cloud infrastructure and platform capabilities. this role emphasizes platform engineering: designing and delivering a scalable, reliable, secure, observable, and cost\\-efficient runtime platform (kubernetes, containers, ci/cd, iac, cloud services) used by multiple product teams. you will be embedded in the code and infrastructure to execute the roadmap end\\-to\\-end.\n\n\nyou will shape the foundational platform that powers customer\\-facing services across brands, increasing developer velocity, reducing operational risk, and optimizing cloud spend.\n\n\n**what you will do:**\n\n* design and build scalable features and services for our platform architecture.\n* write high\u2011quality, maintainable code with strong test coverage.\n* monitor and support production systems, resolving issues quickly.\n* drive technical improvements in software performance, and tooling.\n\n**minimum qualifications:**\n\n* 5\\+ years professional software engineering experience with significant hands\\-on experience building and operating distributed cloud services\n* strong, demonstrable experience building and running platforms on aws (eks, ecs, ec2, vpc, iam, s3, rds, elb/alb, auto scaling, route 53, cloudtrail, cloudwatch, cost explorer)\n* deep hands\\-on experience with containerization and kubernetes at scale (eks or comparable)\n* practical experience with infrastructure as code (terraform, cloudformation) and helm charts\n* proven record of contributing production code and platform automation (languages such as go, python, java, or similar)\n* experience building applications with resilience, observability, security and operational automation\n* familiarity with ci/cd tooling and developer workflows (spinnaker, jenkins, github actions, gitlab ci, or similar)\n* demonstrated ability to work in cross\\-functional technical initiatives and architecture.\n* strong communication skills and experience mentoring engineers\n\n**preferred qualifications:**\n\n* prior platform infrastructure building experience\n* knowledge of service meshes (istio, linkerd, vpc lattice), api gateways, and advanced networking patterns\n* experience with security/compliance for cloud environments, secrets management, and policy\\-as\\-code\n* experience with monitoring/observability stacks (prometheus, grafana, datadog, open telemetry, jaeger)\n* proven experience with aws cost optimization strategies and tooling (cost explorer, trusted advisor, billing apis)\n\n\\#li\\-dni\n\n\nthe total cash range for this position in san jose is $157,500\\.00 to $220,500\\.00\\. employees in this role have the potential to increase their pay up to $252,000\\.00, which is the top of the range, based on ongoing, demonstrated, and sustained performance in the role.\nstarting pay for this role will vary based on multiple factors, including location, available budget, and an individual\u2019s knowledge, skills, and experience. pay ranges may be modified in the future.\n\n\nexpedia group is proud to offer a wide range of benefits to support employees and their families, including medical/dental/vision, paid time off, and an employee assistance program. to fuel each employee\u2019s passion for travel, we offer a wellness \\& travel reimbursement, travel discounts, and an international airlines travel agent (iatan) membership. view our full list of benefits.\n\n**accommodation requests**\n\n\nif you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our recruiting accommodations team through the accommodation request.\n\n\nwe are proud to be named as a best place to work on glassdoor in 2024 and be recognized for award\\-winning culture by organizations like forbes, time, disability:in, and others.\n\n\nexpedia group's family of brands includes: brand expedia\u00ae, hotels.com\u00ae, expedia\u00ae partner solutions, vrbo\u00ae, trivago\u00ae, orbitz\u00ae, travelocity\u00ae, hotwire\u00ae, wotif\u00ae, ebookers\u00ae, cheaptickets\u00ae, expedia group\u2122 media solutions, expedia local expert\u00ae, carrentals.com\u2122, and expedia cruises\u2122. \u00a9 2024 expedia, inc. all rights reserved. trademarks and logos are the property of their respective owners. cst: 2029030\\-50\n\n\nemployment opportunities and job offers at expedia group will always come from expedia group\u2019s talent acquisition and hiring teams. never provide sensitive, personal information to someone unless you\u2019re confident who the recipient is. expedia group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. our email domain is @expediagroup.com. the official website to find and apply for job openings at expedia group is careers.expediagroup.com/jobs.\n\n\nexpedia is committed to creating an inclusive work environment with a diverse workforce. all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. this employer participates in e\\-verify. the employer will provide the social security administration (ssa) and, if necessary, the department of homeland security (dhs) with information from each new employee's i\\-9 to confirm work authorization.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Backend Engineer, AI/ML-powered IoT Platform",
        "company": "JIG-SAW US, Inc.",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 14.4,
        "matched_keywords": [
            "Kinesis",
            "Docker",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "PostgreSQL",
            "MongoDB",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8f03868858d07644",
        "description": "**who we are**\n\njig\\-saw operates 24/7 operations centers in japan and canada that proactively monitor systems, issue alerts, and deliver live incident response\u2014keeping your web services and iot environments secure and running smoothly.\n\n**corporate site:** https://jig\\-saw.com/en/\n\n**neqto.ai:** https://neqto.ai/\n\n**about the role**\n\nhelp build a game\\-changing, ai\\-driven iot platform at enterprise scale. as a senior backend engineer, you\u2019ll own the core data pipeline\u2014evolving our backend from its current foundation (node.js/typescript, postgresql, mongodb, pg\\-boss job queues) toward a high\\-throughput, event\\-driven architecture capable of handling millions of sensor readings per day.\n\nyou\u2019ll architect the migration to kafka\\-centric streaming, implement distributed caching with redis, extend iot protocol support beyond mqtt, and build the resilient, scalable services that power real\\-time dashboards, intelligent alerts, and ai\\-driven anomaly detection for multi\\-tenant enterprise customers.\n\n**key responsibilities**\n\n**high\\-throughput data streaming:** architect the migration from postgresql\\-backed job queues (pg\\-boss) to kafka\\-centric event streaming. design bi\\-directional data flow\u2014ingest from iot protocols (mqtt today, expanding to modbus, bacnet, lorawan) and expose processed data to downstream microservices via optimized topics.\n\n**distributed caching \\& performance:** replace in\\-process caching (node\\-cache) with redis. design multi\\-layer caching strategies to reduce database load for high\\-frequency sensor data. optimize api response times for real\\-time dashboarding\u2014sub\\-second latency at scale.\n\n**scalable backend services:** evolve the worker architecture (http, websocket, mqtt, alerts, jobs) into independently scalable, distributed services. design for horizontal scaling\u2014stateless services, distributed state management, graceful degradation, backpressure handling.\n\n**protocol integration:** build protocol adapters and bridges that normalize heterogeneous device data (modbus tcp/rtu, bacnet/ip, mqtt, lorawan) into standardized schemas with reliable delivery and data integrity.\n\n**api \\& integration layer:** extend the rest api for enterprise integrations and partner platforms. design webhook/event delivery systems for downstream consumers. support real\\-time data delivery via websocket alongside rest.\n\n**production excellence:** own the backend end\\-to\\-end\u2014from device ingest to dashboard delivery. implement circuit breakers, retry strategies, dead\\-letter queues, and observability hooks across all services.\n\n**infrastructure as code:** help manage cloud resources via terraform, cloudformation and crossplane, enabling self\\-service infrastructure provisioning, drift detection, and gitops\\-driven deployments tied to the backend services you own.\n\n**required skills \\& qualifications**\n\n**backend engineering:** 5\\+ years building production backend services in node.js/typescript (or equivalent) for high\\-traffic, distributed systems.\n\n**event streaming:** hands\\-on experience designing and operating kafka (or comparable: pulsar, kinesis, eventhub)\u2014topic design, consumer groups, partitioning, exactly\\-once semantics, schema evolution.\n\n**distributed caching:** production experience with redis or memcached\u2014cluster mode, eviction policies, cache invalidation patterns, pub/sub.\n\n**databases:** deep postgresql experience\u2014query optimization, partitioning, connection pooling (pgbouncer), replication. mongodb experience\u2014schema design for time\\-series/iot data, sharding, aggregation pipelines.\n\n**distributed systems:** service decomposition, eventual consistency, backpressure handling, circuit breakers, idempotency patterns. experience building systems that degrade gracefully under load.\n\n**api design:** restful conventions, versioning, rate limiting, authentication patterns, websocket architecture.\n\n**cloud \\& containers:** aws, gcp, azure, docker in production, ci/cd pipelines (github actions or similar).\n\n**education:** bachelor\u2019s degree or higher in computer science (or closely related field) is required.\n\n**experience guidelines**\n\n**5\\+ years of backend engineering:** building and operating high\\-throughput data services in production, including event streaming, caching layers, and database optimization.\n\n**3\\+ years with iot or real\\-time data systems:** high\\-frequency ingest, time\\-series patterns, device connectivity, protocol handling.\n\n**2\\+ years with kafka or equivalent:** designing topics, managing consumer groups, handling schema evolution and exactly\\-once delivery in production.\n\n**nice\\-to\\-have**\n\n**iot protocols:** hands\\-on experience with mqtt brokers (hivemq), modbus tcp/rtu, bacnet/ip, lorawan, opc\\-ua, certificate\\-based device authentication.\n\n**time\\-series data:** timescaledb, influxdb, or partitioned postgresql for high\\-volume sensor data; data retention and downsampling strategies.\n\n**migration experience:** strangler fig pattern, dual\\-write, shadow traffic\u2014incrementally evolving production systems without downtime.\n\n**multi\\-tenant saas:** data isolation patterns, tenant\\-aware routing, noisy\\-neighbor mitigation.\n\n**ai/ml integration:** experience feeding data pipelines into ml models (anomaly detection, forecasting) or building feature stores.\n\n**prisma orm:** production experience with prisma on postgresql.\n\n**security:** pii handling, secrets management, rbac, audit logging, soc 2 awareness.\n\n**compensation \\& benefits**\n\n* competitive salary (market\\-based; location\\-dependent)\n* health, dental, vision insurance (u.s.)\n* paid time off (pto): vacation, sick leave, company holidays\n* remote work\n\njob type: full\\-time\n\npay: $100,000\\.00 \\- $150,000\\.00 per year\n\nbenefits:\n\n* dental insurance\n* health insurance\n* paid time off\n* vision insurance\n\neducation:\n\n* bachelor's (required)\n\nexperience:\n\n* production\\-level mongodb: 5 years (required)\n* production\\-level postgresql: 5 years (required)\n* aws architecture \\& ops (professional): 5 years (required)\n\nlicense/certification:\n\n* aws certified solutions architect \u2013 professional (required)\n\nwork location: remote",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Python / AI Engineer",
        "company": "Cognizant Technology Solutions",
        "location": "Washington, DC, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Gemini",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Data Lake",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d3e8a0028e588dd3",
        "description": "**background \\& objectives**\n\n\nthe information and technology solutions (its) vice presidency is responsible for providing high\\-quality information and technology solutions to the world bank group. within its, the operations products team (itsop) focuses on leveraging advanced ai technologies to enhance the bank's operations and services. the ai engineer will play a critical role in developing and implementing ai\\-powered solutions, including generative ai models like chatgpt and gemini, to support various initiatives within the world bank.\n\n**scope of work**\n\n\nthe ai engineer will be responsible for designing, developing, and deploying ai solutions that address the needs of the world bank. this includes working on projects that involve natural language processing, machine learning, generative ai and other ai technologies. the engineer will collaborate with various teams to understand their requirements and deliver ai solutions that improve efficiency, decision\\-making, and service delivery.\n\n**key responsibilities**\n\n* lead the design and implementation of scalable, secure cloud architectures for enterprise solutions leveraging llm models from google and openai.\n* build applications with vector databases and langchain and understand text embeddings.\n* develop and maintain cloud\\-based solutions that integrate llm models to enhance search capabilities and user experience.\n* demonstrate experience in azure technologies, including ase, azure functions, azure api management, azure service bus, logic apps, azure storage, azure cognitive services, azure cosmos db, azure data factory, databricks, azure data lake, and caching technologies.\n* provide technical leadership and mentorship to team members, ensuring best practices in cloud architecture and llm model integration.\n* ensure compliance with security standards and best practices in cloud architecture and data handling, particularly when dealing with sensitive information processed by llm models.\n* address performance and production issues, with extensive knowledge in logging and monitoring using tools such as splunk and appinsights.\n* possess sound technical knowledge and understanding of infrastructure design, including private and public cloud.\n* establish systems to supervise the operating efficiency of existing application systems and provide proactive maintenance.\n* participate in systems design, working within an established framework, and provide direction to a team of staff and contractors in their area of expertise.\n* stay updated with the latest industry trends, cloud technologies, and advancements in llm models to continually improve enterprise search solutions.\n* exhibit strong leadership and team\\-building skills, with the ability to mentor and guide technical teams, and excellent communication skills to articulate complex technical information to non\\-technical stakeholders.\n* create detailed architectural documentation and design patterns, and possess knowledge of security standards and compliance requirements in a cloud environment.\n\n**required qualifications \\& experience**\n\n* a degree in computer science, data science, ai, or a related field.\n* **technical skills**:\n\t+ proficiency in programming languages such as python.\n\t+ experience with ai frameworks and libraries (e.g., tensorflow, pytorch).\n\t+ knowledge of natural language processing and generative ai models.\n\t+ experience with langchain, langgraph and building platform services.\n\t+ experience in building solutions leveraging agentic ai.\n\t+ knowledge and experience with google cloud services, azure technologies, and enterprise search applications.\n* **professional experience**:\n\t+ at least 3\\-5 years of experience in ai development and implementation.\n\t+ experience in prompt engineering and ai governance.\n\t+ demonstrated experience in cloud\\-native and hybrid web development projects.\n* **soft skills**:\n\t+ strong problem\\-solving skills.\n\t+ excellent communication and collaboration abilities.\n\t+ ability to work in a fast\\-paced and dynamic environment.\n\n**location**\n\n* **location**: the position is based at the world bank headquarters in washington, d.c.\n\n***please note, this role is not able to offer visa transfer or sponsorship now or in the future***",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Cloud Data Engineer",
        "company": "Arbitration Forums Inc.",
        "location": "Tampa, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "CI/CD",
            "Git",
            "Snowflake",
            "Databricks",
            "Tableau",
            "Power BI",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=07cde8b42f3da231",
        "description": "**department:** data insights and innovation\n\n**job title:** senior cloud data engineer\n\n**job code:** scde\n\n**reports to:** lead cloud data engineer\n\n**flsa status:** exempt\n\n**employment type:** full\\-time\n\n**job purpose:**\n\n\nthis role at arbitration forums is as unique as it is rewarding because of the af ipaal values (integrity, passion, accountability, achievement, leadership) and tri model (trust, respect, inclusion).\n\n\nthe senior cloud data engineer will be responsible for designing, developing, and implementing robust, scalable, and secure data pipelines for modern cloud platforms to support analytics and ai/ml needs at arbitration forums, inc. this role will streamline data acquisition from different data sources and set up processes to ensure data quality and data security.\n\n**departmental expectation of employee**\n\n* adheres to af policy and procedures and the af ipaal values and tri model\n* acts as a role model within and outside af.\n* performs duties as workload necessitates.\n* maintains a positive and respectful attitude.\n* communicates regularly with the departmental leader about department issues.\n* demonstrates flexible and efficient time management and ability to prioritize workload.\n* consistently reports to work on time, prepared to perform duties of the position.\n* meets department productivity standards.\n\n**essential duties and responsibilities**\n\n**data engineering \\& pipeline development:**\n\n* design, develop, and implement robust, scalable, and secure data pipelines in a cloud environment.\n* build and manage etl/elt processes to efficiently move and transform large datasets from multiple data sources.\n* implement secure data access, encryption, and data masking policies.\n* develop automated processes to validate data quality and data accuracy.\n* document and maintain data workflows and diagrams.\n* work with data scientists and ai specialists to automate model deployment lifecycles (mlops).\n\n**data pipeline/warehouse management**\n\n* configure and maintain cloud\\-based data warehousing solutions.\n* optimize data warehouse storage strategies to support analytics and data science needs.\n* set up monitoring tools and alerts to maintain data warehouse availability and reliability.\n* troubleshoot, profile, and optimize data pipelines for performance issues to minimize latency.\n\n**collaboration**\n\n* work closely with data architects, data analysts and data scientists to understand their data needs and translate them into technical designs.\n* mentor and guide junior data engineers, perform code reviews, and establish best practices for could data engineering.\n* collaborate with devops and itops to implement ci/cd pipelines and robust dr strategies.\n\n**qualifications**\n\n* bachelor\u2019s degree in computer science, computer engineering, information systems, or a related field.\n* 7\\+ years of experience in data engineering with a focus on cloud data engineering.\n\n**technical skills:**\n\n* profound understanding of major cloud platforms (aws, gcp, azure) and major cloud data platforms like snowflake and databricks.\n* hands\\-on experience with data services offered by cloud platforms.\n* expertise in programming languages such as python, java, or scala with strong sql skills.\n* experience with etl/elt tools like talend, dbt, azure data factory, etc.\n* experience with ci/cd tools like gitlab/github.\n* strong knowledge of data governance, data security, and compliance practices.\n* experience supporting data science and machine learning operations.\n* familiarity with data visualization and reporting tools (e.g., power bi, tableau).\n\n**soft skills:**\n\n* excellent analytical and problem\\-solving abilities.\n* strong communication and interpersonal skills to collaborate with cross\\-functional teams.\n* auto insurance claims industry experience preferred.\n\n**americans with disability specifications**\n\n**physical demands**\n\n\nthe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.\n\n\nwhile performing the duties of this job, the employee is occasionally required to stand; walk; sit; use hands to finger, handle, or feel objects, tools or controls; reach with hands and arms; climb stairs; balance; stoop, kneel, crouch or crawl; talk or hear; taste or smell. the employee must occasionally lift and/or move up to 25 pounds. specific vision abilities required by the job include close vision, distance vision, color vision, peripheral vision, depth perception, and the ability to adjust focus.\n\n**work environment**\n\n\nthis is a fully remote position requiring reliable high\\-speed internet access and a dedicated workspace.\n\n\nreasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer III",
        "company": "Mulligan Funding, LLC",
        "location": "San Diego, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Data Lake",
            "Docker",
            "Kubernetes",
            "Git",
            "PostgreSQL",
            "Polars",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b83efe663e7f5074",
        "description": "*headquartered in san diego, mulligan funding serves as a leading provider of working capital (up to $5m) to the small and medium\\-sized businesses that fuel our country. since 2008, we have prided ourselves on our collaborative, innovative, and customer\\-focused approach. enjoying a period of unprecedented growth, driven by the combination of cutting\\-edge technology, human touch, and unwavering integrity, we are looking to add to our people first culture, with highly motivated and results\\-oriented professionals, to push the limits of what\u2019s possible while creating value for all of our partners.*  \n\nas a *data engineer iii*, you are responsible for leveraging data to drive business initiatives and ensuring the reliability, scalability, and accuracy of the data pipeline. you will collaborate closely with data analysts and data scientists to generate and validate data used for business decision\\-making and predictive modeling. this role operates within high\\-volume data environments integrating postgresql, azure cosmos db, and a modern data lakehouse architecture.\n### **you will:**\n\n* design, implement, and optimize etl and elt pipelines using dbt to extract, transform, and load data into the data warehouse.\n* build, evolve, and maintain infrastructure for optimal data transformation and integration using azure event hub or event grid, postgresql, and azure cosmos db.\n* manage and monitor complex data processing and storage infrastructure, including data lakes leveraging apache iceberg table formats.\n* perform advanced manipulation and analysis of large datasets using trino for distributed sql querying across diverse data sources.\n* utilize python for high\\-performance data manipulation, scripting, and process automation.\n* containerize and orchestrate data workloads using docker and kubernetes to ensure scalability and reliability.\n* develop architectural frameworks and tools that enable data science teams to build, train, and scale machine learning models on the azure machine learning platform.\n* automate and optimize data processes while implementing robust logging and monitoring practices.\n* partner strategically with analytics and cross\\-functional teams to deliver scalable data solutions within agreed timelines.\n* monitor, troubleshoot, and remediate complex data quality issues, including database schema modifications.\n* support data governance initiatives, including cataloging and lineage tracking.\n* collaborate with senior engineers to design data models, develop summarized datasets, and build data apis with minimal guidance.\n* coach and mentor junior engineers on complex technical challenges.\n* write scripts and queries to support ad hoc data analysis.\n* perform other duties as assigned.\n\n### **you have:**\n\n* 5\\+ years of relevant data engineering experience.\n* a bachelor\u2019s degree in a related field.\n* expert\\-level sql and advanced experience with postgresql schema design and query optimization.\n* deep proficiency in dbt for developing and managing modular, documented etl and elt pipelines.\n* advanced python skills, including experience with pandas and polars for high\\-performance data manipulation.\n* hands\\-on experience with azure event hub or event grid subscriptions and azure cosmos db for streaming data storage.\n* experience managing data lake environments using apache iceberg table formats.\n* proficiency with trino for federated sql queries across disparate data sources.\n* experience with git and modern source code management practices.\n* strong understanding of data management systems, data warehousing, and data modeling concepts.\n* ability to communicate effectively with both technical and non\\-technical stakeholders.\n\n### **you may also have:**\n\n* familiarity with docker and kubernetes for orchestrating data workloads and microservices.\n* experience supporting machine learning teams through the azure machine learning platform.\n* familiarity with r, scala, or other scripting languages for automation.\n* experience with data governance tools for cataloging and lineage tracking.\n\n### **we offer:**\n\n* comprehensive medical, vision and dental benefits that give you peace of mind.\n* flexible spending accounts (fsa) that let you use pre\\-tax dollars to cover healthcare expenses.\n* a fantastic 401k with matching contributions that helps you plan for retirement and build wealth over time.\n* generous sick, vacation, and holiday benefits that give you the time and flexibility you need to enjoy life.\n* a gym membership contribution that supports your well\\-being, and helps you stay energized and focused.\n* an internal referral program that rewards you for bringing talented people to the team.\n* company events that foster a positive and inclusive culture, and create opportunities to bond and grow with your colleagues.\n\n\na reasonable estimate of the salary range for this role is $126,100 to $159,200 per year. in order to provide a competitive compensation package, mulligan funding takes into account a variety of factors including but not limited to market compensation data, relevant experience, skills, education, and certifications.\nmulligan funding is an equal opportunity employer (eoe) and takes great pride in building a diverse work environment. qualified applicants are considered for employment without regard to age, race, religion, gender, national origin, sexual orientation, disability or veteran status.\n\nwe may use artificial intelligence (ai) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. these tools assist our recruitment team but do not replace human judgment. final hiring decisions are ultimately made by humans. if you would like more information about how your data is processed, please contact us.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Machine Learning Data Scientist Intern",
        "company": "Enlyte",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Gemini",
            "Copilot",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Git",
            "PySpark"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=92fceff63553264e",
        "description": "company overview:\n\nat enlyte, we combine innovative technology, clinical expertise, and human compassion to help people recover after workplace injuries or auto accidents. we support their journey back to health and wellness through our industry\\-leading solutions and services. whether you're supporting a fortune 500 client or a local business, developing cutting\\-edge technology, or providing clinical services you'll work alongside dedicated professionals who share your commitment to excellence and make a meaningful impact. join us in fueling our mission to protect dreams and restore lives, while building your career in an environment that values collaboration, innovation, and personal growth. **be part of a team that makes a real difference.**\njob description :\n\nwe are seeking passionate graduate\\-level ml/ai interns to join our auto physical damage (apd) intelligent solutions team for summer 2026 to work on cutting\\-edge ml/computer vision, generative ai systems, and claims management ai applications, and responsible ai initiatives. you will contribute to real\\-world projects involving computer vision, nlp, large language models, rag, multi\\-agent systems, and ai safety frameworks while gaining hands\\-on experience with the latest ai innovations shaping our property \\& casualty industry.  \n\nyou will have the opportunity to work on several challenging machine learning, generative as well as responsible ai problems, including:* build models at scale using vast amounts of structured and unstructured heterogeneous data.\n* ensure high accuracy based on industry\u2019s stringent requirements around precision or recall and with minimum type i and type ii errors.\n* generate predictions for millions of rows of data with high response time, and low latency.\n* deal with high data diversity, very high dimensionality, and noisy data.\n* integrate trustworthy machine learning, ai ethics, and governance.\n* learn about causal inference for ai systems\n\n**key focus areas*** generative ai \\& foundation models\n* design and fine\\-tune large language models and multimodal ai systems\n* implement rag (retrieval\\-augmented generation) pipelines and prompt engineering strategies\n* work with open\\-source, open\\-weights and proprietary llms (nova, titan, claude, \\& gemini)\n* integrate responsible ai \\& bias mitigation\n* conduct fairness audits and bias testing across protected attributes (race, gender, age, license plate etc.)\n* implement ai governance frameworks and risk assessment protocols\n* develop evaluation metrics for model safety, hallucination detection, and alignment\n* agentic ai systems\n* build autonomous ai agents using frameworks like aws strands, aws agent core or crewai\n* design multi\\-agent orchestration for complex task automation\n* create evaluation benchmarks for agent reliability and performance\n\n\nqualifications:\n**education \\& timing*** pursuing a ms/phd in computer science, ai/ml, data science, or related field\n* gpa of 3\\.5 or higher\n* graduating summer/fall 2026 or spring 2027\n* ability to commit 40 hours per week for 12 weeks during the 2026 summer\n\n**technical skills*** foundation in transformers, attention mechanisms, and neural net architectures\n* experience with tensorflow or pytorch\n* proficiency in python and modern ml ops tools\n* coursework in: deep learning, nlp, ai ethics, or trustworthy ai\n* coding assisting tools like github copilot, cline, and knowledge around using scientific, distributed programming and scripting languages like python, pyspark and/or java preferred.\n* strong foundation in mathematics, statistics, and machine learning algorithms\n* knowledge of human\\-ai interaction design\n\n**preferred experience*** published research or significant projects in generative ai applications\n* contributions to open\\-source ai safety tools or bias detection frameworks\n* familiarity with constitutional ai, rlhf, or interpretability methods\n* ai ethics certifications or responsible ai coursework\n\n**learn more about our summer internship program:****https://careers.enlyte.com/internship**  \n\n\nbenefits:\n\ncompensation depends on the applicable us geographic market. the expected base pay for this position is $35 per hour and will be based on a number of additional factors including skills, experience, and education.  \n\n*the company is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, gender identity, sexual orientation, age, status as a protected veteran, among other things, or status as a qualified individual with disability.*\n\n  \n\ndon\u2019t meet every single requirement? studies have shown that women and underrepresented minorities are less likely to apply to jobs unless they meet every single qualification. we are dedicated to building a diverse, inclusive, and authentic workplace, so if you\u2019re excited about this role but your past experience doesn\u2019t align perfectly with every qualification in the job description, we encourage you to apply anyway. you may be just the right candidate for this or other roles.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer - Platform (Frameworks, AI, Automation) Sr. Associate New",
        "company": "Sixth Street",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "S3",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "NoSQL",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ba0529d8ff520672",
        "description": "**the role**\n\n\n\nthis is a full\\-time role based in austin, tx as part of sixth street's growing technology team. we are building and evolving a cloud\u2011native platform that enables teams across the firm to build, ship, and operate secure, scalable software. we are a lean, high\\-impact team where everyone wears many hats. we do not work in isolation; we are deeply connected with our developers to ensure the platform we build solves real\\-world problems. on our team, you will treat our platform as a product. you will be responsible for the \"golden paths\" that reduce cognitive load for our developers, while balancing technical excellence with deep developer empathy. you will work closely with our infrastructure and security teams on behalf of our developers and analysts who use our platform.\n\n\n**about our stack**\n\n\n* aws\\-centric: lambda, ecs (fargate), rds, s3, cloudfront, elasticache\n* infrastructure as code: cdk for iac, terraform a plus\n* tooling: ownership of gitops and ci/cd tooling; github actions\n* backend: python\\-centric (with some java and julia); frontend: typescript/react\n* observability: opentelemetry standards and shared collectors\n* ai: ai gateway and shared ai platform services for secure model access and governance\n\n\n**core responsibilities**\n\n  \n\n\n* platform strategy: drive cross\\-team platform initiatives; facilitate alignment, adoption, and migration plans across the engineering organization.\n* self\\-service \\& devex: build and evolve self\\-service platform capabilities (e.g., \u201cgolden paths\u201d) that reduce cognitive load and accelerate delivery via templates, libraries, and sample applications.\n* mentorship \\& standards: mentor engineers, analysts, and non\\-technical stakeholders with empathy. set high\\-bar engineering standards for platform codebases and practice a disciplined approach to problem\\-solving (solid, ddd, etc.).\n* product mindset: treat the platform as a product with active involvement in our roadmap, feedback loops with developers, documentation, and iterative ux improvements.\n* observability \\& telemetry: refine and optimize the org\\-wide instrumentation \"golden path\" for metrics, logs, and traces. enable teams through high\\-quality documentation, examples, and targeted one\\-on\\-one partnering.\n* ai enablement: evolve our ai platform capabilities, including ai gateway services and governance patterns. use ai tools to plan, develop, and validate solutions as a key member of one of the firm\u2019s primary ai teams.\n* deployment \\& standards: in collaboration with our developers, evolve the gitops and ci/cd tooling strategy. encourage \"golden path\" deployment patterns, secrets management best practices, configuration, and runtime standards.\n* strategic partnerships: maintain a close partnership with the security and cloud infrastructure teams to ensure platform patterns are secure and resilient.\n* operational excellence: help define the future of our platform\u2019s reliability. this is an area ripe for ownership\u2014you will have the opportunity to shape our evolving slo/sli strategy, incident response patterns, and on\\-call expectations.\n\n\n**preferred**\n\n\n* 7\\+ years\u2019 experience in professional software development. degree in computer science or stem field preferred.\n* curiosity and hunger to keep learning\n* excellent programming skills in a major programming language. python experience strongly preferred\n* experience building internal platforms, shared services, or developer tooling\n* passion for ci/cd and shipping well\\-engineered code in a repeatable and reliable way.\n* experience designing restful apis, and working with relational and \u201cnosql\u201d data stores\n* comfortable working in a unix shell\n* strong sense of urgency; ability to execute quickly and efficiently with strong attention to detail\n* ability to collaborate effectively across multiple teams\n* strong prioritization and project management skills\n* comfort working in a high growth, iterative environment\n\n  \n\n\n**what we value**\n\n\n* highly motivated, entrepreneurial, and team\\-oriented candidates with strong problem\\-solving skills\n* strong developer empathy and a passion for improving developer experience\n* team oriented and ability to influence through patterns, frameworks, and collaboration\n* high trust and integrity\n* strategic / entrepreneurial mindset\n* strong organizational and communication skills\n* cross\\-team collaboration and the ability to operate across engineering, analytics, risk, and infrastructure functions\n\n  \n\n\n**about sixth street**\n\n\n\nfounded in 2009, sixth street is a leading global investment firm dedicated to developing themes and offering solutions to companies across all stages of growth.  \n\n  \n\nour firm is designed for cross\\-platform collaboration at scale: we build businesses, invest for growth, acquire assets, provide direct financing, identify value in public markets, purchase royalty streams, and regularly develop first\\-of\\-their\\-kind structures to meet the strategic objectives of management teams.  \n\n  \n\nwe do all this globally and as one unified team of investment and control\\-side professionals working on behalf of our institutional investors and their beneficiaries around the world.  \n\n  \n\nwe believe adhering to our core values is a competitive advantage. everyone on our team contributes their perspectives and expertise to help us solve complex challenges and get to the right answer. at sixth street, the best idea wins.\n\n  \n\n\n*sixth street is proud to be an equal opportunity employer. we do not discriminate based upon race, religion, color, national origin, genetic history, marital status, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, veteran or military status, disability, genetic predisposition, status as a victim of domestic violence, a sex offense or stalking, or any other class or status in accordance with applicable federal, state and local laws.*\n\n\n*pursuant to the san francisco fair chance ordinance, sixth street will consider for employment qualified applicants with arrest and conviction records.*\n\n*if you need a reasonable accommodation to fill out this application, please contact* *cindy bombara**.*\n\n\n\nplease refer to the privacy notice on our website for additional information regarding our obligations under the california consumer privacy act (\u201cccpa\u201d).",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Full Stack .NET Architect - GenAI Experience (Remote)",
        "company": "Avanade",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Hugging Face",
            "Prompt Engineering",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3e2c74a9e493b69f",
        "description": "job description\n  \n***this position is open to candidates located anywhere in the united states. candidates are welcome to work remotely or out of one of our regional offices.***\n\nwe are seeking a visionary full stack engineering architect with proven expertise in generative ai (gen ai) to drive the design and implementation of groundbreaking intelligent applications. the ideal candidate will possess deep technical acumen across front\\-end and back\\-end development, cloud architectures, and the latest advancements in ai and machine learning, particularly gen ai platforms and techniques.\n\n\n  \nqualification\n  \n* architect, design, and oversee development of scalable, secure, and robust full stack solutions integrating gen ai capabilities.\n* guide cross\\-functional engineering teams in building end\\-to\\-end web and mobile applications leveraging modern frameworks and cloud services.\n* lead the integration of state\\-of\\-the\\-art gen ai models (e.g., llms, transformers, generative image and text models) into core business products.\n* collaborate with product managers, data scientists, and ux/ui designers to deliver seamless, user\\-centric ai experiences.\n* establish and enforce software architecture best practices, coding standards, and agile development methodologies.\n* evaluate emerging ai technologies and tools, providing strategic recommendations for adoption and implementation.\n* mentor engineers, championing technical excellence and continuous learning within the team.\n* ensure solutions are compliant with security, privacy, and regulatory requirements relevant to gen ai applications.\n\n  \n\nqualifications\n\n\n* bachelor\u2019s or master\u2019s degree in computer science, engineering, or related field.\n* 10\\+ years of experience in full stack development .net, cloud computing (azure), and modern devops practices.\n* experience with microsoft azure openai service.\n* demonstrated hands\\-on expertise in deploying and scaling gen ai models (openai, hugging face, custom llms, etc.).\n* strong understanding of microservices, api design, containerization (docker, kubernetes), and ci/cd pipelines.\n* experience architecting solutions with restful and graphql apis, nosql/sql databases, and real\\-time data processing.\n* track record of leading engineering teams and delivering complex projects from conception to production.\n* excellent problem\\-solving, communication, and leadership skills.\n* passion for innovation and staying ahead of gen ai trends.\n\npreferred skills\n\n\n* knowledge of agentic ai concepts, frameworks, and their practical applications.\n* experience with prompt engineering, fine\\-tuning, and deploying custom gen ai models.\n* knowledge of ethical ai practices, responsible ai frameworks, and related governance.\n* background in building ai\\-powered chatbots, recommendation engines, or creative content tools.\n* familiarity with mlops, model monitoring, and performance optimization.\n\nwe anticipate this job posting will be posted on 2/16/2025 and open for at least 14 days.\n\n\navanade offers a market competitive suite of benefits including medical, dental, vision, life, and long\\-term disability coverage, a 401(k) plan, bonus opportunities, paid holidays, and paid time off. see more information on our benefits here:\n\n\nu.s. employee benefits \\| avanade\n\n\ncalifornia $155,000\\-190,000\n\n\ncleveland $140,000\\-170,000\n\n\ncolorado $140,000\\-170,000\n\n\ndistrict of columbia $155,000\\-190,000\n\n\nillinois $150,000\\-180,000\n\n\nmaryland $140,000\\-170,000\n\n\nmassachusetts $140,000\\-170,000\n\n\nminnesota $150,000\\-180,000\n\n\nnew york $155,000\\-190,000\n\n\nnew jersey $155,000\\-190,000\n\n\nwashington $155,000\\-190,000\n\n\nat avanade, we are committed to ensure our people feel appreciated and empowered to succeed both personally and professionally.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer",
        "company": "Books-A-Million",
        "location": "Birmingham, AL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "S3",
            "Redshift",
            "BigQuery",
            "Data Lake",
            "Git",
            "BigQuery",
            "Redshift",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7bb1becaa3bb4909",
        "description": "**about the job**\n\n\nthis position is located at our birmingham, al home office.\n\n\n**role summary**\n\n\nbooks\\-a\\-million, one of the nation\u2019s leading book retailers, headquartered in birmingham, is seeking a skilled **data engineer** to support a major digital transformation initiative focused on building a modern, enterprise\\-grade data lakehouse. this role will design, develop, and maintain scalable data pipelines and architecture, ensuring data integrity, reliability, and performance to support advanced analytics and future data science efforts. the ideal candidate thrives in greenfield environments and is passionate about building high\\-quality, scalable data systems; experience with data science concepts is a plus.\n\n\n**role responsibilities**\n\n\n**digital transformation \\& data architecture**\n\n\n* support books\\-a\\-million\u2019s enterprise digital transformation initiative\n* design and build a modern, enterprise\\-grade data lakehouse architecture\n* establish scalable, foundational data infrastructure in a greenfield environment\n\n\n**data engineering \\& pipeline development**\n\n\n* develop, deploy, and maintain robust, scalable data pipelines\n* implement optimized data storage solutions\n* ensure efficient data ingestion, transformation, and integration processes.\n\n\n**data quality, governance \\& performance**\n\n\n* ensure data integrity, reliability, and system performance\n* maintain high standards for data quality and consistency\n* support sustainable, scalable data operations\n\n\n**analytics \\& advanced capabilities enablement**\n\n\n* build infrastructure to support advanced analytics and future data science initiatives\n* collaborate with data \\& analytics teams to enable reporting and insights\n* contribute to expanding enterprise data capabilities\n\n\n**innovation \\& continuous improvement**\n\n\n* thrive in build\\-from\\-scratch environments\n* identify opportunities to improve scalability and system efficiency\n* apply forward\\-thinking, problem\\-solving approaches to data system design\n\n\n**role qualifications**\n\n\n* bachelor\u2019s degree in computer science, engineering, or a related technical field or appropriate experience.\n\n\n* proven experience as a data engineer, etl developer, or in a similar role.\n\n\n* strong programming skills in a language such as python, java, or scala.\n\n\n* proficiency in sql and experience with relational and non\\-relational databases.\n\n\n* hands\\-on experience with cloud platforms (e.g., aws, azure, gcp) and their data services (e.g., s3, redshift, bigquery).\n\n\n* experience with data pipeline and workflow orchestration tools (e.g., airflow, prefect, dagster, fivetran).\n\n\n* familiarity with data modeling concepts and best practices.\n\n\n* a huge plus: experience or knowledge in data science, machine learning, or statistical modeling.\n\n\n* strong problem\\-solving skills and the ability to work independently and as part of a team.\n\n\n**the perks**\n\n\nat books\\-a\\-million, we believe in taking care of our employees. as part of the team, you\u2019ll enjoy a comprehensive benefits package, including:\n\n\n* **competitive compensation**: we offer competitive pay and performance\\-based incentives.\n* **health \\& wellness**: medical, dental, and vision insurance to keep you healthy.\n* **paid time off (pto)**: enjoy generous pto and paid holidays.\n* **employee discounts**: get discounts on books and other products across our stores.\n* **retirement savings**: build your future with our 401(k) plan and company contributions.\n\n\n**equal employment opportunity (eeo) statement**\n\n\nbooks\\-a\\-million is an equal opportunity employer committed to fostering a diverse, inclusive, and welcoming workplace. we celebrate and embrace differences, and we strive to create an environment where every individual feels valued and empowered. we do not discriminate based on race, color, religion, gender, gender identity or expression, sexual orientation, age, national origin, disability, veteran status, or any other characteristic protected by federal, state, or local law.\n\n\nall qualified applicants will receive consideration for employment without regard to their background or personal characteristics. we are dedicated to creating a culture of respect, equity, and opportunity for all employees, and we encourage individuals from all backgrounds to apply.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Full-Stack Senior Software Engineer",
        "company": "nan",
        "location": "Boulder, CO, US USA",
        "posted_at": "2026-02-24",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "PostgreSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6d1ef7cc52780120",
        "description": "**about radicl**\n\n  \n\nas the leading provider of cybersecurity\\-as\\-a\\-service (csaas), radicl is revolutionizing advanced cyberthreat protection for small and medium\\-sized businesses (smbs) in the u.s. defense industrial base (dib) and other regulated industries. no longer should smbs be satisfied with lack luster solutions delivering lack luster protection. radicl is ushering in a new era of turn\\-key and affordable cyberthreat protection via its ai\\-powered virtual soc platform that allows human and digital agents to quickly and seamlessly become smbs\u2019 day\\-to\\-day compliance and security operations team.\n\n  \n\nradicl\u2019s turn\\-key csaas offering uniquely combines compliance management with ai and expert\\-driven 24/7 security operations. we guide customers to regulatory and best practice adherence with standards like cmmc and nist csf while also delivering 24/7 threat monitoring, deep\\-spectrum\u2122 threat hunting, incident response, vulnerability management, and security awareness training.\n\n  \n\nwith radicl, customers can stay mission focused, confident their front, rear, and flank are protected, affordably and without compromise.\n\n  \n\nif you\u2019re excited about working with industry experts to help smbs focus on growing their businesses without the constant worry of security and compliance risks, we invite you to join us in our mission to protect american businesses and drive innovation in cybersecurity.\n\n **about the role**\n\n**as a radicl senior full\\-stack software engineer, you will:**\n\n* lead the design and delivery of scalable, secure, cloud\\-native systems powering radicl\u2019s cybersecurity platform. this role owns features end\\-to\\-end\u2014from architecture and backend services to frontend user experience\u2014 while helping shape engineering standards, system direction, and technical strategy.\n* you will operate as a senior technical contributor responsible for building resilient distributed systems, maintaining the highest\\-quality engineering standards, and driving execution across the full application stack.\n\n**responsibilities:**\n\n* build and evolve modern frontend applications using **typescript, react and nodejs**\n* architect, design, and implement scalable backend services using **golang**\n* own features end\\-to\\-end across api, service, infrastructure, and ui layers\n* lead technical design discussions and influence system architecture decisions\n* improve platform scalability, observability, reliability, and security posture\n* collaborate cross\\-functionally with product, security, and operations teams\n* opportunity to mentor engineers and provide technical leadership across the team\n* contribute to incident response, performance tuning, and production troubleshooting\n* maintain architectural documentation and technical decision records\n\n**required qualifications:**\n\n* 7\\+ years of professional software engineering experience\n* strong full\\-stack development experience using react, api (nodejs), and a relational database\n* expert proficiency in **typescript**, **golang**, or comparable modern languages\n* production experience with cloud platforms (aws, gcp, or azure)\n* deep understanding of api design, system scalability, and performance optimization\n* experience with containerization and orchestration (docker, kubernetes)\n* strong experience with git workflows and modern sdlc practices excellent written communication skills\n\n**preferred qualifications:**\n\n* experience building security, compliance, or mission\\-critical platforms\n* familiarity with **postgresql**, **kafka**, **elasticsearch**, or large\\-scale data systems\n* experience with **github actions** or modern ci/cd tooling\n* infrastructure\\-as\\-code experience (terraform, ansible)\n* experience operating high\\-availability saas platforms\n* background in cybersecurity, defense, or regulated environments startup or high\\-growth company experience\n\n  \n\nif the above excites you, radicl defense is seeking high performing, motivated individuals to join our mission. as an early member, you will work closely alongside an experienced founding team and realize the life\\-changing experience of building a company. you will work with the latest technologies in software, cybersecurity, and cloud and will have a significant impact on the formation of our platform and offering.\n\n **about you**  \n\nyou enjoy fast\\-paced environments, bring a positive attitude, and excel at getting things done. you enjoy being part of a high performing team and are also able to self\\-direct and self\\-start. you consider yourself to be top tier talent and are eager to help others raise their game. you enjoy working with customers, are an excellent communicator, and able to engage and interact with people of various backgrounds and skill levels. you want your work to have meaning, to be important. you want to be part of creating something great.  \n\n  \n\n**about the workplace**  \n\nat radicl, we prioritize our culture and believe the strongest teams are built through daily, side\\-by\\-side collaboration and experiential sharing. we also value individual freedom and flexibility. for this reason, we have a hybrid work model. as a team, we are in office m/w/th with work\\-from\\-home on tuesdays and fridays. for remote positions, periodic travel to boulder will be expected to participate in company events and meaningful side\\-by\\-side collaboration opportunities.\n\n  \n\nradicl offices are in downtown boulder, colorado with easy\\-to\\-access employee parking provided by the company. we offer comprehensive, competitive benefits including health, dental, and vision as well as 401k and a responsible pto plan.\n\n  \n\nwe encourage motivated, talented, mission\\-oriented, and fun people to apply. **let\u2019s do this!**\n\n **the pay range for this role is:**  \n\n150,000 \\- 200,000 usd per year(walnut st.)",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Observability Platform Engineer",
        "company": "Klaviyo",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-24",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Cortex",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Git",
            "Kafka",
            "MySQL",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6db8b13f873a0ab3",
        "description": "*at klaviyo, we value the unique backgrounds, experiences and perspectives each klaviyo (we call ourselves klaviyos) brings to our workplace each and every day. we believe everyone deserves a fair shot at success and appreciate the experiences each person brings beyond the traditional job requirements. if you're a close but not exact match with the description, we hope you'll still consider applying. want to learn more about life at klaviyo? visit* *klaviyo.com/careers* *to see how we empower creators to own their own destiny.*\n\n\nat klaviyo, **platform engineering** is what you get when you treat operating complex systems as a **software engineering** problem. our **observability platform** group applies that philosophy to how we collect, store, and surface signals about the health of our products and infrastructure. we build and run the shared observability stack\u2014metrics, logs, traces, alerting, and developer\\-facing tooling\u2014that enables every product and platform team at klaviyo to understand how their systems behave in production and to ship changes with confidence.\n\n\n\nas a **senior observability platform engineer**, you will design, build, and operate the core observability services that power klaviyo's monitoring and incident response. you'll partner closely with product engineering, other platform teams, and security to define how we instrument services, standardize telemetry, and make it easy for engineers to debug issues in a fast\\-growing, distributed environment.\n\n\n### **how you'll make an impact**\n\n\n* **own observability platforms end\\-to\\-end** \u2013 design, implement, and operate scalable, highly available systems for metrics, logging, tracing, and alerting (e.g., prometheus\\-compatible metrics, time\u2011series storage, log pipelines, distributed tracing backends).\n* **build opinionated developer experiences** \u2013 create libraries, dashboards, runbooks, and self\\-service tooling that make \"doing the right thing\" for observability the easiest path for klaviyo engineers.\n* **set standards for telemetry** \u2013 define and evangelize best practices for instrumentation, slos, alerting, and incident readiness across services and teams.\n* **drive reliability through data** \u2013 use observability data to identify performance bottlenecks, reliability risks, and architectural improvements, and collaborate with teams to address them.\n* **automate everything** \u2013 treat infrastructure as code; build automation for provisioning, configuration, scaling, and upgrades of observability components.\n* **mentor and multiply** \u2013 partner with engineers across klaviyo to level up skills in debugging distributed systems, designing effective alerts, and using observability tools to make better product and reliability decisions.\n* **utilize ai** \u2013 you've already experimented with ai in work or personal projects, and you're excited to dive in and learn fast. you're hungry to responsibly explore new ai tools and workflows, finding ways to make your work smarter and more efficient.\n\n### **what we're looking for**\n\n\n* strong **software engineering** experience in at least one modern language (e.g., go, python, java) and comfort working in linux\\-based production environments.\n* hands\\-on experience **designing and operating observability systems** at scale (for example: prometheus / cortex / thanos / mimir, opentelemetry, grafana, alerting pipelines, log aggregation systems, or distributed tracing backends).\n* a track record of **improving reliability and performance** of complex, distributed applications using telemetry and data\\-driven insights.\n* experience with **infrastructure\\-as\\-code** and modern cloud\\-native tooling (e.g., terraform, kubernetes, service meshes, ci/cd systems).\n* strong **technical communication and collaboration** skills\u2014you're comfortable partnering with many teams, writing clear documentation, and leading technical discussions.\n* a mindset that values **simple, well\u2011understood systems**, iterative improvement, and a bias toward empowering other engineers rather than being on the critical path for every change.\n\n### **technologies we use (not exhaustive):**\n\n\n* **backend:** python, django, go\n* **observability platform:** chronosphere, cortex, prometheus, otel\n* **testing frameworks:** pytest\n* **infrastructure and ci:** aws, kubernetes, terraform, helm, buildkite\n* **data:** mysql, redis, kafka\n\n\nklaviyo is growing fast and we have opportunities for engineers who care deeply about reliability, developer experience, and building strong foundational platforms. learn more about our engineering culture at https://klaviyo.tech/.\n\n\n\nwe use covey as part of our hiring and / or promotional process. for jobs or candidates in nyc, certain features may qualify it as an aedt. as part of the evaluation process we provide covey with job requirements and candidate submitted applications. we began using covey scout for inbound on april 3, 2025\\.\n\n\n\nplease see the independent bias audit report covering our use of covey here\n\n **get to know klaviyo**\n\n\n\nwe're klaviyo (pronounced clay\\-vee\\-oh). we empower creators to own their destiny by making first\\-party data accessible and actionable like never before. we see limitless potential for the technology we're developing to nurture personalized experiences in ecommerce and beyond. to reach our goals, we need our own crew of remarkable creators\u2014ambitious and collaborative teammates who stay focused on our north star: delighting our customers. if you're ready to do the best work of your career, where you'll be welcomed as your whole self from day one and supported with generous benefits, we hope you'll join us.\n\n\n*ai fluency at klaviyo includes responsible use of ai (including privacy, security, bias awareness, and human\\-in\\-the\\-loop). we provide accommodations as needed.*\n\n\n*by participating in klaviyo's interview process, you acknowledge that you have read, understood, and will adhere to our* *guidelines for using ai in the klaviyo interview process**. for more information about how we process your personal data, see our* *job applicant privacy notice**.*\n\n  \n\n*klaviyo is committed to a policy of equal opportunity and non\\-discrimination. we do not discriminate on the basis of race, ethnicity, citizenship, national origin, color, religion or religious creed, age, sex (including pregnancy), gender identity, sexual orientation, physical or mental disability, veteran or active military status, marital status, criminal record, genetics, retaliation, sexual harassment or any other characteristic protected by applicable law.*\n\n\n\n*important notice: our company takes the security and privacy of job applicants very seriously. we will never ask for payment, bank details, or personal financial information as part of the application process. all our legitimate job postings can be found on our official career site. please be cautious of job offers that come from non\\-company email addresses (@klaviyo.com), instant messaging platforms, or unsolicited calls.*\n\n**by clicking \"submit application\" you consent to klaviyo processing your personal data in accordance with our job applicant privacy notice. if you do not wish for klaviyo to process your personal data, please do not submit an application.***you can find our job applicant privacy notice* *here* *and* *here* *(fr).*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AWS AI & DevOps Intern",
        "company": "Network Distribution",
        "location": "Schaumburg, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 14.4,
        "matched_keywords": [
            "Generative AI",
            "Copilot",
            "TensorFlow",
            "PyTorch",
            "S3",
            "EC2",
            "Glue",
            "CI/CD",
            "Terraform",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f896c6da0dfabb9a",
        "description": "do you want to **gain exposure to it and devops projects associated with ai?**\n\n\n\nare you great at **collaborating** with team members to **creatively problem solve?**\n\n\n\ndo you want to work for a **chicago\u2019s best \\& brightest company to work for?**\n\n\n\ndoes working in a **highly\\-engaged organization***,* one that\u2019s **committed to growth, collaboration and innovation** interest you?\n\n\n**if so, read on.**\n\n\n\nthis is an in person internship where you will gain hands\\-on experience in an office setting! the aws ai developer intern will support the it and devops team in designing, developing, and deploying artificial intelligence and machine learning solutions on amazon web services (aws). this role offers hands\\-on experience with cloud\\-native ai/ml services, infrastructure automation, and real\\-world business applications of generative ai. the intern will work closely with senior engineers, the associate devops engineer, and cross\\-functional stakeholders to build intelligent solutions that drive operational efficiency and innovation.\n\n\n**what you\u2019ll do:**\n\n\n**ai/ml development \\& deployment:**\n\n\n* design, build, and deploy ai/ml models and applications using aws services such as amazon sagemaker, bedrock, lambda, and step functions.\n* develop and integrate generative ai capabilities (e.g., amazon bedrock, foundation models) into internal tools and customer\\-facing applications.\n* build and maintain data pipelines for model training and inference using aws glue, s3, and dynamodb.\n* monitor model performance and implement logging, alerting, and retraining workflows using amazon cloudwatch and sagemaker model monitor.\n* research and evaluate emerging aws ai/ml services and industry best practices to recommend adoption.\n\n\n**devops integration \\& infrastructure automation:**\n\n\n* assist in creating ci/cd pipelines for ml model deployment using aws codepipeline, codebuild, and infrastructure as code (cloudformation/terraform).\n* implement api endpoints and serverless architectures (api gateway, lambda) to expose ai/ml model predictions.\n* support automation of repetitive tasks through intelligent workflows combining ai services with existing devops tooling.\n* collaborate with the devops team on infrastructure provisioning, monitoring, and deployment best practices.\n* assist with aws cloud environment management including ec2, s3, rds, lambda, and vpc as needed.\n* support infrastructure as code (iac) efforts using terraform, cloudformation\n\n\n**microsoft 365 copilot rollout \\& ai integration:**\n\n\n* support the planning, configuration, and rollout of microsoft 365 copilot across the organization, including user enablement, licensing coordination, and adoption tracking.\n* develop m365 copilot agents, custom prompts, and copilot studio solutions to enhance productivity across departments (e.g., teams, outlook, sharepoint, and power platform).\n* collaborate with the team to evaluate copilot usage analytics, gather user feedback, and identify opportunities to expand ai\\-driven productivity improvements across the business.\n\n\n**documentation \\& collaboration:**\n\n\n* document technical solutions, architecture diagrams, and runbooks for knowledge sharing across the team.\n* participate in code reviews, agile ceremonies, and team planning sessions.\n* contribute to confluence knowledge base articles and internal training materials.\n\n  \n\n  \n\n**what you\u2019ll need:**\n\n\n* currently enrolled in an associate's or bachelor's degree program in information technology, computer science, or a related field.\n* proficiency in one or more programming languages such as python (preferred), javascript/typescript, java, c\\#, go, or rust.\n* experience with python ml/ai libraries (pandas, numpy, scikit\\-learn, pytorch, or tensorflow) is strongly preferred.\n* foundational understanding of machine learning concepts: supervised/unsupervised learning, neural networks, nlp, and computer vision.\n* exposure to aws cloud services (ec2, s3, lambda, iam) or willingness to obtain aws cloud practitioner certification prior to start.\n* experience with version control (git/github) and basic command\\-line/linux operations.\n* strong analytical and problem\\-solving abilities with attention to detail.\n* excellent written and verbal communication skills.\n* self\\-motivated with the ability to manage time effectively and work independently.\n* collaborative team player with a growth mindset and eagerness to learn.\n\n  \n\n  \n\n**our internship program:**\n\n\n\nour highly engaged internship program allows you to focus on projects related to information technology that will help prepare you for your career. you\u2019ll also get to collaborate and get to know interns in other functions to gain key business skills and help you succeed in the corporate world. as a capstone to the internship program, you will have the opportunity to present your summer experience and projects to our executive leadership team.\n\n\n\nthis is a paid internship. interns will work a 40 hour work\\-week at $18/hour.\n\n\n**what former interns say:**\n\n\n*\u201cdiving into your career can be a bit intimidating, but at network everyone made me feel extremely welcome and provided me with great guidance to becoming successful with where i wanted to take my career. i truly enjoyed my internship with network and i feel that it gave me the right resources for my career moving forward.\u201d*\n\n\n\n\u2013 former summer intern\n\n\n***network*** **is proud to be an equal opportunity employer. we are committed to creating a diverse workforce.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer, Data Services",
        "company": "Fox Corporation",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 14.4,
        "matched_keywords": [
            "Copilot",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "NoSQL",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8a2b768ed31bba34",
        "description": "overview of the company\nfox corporation\nunder the fox banner, we produce and distribute content through some of the world\u2019s leading and most valued brands, including: fox news media, fox sports, fox entertainment, fox television stations and tubi media group. we empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise and strategic thinking.\njob description\nabout the role\nfox is building the next generation of high\\-scale, real\\-time platforms that power news, weather, elections, and data\\-driven experiences across the company. we are seeking a senior software engineer to join the data services team and lead the architecture, reliability, and evolution of critical backend services and data pipelines.\nthis role blends hands\\-on engineering with technical leadership. reporting to the head of data services, you will own the architecture of high\\-traffic services and event\\-driven systems, establish engineering standards, mentor teammates, and drive consistency across backend platforms that support multiple fox business units.\na snapshot of your responsibilities* own the architecture and technical direction for backend services and data pipelines; define contracts for http and rpc/grpc systems\n* lead aws\\-first platform design across eks/kubernetes, lambda, dynamodb, memorydb/redis, and postgres\n* drive infrastructure\\-as\\-code and ci/cd practices (terraform, github actions) with strong operational ownership\n* mentor engineers, lead design reviews, and standardize patterns across services and repositories\n* partner with product, data, and platform stakeholders to translate business needs into scalable technical roadmaps\n* serve as a technical leader for c\\#/.net services, including azure integrations (functions, app service, service bus, aks, azure sql/cosmos db)\n* strengthen reliability, security, observability, and performance across distributed systems\n\n\nwhat you will need* 5\\+ years of experience building and operating production backend systems at scale\n* strong go expertise, with python experience supporting data and automation workloads\n* demonstrated experience designing and maintaining c\\#/.net services in production\n* deep knowledge of kubernetes, containers, and cloud\\-native architecture (aws primary; azure strongly valued)\n* strong data modeling experience across relational and nosql systems (postgres, redis/memorydb, dynamodb)\n* ownership of ci/cd pipelines, terraform\\-based infrastructure, and production reliability practices\n* solid foundation in security, compliance, and cloud best practices\n* proven ability to mentor engineers and influence architectural direction\n* regular, on\\-site attendance at the workplace a minimum of 3 days per week is an essential function of the position. selected candidate must be able to reliably meet this requirement.\n\n\nnice to have, but not a dealbreaker* experience in real\\-time, high\\-traffic domains (sports, elections, media, or finance)\n* background standardizing platforms or shared services across teams\n* advanced observability and cloud cost optimization experience\n* cross\\-cloud system ownership (aws and azure)\n* familiarity with ai\\-assisted development tools (cursor, copilot)\n\n\n\\#ll\\-hybrid*we are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. we will consider for employment qualified applicants with criminal histories consistent with applicable law.*\npursuant to state and local pay disclosure requirements, the pay rate/range for this role, with final offer amount dependent on education, skills, experience, and location is $143,000\\.00\\-170,000\\.00 annually. this role is also eligible for an annual discretionary bonus, various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents. benefits for union represented employees will be in accordance with the applicable collective bargaining agreement.\nview more detail about fox benefits.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Mid-Level AI / Machine Learning Software Engineer",
        "company": "MTSI",
        "location": "Huntsville, AL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Hugging Face",
            "TensorFlow",
            "PyTorch",
            "Keras",
            "Docker",
            "Git",
            "Matplotlib",
            "Seaborn",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=96ede8f4dfa331b6",
        "description": "we are seeking a mid\\-level ai / machine learning software engineer to support development of scalable data analysis and machine learning capabilities across large datasets and real\\-time data streams. the role focuses on designing, implementing, and optimizing machine learning models and data pipelines using python and modern deep learning frameworks.\nthe ideal candidate has strong programming fundamentals, hands\\-on model development experience, and is comfortable working with large structured and unstructured datasets in production environments.\n**primary responsibilities*** design, develop, and maintain python\\-based data processing and analytics solutions\n* implement and optimize machine learning and deep learning models\n* work with large datasets and streaming data sources\n* develop reusable data structures and efficient algorithms for analysis workflows\n* build and evaluate models for classification, prediction, and pattern recognition\n* integrate ai/ml capabilities into software systems and pipelines\n* collaborate with software engineers, data engineers, and analysts to deploy solutions\n* perform model validation, performance tuning, and debugging\n* document architecture, implementation, and usage of developed tools\n\n\n**required qualifications*** 3\\+ years of professional software development experience\n* strong python development skills\n* experience working with large datasets and/or streaming data\n* proficiency in machine learning and deep learning frameworks:\n* pytorch\n* tensorflow\n* keras\n* hugging face transformers\n* understanding of machine learning concepts and model architectures, including:\n* decision trees / random forests\n* lstm / sequence models\n* experience implementing, training, and evaluating ml models\n* knowledge of data structures, algorithms, and performance optimization\n* familiarity with version control (git) and collaborative development workflows\n\n\n**desired / preferred qualifications*** experience with retrieval\\-augmented generation (rag)\n* experience with model context protocols (mcp) or similar agent/tool interaction frameworks\n* experience with gpu acceleration and cuda architecture\n* drivers, runtime, and apis\n* experience with deep learning and reinforcement learning libraries\n* experience building or consuming real\\-time data pipelines\n* data visualization and exploratory analysis (matplotlib, seaborn, plotly, etc.)\n* familiarity with model deployment and inference optimization\n* experience working in containerized or distributed environments\n\n **education*** bachelor\u2019s degree (or working toward a degree) in computer science, data science, engineering, mathematics, or related field\n* (equivalent practical experience considered)\n\n\n**nice\\-to\\-know technologies*** linux development environments\n* jupyter notebooks\n* docker or container basics\n* basic command line usage\n\n  \n\n\\#li\\-as1\n  \n\n**perks and benefits**\n======================\n\n* *vacation:*\nnew hires accrue 20 days of pto and 10 holidays per year\n* *health insurance:*\nzero deductible health plans\n* *flexible schedules:*\nflex schedules\n* *professional development:*\nup to $10,000 annual education/training reimbursement\n* *esop:*\nfunded stock ownership plan\n* *401k match\\+:*\n6% 401k match \\+ immediate vesting\n* *bonus program:*\nsemi\\-annual bonus opportunity\n* *mentorship:*\ncareer mentorship programs\n\n### **why is mtsi a great place to work**\n\n* *interesting work:*\nour co\\-workers support some of the most important and critical programs to our national defense and security.\n* *values:*\nour first core value is that employees come first. we challenge our co\\-workers to provide the highest level of support and service, and reward them with some of the best benefits in the industry.\n* *100% employee owned:*\nwe have a stake in each other's success, and the success of our customers. it's also nice to know what's going on across the company; we have company wide town\\-hall meetings three times a year.\n* *great benefits \\- most full\\-time staff are eligible for:*\n* + starting pto accrual of 20 days pto/year \\+ 10 holidays/year\n\t+ flexible schedules\n\t+ 6% 401k match with immediate vesting up to $9k annually\n\t+ semi\\-annual bonus eligibility (july and decemeber)\n\t+ company funded employee stock ownership plan (esop) \\- a separate qualified retirement account\n\t+ up to $10,000 in annual educational reimbursement\n\t+ other company funded benefits, like life and disability insurance\n\t+ optional zero deductible blue cross/blue shield health insurance plan\n* *track record of success:*\nwe have grown every year since our founding in 1993\\.\n\nmodern technology solutions, inc. (mtsi) is a 100% employee\\-owned engineering services and solutions company that provides high\\-demand technical expertise in digital transformation, modeling and simulation, rapid capability development, test and evaluation, artificial intelligence, autonomy, cybersecurity and mission assurance\n\n\nmtsi delivers capabilities to solve problems of global importance. founded in 1993, mtsi today has employees at over 20 offices and field sites worldwide.\n\n\nfor more information about mtsi, please visitwww.mtsi\\-va.com\n\n#### **eeo statement**\n\n\nmtsi embraces nine core values including our first core value of employees come first. consistent with our core values, we are committed to equal opportunity, making decisions without regard to race, color, religion, sex, national origin, age, military/veteran status, disability, or any other characteristics protected by applicable law. mtsi is committed to equal employment opportunity and providing reasonable accommodations to applicants and employees with physical and/or mental disabilities.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI/ML Research Engineer, LLM Post-Training & Evaluation",
        "company": "Innodata",
        "location": "Ridgefield Park, NJ, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "RAG",
            "Hugging Face",
            "TensorFlow",
            "PyTorch",
            "CI/CD",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d862aacbcf2d942a",
        "description": "**who we are:**\n\n\ninnodata (nasdaq: inod) is a leading data engineering company. with more than 2,000 customers and operations in 13 cities around the world, we are the ai technology solutions provider\\-of\\-choice to 4 out of 5 of the world\u2019s biggest technology companies, as well as leading companies across financial services, insurance, technology, law, and medicine.\n\n\nby combining advanced machine learning and artificial intelligence (ml/ai) technologies, a global workforce of subject matter experts, and a high\\-security infrastructure, we\u2019re helping usher in the promise of clean and optimized digital data to all industries. innodata offers a powerful combination of both digital data solutions and easy\\-to\\-use, high\\-quality platforms.\n\n\nour global workforce includes over 3,000 employees in the united states, canada, united kingdom, the philippines, india, sri lanka, israel and germany. we\u2019re poised for a period of explosive growth over the next few years.\n\n**position summary:**\n\n\ninnodata is expanding its team of technical experts in llm training, post\\-training, and evaluation systems. as an ai/ml research engineer, llm training \\& evaluation, you will build and optimize the technical foundations that power model improvement for foundation model builders and leading labs.\n\n\nthis role is ideal for someone who has hands\\-on experience fine\\-tuning and evaluating large language models (and ideally multimodal models), and who can bridge research and engineering in real\\-world customer environments. you will work closely with language data scientists, applied research scientists, data engineers, and client technical stakeholders to design and implement robust training/evaluation pipelines using both human\\-in\\-the\\-loop and ai\\-augmented methods.\n\n\nthe ideal candidate brings a strong computer science / machine learning engineering background, experience with modern llm post\\-training workflows, and the ability to engage credibly with technical counterparts at leading ai organizations.\n\n**who we\u2019re looking for:**\n\nyou have at least 2\\-3 years of relevant experience in machine learning engineering, applied ml systems, or research engineering, with substantial hands\\-on work in llms and multimodal foundation models. you have built, adapted, or optimized model training and evaluation pipelines, and you understand the practical realities of experimentation at scale: reproducibility, debugging, metrics quality, and iteration speed.\n\n\nyou are comfortable operating in ambiguous, high\\-complexity environments and can move from problem framing to implementation. you can collaborate effectively with both researchers and engineers, and you are credible in technical conversations with sophisticated customer stakeholders (e.g., ai researchers, ml engineers, technical product leads).\n\n\nyou bring a builder mindset and strong engineering judgment, while also understanding that evaluation quality and data quality are central to model improvement. you are excited to partner with human evaluation experts and language data scientists to create integrated post\\-training and evaluation systems.\n\n**tell me more:**\n\n\nas an ai/ml research engineer, llm training \\& evaluation, you will design and implement the pipelines and tooling that connect data, evaluation, and post\\-training. you will help customers and internal teams move from evaluation findings to measurable model improvements.\n\n\nyour work may include building fine\\-tuning workflows (e.g., supervised fine\\-tuning and preference\\-based optimization), integrating evaluation harnesses into model development loops, improving experiment reliability and throughput, and supporting advanced evaluation scenarios such as long\\-context, cross\\-modal, and dynamic multi\\-turn interactions.\n\n\nyou will also contribute to innodata\u2019s internal r\\&d efforts, including benchmark datasets, evaluation frameworks, and reusable infrastructure for model assessment and post\\-training experimentation.\n\n**responsibilities:**\n\n* lead or co\\-lead technically complex ml engineering projects from initial customer discussions through implementation and delivery\n* design, build, and improve llm training and post\\-training pipelines, including data ingestion, preprocessing, fine\\-tuning, evaluation, and experiment tracking\n* implement and optimize evaluation systems for llms and multimodal models, including offline benchmarks and task\\-specific test harnesses\n* integrate human\\-in\\-the\\-loop and ai\\-augmented evaluation signals into model development workflows\n* build robust infrastructure and tooling for reproducible experimentation, metrics logging, and regression monitoring\n* diagnose model behavior and pipeline failures, including data issues, training instability, metric inconsistencies, and evaluation drift\n* collaborate with language data scientists and applied research scientists to translate evaluation frameworks into executable systems\n* work closely with customer technical stakeholders to understand goals, constraints, and success criteria; propose and implement technically sound solutions\n* contribute to internal research and platform development, including benchmark frameworks, evaluation tooling, and post\\-training workflow improvements\n* contribute to best practices and standards for llm training, evaluation, and quality assurance across projects\n* mentor junior engineers and contribute to technical design reviews, documentation, and engineering rigor across the team\n* bs/ms/phd in computer science, machine learning, ai, applied mathematics, or a related quantitative technical field (ms/phd preferred)\n* 2\\-3 years of relevant industry or research engineering experience in ml/ai systems\n* hands\\-on experience with llm training / fine\\-tuning / post\\-training, including at least one of:\n\t+ supervised fine\\-tuning (sft)\n\t+ preference optimization (e.g., dpo or related methods)\n\t+ rlhf / rlaif\\-style workflows\n\t+ task\\- or domain\\-adaptation of foundation models\n* strong programming skills in python and experience building production\\-quality ml code\n* experience with modern ml frameworks (e.g., pytorch, jax, tensorflow) and model libraries/tooling (e.g., hugging face ecosystem, vllm, distributed training stacks)\n* experience designing and implementing evaluation pipelines for llm/ml systems, including metrics computation, dataset handling, and experiment comparisons\n* strong understanding of data pipelines and ml systems engineering, including reproducibility, observability, and debugging\n* experience with large\\-scale distributed ml systems and performance optimization for training/evaluation workloads (gpu/accelerator environments preferred)\n* experience with large\\-scale data processing and workflow orchestration in support of model training/evaluation\n* ability to collaborate directly with technical stakeholders including research scientists, ml engineers, data engineers, and customer technical leads\n* strong written and verbal communication skills, including the ability to explain complex technical tradeoffs to both technical and non\\-technical audiences\n\n**technical skills**\n\n\nml / llm engineering\n\n* experience training, fine\\-tuning, and evaluating transformer\\-based models\n* understanding of post\\-training workflows and model iteration loops\n* familiarity with inference\\-time considerations (latency, throughput, memory/performance tradeoffs) where relevant to evaluation or deployment\n\n\nevaluation \\& experimentation\n\n* experience implementing automated evaluation pipelines and test harnesses\n* experience with experiment tracking, versioning, and reproducibility practices\n* ability to assess metric quality and ensure consistency across model comparisons\n\n\nsoftware / data engineering\n\n* proficiency in python and strong software engineering fundamentals\n* experience with data processing pipelines, storage formats, and scalable dataset workflows\n* familiarity with ci/cd, testing, and engineering quality practices for ml systems\n\n**preferred skills**\n\n* experience with multimodal model training/evaluation (text \\+ image/audio/video)\n* experience with long\\-context evaluation and/or model adaptation for long\\-context tasks\n* experience with agentic or multi\\-turn evaluation harnesses, tool\\-use simulation, or interactive environment testing\n* experience working in customer\\-facing technical consulting, solutions engineering, or applied research delivery\n* familiarity with llm safety, alignment, robustness, or red\\-teaming evaluation approaches\n* contributions to open\\-source ml/llm tooling or published technical work in relevant areas\n\n**how this role partners with the team**\n\n\nthis role works closely with:\n\n* language data scientists, who lead human evaluation design, language/data process excellence, and annotation/synthetic workflows\n* applied research scientists, who lead evaluation methodology, benchmarking research, and experimental design\n* data engineers / platform teams, who support scalable data and infrastructure foundations\n* customer technical stakeholders, who rely on innodata for expert guidance and implementation support in advanced genai development\n\n*please be aware of recruitment scams involving individuals or organizations falsely claiming to represent employers. innodata will never ask for payment, banking details, or sensitive personal information during the application process. to learn more on how to recognize job scams, please visit the federal trade commission\u2019s guide at* https://consumer.ftc.gov/articles/job\\-scams.\n\n*if you believe you\u2019ve been targeted by a recruitment scam, please report it to innodata at* verifyjoboffer@innodata.com *and consider reporting it to the ftc at* reportfraud.ftc.gov*.*\n\n\n\\#li\\-ns1",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Database Reliability Engineer",
        "company": "Filevine",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Git",
            "Snowflake",
            "PostgreSQL",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a200fed649475457",
        "description": "filevine is a legal ai company delivering legal operating intelligence for the future of legal work. grounded in a singular system of truth, filevine brings together data, documents, workflows, and teams into one unified platform\u2014where modern legal work happens with clarity and consistency.  \n\npowered by **lois**, the legal operating intelligence system, filevine connects context across every matter to transform legal operations from reactive to proactive. lois reads, understands, and reasons across your data to surface insight, automate complexity, and give professionals the clarity and confidence to see more, know more, and do more. fueled by a team of exceptional collaborators and innovators, filevine\u2019s rapid growth has earned ai awards and recognition from deloitte and inc. as one of the most innovative and fastest\\-growing technology companies in the country. **role summary:**  \n\nas a senior dbre, you are the guardian of our data\u2019s performance, availability, and scalability. you aren't just managing instances; you are automating the entire lifecycle of our data platform. we are looking for an engineer who is **eager to explore how ai\\-driven automation and mcp (model context protocol) integrations** can take our operations to the next level\u2014from predictive scaling to streamlining incident response. from managing mission\\-critical **sql server** and **postgres** to provisioning via **terraform, docker, and kubernetes**, you will be the backbone of our data reliability strategy.\n### **responsibilities**\n\n* **automation evolution:** proactively explore and implement ai tools, llm integrations, and **mcp (model context protocol)** to reduce routine database toil, optimize query performance, and accelerate incident resolution. we want a creative mind to help evolve our operational duties.\n* **modern data stack:** support our data warehouse ecosystem by optimizing **snowflake** performance, including application packaging and testing.\n* **performance engineering:** own the deep\\-level optimization of **mssql** (crucial for on\\-call stability) and **postgresql** at the server, database, and query levels.\n* **capacity planning \\& cost optimization:** forecast resource utilization across platforms. identify cost\\-saving opportunities, optimize snowflake credit usage, and right\\-size aws infrastructure.\n* **infrastructure as code (iac) \\& orchestration:** automate all data infrastructure using **terraform**, **aws**, **docker**, and **kubernetes**. you will manage containerized data services and stateful workloads.\n* **ci/cd \\& release management:** manage and optimize deployment pipelines using **gitlab** and **octopus deploy**, ensuring safe, repeatable database schema changes.\n* **documentation \\& knowledge sharing:** create technical documentation, including runbooks, \"how\\-to\" guides for developer self\\-service, and clear architectural diagrams.\n* **on\\-call \\& triage:** serve as the subject matter expert for sql server, postgres, and snowflake in a 24/7/365 on\\-call rotation.\n\n### **qualifications**\n\n* **dba fundamentals:** 8\\+ years of experience maintaining and optimizing high\\-traffic production databases, specifically **microsoft sql server**, **postgresql**\n* **expert** knowledge in **snowflake**.\n* **devops expertise:** professional experience with **terraform**, **aws**, **gitlab**, and **octopus deploy**.\n* **containerization \\& orchestration:** expert\\-level knowledge of **docker** and production experience running stateful applications within **kubernetes (k8s)**.\n* **ai \\& integration:** knowledge of **mcp (model context protocol)** and experience using llms to assist in coding or system automation.\n* **scripting \\& programming:** expert\\-level **python** and **powershell** skills. familiarity with **c\\#** and object\\-oriented programming (oop).**orm optimization:** expert in tuning queries generated from orms (entity framework, dapper, etc.).\n* **the data stack:** hands\\-on experience with **snowflake** (packaging/testing) and **dynamodb**.\n* **familiarity with opensearch, redis and dynamodb.**\n* **education:** b.s./m.s. in computer science, information systems, or equivalent direct work experience.\n\n*compensation information: $145k \\- 180k*  \n\n*the base salary range represents the low and high end of the salary range for this position. the total compensation package for this position will be determined by each individual\u2019s location, qualifications, education, work experience, skills and performance. we believe in the importance of pay equity \\- the range listed is just one component of filevine\u2019s total compensation package for employees. this position is also eligible for a paid time off policy, as well as a comprehensive benefits package.* **cool company benefits:*** a dynamic, rapidly growing company, focused on helping organizations thrive\n* medical, dental, \\& vision insurance (for full\\-time employees)\n* competitive \\& fair pay\n* maternity \\& paternity leave (for full\\-time employees)\n* short \\& long\\-term disability\n* opportunity to learn from a dedicated leadership team\n* top\\-of\\-the\\-line company swag\n\n **privacy policy notice**\nfilevine will handle your personal information according to what\u2019s outlined in our privacy policy.  \n\ncommunication about this opportunity, or any open role at filevine, will *only* come from representatives with email addresses using \"filevine.com\". other addresses reaching out are *not affiliated* with filevine and should not be responded to.\nwe may use artificial intelligence (ai) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. these tools assist our recruitment team but do not replace human judgment. final hiring decisions are ultimately made by humans. if you would like more information about how your data is processed, please contact us.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Generative AI Developer-1",
        "company": "Realign",
        "location": "Irving, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5221b874a58d1dd5",
        "description": "irving, texas 75014 posted february 24th, 2026\n looking for more job opportunities? click here!\n\n\n  \njob type: full time\n\n\njob category: it\n\n\njob description\n\n\n**job title:** senior generative ai developer  \n\n**location** \u2013 irving, tx / tampa, fl / jersey city, nj (3 days onsite/week)  \n\n**fte**  \n\n**job description:**  \n\nwe are seeking an experienced senior generative ai developer to design and implement cutting\\-edge ai solutions leveraging retrieval\\-augmented generation (rag) techniques. the ideal candidate will have strong expertise in python programming, fastapi, and cloud platforms (aws, azure, or gcp). this role requires a deep understanding of system architecture design, scalable apis, and end\\-to\\-end ai solution development. **key responsibilities:**  \n\narchitect and develop generative ai applications using rag frameworks for enterprise\\-scale solutions.  \n\ndesign and implement robust system architectures for ai\\-driven platforms ensuring scalability, security, and performance.  \n\nbuild and optimize apis using fastapi for seamless integration with ai models and data pipelines.  \n\ncollaborate with cross\\-functional teams to integrate ai solutions into existing systems and workflows.  \n\nimplement data ingestion, preprocessing, and retrieval mechanisms for large\\-scale knowledge bases.  \n\nensure compliance with best practices for cloud deployment (aws, azure, or gcp).  \n\nconduct performance tuning and optimization of ai models and apis.  \n\nstay updated with the latest advancements in generative ai, llms, and rag methodologies. **required skills \\& qualifications**  \n\n8\\+ years of professional experience in software development and system design.  \n\nstrong proficiency in python and experience with fastapi for api development.  \n\nhands\\-on experience with generative ai frameworks and rag architectures.  \n\nsolid understanding of system and architecture design principles for distributed applications.  \n\nexperience deploying solutions on any major cloud platform (aws, azure, gcp).  \n\nfamiliarity with vector databases, embedding models, and retrieval pipelines.  \n\nstrong problem\\-solving skills and ability to work in a fast\\-paced environment. **preferred qualifications**  \n\nexperience with llm fine\\-tuning, prompt engineering, and model evaluation.  \n\nknowledge of containerization (docker) and orchestration (kubernetes).  \n\nexposure to ci/cd pipelines and devops practices.  \n\n\nrequired skills\n\n\ncloud developer\n\n sql application developer",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Generative AI Developer-1",
        "company": "Realign",
        "location": "Tampa, FL, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fbe61600116cf690",
        "description": "tampa, florida 33592 posted february 24th, 2026\n looking for more job opportunities? click here!\n\n\n  \njob type: full time\n\n\njob category: it\n\n\njob description\n\n\n**job title:** senior generative ai developer  \n\n**location** \u2013 irving, tx / tampa, fl / jersey city, nj (3 days onsite/week)  \n\n**fte**  \n\n**job description:**  \n\nwe are seeking an experienced senior generative ai developer to design and implement cutting\\-edge ai solutions leveraging retrieval\\-augmented generation (rag) techniques. the ideal candidate will have strong expertise in python programming, fastapi, and cloud platforms (aws, azure, or gcp). this role requires a deep understanding of system architecture design, scalable apis, and end\\-to\\-end ai solution development. **key responsibilities:**  \n\narchitect and develop generative ai applications using rag frameworks for enterprise\\-scale solutions.  \n\ndesign and implement robust system architectures for ai\\-driven platforms ensuring scalability, security, and performance.  \n\nbuild and optimize apis using fastapi for seamless integration with ai models and data pipelines.  \n\ncollaborate with cross\\-functional teams to integrate ai solutions into existing systems and workflows.  \n\nimplement data ingestion, preprocessing, and retrieval mechanisms for large\\-scale knowledge bases.  \n\nensure compliance with best practices for cloud deployment (aws, azure, or gcp).  \n\nconduct performance tuning and optimization of ai models and apis.  \n\nstay updated with the latest advancements in generative ai, llms, and rag methodologies. **required skills \\& qualifications**  \n\n8\\+ years of professional experience in software development and system design.  \n\nstrong proficiency in python and experience with fastapi for api development.  \n\nhands\\-on experience with generative ai frameworks and rag architectures.  \n\nsolid understanding of system and architecture design principles for distributed applications.  \n\nexperience deploying solutions on any major cloud platform (aws, azure, gcp).  \n\nfamiliarity with vector databases, embedding models, and retrieval pipelines.  \n\nstrong problem\\-solving skills and ability to work in a fast\\-paced environment. **preferred qualifications**  \n\nexperience with llm fine\\-tuning, prompt engineering, and model evaluation.  \n\nknowledge of containerization (docker) and orchestration (kubernetes).  \n\nexposure to ci/cd pipelines and devops practices.  \n\n\nrequired skills\n\n\ncloud developer\n\n sql application developer",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Generative AI Developer-1",
        "company": "Realign",
        "location": "Jersey City, NJ, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5c15b9e57ef4bd9f",
        "description": "jersey city, new jersey 07030 posted february 24th, 2026\n looking for more job opportunities? click here!\n\n\n  \njob type: full time\n\n\njob category: it\n\n\njob description\n\n\n**job title:** senior generative ai developer  \n\n**location** \u2013 irving, tx / tampa, fl / jersey city, nj (3 days onsite/week)  \n\n**fte**  \n\n**job description:**  \n\nwe are seeking an experienced senior generative ai developer to design and implement cutting\\-edge ai solutions leveraging retrieval\\-augmented generation (rag) techniques. the ideal candidate will have strong expertise in python programming, fastapi, and cloud platforms (aws, azure, or gcp). this role requires a deep understanding of system architecture design, scalable apis, and end\\-to\\-end ai solution development. **key responsibilities:**  \n\narchitect and develop generative ai applications using rag frameworks for enterprise\\-scale solutions.  \n\ndesign and implement robust system architectures for ai\\-driven platforms ensuring scalability, security, and performance.  \n\nbuild and optimize apis using fastapi for seamless integration with ai models and data pipelines.  \n\ncollaborate with cross\\-functional teams to integrate ai solutions into existing systems and workflows.  \n\nimplement data ingestion, preprocessing, and retrieval mechanisms for large\\-scale knowledge bases.  \n\nensure compliance with best practices for cloud deployment (aws, azure, or gcp).  \n\nconduct performance tuning and optimization of ai models and apis.  \n\nstay updated with the latest advancements in generative ai, llms, and rag methodologies. **required skills \\& qualifications**  \n\n8\\+ years of professional experience in software development and system design.  \n\nstrong proficiency in python and experience with fastapi for api development.  \n\nhands\\-on experience with generative ai frameworks and rag architectures.  \n\nsolid understanding of system and architecture design principles for distributed applications.  \n\nexperience deploying solutions on any major cloud platform (aws, azure, gcp).  \n\nfamiliarity with vector databases, embedding models, and retrieval pipelines.  \n\nstrong problem\\-solving skills and ability to work in a fast\\-paced environment. **preferred qualifications**  \n\nexperience with llm fine\\-tuning, prompt engineering, and model evaluation.  \n\nknowledge of containerization (docker) and orchestration (kubernetes).  \n\nexposure to ci/cd pipelines and devops practices.  \n\n\nrequired skills\n\n\ncloud developer\n\n sql application developer",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer, Machine Learning",
        "company": "MNTN",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "RAG",
            "PyTorch",
            "BigQuery",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1d09521a9228f78a",
        "description": "at mntn, we put our people first, full stop. this allows our company culture to be defined by our team members and their shared values, like trust, ambition, quality, radical honesty, and compassionate leadership. it's why we all *really* love working for the hardest working software in television\u2122 (and also why we were named one of ad age's best places to work in 2025\\.)\n\n\n\nwe pride ourselves on bringing unrivaled performance and simplicity to connected tv advertising. our self\\-serve technology makes running tv ads as easy as search and social, helping brands drive measurable conversions, revenue, site visits, and more. it's what led mntn to being named one of fast company's most innovative companies in 2023\\. you can learn more about us and everything we do by visiting https://mountain.com/.\n\n\n\nwe're committed to innovation that empowers, not replaces. at mntn, ai is a tool for growth, enhancing efficiency while keeping a people\\-first approach. our goal is to streamline workflows and drive new solutions\u2014without compromising the human element that makes our company great.\n\n\n\nso if wanting to do more, own more, and make a bigger impact comes naturally to you, then you may be the person we're looking for to join us in our next stage of growth.\n\n\nthe mntn performance ml team helps brands reach the right customers with software that turns petabytes of data into meaningful campaign strategies. our engineers, data scientists and analysts build software that serves content to millions of people every day. as a senior machine learning engineer, you will focus on operationalizing machine learning models by taking ownership of prototypes built by data scientists and turning them into robust, scalable production systems. you will lead the deployment, monitoring, and maintenance of ml solutions that power campaign optimizations at scale. this role emphasizes strong software engineering practices, designing for reliability and performance, and working with large\\-scale data pipelines and infrastructure. you'll collaborate across functions to ensure models are not just accurate but production\\-ready, scalable, and cost\\-effective. this is a senior machine learning role with an emphasis on building production\\-ready models. it is not a pure research role. you are expected to ship production\\-grade implementations and own outcomes in production.\n\n\n**what you'll do:**\n\n\n* design and build a robust marketing platform that reaches the right audience, anywhere, anytime\n* build high volume services that are reliable at scale\n* develop big data solutions using open source frameworks\n* collaborate with and explain complex technical issues to product and project leads\n* joint production ownership with shared on\\-call participation\n* focus on:\n\t+ improving service reliability, latency, and observability\n\t+ improving model quality (including false positives, thresholds, calibration)\n\t+ speeding up model testing loops\n\t+ releasing model and service changes faster, with confidence\n\t+ improving data/pipeline freshness and delivery velocitydesign, train, evaluate, and improve models for deliverability, forecasting, and optimization.\n* improve thresholding, calibration, and guardrail logic to reduce false positives and decision noise.\n* build robust offline/online evaluation workflows tied to business outcomes.\n* work directly in production codepaths to ship model improvements safely.\n* partner closely with platform\\-leaning mles on reliability, rollout safety, and observability.\n* share on\\-call responsibility for production ml services.\n\n\n**what success looks like:**\n\n\n* model quality improves on agreed business and operational metrics.\n* false positives and unstable decision behavior are reduced in key flows.\n* model testing/evaluation cycles become materially faster.\n* more model improvements reach production safely and predictably.\n\n\n**what you'll bring:**\n\n\n* 5\\+ years building ml models that were deployed and operated in production.\n* strong applied ml fundamentals (classification/regression/forecasting \\+ evaluation rigor).\n* strong python and sql with production engineering discipline (testing, maintainability, performance).\n* experience balancing model quality, system constraints, and speed\\-to\\-production.\n* strong experience with ownership and cross\\-functional collaboration.\n* experience in ad tech, growth analytics, personalization, or performance marketing\n* proficiency working with real\\-time or near\\-real\\-time data pipelines\n* experience with experimentation frameworks and production model monitoring.\n* experience with kedro, autogluon, pytorch, polars, bigquery/gcp, and airflow/sqlmesh ecosystems.\n\n**mntn perks**\n\n\n* 100% remote within the us\n* flexible vacation policy\n* annual vacation allowance for travel related expenses\n* three\\-day weekend every month of the year\n* competitive compensation\n* 100% healthcare coverage\n* 401k plan\n* flexible spending account (fsa) for dependent, medical, and dental care\n* access to coaching, therapy, and professional development\n\n\n**about mntn**\n\n\n\nour recruiters will always reach out using an email address ending with @mountain.com or @mntn.com. if you're contacted by someone without that address and they mention a reference code (which we never use), then *that ain't us folks.* tell those trolls to take a hike\u2013you're waiting to climb a mntn.\n\n\n\nmntn provides advertising software for brands to reach their audience across connected tv, web, and mobile. mntn performance tv has redefined what it means to advertise on television, transforming connected tv into a direct\\-response, performance marketing channel. our web retargeting has been leveraged by thousands of top brands for over a decade, driving billions of dollars in revenue.\n\n\n\nour solutions give advertisers total transparency and complete control over their campaigns all with the fastest go\\-live in the industry. as a result, thousands of top brands have partnered with mntn, including tarte, decked, and national university.\n\n\n\n\\#li\\-remote",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Wells Fargo",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "Synapse",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Databricks",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=da661d8fb0ece801",
        "description": "**about this role:**  \n\n  \n\nwells fargo is seeking a senior software engineer within core infrastructure services (cis) to enable and manage safe, secure, and efficient public cloud environments across microsoft azure and google cloud platform. cis accelerates enterprise application transformation by empowering engineers to build and run applications securely on cloud platforms. this position focuses on cloud platform enablement, infrastructure automation, and secure\\-by\\-default foundations that support large\\-scale, regulated workloads. the role will collaborate closely with platform engineers, security, and delivery teams to drive reliable cloud adoption.  \n\n  \n\n**in this role, you will:*** lead moderately complex initiatives and deliverables within cloud and infrastructure technical domains.\n* contribute to large\\-scale planning of cloud platform strategies and foundational services.\n* design, code, test, debug, document, and deploy infrastructure and automation for cloud platforms, including upgrades and deployments.\n* review and resolve moderately complex technical challenges requiring in\\-depth evaluation of cloud technologies, tooling, and procedures.\n* collaborate and consult with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve goals\n* build and operate secure\\-by\\-default azure and gcp resources aligned with enterprise security and compliance requirements.\n* lead projects, act as an escalation point, and provide guidance to less experienced engineers.\n* collaborate with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve shared goals.\n* enable ci/cd and event\\-driven automation for infrastructure using modern devops practices.\n\n**required qualifications:*** 4\\+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n* 4\\+ years of software development lifecycle (sdlc) experience from planning through deployment and optimization\n* 2\\+ years of hands\\-on experience enabling microsoft azure cloud services with knowledge in google cloud platform (gcp).\n* 2\\+ years of experience with infrastructure as code using terraform, including module development, versioning, reusable patterns, and compliance controls.\n* 2\\+ years of experience designing and implementing enterprise\\-grade devops and ci/cd automation.\n\n**desired qualifications:*** hands\\-on experience enabling google cloud platform (gcp) services.\n* hands on experience with azure (vnet, private link, key vault, data factory, data share, data explorer, data fabric, databricks, synapse, purview, analysis services) and knowledge of gcp (vpc, private service connect, cloud kms).\n* strong understanding of iam/entra id, service accounts, managed identities, and least privilege access.\n* experience with remote state, workspace organization, and module registries, ideally using terraform enterprise (tfe).\n* ability to build secure\\-by\\-default cloud resources.\n* knowledge of policy products azure policy, hashicorp sentinel, prisma, and gcp organization policy to enforce guardrails.\n* knowledge of logging, audit, encryption, and secret management best practices.\n* understanding of cloud networking concepts: vpc/vnet design, private connectivity, firewalls, routing, dns, and segmentation.\n* experience with github actions, jenkins, harness for automated terraform pipelines.\n* experience with event\\-driven automation (azure functions, logic apps, event grid)\n* modern auth patterns (oidc, workload identity federation), no static credentials.\n* strong coding/scripting skills in python, powershell, hcl or other scripting languages.\n* solid git workflows, code review practices, and documentation habits.\n* ability to design deployable, scalable cloud patterns aligned to landing zone or foundation models.\n* experience building internal platform modules, templates, or paved paths for product teams.\n* familiar with iac testing and quality tools (tflint, terraform compliance, terraform testing framework).\n* experience in designing or developing ai, generative ai, or agentic automation solutions.\n* azure and gcp certifications in relevant areas.\n\n**job expectations:*** this position is not eligible for visa sponsorship\n* this is a hybrid position that requires working on\\-site at one of the specified locations, with no fully remote option available.\n\n**pay range**  \n\n  \n\nreflected is the base pay range offered for this position. pay may vary depending on factors including but not limited to demonstrated examples of prior performance, skills, experience, or work location. employees may also be eligible for incentive opportunities.  \n\n$100,000\\.00 \\- $179,000\\.00  \n\n**benefits**  \n\n  \n\nwells fargo provides eligible employees with a comprehensive set of benefits, many of which are listed below. visit benefits \\- wells fargo jobs for an overview of the following benefit plans and programs offered to employees.* health benefits\n* 401(k) plan\n* paid time off\n* disability benefits\n* life insurance, critical illness insurance, and accident insurance\n* parental leave\n* critical caregiving leave\n* discounts and savings\n* commuter benefits\n* tuition reimbursement\n* scholarships for dependent children\n* adoption reimbursement\n\n**posting end date:**  \n\n27 feb 2026  \n\n* **job posting may come down early due to volume of applicants.**\n\n  \n\n**we value equal opportunity**  \n\n  \n\nwells fargo is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.  \n\n  \n\nemployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance\\-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. they are accountable for execution of all applicable risk programs (credit, market, financial crimes, operational, regulatory compliance), which includes effectively following and adhering to applicable wells fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. there is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.  \n\n  \n\n**applicants with disabilities**  \n\n  \n\nto request a medical accommodation during the application or interview process, visit disability inclusion at wells fargo .  \n\n  \n\n**drug and alcohol policy**  \n\n  \n\nwells fargo maintains a drug free workplace. please see our drug and alcohol policy to learn more.  \n\n  \n\n**wells fargo recruitment and hiring requirements:**  \n\n  \n\na. third\\-party recordings are prohibited unless authorized by wells fargo.  \n\n  \n\nb. wells fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Wells Fargo",
        "location": "Minneapolis, MN, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "Synapse",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Databricks",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f03d32c795783e6d",
        "description": "**about this role:**  \n\n  \n\nwells fargo is seeking a senior software engineer within core infrastructure services (cis) to enable and manage safe, secure, and efficient public cloud environments across microsoft azure and google cloud platform. cis accelerates enterprise application transformation by empowering engineers to build and run applications securely on cloud platforms. this position focuses on cloud platform enablement, infrastructure automation, and secure\\-by\\-default foundations that support large\\-scale, regulated workloads. the role will collaborate closely with platform engineers, security, and delivery teams to drive reliable cloud adoption.  \n\n  \n\n**in this role, you will:*** lead moderately complex initiatives and deliverables within cloud and infrastructure technical domains.\n* contribute to large\\-scale planning of cloud platform strategies and foundational services.\n* design, code, test, debug, document, and deploy infrastructure and automation for cloud platforms, including upgrades and deployments.\n* review and resolve moderately complex technical challenges requiring in\\-depth evaluation of cloud technologies, tooling, and procedures.\n* collaborate and consult with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve goals\n* build and operate secure\\-by\\-default azure and gcp resources aligned with enterprise security and compliance requirements.\n* lead projects, act as an escalation point, and provide guidance to less experienced engineers.\n* collaborate with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve shared goals.\n* enable ci/cd and event\\-driven automation for infrastructure using modern devops practices.\n\n**required qualifications:*** 4\\+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n* 4\\+ years of software development lifecycle (sdlc) experience from planning through deployment and optimization\n* 2\\+ years of hands\\-on experience enabling microsoft azure cloud services with knowledge in google cloud platform (gcp).\n* 2\\+ years of experience with infrastructure as code using terraform, including module development, versioning, reusable patterns, and compliance controls.\n* 2\\+ years of experience designing and implementing enterprise\\-grade devops and ci/cd automation.\n\n**desired qualifications:*** hands\\-on experience enabling google cloud platform (gcp) services.\n* hands on experience with azure (vnet, private link, key vault, data factory, data share, data explorer, data fabric, databricks, synapse, purview, analysis services) and knowledge of gcp (vpc, private service connect, cloud kms).\n* strong understanding of iam/entra id, service accounts, managed identities, and least privilege access.\n* experience with remote state, workspace organization, and module registries, ideally using terraform enterprise (tfe).\n* ability to build secure\\-by\\-default cloud resources.\n* knowledge of policy products azure policy, hashicorp sentinel, prisma, and gcp organization policy to enforce guardrails.\n* knowledge of logging, audit, encryption, and secret management best practices.\n* understanding of cloud networking concepts: vpc/vnet design, private connectivity, firewalls, routing, dns, and segmentation.\n* experience with github actions, jenkins, harness for automated terraform pipelines.\n* experience with event\\-driven automation (azure functions, logic apps, event grid)\n* modern auth patterns (oidc, workload identity federation), no static credentials.\n* strong coding/scripting skills in python, powershell, hcl or other scripting languages.\n* solid git workflows, code review practices, and documentation habits.\n* ability to design deployable, scalable cloud patterns aligned to landing zone or foundation models.\n* experience building internal platform modules, templates, or paved paths for product teams.\n* familiar with iac testing and quality tools (tflint, terraform compliance, terraform testing framework).\n* experience in designing or developing ai, generative ai, or agentic automation solutions.\n* azure and gcp certifications in relevant areas.\n\n**job expectations:*** this position is not eligible for visa sponsorship\n* this is a hybrid position that requires working on\\-site at one of the specified locations, with no fully remote option available.\n\n**pay range**  \n\n  \n\nreflected is the base pay range offered for this position. pay may vary depending on factors including but not limited to demonstrated examples of prior performance, skills, experience, or work location. employees may also be eligible for incentive opportunities.  \n\n$100,000\\.00 \\- $179,000\\.00  \n\n**benefits**  \n\n  \n\nwells fargo provides eligible employees with a comprehensive set of benefits, many of which are listed below. visit benefits \\- wells fargo jobs for an overview of the following benefit plans and programs offered to employees.* health benefits\n* 401(k) plan\n* paid time off\n* disability benefits\n* life insurance, critical illness insurance, and accident insurance\n* parental leave\n* critical caregiving leave\n* discounts and savings\n* commuter benefits\n* tuition reimbursement\n* scholarships for dependent children\n* adoption reimbursement\n\n**posting end date:**  \n\n27 feb 2026  \n\n* **job posting may come down early due to volume of applicants.**\n\n  \n\n**we value equal opportunity**  \n\n  \n\nwells fargo is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.  \n\n  \n\nemployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance\\-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. they are accountable for execution of all applicable risk programs (credit, market, financial crimes, operational, regulatory compliance), which includes effectively following and adhering to applicable wells fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. there is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.  \n\n  \n\n**applicants with disabilities**  \n\n  \n\nto request a medical accommodation during the application or interview process, visit disability inclusion at wells fargo .  \n\n  \n\n**drug and alcohol policy**  \n\n  \n\nwells fargo maintains a drug free workplace. please see our drug and alcohol policy to learn more.  \n\n  \n\n**wells fargo recruitment and hiring requirements:**  \n\n  \n\na. third\\-party recordings are prohibited unless authorized by wells fargo.  \n\n  \n\nb. wells fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Wells Fargo",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "Synapse",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Databricks",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7ce987d03cd6c59b",
        "description": "**about this role:**  \n\n  \n\nwells fargo is seeking a senior software engineer within core infrastructure services (cis) to enable and manage safe, secure, and efficient public cloud environments across microsoft azure and google cloud platform. cis accelerates enterprise application transformation by empowering engineers to build and run applications securely on cloud platforms. this position focuses on cloud platform enablement, infrastructure automation, and secure\\-by\\-default foundations that support large\\-scale, regulated workloads. the role will collaborate closely with platform engineers, security, and delivery teams to drive reliable cloud adoption.  \n\n  \n\n**in this role, you will:*** lead moderately complex initiatives and deliverables within cloud and infrastructure technical domains.\n* contribute to large\\-scale planning of cloud platform strategies and foundational services.\n* design, code, test, debug, document, and deploy infrastructure and automation for cloud platforms, including upgrades and deployments.\n* review and resolve moderately complex technical challenges requiring in\\-depth evaluation of cloud technologies, tooling, and procedures.\n* collaborate and consult with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve goals\n* build and operate secure\\-by\\-default azure and gcp resources aligned with enterprise security and compliance requirements.\n* lead projects, act as an escalation point, and provide guidance to less experienced engineers.\n* collaborate with peers, colleagues, and mid\\-level managers to resolve technical challenges and achieve shared goals.\n* enable ci/cd and event\\-driven automation for infrastructure using modern devops practices.\n\n**required qualifications:*** 4\\+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n* 4\\+ years of software development lifecycle (sdlc) experience from planning through deployment and optimization\n* 2\\+ years of hands\\-on experience enabling microsoft azure cloud services with knowledge in google cloud platform (gcp).\n* 2\\+ years of experience with infrastructure as code using terraform, including module development, versioning, reusable patterns, and compliance controls.\n* 2\\+ years of experience designing and implementing enterprise\\-grade devops and ci/cd automation.\n\n**desired qualifications:*** hands\\-on experience enabling google cloud platform (gcp) services.\n* hands on experience with azure (vnet, private link, key vault, data factory, data share, data explorer, data fabric, databricks, synapse, purview, analysis services) and knowledge of gcp (vpc, private service connect, cloud kms).\n* strong understanding of iam/entra id, service accounts, managed identities, and least privilege access.\n* experience with remote state, workspace organization, and module registries, ideally using terraform enterprise (tfe).\n* ability to build secure\\-by\\-default cloud resources.\n* knowledge of policy products azure policy, hashicorp sentinel, prisma, and gcp organization policy to enforce guardrails.\n* knowledge of logging, audit, encryption, and secret management best practices.\n* understanding of cloud networking concepts: vpc/vnet design, private connectivity, firewalls, routing, dns, and segmentation.\n* experience with github actions, jenkins, harness for automated terraform pipelines.\n* experience with event\\-driven automation (azure functions, logic apps, event grid)\n* modern auth patterns (oidc, workload identity federation), no static credentials.\n* strong coding/scripting skills in python, powershell, hcl or other scripting languages.\n* solid git workflows, code review practices, and documentation habits.\n* ability to design deployable, scalable cloud patterns aligned to landing zone or foundation models.\n* experience building internal platform modules, templates, or paved paths for product teams.\n* familiar with iac testing and quality tools (tflint, terraform compliance, terraform testing framework).\n* experience in designing or developing ai, generative ai, or agentic automation solutions.\n* azure and gcp certifications in relevant areas.\n\n**job expectations:*** this position is not eligible for visa sponsorship\n* this is a hybrid position that requires working on\\-site at one of the specified locations, with no fully remote option available.\n\n**pay range**  \n\n  \n\nreflected is the base pay range offered for this position. pay may vary depending on factors including but not limited to demonstrated examples of prior performance, skills, experience, or work location. employees may also be eligible for incentive opportunities.  \n\n$100,000\\.00 \\- $179,000\\.00  \n\n**benefits**  \n\n  \n\nwells fargo provides eligible employees with a comprehensive set of benefits, many of which are listed below. visit benefits \\- wells fargo jobs for an overview of the following benefit plans and programs offered to employees.* health benefits\n* 401(k) plan\n* paid time off\n* disability benefits\n* life insurance, critical illness insurance, and accident insurance\n* parental leave\n* critical caregiving leave\n* discounts and savings\n* commuter benefits\n* tuition reimbursement\n* scholarships for dependent children\n* adoption reimbursement\n\n**posting end date:**  \n\n27 feb 2026  \n\n* **job posting may come down early due to volume of applicants.**\n\n  \n\n**we value equal opportunity**  \n\n  \n\nwells fargo is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.  \n\n  \n\nemployees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance\\-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. they are accountable for execution of all applicable risk programs (credit, market, financial crimes, operational, regulatory compliance), which includes effectively following and adhering to applicable wells fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. there is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.  \n\n  \n\n**applicants with disabilities**  \n\n  \n\nto request a medical accommodation during the application or interview process, visit disability inclusion at wells fargo .  \n\n  \n\n**drug and alcohol policy**  \n\n  \n\nwells fargo maintains a drug free workplace. please see our drug and alcohol policy to learn more.  \n\n  \n\n**wells fargo recruitment and hiring requirements:**  \n\n  \n\na. third\\-party recordings are prohibited unless authorized by wells fargo.  \n\n  \n\nb. wells fargo requires you to directly represent your own experiences during the recruiting and hiring process.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Engineer",
        "company": "McAfee",
        "location": "Frisco, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Snowflake",
            "Databricks",
            "Kafka",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4210181db0262688",
        "description": "***role overview:***\nas a data engineer at mcafee, you will be a key member of our data innovation team, responsible for designing, building, and overseeing the deployment and operation of technology architecture, solutions, and software that unlock the full potential of our data assets. this role combines hands\\-on technical implementation with strategic problem\\-solving to drive data\\-driven innovation across the organization.\n  \n\n  \n\nyou will establish and build processes and structures to capture, manage, store, and utilize structured and unstructured data from diverse internal and external sources, creating scalable solutions that span from cloud\\-based architectures to traditional databases. working at the intersection of data engineering, data science, and data quality, you will leverage artificial intelligence, machine learning, and big\\-data techniques to transform raw data into actionable insights that drive business value.\n  \n\n  \n\nthis is a collaborative role where you will partner closely with business stakeholders, data scientists and product teams to solve complex problems, enable company\\-wide data solutions, and establish the foundation for data\\-driven decision making across mcafee.\n  \n\n  \n\nthis is a hybrid position based in texas. we are only considering candidates who are currently in texas and are not offering relocation.**about the role:**\n\n* partner with business stakeholders to understand data requirements and translate them into scalable technical solutions that drive operational efficiency and strategic insights\n* lead data innovation initiatives by identifying opportunities to leverage data assets for new business capabilities and competitive advantages\n* review internal and external business and product requirements for data operations and recommend strategic changes and upgrades to systems and storage\n* collaborate with data scientists to enable advanced analytics, predictive modeling, and machine learning initiatives that solve complex business problems\n* work with professional services teams on client\\-focused data solutions, ensuring alignment with business objectives and customer needs\n* design and oversee the deployment of comprehensive data architecture that captures, manages, and stores structured and unstructured data from multiple internal and external sources\n* build resilient etl/elt pipelines that channel data from multiple inputs, route appropriately, and store using cloud structures, local databases, and other applicable storage forms\n* establish processes and structures based on business and technical requirements to ensure optimal data flow across systems\n* create and maintain well\\-documented data services and interfaces for efficient data access across the organization\n* develop company\\-wide, web\\-enabled solutions that democratize data access and empower self\\-service analytics\n* develop technical tools and programming leveraging artificial intelligence, machine learning, and big\\-data techniques to cleanse, organize, and transform data on an automated basis\n* implement comprehensive data quality frameworks including validation checks, monitoring, and automated recovery strategies to maintain data accuracy, completeness, and freshness\n* apply business logic to cleanse, enrich, and structure raw data, ensuring consistency and quality across domains\n* leverage model context protocol (mcp) to connect with top enterprise applications, enabling seamless automation of data flows and improving operational efficiency\n* utilize copilot and anthropic models to accelerate development, automate documentation, and enhance code quality and review processes\n* create and establish design standards and assurance processes for software, systems, and applications development to ensure compatibility and operability of data connections, flows, and storage requirements\n* ensure secure, scalable, and auditable data ingestion processes, with appropriate handling of pii and compliance requirements\n* uphold sdlc best practices across development and delivery stages to ensure reliability, maintainability, and scalability\n* maintain and defend data structures and integrity on an automated basis, implementing proactive monitoring and alerting systems\n* troubleshoot pipeline issues and collaborate with platform teams to optimize performance and recovery strategies\n* participate in on\\-call rotations to ensure 24/7 reliability of critical data systems\n* continuously evaluate and implement new technologies and methodologies to improve data engineering capabilities\n* mentor junior team members and contribute to the growth of the data engineering practice\n\n **about you:**\n\n* 5\\+ years of hands\\-on experience in developing etl/elt pipelines across varied data sources, with demonstrated ability to work across the full spectrum of data engineering challenges\n* experience with copilot and claude anthropic models to enhance development speed, code quality, and documentation\n* strong programming skills in languages such as python, scala, or java, with ability to write production\\-quality code\n* experience with modern data platforms and tools (e.g., snowflake, databricks, apache spark, kafka, airflow)\n* practical knowledge of model context protocol (mcp) to connect enterprise applications and automate data workflows\n* experience with cloud platforms (aws, azure, gcp) and their native data services\n* knowledge of containerization and orchestration technologies (docker, kubernetes)\n* strong expertise in data integration, transformation, and curation with a focus on quality and consistency\n* experience with real\\-time data processing and streaming architectures\n* background in data science or analytics, with ability to collaborate effectively with data scientists\n* experience in client\\-facing or professional services roles\n* familiarity with dataops and mlops practices\n* a mindset focused on operational efficiency, automation, and continuous improvement\n* strong business acumen with ability to translate technical capabilities into business value\n* commitment to sdlc best practices and structured development processes\n* excellent communication and collaboration skills, with ability to work effectively with both technical and non\\-technical stakeholders\n* proactive approach to problem\\-solving with strong analytical and critical thinking skills\n* passion for innovation and staying current with emerging technologies and industry trends\n* proven experience with both structured and unstructured data, including design and implementation of solutions that leverage both traditional databases and modern cloud architectures\n* experience managing sensitive data, including pii, with attention to compliance and governance requirements\n* demonstrated ability to work with artificial intelligence, machine learning, and big\\-data techniques\n* solid understanding of data modeling, data warehousing concepts, and dimensional modeling\n\n  \n\n\\#li\\-hybrid\n\n ***company overview***\nmcafee is a leader in personal security for consumers. focused on protecting people, not just devices, mcafee consumer solutions adapt to users\u2019 needs in an always online world, empowering them to live securely through integrated, intuitive solutions that protects their families and communities with the right security at the right moment.\n\n***company benefits and perks:***\nwe work hard to embrace diversity and inclusion and encourage everyone at mcafee to bring their authentic selves to work every day. we\u2019re proud to be great place to work\u00ae certified in 10 countries, a reflection of the supportive, empowering environment we\u2019ve built where people feel seen, valued, and energized to reach their full potential and thrive.\n\n\nwe offer a variety of social programs, flexible work hours and family\\-friendly benefits to all of our employees.\n\n* bonus program\n* pension and retirement plans\n* medical, dental and vision coverage\n* paid time off\n* paid parental leave\n* support for community involvement\n\n\nwe're serious about our commitment to diversity which is why mcafee prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "HPC Software Engineer",
        "company": "Cadre5",
        "location": "Knoxville, TN, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Keras",
            "Docker",
            "Kubernetes",
            "Git",
            "Dask",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=51ad2b47ec2abe67",
        "description": "**hpc software engineer**\n=========================\n\n\nfounded in 1999 in the beautiful smoky mountains of east tennessee, cadre5 provides innovative technical solutions to our customers locally and nationally. our cadre5 lab partners division has partnered with the emerging technologies \\& computing (etac) group in the research computing support division (rcsd) of the information technology services directorate (itsd) at oak ridge national laboratory (ornl) to recruit a qualified hpc software engineer to support the integration of computing hardware and software tools for accomplishing research tasks across a variety of scientific research areas.  \n\n\nornl delivers scientific discoveries and technical breakthroughs needed to realize solutions in energy and national security and provides economic benefit to the nation. this premier research institution located near knoxville in oak ridge, tn, addresses national needs through impactful research and world\\-leading research centers.\n*\\#cj*\n**this is a full\\-time, permanent position follows a hybrid working model.**\n**why cadre5?**\n===============\n\n\n* working with highly talented team members\n* 3 weeks\u2019 vacation\n* excellent medical insurance, including employer\\-paid benefits\n\n**what will you be doing?**\n---------------------------\n\n\n**etac focuses on supporting ornl researcher\u2019s hpc computing, data engineering and management, infrastructure as a service, and new technology needs. you'll be working directly with researchers, supporting their science and understanding scientific problems and the application of advanced research computing tools to help achieve research outcomes. the hpc scientific software engineers play a crucial role in optimizing computational methods and facilitating groundbreaking research across multiple scientific areas. as an hpc team member, you will recommend computational and/or visualization tools, techniques, and methodologies for the scientific computing aspect of research investigations.**\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n  \n\n**job responsibilities:**\n-------------------------\n\n\n* scientific software and application management:\n\t+ understand scientific software users\u2019 requirements: work closely with researchers to understand their computational needs and translate them into efficient hpc applications. analyze application performance to identify bottlenecks and develop strategies to improve scalability and efficiency on hpc systems. this may involve profiling code, analyzing communication patterns, and tuning system parameters.\n\t+ install and manage scientific software: deploy and maintain a wide range of scientific applications, libraries, and development tools on hpc systems to support research activities.\n\t+ develop custom tools and scripts: develop tools to automate common tasks, improve systems management, and facilitate sophisticated computational workflows. develop, maintain, and install software for hpc and data intensive architectures, including graphic processing units (gpus), parallel systems, and other computing environments.\n* user support and collaboration:\n\t+ provide software technical support: collaborate with hpc support and scientists on technical issues related to scientific software problems. following industry standards, implement hpc software with novel programming and optimization techniques. provide solutions and technical recommendations for code optimization, resource utilization, and system tuning.\n\t+ collaborate on research projects: work closely with researchers to understand their computational requirements and assist in developing efficient computational strategies, code optimization, and parallelization. this includes working with a highly diverse and multidisciplinary team (such as mathematicians, physicists, computer scientists, and engineers) in the research, development, integration, testing, and deployment of research software, data platforms, and machine learning systems for large\\-scale data analysis.\n\t+ research information dissemination: support research staff in disseminating results in peer\\-reviewed journals, technical reports, relevant conferences, and open\\-source software project repos.\n* research and development:\n\t+ stay informed about latest research in hpc and ai.\n\t+ develop and recommend ideas for new programs, products, and features by staying abreast of new technology developments and trends.\n* partnerships and collaboration:\n\t+ as applicable/possible\\- establish and maintain partnerships and collaborations with industry, other groups at ornl, and hpc networks to share knowledge and best practices.\n\n**basic qualifications:**\n-------------------------\n\n\n* a bs in computer science, computer engineering, information systems, or a related field of study and five (5\\) to seven (7\\) years of proven and aligned experience is required. an overall combination of equivalent experience may be considered.\n* three (3\\) or more years of demonstrated abilities in the following areas:\n\t+ high performance computing (hpc) environments and hpc scheduling software.\n\t+ software development including version control using gitwith open\\-source tools and software.\n\t+ python and data analysis modules such as pandas, numpy, and dask.\n\t+ developing software in c/c\\+\\+, fortran or other programming languages\n* the ability to obtain and maintain a department of energy \"q\" clearance is required. this requires us citizenship.\n\n\n**preferred qualifications:**\n-----------------------------\n\n\n* in\\-depth understanding of hpc architectures and their optimization techniques.\n* experience in the following areas:\n\t+ optimizing and parallelizing software products for hpc using mpi or other open\\-source tools.\n\t+ hpc debugging tools such as ddt, gdb or valgrind.\n\t+ ai toolkits such as pytorch, rapidsai, tensorflow, or keras.\n\t+ statistical analysis software such as python or r.\n\t+ building and running containerized applications in an hpc environment.\n\t+ cluster deployment tools such as warewulf, pxeboot, and/or bright.\n\t+ managing systems.\n\t+ working in a government, scientific, or other highly technical environment.\n* knowledge of multiple operating systems including linux.\n* exposure to microservices concepts and understanding of container environments including podman, docker, and kubernetes.\n* proven ability to balance sophisticated research and security requirements.\n\n\n**benefits**\ncadre5 offers excellent pay and benefits, to include full medical, dental, and vision coverage coupled with 401k match, 15 days pto, and 10 holidays.\n ***cadre5 is an equal opportunity employer. all qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. cadre5 is an e\\-verify employer.***",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Machine Learning Engineer",
        "company": "The Walt Disney Company",
        "location": "Lake Buena Vista, FL, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "AI Engineer",
            "Machine Learning Engineer",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=296b70730c29edd4",
        "description": "**department description:**\n\n\nat disney, we\u2019re storytellers. we make the impossible, possible. the walt disney company is a world\\-class entertainment and technological leader. walt\u2019s passion was to continuously envision new ways to move audiences around the world\u2014a passion that remains our touchstone in an enterprise that stretches from theme parks, resorts and a cruise line to sports, news, movies and a variety of other businesses. uniting each endeavor is a commitment to creating and delivering unforgettable experiences \u2014 and we\u2019re constantly looking for new ways to enhance these exciting experiences.\n\n\nthe enterprise technology mission is to deliver technology solutions that align to business strategies while enabling enterprise efficiency and promoting cross\\-company collaborative innovation. our group drives competitive advantage by enhancing our consumer experiences, enabling business growth, and advancing operational excellence.\n\n**team description:**\n\n\nreporting to the director of automation, tooling, and observability within global network engineering \\& operations (gneo), the machine learning / software engineer plays a critical role in designing, developing, and implementing self\\-healing infrastructure management systems for enterprise\\-wide, production environments. this role combines deep expertise in machine learning, ai technology, software engineering, and devops to create reusable patterns, frameworks, and services to improve reliability across services and platforms. the candidate will serve as a thought leader, identifying opportunities for and applying advanced analytics, predictive modeling, and ai to large\\-scale telemetry, changes, events and incident data to derive actionable insights. the role focuses on building, deploying, and operating machine learning models that proactively detect issues, predict failures, and drive automated, self\\-healing remediation across enterprise systems. the role is intentionally machine learning and ai heavy and is intended to be a strategic driver in that space.\n\n**what you\u2019ll do:**\n\n* work alongside our first\\-class applications, infrastructure \\& operations teams to understand current manual processes and business requirements\n* architect, design, and implement reusable machine learning frameworks, patterns, and services that integrate into the enterprise automation and observability platforms\n* design, train, and deploy machine learning models for anomaly detection, forecasting, predictive analytics, event correlation, pattern recognition, classification, causal analysis, and more in distributed environments that can be used to surface leading indicators of failure\n* build near\\-real\\-time inference pipelines that generate actionable insights from live telemetry, including continuous streams of metrics, logs, traces, and operational events\n* create data abstractions and perform feature engineering on high\\-volume, high\\-cardinality telemetry data\n* evaluate model performance using real production signals and continuously iterate to improve accuracy and reliability\n* build closed\\-loop, event\\-driven systems where model signals trigger automated remediation actions\n* partner with infrastructure and sre teams to identify opportunities and integrate machine learning and ai\\-driven insights into operational tools, workflows, and dashboards\n* analyze incident and historical data to uncover leading indicators and predictive signals\n* own the full machine learning lifecycle: experimentation, validation, deployment, monitoring, and retraining\n* breakdown targeted, manual processes into reusable software modules that leverage machine learning models\n* build emulation and simulation environments (digital twins) of the infrastructure to test ai/ml\\-driven automation under realistic scenarios and allow for faster ideation and iteration for architects and engineers.\n* develop algorithms and frameworks to integrate machine learning and ai technologies into our orchestration platform\n* ensure service reliability, performance, and operational uptime through code\\-driven solutions.\n* conduct root cause analysis, design fault\\-tolerant architectures, and enable self\\-healing automation.\n* implement monitoring dashboards and kpis to provide visibility into automation and tooling performance.\n* collaborate with cross\\-functional teams including network engineers, software developers, machine learning engineers, and operations teams across the enterprise.\n* support the integration of commercial and open\\-source tools while maintaining a vendor\\-agnostic implementation\n\n**required qualifications \\& skills:**\n\n* 7\\+ years of software engineering experience, with expertise in automation, machine learning, and ai technologies\n* proven hands\\-on experience building production\\-grade ml models and inference pipelines; strong proficiency with modern ml frameworks such as pytorch, tensorflow, scikit\\-learn, etc.\n* design, train, and deploy machine learning models for anomaly detection, forecasting, predictive analytics, event correlation, pattern recognition, classification, causal analysis, and more in distributed environments that can be used to surface leading indicators of failure\n* proven hands\\-on experience using software to build frontend, apis and backend functionality; strong proficiency with python, javascript, typescript, go, or rust\n* build emulation and simulation environments (digital twins) of the infrastructure to test ai/ml\\-driven automation under realistic scenarios and allow for faster ideation and iteration for architects and engineers.\n* strong hands\\-on experience building and deploying event\\-driven or streaming data, machine learning models in production\n* solid foundation in statistics, data analysis, and applied machine learning techniques\n* experience working with large\\-scale, real\\-world datasets (noisy, incomplete, non\\-standardized, and evolving)\n* experience operationalizing models in distributed, production environments\n* ability to translate ambiguous operational problems into solvable machine learning use cases\n* experience with modern cloud platforms, container orchestration (kubernetes/docker), identity/auth frameworks, data and workflow orchestration.\n* experience with ai/ml technologies and data engineering concepts. preferred: proven hands\\-on building ai agents.\n* demonstrated success designing and building enterprise\\-scale systems and reusable software frameworks.\n* strong communication, collaboration and leadership skills\n* applies systems thinking to understand how individual components fit into larger, more holistic solutions.\n* capable of quickly shifting between detailed, hands\\-on work and high\\-level strategic thinking.\n\n**preferred qualifications:**\n\n* certifications such as kubernetes (cka/ckad), aws/azure/gcp certifications, ccnp/devnet or nvidia ai engineer.\n* experience developing low\\-code/no\\-code automation platforms or reusable developer toolkits.\n* contributions to open\\-source automation, machine learning, ai, observability, or devops communities.\n* applying unsupervised and semi\\-supervised learning for anomaly detection and signal discovery\n* applying complex event processing and event correlation techniques\n* building time\\-series forecasting models for capacity, latency, and failure prediction\n* experience with feature stores, offline/online feature pipelines, and feature reuse\n* implementing model monitoring for drift, bias, and performance degradation\n* experience with reinforcement learning or decision models for automated remediation and optimization\n* working with real\\-time or near\\-real\\-time inference pipelines\n* experience labeling, curating, and managing training data derived from production telemetry\n* experience mentoring engineers, sharing knowledge, and fostering a learning culture\n* demonstrated curiosity and continuous learning mindset, with a passion for exploring emerging ai/ml, automation, and platform technologies\n\n**required education:**\n\n* bachelor\u2019s degree in computer science, information systems, software, electrical or electronics engineering, or comparable field of study, and/or equivalent work experience\n\n**preferred education:**\n\n* master\u2019s degree in computer science, engineering, or related discipline.\n\n\n\\#disneytech\n\n  \n\nthe hiring range for this position in burbank, ca is $155,700 \\- $208,700 per year and in seattle is $163,100 \\- $218,700 per year. the base pay actually offered will take into account internal equity and also may vary depending on the candidate\u2019s geographic region, job\\-related knowledge, skills, and experience among other factors. a bonus and/or long\\-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "ML Infrastructure Architect",
        "company": "Openkyber",
        "location": "AK, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Data Lake",
            "Databricks",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5be5e23fbb3c6051",
        "description": "title: ai engineer location: washington, d.c 4 days\\- hybrid duration: 12 months\n\n\njob description the ai engineer will be responsible for designing, developing, and deploying ai solutions that address the needs of the world bank. this includes working on projects that involve natural language processing, machine learning, generative ai and other ai technologies. the engineer will collaborate with various teams to understand their requirements and deliver ai solutions that improve efficiency, decision\\-making, and service delivery.\n\n\n**key responsibilities**\n\n\n* lead the design and implementation of scalable, secure cloud architectures for enterprise solutions leveraging llm models from google and openai.\n* build applications with vector databases and langchain and understand text embeddings.\n* develop and maintain cloud\\-based solutions that integrate llm models to enhance search capabilities and user experience.\n* demonstrate experience in azure technologies, including ase, azure functions, azure api management, azure service bus, logic apps, azure storage, azure cognitive services, azure cosmos db, azure data factory, databricks, azure data lake, and caching technologies.\n* provide technical leadership and mentorship to team members, ensuring best practices in cloud architecture and llm model integration.\n* ensure compliance with security standards and best practices in cloud architecture and data handling, particularly when dealing with sensitive information processed by llm models.\n* address performance and production issues, with extensive knowledge in logging and monitoring using tools such as splunk and appinsights.\n* possess sound technical knowledge and understanding of infrastructure design, including private and public cloud.\n* establish systems to supervise the operating efficiency of existing application systems and provide proactive maintenance.\n* participate in systems design, working within an established framework, and provide direction to a team of staff and contractors in their area of expertise.\n* stay updated with the latest industry trends, cloud technologies, and advancements in llm models to continually improve enterprise search solutions.\n* exhibit strong leadership and team\\-building skills, with the ability to mentor and guide technical teams, and excellent communication skills to articulate complex technical information to non\\-technical stakeholders.\n* create detailed architectural documentation and design patterns, and possess knowledge of security standards and compliance requirements in a cloud environment.\n\n**required qualifications \\& experience**\n\n\n* a degree in computer science, data science, ai, or a related field.\n* technical skills : proficiency in programming languages such as python.\n* experience with ai frameworks and libraries (e.g., tensorflow, pytorch).\n* knowledge of natural language processing and generative ai models.\n* experience with langchain, langgraph and building platform services.\n* experience in building solutions leveraging agentic ai.\n* knowledge and experience with google cloud services, azure technologies, and enterprise search applications.\n* professional experience : at least 3\\-5 years of experience in ai development and implementation.\n* experience in prompt engineering and ai governance.\n* demonstrated experience in cloud\\-native and hybrid web development projects.\n* soft skills : strong problem\\-solving skills.\n* excellent communication and collaboration abilities.\n* ability to work in a fast\\-paced and dynamic environment.\n\neeo: openkyber is an equal opportunity employer and does not discriminate in employment on the basis of minority/gender/disability/religion/lgbtqi/age/veterans.\n\n\n**for applications and inquiries, contact:** hirings@openkyber.com",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Deployment Engineer",
        "company": "Openkyber",
        "location": "AK, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "LLaMA",
            "Gemini",
            "Prompt Engineering",
            "S3",
            "Redshift",
            "Terraform",
            "Redshift",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=403a6d5b03cee33e",
        "description": "job title: agentic ai engineer location : ny/nj/ dallas job type : fte's job overview: in this role, you will be responsible for launching and implementing genai agentic solutions aimed at reducing the risk and cost of managing large\\-scale production environments with varying complexities. you will address various production runtime challenges by developing agentic ai solutions that can diagnose, reason, and take actions in production environments to improve productivity and address issues related to production support.\n\n\nwhat you'll do: build agentic ai systems: design and implement tool\\-calling agents that combine retrieval, structured reasoning, and secure action execution (function calling, change orchestration, policy enforcement) following mcp protocol. engineer robust guardrails for safety, compliance, and least\\-privilege access. productionize llms: build evaluation framework for open\\-source and foundational llms; implement retrieval pipelines, prompt synthesis, response validation, and self\\-correction loops tailored to production operations. integrate with runtime ecosystems: connect agents to observability, incident management, and deployment systems to enable automated diagnostics, runbook execution, remediation, and post\\-incident summarization with full traceability. collaborate directly with users: partner with production engineers, and application teams to translate production pain points into agentic ai roadmaps; define objective functions linked to reliability, risk reduction, and cost; and deliver auditable, business\\-aligned outcomes. safety, reliability, and governance: build validator models, adversarial prompts, and policy checks into the stack; enforce deterministic fallbacks, circuit breakers, and rollback strategies; instrument continuous evaluations for usefulness, correctness, and risk. scale and performance: optimize cost and latency via prompt engineering, context management, caching, model routing, and distillation; leverage batching, streaming, and parallel tool\\-calls to meet stringent slos under real\\-world load. build a rag pipeline: curate domain\\-knowledge; build data\\-quality validation framework; establish feedback loops and milestone framework maintain knowledge freshness. raise the bar: drive design reviews, experiment rigor, and high\\-quality engineering practices; mentor peers on agent architectures, evaluation methodologies, and safe deployment patterns role requirements: understand what skills, experience, and qualities you are looking for.\n\n\n**essential skills:**\n\n\n* 5\\+ years of software development in one or more languages (python, c/c\\+\\+, go, java); strong hands\\-on experience building and maintaining large\\-scale python applications preferred.\n* 3\\+ years designing, architecting, testing, and launching production ml systems, including model deployment/serving, evaluation and monitoring, data processing pipelines, and model fine\\-tuning workflows.\n* practical experience with large language models (llms): api integration, prompt engineering, fine\\-tuning/adaptation, and building applications using rag and tool\\-using agents (vector retrieval, function calling, secure tool execution).\n* understanding of different llms, both commercial and open source, and their capabilities (e.g., openai, gemini, llama, qwen, claude).\n* solid grasp of applied statistics, core ml concepts, algorithms, and data structures to deliver efficient and reliable solutions.\n* strong analytical problem\\-solving, ownership, and urgency; ability to communicate complex ideas simply and collaborate effectively across global teams with a focus on measurable business impact.\n* preferred: proficiency building and operating on cloud infrastructure (ideally aws), including containerized services (ecs/eks), serverless (lambda), data services (s3, dynamodb, redshift), orchestration (step functions), model serving (sagemaker), and infra\\-as\\-code (terraform/cloudformation).\n\n**for applications and inquiries, contact:** hirings@openkyber.com",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "DevOps Engineer",
        "company": "SWAP",
        "location": "US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "NoSQL",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9db067fbe776dd20",
        "description": "location: austin, texas (remote)\n\n**about swap**\n--------------\n\n\nswap is the infrastructure behind modern agentic commerce. the only ai\\-native platform connecting backend operations with a forward\\-thinking storefront experience.  \n\n  \n\nbuilt for brands that want to sell anything \\- anywhere, swap centralises global operations, powers intelligent workflows, and unlocks margin\\-protecting decisions with real\\-time data and capability. our products span cross\\-border, tax, returns, demand planning, and our next\\-generation agentic storefront, giving merchants full transparency and the ability to act with confidence.  \n\n  \n\nat swap, we\u2019re building a culture that values clarity, creativity, and shared ownership as we redefine how global commerce works.\n\n**about the role**\n------------------\n\n\nswap commerce is a leading innovator in e\\-commerce, dedicated to revolutionising online retail experiences. we are looking for a highly skilled and motivated **devops engineer** to join our growing team and help us build and maintain scalable, reliable, and efficient infrastructure. as a devops engineer, you will be a key player in designing, implementing, and maintaining our cloud infrastructure and ci/cd pipelines. you will report directly to the head of devops and work closely with development and operations teams to automate processes, improve system performance, and ensure the security and stability of our applications.\n\n**responsibilities**\n--------------------\n\n* design, implement, and maintain scalable, secure, and highly available cloud infrastructure.\n* develop and manage ci/cd pipelines to automate software delivery and deployment.\n* implement and manage monitoring, logging, and alerting solutions to ensure system health and performance.\n* collaborate with development teams to optimise application performance and troubleshoot production issues.\n* implement and enforce security best practices across our infrastructure and applications.\n* automate operational tasks and processes to improve efficiency and reduce manual effort.\n* participate in future on\\-call rotations for critical incidents and provide timely resolution.\n* mentor junior devops engineers and contribute to the growth of the team.\n* stay up\\-to\\-date with emerging technologies and industry best practices.\n\n### **qualifications**\n\n* bachelor's degree in computer science, engineering, or a related field, or equivalent practical experience.\n* 5\\+ years of experience in a devops or sre role.\n* strong experience with gcp.\n* proficiency in scripting languages (e.g., python, bash, javascript).\n* extensive experience with ci/cd tool github actions.\n* solid understanding of containerisation technologies (e.g., docker, kubernetes).\n* experience with infrastructure as code tool terraform. bonus for being able to build customer providers from scratch.\n* familiarity with monitoring and logging tools prometheus, new relic\n* strong understanding of networking concepts and security principles.\n* excellent problem\\-solving, communication, and collaboration skills.\n\n### **preferred qualifications**\n\n* certifications in relevant cloud platforms\n* experience with microservices architecture.\n* knowledge of database administration (e.g., sql, nosql).\n* experience with performance tuning and optimisation.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Information Technology - BI Data Architect",
        "company": "TCC Verizon Authorized Retailer",
        "location": "Fishers, IN, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "Copilot",
            "Synapse",
            "Git",
            "Databricks",
            "PySpark",
            "Power BI",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a149a432f720bb1c",
        "description": "purpose\n  \nas a key member of the data analytics and reporting team, the data architect will lead the design, implementation, and optimization of an enterprise data platform built on the microsoft fabric ecosystem. this role is responsible for translating business and analytical requirements into scalable, secure, and high\\-performance data architectures that enable advanced analytics, ai\\-driven insights, and enterprise reporting.\n  \nthe ideal candidate will possess deep expertise in microsoft fabric, modern cloud data engineering, and advanced analytics technologies. they will drive the evolution of lakehouse, warehouse, and semantic models while enabling ai and machine learning workloads through trusted, governed, and high\\-quality data assets.\n  \nthis role partners closely with data engineers, analytics developers, data scientists, and business stakeholders to ensure the organization\u2019s data platform supports predictive analytics, automation, and intelligent decision\\-making.\n  \n  \n\nessential duties and responsibilities\n  \n* lead the architecture, design, and governance of the microsoft fabric environment, including lakehouse, data warehouse, onelake, and semantic models.\n* design and maintain scalable data models optimized for bi, advanced analytics, and ai workloads.\n* architect end\\-to\\-end data pipelines using fabric data factory, notebooks, and streaming capabilities.\n* implement and optimize data ingestion, transformation, and orchestration processes for structured and unstructured data.\n* enable advanced analytics, machine learning, and ai initiatives through well\\-designed data foundations and feature stores.\n* ensure quality, consistency and security of database design, and ability to meet enterprise requirements in alignment with dba, enterprise architect, and integration teams\n* establish best practices for data modeling, performance tuning, security, and cost optimization within fabric.\n* develop and maintain enterprise metadata, lineage, and data cataloging using microsoft purview and fabric governance tools.\n* collaborate with data science teams to support model training, deployment, and monitoring.\n* review and govern etl/elt processes, notebooks, and transformation logic.\n* ensure compliance with data security, privacy, and regulatory requirements.\n* lead architectural reviews and approve platform changes to ensure stability and integrity.\n* translate complex business and analytical needs into technical architecture and implementation plans.\n* mentor and guide data engineers, bi developers, and analytics professionals.\n* support real\\-time and near\\-real\\-time analytics use cases.\n* drive adoption of ai\\-powered analytics, copilot, and intelligent reporting capabilities.\n* continuously evaluate emerging data, analytics, and ai technologies and recommend improvements.\n\n\nrequired knowledge, skills, and abilities\n  \ntechnical expertise\n  \n* strong hands\\-on experience with microsoft fabric (lakehouse, warehouse, data factory, notebooks, semantic models).\n* advanced proficiency in azure data services and cloud\\-native architecture.\n* expertise in modern data modeling (dimensional, data vault, lakehouse, feature engineering).\n* experience supporting machine learning, ai, and advanced analytics pipelines.\n* proficiency in sql, pyspark, and python for analytics and data engineering.\n* knowledge of mlops, data versioning, and model lifecycle management.\n* experience with data governance, lineage, and cataloging platforms.\n* strong understanding of data security, identity management, and compliance in cloud environments.\n* experience integrating external ai/ml platforms and apis.\n\n\nprofessional skills\n  \n* strategic thinker with ability to align data architecture with business and ai initiatives.\n* strong communication skills for technical and executive audiences.\n* proven ability to lead cross\\-functional data and analytics projects.\n* analytical and problem\\-solving mindset.\n* ability to manage multiple priorities in a fast\\-paced, innovation\\-driven environment.\n* collaborative leadership style and mentoring capability.\n* innovative mindset with strong interest in emerging ai technologies.\n\n\neducation and experience\n  \n* bachelor\u2019s degree in computer science, data science, information systems, or related field (or equivalent experience).\n* 10\\+ years of experience in data architecture, engineering, or analytics platforms.\n* 5\\+ years of experience designing cloud\\-based data platforms.\n* 3\\+ years of hands\\-on experience with microsoft fabric and/or modern azure analytics services.\n* demonstrated experience supporting advanced analytics, ai, or machine learning solutions.\n* experience working in agile/scrum environments.\n\n\npreferred technologies\n  \n* microsoft fabric\n* azure data factory\n* azure synapse analytics\n* azure databricks\n* power bi\n* microsoft purview\n* azure machine learning\n* python / py spark\n* azure devops\n* git\n\n\n\"the cellular connection is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, or protected veteran status\"\n  \n  \n\n\\#itjobs \\#jobsinit \\#jobsinbi \\#informationtechnology \\#powerbi \\#dataarchitecht \\#newjob \\#nowhiring",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "ML Engineer",
        "company": "Solventum",
        "location": "Austin, PA, US USA",
        "posted_at": "2026-02-24",
        "score": 13.3,
        "matched_keywords": [
            "LangChain",
            "PyTorch",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a3a0ae84f6b87980",
        "description": "thank you for your interest in joining solventum. solventum is a new healthcare company with a long legacy of solving big challenges that improve lives and help healthcare professionals perform at their best. at solventum, people are at the heart of every innovation we pursue. guided by empathy, insight, and clinical intelligence, we collaborate with the best minds in healthcare to address our customers\u2019 toughest challenges. while we continue updating the solventum careers page and applicant materials, some documents may still reflect legacy branding. please note that all listed roles are solventum positions, and our privacy policy: https://www.solventum.com/en\\-us/home/legal/website\\-privacy\\-statement/applicant\\-privacy/ applies to any personal information you submit. as it was with 3m, at solventum all qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.**job description:**\n\n**ml engineer**\n\n**3m health care is now solventum**\n\n**at solventum, we enable better, smarter, safer healthcare to improve lives. as a new company with a long legacy of creating breakthrough solutions for our customers\u2019 toughest challenges, we pioneer game\\-changing innovations at the intersection of health, material and data science that change patients' lives for the better while enabling healthcare professionals to perform at their best. because people, and their wellbeing, are at the heart of every scientific advancement we pursue.**\n\n**we partner closely with the brightest minds in healthcare to ensure that every solution we create melds the latest technology with compassion and empathy. because at solventum, we never stop solving for you.**\n\n**the impact you\u2019ll make in this role**\n\n\nas an **ml engineer**, you will be responsible for building and maintaining the pipelines that power ai in our healthcare information systems (his). we are looking for a practical, detail\\-oriented engineer who is passionate about **mlops, data reliability, and production stability.**\n\n\nin this role, you won\u2019t just be building models; you will be ensuring those models work reliably in the real world. you will help bridge the gap between data science and software engineering by implementing automated workflows, managing cloud infrastructure, and ensuring our ai services are secure and scalable.\n\n**key responsibilities**\n\n**1\\. mlops \\& deployment**\n\n* **pipeline development:** build and maintain ci/cd pipelines for machine learning, focusing on automated testing, model deployment, and version control (using tools like mlflow or git).\n* **model serving:** deploy ml models as scalable apis and microservices, ensuring they meet performance and latency requirements for clinical use.\n* **monitoring:** implement basic monitoring tools to track model performance, data drift, and system health in production.\n\n**2\\. data engineering \\& integration**\n\n* **data pipelines:** develop and optimize etl processes to transform healthcare data (fhir, hl7\\) into clean, usable datasets for model training and inference.\n* **feature management:** help build and maintain feature stores and data layers that ensure consistency between training and production environments.\n* **system integration:** work closely with backend teams to integrate ml outputs into our core healthcare applications.\n\n**3\\. engineering best practices**\n\n* **code quality:** write clean, maintainable, and well\\-documented python code. participate in code reviews to ensure system reliability.\n* **containerization:** use docker and kubernetes to package and orchestrate ml workloads across different environments.\n* **security \\& compliance:** follow established protocols to ensure all data handling and deployments meet hipaa and hitrust security standards.\n\n**your skills and expertise**  \n\nto set you up for success in this role from day one, solventum requires (at a minimum) the following qualifications:\n\n* bachelor\u2019s or master\u2019s degree in computer science, software engineering, data engineering, or a related field.\n* 3\u20135 years of professional experience in software engineering or data engineering, with at least 2 years focused on machine learning production environments.\n\n**and**\n\n* **programming:** strong proficiency in **python** and familiarity with sql. knowledge of a compiled language (like go or java) is a plus.\n* **cloud \\& infrastructure:** hands\\-on experience with at least one major cloud provider (aws, azure, or gcp) and containerization (docker).\n* **ml tools:** familiarity with ml libraries (pytorch or scikit\\-learn) and mlops tools (like airflow, prefect, bentoml, or kubeflow).\n* **data tools:** experience with data processing frameworks (like pandas, spark, or dbt).\n\n\nadditional qualifications that could help you succeed even further in this role include:\n\n* familiarity with deploying large language models (llms) or using frameworks like langchain.\n* experience working in a regulated environment (healthcare, finance, etc.).\n* understanding of api design and microservices architecture.\n\n**work location:**\n\n* **remote**\n\n**travel: may include up to 10% domestic**\n\n**must be legally authorized to work in country of employment without sponsorship for employment visa status (e.g., h1b status).**\n\n**supporting your well\\-being**\n\n\nsolventum offers many programs to help you live your best life \u2013 both physically and financially. to ensure competitive pay and benefits, solventum regularly benchmarks with other companies that are comparable in size and scope.\n\n\nonboarding requirement: to improve the onboarding experience, you will have an opportunity to meet with your manager and other new employees as part of the solventum new employee orientation. as a result, new employees hired for this position will be required to travel to a designated company location for on\\-site onboarding during their initial days of employment. travel arrangements and related expenses will be coordinated and paid for by the company in accordance with its travel policy. applies to new hires with a start date of october 1st 2025 or later.  \n\n\n\nresponsibilities of this position include that corporate policies, procedures and security standards are complied with while performing assigned duties.\nsolventum is committed to maintaining the highest standards of integrity and professionalism in our recruitment process. applicants must remain alert to fraudulent job postings and recruitment schemes that falsely claim to represent solventum and seek to exploit job seekers.\n\n\nplease note that all email communications from solventum regarding job opportunities with the company will be from an email with a domain of **@solventum.com**. be wary of unsolicited emails or messages regarding solventum job opportunities from emails with other email domains.\n\n\nplease note, solventum does not expect candidates in this position to perform work in the unincorporated areas of los angeles county.\nsolventum is an equal opportunity employer. solventum will not discriminate against any applicant for employment on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or veteran status.**please note: your application may not be considered if you do not provide your education and work history, either by: 1\\) uploading a resume, or 2\\) entering the information into the application fields directly.**\n\n**solventum global terms of use and privacy statement**\n\n  \n\ncarefully read these terms of use before using this website. your access to and use of this website and application for a job at solventum are conditioned on your acceptance and compliance with these terms.\n\n\nplease access the linked document. before submitting your application you will be asked to confirm your agreement with the  \n\nterms.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Analytics Engineer",
        "company": "Minute Media",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "Athena",
            "BigQuery",
            "Git",
            "BigQuery",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2aba85d8fb6add85",
        "description": "minute media is a global technology and content company built for the future of sports consumption. minute media's proprietary technology platform enables the creation, distribution and monetization of digital content experiences, which powers minute media's portfolio of trusted content brands, including sports illustrated, the players' tribune, and fansided as well as hundreds of sports leagues, teams, broadcasters, third\\-party publishers and advertisers. the technology platform includes, (i) stn video, an online video platform (ovp), which provides access to a robust sports highlights rights portfolio, and (ii) magnifi, an ai\\-driven saas platform, that enables rights holders to detect key moments and create highlights in real time. minute media is building the future of how the world connects with sport, powered by innovation, shared globally, and trusted by millions of fans and the partners we serve. minute media has offices in the us, uk, israel, brazil, asia and india. *for more information, visit* *www.minutemedia.com**.*\n\n  \n\nwe are seeking an analytical and tech\\-savvy analytics engineer/ bi developer to join our team, focusing on our b2b products and content cost analytics. you will bridge the gap between data and business for our publisher\\-facing solutions. in this role, you\u2019ll develop business intelligence solutions and insights that drive informed decisions for both minute media and our content partners. this includes building dashboards to track b2b product performance and analyzing content costs to help optimize our partnerships. if you enjoy turning data into actionable business intelligence and thrive in a fast\\-paced media environment, we\u2019d love to meet you.\n\n **what you'll do**\n\n**\ufeff**\n\n**content cost analytics \\& reporting**\n\n* analyze and report on content costs (licensing, contributors) to assess impact on revenue, margins, and content roi, and integrate these metrics into centralized data models and the single source of truth in partnership with data engineering.\n* automate monthly and quarterly content cost reporting by replacing manual spreadsheets with dynamic looker dashboards, providing finance and partnerships teams with real\\-time visibility into spend vs. returns.\n\n**bi development for partner solutions**\n\n* design and build scalable data sources, models, and feeds to support b2b analytics, including partner\\-facing reporting in voltax for content performance and earnings.\n* develop and maintain intuitive looker dashboards tracking key b2b kpis (e.g., video usage, syndication metrics, revenue share), ensuring data accuracy, consistency, and stakeholder trust.\n\n**insights \\& collaboration**\n\n* collaborate cross\\-functionally with product, finance, and partnerships to translate business questions into data analysis and actionable insights.\n* identify trends and optimization opportunities in partner usage and content costs, clearly communicating findings internally and occasionally supporting external partner reporting, enablement, and training.\n* proactively improve analytics offerings by staying current on bi and digital media trends and recommending new metrics or visualizations.\n\n  \n\nrequirements:  \n\n  \n\n**what you have**\n\n* 3\\+ years of experience in data analytics, business intelligence, or a related role, ideally in tech, media, or digital content environments.\n* strong sql skills with experience writing complex queries and transforming data in cloud data warehouses (e.g., bigquery, athena), along with a solid understanding of data modeling and etl processes (e.g., rivery or similar tools).\n* proven experience building dashboards and reports in bi tools, with looker strongly preferred (or tableau/power bi with ability to ramp quickly).\n* strong analytical mindset with excellent attention to detail, able to ensure data integrity, uncover insights, and balance hands\\-on technical work with high\\-level analysis and interpretation.\n* strong communication and presentation skills, capable of translating complex data findings into clear, business\\-friendly language and visuals.\n* self\\-motivated and curious, with a strong interest in understanding how products work and how data can drive improvement.\n* ba/bsc or master\u2019s degree in data science, analytics, computer science, or a related field (or equivalent practical experience in a data\\-focused role).\n\n **\ufeffbonus if you have**\n\n* familiarity with the digital media landscape and content economics, including content licensing models and performance metrics.\n* experience working on analytics for b2b or saas products, particularly involving data sharing with external partners or clients.\n* knowledge of voltax or similar online video platforms or content management systems from a data perspective.\n* exposure to financial or accounting concepts to support cost, revenue, and profitability analysis.\n* experience with collaborative analytics and documentation tools (e.g., cursor, data dictionaries, partner guides).\n* basic scripting or programming skills (python, r) for advanced data manipulation or extending bi capabilities.\n\n  \n\nthe expected salary range for this position is between $85,000 \\- $95,000 annually. actual pay will be determined based on skills, experience, and location. the benefits available for this position include a flexible vacation policy, 15 paid holidays, paid parental leave, health insurance, 401(k) retirement plan.\n\n\nnot sure that you're 100% qualified but are up for the challenge? we want you to apply!\n\n  \n\nminute media is committed to creating a diverse and inclusive work environment and is proud to be an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. minute media participates in e\\-verify.\n\n  \n\ncheck the url and email address of any correspondence with minute media. if it is not from **@**minutemedia.com, treat it as fraudulent. to learn how to protect yourself from recruitment fraud click here.\n\n  \n\nbrand:\n  \n\nminute media",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI / Data Engineer, AI/ML-powered IoT Platform",
        "company": "JIG-SAW US, Inc.",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "Pinecone",
            "TensorFlow",
            "PyTorch",
            "MLflow",
            "Git",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=48b6a9e62830f431",
        "description": "**who we are**\n\njig\\-saw operates 24/7 operations centers in japan and canada that proactively monitor systems, issue alerts, and deliver live incident response\u2014keeping web services and iot environments secure and running smoothly.\n\n* **corporate site:** https://jig\\-saw.com/en/\n* **neqto.ai:** https://neqto.ai/\n\n**about the role**\n\nwe are looking for a dual\\-threat **ai and data engineer** to build a game\\-changing, ai\\-driven iot platform. you won\u2019t just be building chatbots; you will be building the \"nervous system\" of industrial environments.\n\nthe core of this role is turning raw, often messy device signals into actionable intelligence. you will own the end\\-to\\-end pipeline: from integrating industrial protocols and normalizing high\\-velocity data streams to deploying rag\\-based insights and predictive ml models. if you enjoy the challenge of making intelligence observable, reliable, and scalable across factories, buildings, and logistics hubs, this role is for you.\n\n**key responsibilities**\n\n* **data pipeline architecture:** design and implement robust data ingestion and processing pipelines that receive, normalize, and transform raw data from diverse iot sources into structured formats for ai consumption.\n* **protocol integration:** implement and manage connectivity for industrial and wireless protocols including **modbus, bacnet, lorawan, and mqtt**.\n* **ai/ml development:** ship insight features combining **llm/rag** with classical ml for anomaly detection, forecasting, and predictive maintenance.\n* **data engineering at scale:** handle \"big data\" challenges\u2014managing time\\-series datasets, stream processing, and ensuring data integrity for downstream models.\n* **retrieval \\& orchestration:** implement robust retrieval pipelines (chunking, embeddings, reranking) and agentic workflows to turn business questions into measurable outcomes.\n* **production excellence:** own the loop end\\-to\\-end\u2014training, deploying, and monitoring models in production with clear kpis and guardrails.\n\n**required skills \\& qualifications**\n\n* **data engineering \\& processing:** expert\\-level experience in etl/elt, data normalization, and processing high\\-throughput streaming data.\n* **llms \\& rag:** deep understanding of langchain/llamaindex, agent frameworks, and tool orchestration for generating insights beyond simple text generation.\n* **ml/dl frameworks:** proven experience using **tensorflow**, pytorch, or scikit\\-learn for time\\-series forecasting and anomaly detection.\n* **cloud \\& vector infra:** proficiency in gcp/aws/azure and vector databases (e.g., pgvector, pinecone, milvus, or weaviate).\n* **education:** bachelor\u2019s degree or higher in computer science or a closely related technical field.\n\n**experience guidelines**\n\n* **5\\+ years** of professional **data engineering/ml** experience: building and operating production\\-grade platforms (ingestion, training, deployment, and monitoring).\n* **5\\+ years** of **industrial iot (iiot)** domain experience: working with smart factories, logistics, energy management, or smart buildings.\n* **3\\+ years** delivering **genai (llm/rag)** features for real\\-world users (beyond hobbyist projects).\n\n**nice\\-to\\-have**\n\n* **mlops:** experience with mlflow, w\\&b, or sagemaker for experiment tracking and model versioning.\n* **industrial connectivity:** hands\\-on experience integrating with **modbus (tcp/rtu), bacnet, lorawan,** and mqtt.\n* **advanced analytics:** experience with temporal fusion transformers (tft), prophet, or state\\-space models for time\\-series at scale.\n* **security:** knowledge of pii handling, rbac, and secure device onboarding.\n\n**compensation \\& benefits**\n\n* **remote work:** fully remote position.\n* **health \\& wellness:** comprehensive health, dental, and vision insurance.\n* **time off:** competitive pto (vacation, sick leave, and company holidays).\n* **salary:** competitive market\\-based salary dependent on location and experience.\n\n**portfolio required:** please include links to your github/gitlab, product sites, or technical case studies. applications without a portfolio will not be considered.\n\njob type: full\\-time\n\npay: $100,000\\.00 \\- $150,000\\.00 per year\n\nbenefits:\n\n* dental insurance\n* health insurance\n* paid time off\n* vision insurance\n\napplication question(s):\n\n* \\[required] this is the first\\-round screening question \\#1\\. if this field is left blank, you will not be considered.\n\nthe key evaluation criterion is whether you can explain your reasoning logically. we value how you think more than what you already know. if a response is detected to have been generated using chatgpt or an equivalent tool, it will be automatically rejected.\n\nfor the function below, propose technical approaches you would use to implement it. assume real production use: consider performance, scalability, operating cost, and llm context\\-window limits.\n\non an iot data dashboard, display:\n\n1\\) insights through the previous day (e.g., findings, early signs of failure, predictive maintenance).\n\n2\\) a list of alerts through the previous day (not simple threshold checks, but early failure signs and predictive maintenance).\n\n* \\[required] this is the first\\-round screening question \\#2\\. if this field is left blank, you will not be considered.\n\nthe key evaluation criterion is the same as \\#1\\.\n\nfor the function below, propose technical approaches you would use to implement it. assume real production use: consider performance, scalability, operating cost, and llm context\\-window limits.\n\nenable users to obtain on\\-demand insights (findings, early signs of failure, predictive maintenance) by asking a chatbot, based on all data collected to date.\n\n* \\[required] this is the first\\-round screening question \\#3\\. if this field is left blank, you will not be considered.\n\nthe key evaluation criterion is the same as \\#1\\.\n\nfor the function below, propose technical approaches you would use to implement it. assume real production use: consider performance, scalability, operating cost, and llm context\\-window limits.\n\nfrom the data format sent by unknown iot devices, auto\\-register/provision devices.\n\n1\\) automatically detect and register the device with its metadata when multiple sensor values compressed/obfuscated by encoding them as binary/hex.\n\n2\\) automatically detect and register the device with its metadata when multiple sensor values are human\\-readable (non\\-binary/non\\-hex) payloads.\n\n* \\[required] this is the first\\-round screening question \\#4\\. if this field is left blank, you will not be considered.\n\nthe key evaluation criterion is the same as \\#1\\.\n\nfrom the data format sent by unknown iot devices, auto\\-detect/map sensor attributes.\n\n1\\) automatically detect sensor attributes (e.g., temperature), normalize them, and associate attributes with the database scheme. assume multiple sensor values compressed/obfuscated by encoding them as binary/hex.\n\n2\\) automatically detect sensor attributes (e.g., temperature), normalize them, and associate attributes with the database scheme. assume multiple sensor values are human\\-readable (non\\-binary/non\\-hex) payloads.\n\n* \\[required] this is the first\\-round screening question \\#5\\. if this field is left blank, you will not be considered.\n\nthe key evaluation criterion is the same as \\#1\\.\n\nfor the function below, propose technical approaches you would use to implement it. assume real production use: consider performance, scalability, operating cost, and llm context\\-window limits.\n\nsettings recommendations: a feature that recommends configuration settings the user tends to prefer.\n\n* \\[required] please describe in detail your professional experience using generative ai in industrial iot environments such as smart factories, smart buildings, logistics, and energy management. if this field is left blank, you will not be considered.\n* \\[required] please describe in detail your professional experience using machine learning in industrial iot environments such as smart factories, smart buildings, logistics, and energy management. if this field is left blank, you will not be considered.\n\neducation:\n\n* bachelor's (required)\n\nexperience:\n\n* professional ml (shipped, deployed, monitored): 5 years (required)\n* professional genai (llms/rag): 3 years (required)\n* industrial iot (e.g., smart factories, logistics): 5 years (required)\n\nwork location: remote",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Analytics Engineer",
        "company": "Minute Media",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Athena",
            "BigQuery",
            "Git",
            "BigQuery",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=78ea236ffc20e955",
        "description": "minute media is a global technology and content company built for the future of sports consumption. minute media's proprietary technology platform enables the creation, distribution and monetization of digital content experiences, which powers minute media's portfolio of trusted content brands, including sports illustrated, the players' tribune, and fansided as well as hundreds of sports leagues, teams, broadcasters, third\\-party publishers and advertisers. the technology platform includes, (i) stn video, an online video platform (ovp), which provides access to a robust sports highlights rights portfolio, and (ii) magnifi, an ai\\-driven saas platform, that enables rights holders to detect key moments and create highlights in real time. minute media is building the future of how the world connects with sport, powered by innovation, shared globally, and trusted by millions of fans and the partners we serve. minute media has offices in the us, uk, israel, brazil, asia and india. *for more information, visit* *www.minutemedia.com**.*\n\n  \n\nwe are seeking an analytical and tech\\-savvy analytics engineer/ bi developer to join our team, focusing on our b2b products and content cost analytics. you will bridge the gap between data and business for our publisher\\-facing solutions. in this role, you\u2019ll develop business intelligence solutions and insights that drive informed decisions for both minute media and our content partners. this includes building dashboards to track b2b product performance and analyzing content costs to help optimize our partnerships. if you enjoy turning data into actionable business intelligence and thrive in a fast\\-paced media environment, we\u2019d love to meet you.\n\n **what you'll do**\n\n**\ufeff**\n\n**content cost analytics \\& reporting**\n\n* analyze and report on content costs (licensing, contributors) to assess impact on revenue, margins, and content roi, and integrate these metrics into centralized data models and the single source of truth in partnership with data engineering.\n* automate monthly and quarterly content cost reporting by replacing manual spreadsheets with dynamic looker dashboards, providing finance and partnerships teams with real\\-time visibility into spend vs. returns.\n\n**bi development for partner solutions**\n\n* design and build scalable data sources, models, and feeds to support b2b analytics, including partner\\-facing reporting in voltax for content performance and earnings.\n* develop and maintain intuitive looker dashboards tracking key b2b kpis (e.g., video usage, syndication metrics, revenue share), ensuring data accuracy, consistency, and stakeholder trust.\n\n**insights \\& collaboration**\n\n* collaborate cross\\-functionally with product, finance, and partnerships to translate business questions into data analysis and actionable insights.\n* identify trends and optimization opportunities in partner usage and content costs, clearly communicating findings internally and occasionally supporting external partner reporting, enablement, and training.\n* proactively improve analytics offerings by staying current on bi and digital media trends and recommending new metrics or visualizations.\n\n  \n\nrequirements:  \n\n  \n\n**what you have**\n\n* 3\\+ years of experience in data analytics, business intelligence, or a related role, ideally in tech, media, or digital content environments.\n* strong sql skills with experience writing complex queries and transforming data in cloud data warehouses (e.g., bigquery, athena), along with a solid understanding of data modeling and etl processes (e.g., rivery or similar tools).\n* proven experience building dashboards and reports in bi tools, with looker strongly preferred (or tableau/power bi with ability to ramp quickly).\n* strong analytical mindset with excellent attention to detail, able to ensure data integrity, uncover insights, and balance hands\\-on technical work with high\\-level analysis and interpretation.\n* strong communication and presentation skills, capable of translating complex data findings into clear, business\\-friendly language and visuals.\n* self\\-motivated and curious, with a strong interest in understanding how products work and how data can drive improvement.\n* ba/bsc or master\u2019s degree in data science, analytics, computer science, or a related field (or equivalent practical experience in a data\\-focused role).\n\n **\ufeffbonus if you have**\n\n* familiarity with the digital media landscape and content economics, including content licensing models and performance metrics.\n* experience working on analytics for b2b or saas products, particularly involving data sharing with external partners or clients.\n* knowledge of voltax or similar online video platforms or content management systems from a data perspective.\n* exposure to financial or accounting concepts to support cost, revenue, and profitability analysis.\n* experience with collaborative analytics and documentation tools (e.g., cursor, data dictionaries, partner guides).\n* basic scripting or programming skills (python, r) for advanced data manipulation or extending bi capabilities.\n\n  \n\nthe expected salary range for this position is between $85,000 \\- $95,000 annually. actual pay will be determined based on skills, experience, and location. the benefits available for this position include a flexible vacation policy, 15 paid holidays, paid parental leave, health insurance, 401(k) retirement plan.\n\n\nnot sure that you're 100% qualified but are up for the challenge? we want you to apply!\n\n  \n\nminute media is committed to creating a diverse and inclusive work environment and is proud to be an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. minute media participates in e\\-verify.\n\n  \n\ncheck the url and email address of any correspondence with minute media. if it is not from **@**minutemedia.com, treat it as fraudulent. to learn how to protect yourself from recruitment fraud click here.\n\n  \n\nbrand:\n  \n\nminute media",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Engineer",
        "company": "CVS Health",
        "location": "Buffalo Grove, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "Git",
            "Kafka",
            "NoSQL",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d9295dd5dd4ae3b5",
        "description": "we\u2019re building a world of health around every individual \u2014 shaping a more connected, convenient and compassionate health experience. at cvs health\u00ae, you\u2019ll be surrounded by passionate colleagues who care deeply, innovate with purpose, hold ourselves accountable and prioritize safety and quality in everything we do. join us and be part of something bigger \u2013 helping to simplify health care one person, one family and one community at a time.\n\n\nif you\u2019re eager to make a real impact in the health care industry through your own meaningful contributions, explore a role in technology with cvs health. our journey calls for technical innovators and data visionaries: come help us pave the way.\n\n\nat cvs health, we possess an extensive repository of healthcare data that spans over 150 million individuals, providing an unparalleled foundation for ambitious data engineers. in this role, you will engage with complex business challenges, harnessing modern tools and technologies to securely store, process, transform, and enrich terabyte to petabyte scale healthcare data. your work will underpin data\\-driven business decisions and contribute to our mission of delivering industry\\-best data products / software with a customer\\-first mindset and team\\-oriented approach.\n\n\nas a senior data engineer, you will be instrumental in designing, developing, and maintaining optimal data pipelines to assemble large and intricate datasets, catering to the business requirements of various cvs lines of business. collaborating closely with teams, you will craft tools to provide actionable insights and integrate them with consumer touchpoints.\n\n\nas leaders in healthcare, our analytics and engineering teams deliver innovative solutions to business problems by collaborating with cross\\-functional teams in a dynamic and agile environment. you will be part of a team that values collaboration and encourages innovative thinking at all levels. you will be intellectually challenged to solve problems associated with large scale complex, structured and unstructured data, that will allow you to grow your technical skills and engineering expertise.\n\n\nthis role will work onsite in our buffalo grove, il office 3 days per week (every other week).\n\n**required qualifications**\n\n* 3\\+ years of experience with sql, nosql\n* 3\\+ years of experience with python\n* 3\\+ years of experience with data warehouses (such as data modeling and technical architectures) and infrastructure components\n* 3\\+ years of experience with etl/elt, and building high\\-volume data pipelines\n* 3\\+ years of experience with reporting/analytic tools\n* 3\\+ years of experience with query optimization, data structures, transformation, metadata, dependency, and workload management\n* 3\\+ years of experience with big data and cloud architecture\n* 3\\+ years of hands\\-on experience building modern data pipelines within a major cloud platform (preferably gcp, open to aws or azure)\n* 3\\+ years of experience with deployment/scaling of apps on containerized environment (i.e. kubernetes, aks)\n* 3\\+ years of experience with real\\-time and streaming technology (i.e. azure event hubs, azure functions, kafka, spark streaming)\n* 1\\+ year(s) of soliciting complex requirements and managing relationships with key stakeholders\n* 1\\+ year(s) of experience independently managing deliverables\n\n**preferred qualifications**\n\n* experience building agentic ai tools\n* experience with git, ci/cd pipeline, and other devops principles/best practices\n* experience with bash shell scripts, unix utilities \\& unix commands\n* understanding of software development methodologies including waterfall and agile\n* ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources\n* knowledge of api development\n* experience with complex systems and solving challenging analytical problems\n* strong collaboration and communication skills within and across teams\n* google professional data engineer certification\n* knowledge of microservices and soa\n* formal safe and/or agile experience\n* previous healthcare experience and domain knowledge\n\n**education**\n\n* bachelor\u2019s degree or equivalent work experience in computer science, information systems, data engineering, data analytics, machine learning, or related field required\n* master\u2019s degree preferred\n\n**anticipated weekly hours**\n\n\n40**time type**\n\n\nfull time**pay range**\n\n\nthe typical pay range for this role is:\n\n\n$101,970\\.00 \\- $203,940\\.00\nthis pay range represents the base hourly rate or base annual full\\-time salary for all positions in the job grade within which this position falls. the actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. this position is eligible for a cvs health bonus, commission or short\\-term incentive program in addition to the base pay range listed above.\n\n  \n\nour people fuel our future. our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\n**great benefits for great people**\n\n\nwe take pride in our comprehensive and competitive mix of pay and benefits \u2013 investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. in addition to our competitive wages, our great benefits include:\n\n* **affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan**.\n* **no\\-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n* **benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\n\n\nfor more information, visit https://jobs.cvshealth.com/us/en/benefits\n\n\nwe anticipate the application window for this opening will close on: 03/30/2026\nqualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Full Stack Software Engineer",
        "company": "Vontier",
        "location": "Remote, US USA",
        "posted_at": "2026-02-13",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Data Lake",
            "CI/CD",
            "Git",
            "Snowflake",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8fa27cef936c5867",
        "description": "**introduction and what you will do (job responsibilities)**\n\n\nthis position is responsible for designing, implementing, testing, deploying, and supporting full stack software solutions for angi energy systems\u2019 remote monitoring platform serving cng, rng, and hydrogen refueling stations. the full stack software engineer will contribute across web applications, backend services, real\\-time data ingestion, cloud messaging, and analytics enablement.\n\n\nthe platform ingests equipment telemetry and alarms from industrial control systems, supports live monitoring and operational workflows, and provides reporting and historical views for customers and internal support teams. the role will support both an existing legacy reporting pipeline and an evolving cloud\\-native architecture using modern iot messaging patterns, serverless compute, managed message queues, and cloud data platforms. the role will also contribute to building an analytics foundation that consolidates equipment telemetry with service/support system data to enable reliability insights, reporting, and future predictive/correlative models.\n\n**responsibilities**\n\n**software development \u2013 web application**\n\n* design, develop, and sustain features in customer\\- and internal\\-facing web applications (dashboards, station views, alarms, reporting, administration tools).\n* implement secure, intuitive user experiences with appropriate authentication/authorization patterns.\n* improve live monitoring capabilities by transitioning from legacy screen\\-scraped/derived data views to structured, service\\-backed live data models.\n* collaborate with cross\\-functional stakeholders to refine requirements and deliver iterative improvements.\n\n**software development \u2013 backend services / apis**\n\n* design and implement backend services and apis that support monitoring, reporting, configuration, and operational workflows.\n* develop data access patterns and schemas in a relational database environment, with emphasis on correctness, performance, and maintainability.\n* implement robust error handling, retries, and idempotent processing where required for event\\-driven systems.\n\n**real\\-time data ingestion and event\\-driven processing**\n\n* build and support near real\\-time ingestion pipelines that process telemetry and alarm events from cloud iot messaging endpoints.\n* validate, transform, and persist event data for downstream reporting and operational use.\n* implement secure device\\-to\\-cloud communication patterns under real\\-world constraints.\n* support cloud\\-to\\-device command patterns for remote operational workflows, with auditing, safeguards, and clear operator feedback.\n\n**notifications and workflow automation**\n\n* build and enhance notification and escalation workflows for alarms and operational events.\n* integrate with enterprise communication platforms and apis to deliver email/notification capabilities and manage recipient rules.\n* use managed messaging infrastructure to decouple producers/consumers and improve reliability.\n\n**data \\& analytics enablement**\n\n* contribute to a cloud\\-based data lake/warehouse to ingest and model equipment telemetry and related operational data.\n* integrate external service/support platform data via apis for cross\\-domain analytics.\n* support data quality, governance, and observability to enable reliable reporting predictive/correlative model development.\n* enable advanced analytics and ai initiatives by preparing curated, well\\-documented datasets suitable for feature engineering and model training (e.g., failure prediction, anomaly detection, alarm correlation, maintenance optimization).\n* collaborate with engineering, service, and analytics stakeholders to operationalize model outputs into applications and workflows (e.g., surfacing risk scores, recommended actions, and alerts), including feedback loops to evaluate and continuously improve model performance over time.\n\n**quality, reliability, and continuous improvement**\n\n* utilize industry\\-standard source control, code review, documentation, and ci/cd practices.\n* implement appropriate automated testing (unit/integration) and monitoring/observability (structured logging, metrics, tracing).\n* participate in production support and incident response; drive root\\-cause analysis and corrective actions to reduce repeat issues.\n* contribute to technical design standards, documentation, and continuous improvement initiatives.\n\n**who you are (qualifications)**\n\n\nbachelor\u2019s degree in software engineering, computer science, or related technical field (bs) from a four\\-year college or university and at least 3 years of related experience and/or training in engineering and design and/or equivalent combination of education and experience.\n\n**essential**\n\n**specialized equipment used:**\n\n* engineering software:\n\t+ proficiency with the programming languages, protocols, and formats c\\#, python, sql, javascript, css, html, json, and mqtt.\n\t+ software ides\n\t+ version control, repository management, bug tracking, and documentation software (git, bitbucket, jira, confluence)\n\t+ microsoft azure\n\t+ snowflake\n\t+ sql server management studio\n\t+ networking systems and secure networking practices\n\n**accountability:**\n\n* quality and reliability of delivered software across web, backend services, and data pipelines.\n* correctness and timeliness of telemetry, alarm, and reporting data.\n* internal \\& external customer satisfaction (usability, supportability, responsiveness).\n* performance to schedule and delivery commitments.\n* process conformance (sdlc, security, documentation, operational readiness).\n* rework costs and reduction of repeat defects/operational incidents.\n\n**other skills and abilities:**\n\n* has knowledge of commonly used concepts, practices and procedures regarding:\n\t+ compressed gas site, dispensing, compression, storage, and controls systems\n\t+ software design, development, and sustainment\n\t+ remote device connectivity and management\n\t+ cloud based applications and saas models\n\t+ training and utilizing ai models\n\t+ relational databases\n\t+ virtual machines\n\t+ serverless compute applications\n\t+ apis\n* relies on experience and judgment to perform the functions of the job.\n* performs a variety of tasks, including project engineer.\n* a certain degree of creativity and latitude is required.\n* works with little supervision. self\\-starting.\n\n\nthe base compensation range for this position is $85,000 to $110,000 per annum. your actual base salary will be determined based upon numerous factors which may include relevant experience, skills, location (labor market data), credentials (education, certifications), and internal equity.\n\n\nfor this specific role, you may be eligible to participate in an annual bonus plan.\n\n\n\\#li\\-lp1 \\#li\\-remote \\#findyourpath \\#fuelyourpassion\n\n\nvontier partners with you and your family on your health and wellness journey. visit vontierbenefits.com to view our benefits. we offer a premium suite of health and wellness programs for you and your family, including medical, dental, vision, disability and life insurance. with programs for family planning from maven clinic to managing diabetes like livongo, coverage for women's health, support for adult and elder care, paid parental leave, a generous 401(k) plan with matching company contributions, and more. vontier is here for all stages of life. we also offer paid time off up to 15 days each year, 12 paid holidays (including 2 floating holidays), and paid sick leave.\\*\n\n**disclaimer:** the salary, other compensation, and benefits information is accurate as of the date of this posting. sick leave amount may vary based on state or local laws applicable to the applicant\u2019s geographic location. the company reserves the right to modify this information at any time, subject to applicable law.\n\n**who is angi**\n\n\nangi energy systems llc (\u2018angi\u2019), a vontier company, is a u.s. based manufacturer of quality engineered gas compression equipment and a leading supplier of compressed natural gas (cng) and renewable natural gas (rng) equipment and systems. angi has a longstanding reputation as a leader and innovator in both the compression and natural gas vehicle (ngv) refueling industries and has over 40 years of experience providing worldwide clients with high quality products and services. in 2022 angi launched its expanded alternative energy platform offering, to include hydrogen refueling station (hrs) solutions as it harnesses its unique position as a multi\\-energy systems supplier ready to support global clients in their decarbonization programs.\n\n\n angi sits within vontier\u2019s alternative energy and sustainable fleets platform, which is focused on providing innovative and sustainable solutions for optimizing and decarbonizing the fleet industry. sister companies include gasboy, teletrac navman, driivz and sparkion. for more information on angi\u2019s alternative energy solutions, visit angienergy.com.\n\n**who is vontier**  \n\nvontier (nyse: vnt) is a global technology company powering the way the world moves. we empower businesses in the transport sector to adapt to a fast\\-changing landscape by uniting productivity, automation and multi\\-energy technologies.\n\n\nour smart, connected solutions serve roadside convenience retail stores, fleet operators, and auto repair technicians. from integrated payments and ev charging software to carwash technology and retail automation, we help customers stay productive and prepared for a rapidly evolving industry.  \n\n  \n\nwith decades of expertise and a balanced portfolio, vontier enables businesses to navigate complexity, unlock growth, and build a cleaner, safer future. driven by continuous improvement and the dedication of team vontier, we empower businesses to think bigger, act boldly, and thrive on the road ahead. learn more at www.vontier.com  \n\n  \n\n**at vontier, we empower you to steer your career in the direction of success with a dynamic, innovative, and inclusive environment.**  \n\n  \n\nour commitment to personal growth, work\\-life balance, and collaboration fuels a culture where your contributions drive meaningful change. we provide the roadmap for continuous learning, allowing creativity to flourish and ideas to accelerate into impactful solutions that contribute to a sustainable future.  \n\n  \n\njoin our community of passionate people working together to navigate challenges and seize new opportunities. at vontier, you are not on this journey alone, we are committed to equipping you with the tools and support you need to fuel your innovation, lead with impact, and thrive both personally and professionally.  \n\n  \n\n**together, let\u2019s power the way the world moves!**\n\n\n\"vontier companies are equal employment employers and evaluate qualified applicants without regard to race, color, national origin, religion, ancestry, sex (including pregnancy, childbirth and related medical conditions), age, marital status, sexual orientation, gender identity or expression, and other characteristics protected by law.\"",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Marketing Data Scientist",
        "company": "webuyanycar.com",
        "location": "Springfield, PA, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "Hadoop",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=bf0786d2dcdf32b3",
        "description": "**why join us**\n\n* direct impact on company growth and customer acquisition strategy.\n* opportunity to shape the evolution of data\\-driven decision\\-making across marketing.\n* competitive salary and comprehensive benefits package including medical, dental, vision, 401(k) with company match, paid time off, parental leave, and employee vehicle programs.\n* collaborative and inclusive culture that values innovation and diverse perspectives.\n\n **get to know us!**\n\n\ncargroup holdings llc, operating as webuyanycar.com\u00ae usa, is a leading car buying service focused on delivering a fast, safe, and transparent selling experience. currently, we are investing in advanced analytics to optimize marketing performance and drive profitable growth.\n\n**the role**\n\n\nthe marketing data scientist will own the analytical and modeling strategy that powers customer acquisition and marketing investment decisions. this role is responsible for transforming large\\-scale marketing and customer data into predictive models, experimentation frameworks, and decision systems that directly impact revenue, customer lifetime value, and return on marketing spend.\n\n\nthis position plays a critical role in optimizing multi\\-channel acquisition strategy and influencing company\\-wide growth initiatives.\n\n**we are unable to offer sponsorship for this position.**\n\n\n\\-\n\n**what you will do**\n\n* design and deploy predictive models that optimize customer acquisition, lifetime value (ltv), and marketing spend allocation across channels.\n* develop scalable machine learning solutions that improve targeting, segmentation, and personalization.\n* build and refine experimentation frameworks, including a/b testing and causal inference methodologies, to measure and validate marketing impact.\n* partner with marketing, finance, and executive leadership to align data insights with profitability and growth objectives.\n* identify opportunities to drive measurable incremental revenue through advanced analytics and modeling.\n* create clear, executive\\-ready insights that translate complex analysis into actionable strategy.\n* establish best practices for marketing analytics, model monitoring, and continuous optimization.\n* collaborate cross\\-functionally with data engineering and it to ensure data integrity, accessibility, and scalability.\n\n\n\\-\n\n**what we\u2019re looking for**\n\n* 3\\+ years of experience in data science, marketing analytics, or a related quantitative role within a commercial business, preferably consumer\\-facing.\n* strong proficiency in python or r and sql and experience with statistical analysis tools (e.g., sas, spss, or similar).\n* experience building and deploying machine learning models in production environments.\n* extensive practical expertise in predictive modeling, regression analysis, clustering, decision trees, neural networks, and classification methods, along with proficiency in frameworks such as tensorflow and scikit\\-learn.\n* skilled in using data visualization tools like tableau and power bi, as well as database query languages such as sql.\n* proficiency in big data technologies like hadoop and spark, as well as cloud platforms such as aws, azure, and google cloud.\n* experience designing and analyzing a/b tests and applying statistical methods for causal inference.\n* deep understanding of marketing performance metrics such as cac, ltv, roi, and attribution.\n* strong business acumen with a demonstrated ability to translate analytics into revenue impact.\n* excellent communication skills with the ability to interpret and communicate complex data analysis results in order to influence non\\-technical stakeholders.\n\n  \n\nadvanced degree (master\u2019s or phd) in data science, statistics, mathematics, engineering, or related quantitative field preferred.\n\n  \n\n\\-\n\n**physical requirements:**\n\n* prolonged periods of sitting at a desk and working on a computer.\n* must be able to lift to 15 pounds at times.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist",
        "company": "Merz Aesthetics",
        "location": "Racine, WI, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "Copilot",
            "Prompt Engineering",
            "CI/CD",
            "Snowflake",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=40b30fc35291d092",
        "description": "senior data scientist\nlocation\nracine, wi, usa\ncategory\nother\njob id\nsenio004540\njob type\nfull time\nposted date\n 02/23/2026\n  \nabout us\n\n\n\nfounded in 1908, merz is a successful, family\\-owned specialty healthcare company with a rich history. as a leading global aesthetics business, our award\\-winning portfolio of injectables, devices, and skincare products empowers healthcare professionals to enhance confidence through aesthetic medicine. our purpose is to fuel confidence by helping people look better, feel better, and live better. we believe you do not have to choose between living life and making a living. live your best life with merz aesthetics.\n\n  \n\n\n\na brief overview\n\n\n\nthis role serves as the organization's first dedicated data scientist for manufacturing analytics and the unified namespace (uns). the role is responsible for establishing a scalable analytics foundation, harmonizing data across operational and enterprise systems, developing robust data models, delivering high value kpis and dashboards, and continuously enhancing data quality, speed, and compliance within a regulated environment.\n\n  \n\n\n\nwhat you will do\n\n\n* uns management own and advance the uns to deliver a centralized, structured, real\\-time and historical view of operational data,define and maintain canonical data models, naming conventions, and data standards across ot and enterprise systems, andensure consistency in data lineage, metadata, and contextualization across hmi/plc, scada, mes, qms, erp, and complaints systems.\n* data modeling and analytics develop and maintain semantic and analytical data models, build production\\-quality kpi dashboards for common operational concepts such as oee, yield, scrap, cycle time, etc.,anddevelop and maintain dashboards presenting real\\-time visualization of on\\-going manufacturing operations.\n* data quality \\& governance implement data quality checks, data contracts, and ongoing monitoring frameworks, andcollaborate with quality and regulatory teams to ensure adherence to gxp data integrity principles (alcoa\\+), audit trails, csv/gamp 5, 21 cfr part 11/820, and iso 13485 requirements.\n* cross\\-functional analytics enablement translate business, operations, and quality challenges or requirements into actionable analytics solutions,develop self\\-service semantic layers to empower business users with trusted, curated data, and identify and recommend new data sources, sensors, and integration patterns to close information gaps.\n* tooling, standards, \\& acceleration build and maintain a practical analytics toolkit, including versioned notebooks, templates, and a governed metrics layer andpartner with data engineering to contribute to ci/cd practices.\n\n\nminimum requirements\n\n\n* bachelor's degree data science\n* bachelor's degree computer science\n* 5\\+ years analytics, data science, or advanced analytics engineering, including at least 2 years in manufacturing, ot/iiot, or regulated environments.\n* 3\\-5 years practical expertise with: uns/iiot frameworks (amqp, opc ua), including contextualization of hmi/plc/scada/mes data. data modeling techniques (kimball/dimensional), sql, python, and bi platforms (power bi/tableau). data quality, governance, lineage, cataloging, and metric definitions.\n* 3\\-5 years knowledge of gxp/csv, alcoa\\+, and regulated analytics delivery requirements.\n\n\npreferred qualifications\n\n\n* working with plc, mes, qms, and erp data sources.\n* working with ignition, litmus, and fuuz.\n* developing, fine tuning, or prompt engineering large language models (e.g., claude, copilot) for domain specific tasks.\n* hands on experience with model evaluation frameworks (e.g., prompt testing, hallucination detection, benchmark scoring, adversarial testing).\n\n\ntechnical \\& functional skills\n\n\n* sql and python for analytics, data preparation, and automation\n* dimensional modeling (kimball), star/snowflake schemas\n* semantic model development for bi and self service analytics\n* contextualization of hmi/plc/scada/mes data\n* time series analysis and historian integration\n* statistical analysis, spc, msa\n* predictive modeling and forecasting\n* data contracts, lineage, cataloging, and metadata management\n* automated data quality rules and monitoring\n* in\\-depth knowledge of the above mentioned skills \\& capability to apply best practices and integrate business knowledge with the own area\n* strategic thinking \\& ability to solve complex problems by applying new perspective\n* ability to explain difficult and/or sensitive information in a clear and structured way \\& adapt communication style to different audiences\n* good ms office package skills \\& familiarity with the most common av technology\n\n\nbenefits:\n\n\n* comprehensive medical, dental, and vision plans\n* 20 days of paid time off\n* 15 paid holidays\n* paid sick leave\n* paid parental leave\n* 401(k)\n* employee bonuses\n* and more!\n\n\n your benefits and pto start the date you're hired with no waiting period!\n\n\n\nthis position is not eligible for employer\\-sponsored work authorization. applicants must be legally authorized to work in the united states without the need for current or future employer\\-sponsored work authorization.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Machine Learning Engineer",
        "company": "Zendesk",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Snowflake",
            "Power BI",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b808411d8487a58f",
        "description": "**job description**\n-------------------\n\nrole: machine learning engineer\n\n\ntbh\\#:\n\n\nlocation: us  \n\n\n\n  \n\nzendesk\u2019s people have one goal in mind: to make customer experience better.  \n\n\n\n  \n\nthe enterprise machine learning team drives organizational value through scalable ml solutions and data\\-driven insights, fundamentally changing how business decisions are made. we collaborate closely with stakeholders, applying the latest advances in machine learning, deep learning, and large language models (llms) to create highly impactful outcomes. our commitment is to advance the state of ai, statistical modeling, and robust system design to enhance and expand our core business capabilities.\n\n\n### **role overview**\n\nas a machine learning engineer, you will serve as a technical and strategic member within the team, driving the development and deployment of advanced data science and machine learning solutions\u2014particularly those harnessing llms and deep learning. you will architect and scale ml systems, foster effective cross\\-functional collaborations, and ensure that business value is embedded in every technical decision. your business acumen allows you to translate complex analytical approaches into actionable insights and stakeholder\\-friendly narratives, strengthening partnership and adoption across the enterprise.\n\n\n### **key responsibilities**\n\n* drive the design, development, and deployment of advanced ml and ai solutions, with an emphasis on large language models (llms), deep learning architectures, and sophisticated statistical modeling.\n* build scalable, robust data science systems\u2014from data ingestion, data curation, data modeling to algorithm development, model deployment and monitoring\u2014meeting enterprise\\-grade performance, reliability, and compliance standards.\n* act as a subject matter expert, collaborating with data scientists, ml engineers, analysts, and business stakeholders to understand needs, define requirements, and deliver practical solutions with measurable business impact.\n* effectively articulate complex technical concepts to non\\-technical partners, bridging gaps between technical teams and business operations for maximum results.\n* drive adoption of best practices in mlops, including ci/cd pipelines, containerization, orchestration, observability, and reproducibility.\n* oversee and enhance the integrity, security, and compliance of all data science workflows and contracts.\n* stay abreast of the latest industry advancements in ml, llms, deep learning, cloud data engineering, and mlops solutions (aws, kubernetes, snowflake, etc.).\n* fostering technical excellence and ensuring alignment with business objectives.\n\n### **what we\u2019re looking for**\n\n* education \\& experience:\n\n\n\n\t+ 3\\+ years\u2019 experience in data science, machine learning, or a related field\n\t+ ba/bs in computer science, data science, or related discipline (advanced degree is highly preferred)\n* technical expertise:\n\n\n\n\t+ deep expertise in statistical modeling, machine learning, and deep learning (including practical experience with llms and transformers)\n\t+ strong programming skills (python preferred; java, scala, or similar also valued)\n\t+ proven ability to build and optimize scalable data science solutions\u2014end\\-to\\-end\u2014from data pipelines (dbt, astronomer, snowflake, aws) to deployment and monitoring (docker, kubernetes, ci/cd, mlops best practices)\n\t+ experience handling and analyzing large datasets, with a preference for experience in cloud data warehouses (snowflake)\n* business acumen:\n\n\n\n\t+ demonstrated success in translating business needs into analytical solutions, driving quantifiable impact\n\t+ strong stakeholder engagement skills, with a track record of building trusted business partnerships and driving adoption of data science initiatives\n* communication \\& collaboration:\n\n\n\n\t+ exceptional ability to simplify and communicate complex data science concepts to technical and non\\-technical audiences alike\n\t+ experience working cross\\-functionally with engineers, analysts, and product leaders\n\t+ steadfast commitment to continuous learning, collaboration, and fostering an inclusive, innovative team environment\n\n### **why you\u2019ll thrive here**\n\n* opportunity to develop and scale of llm and deep learning solutions with real\\-world business impact\n* an environment that values innovation, ownership, and professional growth\n* the chance to work on high\\-visibility, high\\-impact projects at scale alongside a passionate multidisciplinary team\n\n  \n\n  \n\nzendesk builds software for better customer relationships. it empowers organizations to improve customer engagement and better understand their customers. zendesk products are easy to use and implement. they give organizations the flexibility to move quickly, focus on innovation, and scale with their growth. based in san francisco, zendesk has operations in the united states, europe, asia, australia, and south america. learn more at www.zendesk.com .  \n\n\n\n  \n\nindividuals seeking employment at zendesk are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, or sexual orientation.\n\n\nthe us annualized base salary range for this position is $206,000\\.00\\-$308,000\\.00\\. this position may also be eligible for bonus, benefits, or related incentives. while this range reflects the minimum and maximum value for new hire salaries for the position across all us locations, the offer for the successful candidate for this position will be based on job related capabilities, applicable experience, and other factors such as work location. please note that the compensation details listed in us role postings reflect the base salary only (or ote for commissions based roles), and do not include bonus, benefits, or related incentives.\nhybrid: in this role, our hybrid experience is designed at the team level to give you a rich onsite experience packed with connection, collaboration, learning, and celebration \\- while also giving you flexibility to work remotely for part of the week. this role must attend our local office for part of the week. the specific in\\-office schedule is to be determined by the hiring manager.\n\n\n**the intelligent heart of customer experience**\n\nzendesk software was built to bring a sense of calm to the chaotic world of customer service. today we power billions of conversations with brands you know and love.\n\n\nzendesk believes in offering our people a fulfilling and inclusive experience. our hybrid way of working, enables us to purposefully come together in person, at one of our many zendesk offices around the world, to connect, collaborate and learn whilst also giving our people the flexibility to work remotely for part of the week.\n\n\nas part of our commitment to fairness and transparency, we inform all applicants that artificial intelligence (ai) or automated decision systems may be used to screen or evaluate applications for this position, in accordance with company guidelines and applicable law.\n\n\nzendesk is an equal opportunity employer, and we\u2019re proud of our ongoing efforts to foster global diversity, equity, \\& inclusion in the workplace. individuals seeking employment and employees at zendesk are considered without regard to race, color, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, disability, military or veteran status, or any other characteristic protected by applicable law. we are an aa/eeo/veterans/disabled employer. if you are based in the united states and would like more information about your eeo rights under the law, please click here .\n\n\nzendesk endeavors to make reasonable accommodations for applicants with disabilities and disabled veterans pursuant to applicable federal and state law. if you are an individual with a disability and require a reasonable accommodation to submit this application, complete any pre\\-employment testing, or otherwise participate in the employee selection process, please send an e\\-mail to peopleandplaces@zendesk.com with your specific accommodation request.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Developer 4 - Redwood City, CA (in-office role)",
        "company": "Oracle",
        "location": "Redwood City, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "XGBoost",
            "MLflow",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "PySpark",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=701b0e63a9a8e050",
        "description": "**\\*please note, this is an in\\-office role in oracle's redwood city, ca office.**\n\nai/ml engineer with hands\\-on experience to build production\\-grade multi\\-agent ai applications, agentic rag knowledge assistants, and scalable ml pipelines. candidate should have strong applied ai background in distributed reasoning, retrieval\\-augmented architectures, and cloud\\-native deployment (kubernetes, docker, fastapi), plus data science experience with large\\-scale consumer data modeling. good to have experience experience with impact improving reliability, latency, and model quality (e.g., reduced hallucinations, faster inference, higher multi\\-hop accuracy)\n\n\n  \nas part of oracle applied ai team, you will work on below list of technologies\n\n\n* llm \\& agentic systems: multi\\-agent orchestration, tool use, routing, verification/refinement loops; react/cot prompting and synthetic data generation; self\\-consistency distillation\n* rag / knowledge automation: graphrag, vector dbs (qdrant, chroma), semantic chunking, query rewriting, reranking, grounded response generation\n* model training \\& optimization: peft/lora, bitsandbytes quantization, onnx runtime optimization; experimentation tracking and reproducibility (weights \\& biases, mlflow)\n* data science \\& ml at scale: pyspark pipelines, feature selection (pca, clustering, xgboost\\-based screening, rfe), oot validation, drift monitoring (psi/iv, shap shift), auroc optimization\n* backend / cloud\\-native engineering: fastapi, pydantic, grpc/protobuf, microservices patterns, kubernetes/docker, service mesh (istio), ci/cd and monitoring (prometheus/grafana)",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Cloud Engineer II",
        "company": "GSK",
        "location": "Philadelphia, PA, US USA",
        "posted_at": "2026-02-19",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3258391b26d34a0f",
        "description": "**site name:** upper providence, cambridge 300 technology square, philadelphia walnut street  \n\n**posted date:** feb 23 2026  \n\nthe onyx research data platform organization represents a major investment by gsk r\\&d and digital \\& tech, designed to deliver a step\\-change in our ability to leverage data, knowledge, and prediction to find new medicines.\n\n\nwe are a full\\-stack shop consisting of product and portfolio leadership, data engineering, infrastructure and devops, data / metadata / knowledge platforms, and ai/ml and analysis platforms, all geared toward:\n\n* building a next\\-generation, metadata\\- and automation\\-driven data experience for gsk\u2019s scientists, engineers, and decision\\-makers, increasing productivity and reducing time spent on \u201cdata mechanics\u201d\n* providing best\\-in\\-class ai/ml and data analysis environments to accelerate our predictive capabilities and attract top\\-tier talent\n* aggressively engineering our data at scale, as one unified asset, to unlock the value of our unique collection of data and predictions in real\\-time\n\n\nour cloud infrastructure team is dedicated to designing and maintaining a robust and scalable cloud platform that forms the backbone of our organization's digital ecosystem. this team is responsible for implementing and managing core cloud services, ensuring seamless integration with other systems, and maintaining the overall reliability and availability of the platform. through collaboration with cross\\-functional teams, they drive the adoption of devops practices, automation, and continuous improvement initiatives to facilitate a smooth and agile development lifecycle.\n\n\na cloud engineer ii is a technical contributor who can consistently take a business or technical problem, work it to a well\\-defined specifications, and execute on it at a high level. they have a strong focus on metrics, both for the impact of their work and for its inner workings/operations. they are a model for the team on best practices for software development in general (and their specialization in particular), including code quality, documentation, devops, and testing. they ensure robustness of our services and serve as an escalation point in the operation of existing services, pipelines, and modules.\n\n\na cloud engineer ii should be familiar with the tools of their specialization and their customers and engaged with the open\\-source community surrounding them \u2013 potentially, even to the level of contributing pull requests\n\n**key responsibilities include:**\n=================================\n\n* design, implement, and manage infrastructure solutions based on best practices, ensuring scalability, reliability, and performance.\n* collaborate with cross\\-functional teams to deploy and operate applications and platforms in a cloud native environment.\n* develop and maintain ci/cd pipelines for automated infrastructure deployments.\n* automate infrastructure provisioning and deployment processes using infrastructure as code (iac) tools.\n* contribute to the creation and maintenance of documentation related to cloud architectures, configurations, and processes.\n* stay aligned with industry best practice in cloud infrastructure, continuously improving our architecture and security posture.\n* actively seek feedback, learn from experiences, and contribute to continuous improvement initiatives within the team\n\n**why you?**\n============\n\n**basic qualifications:**\n=========================\n\n\nwe are looking for professionals with these required skills to achieve our goals:\n\n* bachelor\u2019s degree in computer science, software engineering, or a related discipline with 4\\+ years of relevant experience \u2014 or a master\u2019s degree with 2\\+ years of relevant experience\n* experience with cloud platforms (e.g., google cloud, microsoft azure, aws), including infrastructure as code (terraform)\n* experience with ci/cd implementation (e.g., github actions, azure devops, or cloud build)\n* experience of agile software development environments\n* experience with at least one common programming language (e.g., python, rust, c\\+\\+, scala, go, java), including toolchains that support their eco\\-systems and operations such as documentation, testing, and operations/observability.\n* experience with declarative programming (e.g\\- terraform, ansible, chef)\n\n**preferred qualifications:**\n=============================\n\n\nif you have the following characteristics, it would be a plus:\n\n* experience in modern software development tools and ways of working (e.g. git/github, devops tools, metrics/monitoring, \u2026)\n* experience with docker, kubernetes, and the larger cncf ecosystem including experience with deployment tools such as helm\n* basic understanding of networking, virtualization, storage, containers, and serverless\n* experience using tools like jira and confluence\n* familiarity with tools, techniques, relevant to their specialization area, including engagement with open\\-source communities.\n\n\n\\#gsk\\-li \\#r\\&dtechproject\n\n\n\\#gskonyx\n\n* if you are based in cambridge, ma; waltham, ma; rockville, md; or san francisco, ca, the annual base salary for new hires in this position ranges $117,975 to $196,625\\.\n\n  \n\nthe us salary ranges take into account a number of factors including work location within the us market, the candidate\u2019s skills, experience, education level and the market rate for the role. in addition, this position offers an annual bonus and eligibility to participate in our share based long term incentive program which is dependent on the level of the role. available benefits include health care and other insurance benefits (for employee and family), retirement benefits, paid holidays, vacation, and paid caregiver/parental and medical leave.  \n\n  \n\nif salary ranges are not displayed in the job posting for a specific country, the relevant compensation will be discussed during the recruitment process.\nplease visit gsk us benefits summary to learn more about the comprehensive benefits program gsk offers us employees.\n\n\n**why gsk?**\n\n**uniting science, technology and talent to get ahead of disease together.**\n\n\ngsk is a global biopharma company with a purpose to unite science, technology and talent to get ahead of disease together. we aim to positively impact the health of 2\\.5 billion people by the end of the decade, as a successful, growing company where people can thrive. we get ahead of disease by preventing and treating it with innovation in specialty medicines and vaccines. we focus on four therapeutic areas: respiratory, immunology and inflammation; oncology; hiv; and infectious diseases \u2013 to impact health at scale.\n\n\npeople and patients around the world count on the medicines and vaccines we make, so we\u2019re committed to creating an environment where our people can thrive and focus on what matters most. our culture of being ambitious for patients, accountable for impact and doing the right thing is the foundation for how, together, we deliver for patients, shareholders and our people.\n\n\nshould you require any adjustments to our process to assist you in demonstrating your strengths and capabilities contact us at hr.americassc\\-cs@gsk.com where you can also request a call.\n\n\nplease note should your inquiry not relate to adjustments, we will not be able to support you through these channels. however, we have created a recruitment faq guide. click the link where you will find answers to multiple questions we receive\n\n\ngsk is an equal opportunity employer. this ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, genetic information (including family medical history), military service or any basis prohibited under federal, state or local law.\n\n**important notice to employment businesses/ agencies**\n\n\ngsk does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. all employment businesses/agencies are required to contact gsk's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to gsk. the obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and gsk. in the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of gsk. gsk shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n\n\nplease note that if you are a us licensed healthcare professional or healthcare professional as defined by the laws of the state issuing your license, gsk may be required to capture and report expenses gsk incurs, on your behalf, in the event you are afforded an interview for employment. this capture of applicable transfers of value is necessary to ensure gsk\u2019s compliance to all federal and state us transparency requirements. for more information, please visit the centers for medicare and medicaid services (cms) website at https://openpaymentsdata.cms.gov/",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Backend Engineer",
        "company": "Glassbox",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "Copilot",
            "Kubernetes",
            "Git",
            "Kafka",
            "Cassandra",
            "NoSQL",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a7eed342df94f18f",
        "description": "**senior backend engineer**\n===========================\n\n#### **remote \\- r\\&d \\- full\\-time \\- intermediate**\n\n\nrefer a friend\n\nglassbox is looking for a backend developer to join our global r\\&d team.  \n\nwe are glassbox, and our mission is to reveal the insights that empower organizations to deliver exceptional digital customer experiences.  \n\nwe are growing and have been recognized by g2 as one of 2024's top 50 software companies in the world.  \n\nour customers are the best of the best and include six out of the ten largest global banks, the world\u2019s largest hotel chain, the largest healthcare, and the largest telecommunications company in the u.s.  \n\nnow is the perfect time to come to glassbox and help us accelerate our global leadership position!  \n\nif you are a dynamic, successful, experienced metrics\\-driven leader, glassbox might be a great fit.  \n\nwill you join us on this journey?  \n\n  \n\n**what you will do**\n* design, develop, and maintain java\\-based backend systems that handle large\\-scale data processing and analysis\n* optimize existing systems for performance, scalability, and reliability\n* collaborate with cross\\-functional teams, including data scientists, frontend developers, and product managers, to define and implement new features and data\\-driven solutions\n* write high\\-quality, maintainable, and efficient code that adheres to best practices and coding standards\n* participate in code and design reviews to ensure quality and alignment with project requirements\n* troubleshoot and resolve issues in production systems\n* continuously learn and stay up to date with the latest industry trends, technologies, and frameworks\n* work in a scrum team using the scrum methodology\n\n**what you will need**\n* 4\\+ years of experience in java backend development\n* deep familiarity with spring\\-based ecosystems\n* experience working with micro services and data\\-intensive systems\n* knowledge of cloud platforms, such as aws, gcp, or azure\n* experience with nosql databases such as cassandra and relational databases as postgres\n* strong background with kubernetes, restful apis, and cloud\\-native architecture.\n* experience with distributed messaging systems like apache kafka, rabbitmq, or activemq.\n* familiarity with agile development methodologies and tools (e.g., scrum, jira, git)\n* strong problem\\-solving skills, attention to detail, and ability to work independently as well as in a team environment\n* excellent communication and interpersonal skills\n* clear and fluent english\n\n**advantage**\n* experience with big data technologies, such as spark, kafka, or elasticsearch\n* experience building or working with ai\\-enhanced platforms, copilot agents, or intelligent product features.\n* experience working with node.js",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Development Engineer in Test",
        "company": "Alteryx",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=01a62888fe35817c",
        "description": "we\u2019re looking for problem solvers, innovators, and dreamers who are searching for anything but business as usual. like us, you\u2019re a high performer who\u2019s an expert at your craft, constantly challenging the status quo. you value inclusivity and want to join a culture that empowers you to show up as your authentic self. you know that success hinges on commitment, that our differences make us stronger, and that the finish line is always sweeter when the whole team crosses together.\n\n\n**sdet responsibilities**\n\n* analyze requirements to produce a comprehensive test strategy and detailed test cases.\n* design and implement manual and automated tests at all levels in the test pyramid, including **unit, component, integration,** and **end\\-to\\-end** levels **.**\n* design, build, and enhance **scalable automated test frameworks** to support feature testing and ai\\-driven product workflows.\n* develop and maintain automation for **ui and api tests** using tools such as selenium, pytest, and playwright.\n* conduct **performance and load testing** (using tools like locust) to ensure product scalability.\n* build test reports using reporting tools such as **allure** .\n* report regressions to a corresponding dev team.\n* influence design decisions by providing timely **feedback** on design documents and testability.\n* **collaborate** closely with other members of an agile team, including engineers, product management, and ux, to become a valued member of an autonomous, cross\\-functional team.\n\nrequirements\n\n\n* **education** : bs/be/btech in computer science, or equivalent experience.\n* **experience** : 4\\+ years of experience as a software development engineer in test, qa engineer, or equivalent.\n* passion for delivering high\\-quality software with a focus on testability, robustness, and performance .\n* strong proficiency in **python** and **javascript** (both required) for test automation\n\n\n\n\t+ practical experience with automation frameworks such as **pytest** **,** **selenium** **, and** **playwright**\n\t+ experience with **locust** is a plus\n* strong understanding of **sdlc** and **testing pyramid** concepts.\n* experience developing or enhancing **automation frameworks** for ui/api/component/unit tests.\n* experience with **git** (must have) and **ci/cd systems** such as gitlab ci (preferred) or jenkins.\n* experience working on **ai products** with non\\-deterministic behavior is a strong plus.\n* experience with **docker** and **kubernetes** .\n* exposure to **cloud platforms** (aws required; gcp/azure nice to have).\n* familiarity with **unix/linux/mac os development environments** and **shell scripting (bash required)** .\n* **powershell** knowledge is a strong plus (for windows automation workflows).\n* understanding of **microservices\\-based web applications** including frontend, backend, and database layers.\n* ability to write complex **sql** queries.\n* experience with developing **ansible playbooks** is a plus.\n* excellent communication, organizational, and collaboration skills to work with global and cross\\-functional teams.\n\n**compensation**\n\nalteryx is committed to fair, equitable, and transparent compensation. final compensation is determined by several factors, including but not limited to relevant work experience, education, certifications, skills, and geographic location.\n\n\nthe base salary range for this role in the united states is **$103,300 \\- $133,850** . this role is also eligible for a target annual bonus of 10% of base salary, based on individual and company performance.\n\n\nin addition to base pay and bonus eligibility, this role includes clear forms of additional compensation, such as:\n\n\n* a monthly connectivity plus stipend of $150 to support remote work\\-related expenses\n* an annual $200 home office reimbursement\n\nalteryx offers a comprehensive benefits package designed to support your health, financial security, and overall well\\-being, including:\n\n\n* medical, dental, and vision coverage\n* 401(k) with company match\n* paid parental leave, caregiver leave, and flexible time off\n* mental health support and wellness reimbursement\n* career development and education assistance\n\nfind yourself checking a lot of these boxes but doubting whether you should apply? at alteryx, we support a growth mindset for our associates through all stages of their careers. if you meet some of the requirements and you share our values, we encourage you to apply. as part of our ongoing commitment to a diverse, equitable, and inclusive workplace, we\u2019re invested in building teams with a wide variety of backgrounds, identities, and experiences.\n\n\nfind yourself checking a lot of these boxes but doubting whether you should apply? at alteryx, we support a growth mindset for our associates through all stages of their careers. if you meet some of the requirements and you share our values, we encourage you to apply. as part of our ongoing commitment to a diverse, equitable, and inclusive workplace, we\u2019re invested in building teams with a wide variety of backgrounds, identities, and experiences .\n\n\n**benefits \\& perks:**\n\nalteryx has amazing benefits for all associates which can be viewed here .\n\n\nfor roles in san francisco and los angeles: pursuant to the san francisco fair chance ordinance and the los angeles fair chance initiative for hiring, alteryx will consider for employment qualified applicants with arrest and conviction records.\n\n\nthis position involves access to software/technology that is subject to u.s. export controls. any job offer made will be contingent upon the applicant\u2019s capacity to serve in compliance with u.s. export controls.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Just Appraised",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Docker",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "PostgreSQL",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b2c46b380ee0c308",
        "description": "just appraised is modernizing local government processes related to real estate and property taxes. our cutting\\-edge, ai\\-powered software, which leverages natural language processing (nlp), replaces manual data entry to eliminate delays, backlogs, and errors. with hundreds of government entities across the united states relying on us, your work will directly help drive essential revenues and provide critical services to society.\n\n\njoin a high\\-growth, yc\\-backed, and **fully remote** startup. as a full stack **senior software engineer** on an agile team, you'll have significant ownership in defining the technical future of our products. this is an opportunity to lead technical decisions and directly influence products making a real\\-world impact. **this is a full\\-time, salaried position.**\n\n **what you will work on**\n\n* design, develop, and maintain robust, scalable backend systems using technologies like java, postgresql, aws, docker, and terraform.\n* build and enhance front\\-end features using react and typescript to deliver an intuitive user experience.\n* collaborate with cross\\-functional teams (customer success, sales, operations) to define, design, and ship new features end\\-to\\-end.\n* work directly with customers and business stakeholders to translate needs into technical specifications and clear documentation.\n* optimize application performance, reliability, and scalability while writing clean, maintainable, and well\\-tested code.\n* proactively identify technical debt and performance bottlenecks, leading iterative product improvements.\n* mentor and support engineering team members to foster a culture of efficiency and growth.\n\n**what we\u2019re looking for**\n\n* 5\\+ years of professional software development experience, with strong proficiency in java programming\n* 5\\+ years of experience working with and optimizing relational databases (e.g., sql, postgresql).\n* proven experience developing scalable, modern front\\-end applications, preferably using react and typescript.\n* 2\\+ years of hands\\-on experience with cloud services, preferably amazon web services (aws).\n* strong command of api design, data modeling, object\\-oriented programming, and relational database design principles.\n* expertise in version control (e.g., git), containerization (e.g., docker), ci/cd pipelines (e.g., github actions), and modern automated testing frameworks.\n* experience in customer\\-facing product engineering (b2b or b2c).",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Data Architect",
        "company": "Bank of America",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "RAG",
            "Synapse",
            "Data Lake",
            "Snowflake",
            "Databricks",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3c087e00bef6a423",
        "description": "**job description:**\n\n\nat bank of america, we are guided by a common purpose to help make financial lives better through the power of every connection. we do this by driving responsible growth and delivering for our clients, teammates, communities and shareholders every day.  \n\n  \n\nbeing a great place to work is core to how we drive responsible growth. this includes our commitment to being an inclusive workplace, attracting and developing exceptional talent, supporting our teammates\u2019 physical, emotional, and financial wellness, recognizing and rewarding performance, and how we make an impact in the communities we serve.  \n\n  \n\nbank of america is committed to an in\\-office culture with specific requirements for office\\-based attendance and which allows for an appropriate level of flexibility for our teammates and businesses based on role\\-specific considerations.  \n\n  \n\nat bank of america, you can build a successful career with opportunities to learn, grow, and make an impact. join us!\n\n **job description:**  \n\nthis job is responsible for the execution of data architectural solutions for complex initiatives that span multiple lines of business and control functions. key responsibilities include facilitating solution driven discussions, working with stakeholders to support adherence to the enterprise data management policy and standards, and supporting architecture design reviews to ensure integration of data architecture principles in technology solutions. job expectations include educating data management teams on enterprise data architectural principles and data management processes and routines.\n\n\nwe are seeking an experienced **sr. data architect** with deep expertise in **ai controls, responsible ai, \\& enterprise data governance** to architect secure, compliant, and trustworthy ai ecosystems. this senior role is responsible for designing technical guardrails, assessing ai control capabilities across platforms and tools, and ensuring the safe adoption of ai technologies enterprise\u2011wide.\n\n\nthe ideal candidate combines advanced data/ai architecture experience with an expert understanding of ai risks, model governance, \\& the technical control frameworks required in regulated environments.\n\n**responsibilities:**\n\n* partners with various technology teams to establish data solutions and architecture for large, complex initiatives that align with the enterprise data architecture strategy and strategic technology and platform decisions\n* develops clear and concise responses to questions from senior management and control partners regarding the enterprise data architecture strategy\n* manages multiple priorities in a matrixed environment with attention to detail and accuracy\n* communicates effectively to influence agreement between partners and escalates items, as needed, to ensure execution activities remain on track\n* ensures all relevant risk, financial, and compliance policies and standards are met\n* manages relationships with business and technology partners and creates an inclusive and healthy working environment to resolve organizational impediments and blockers\n* educates data management teams on enterprise data architectural principles, processes, and routines\n\n**ai controls architecture \\& governance**\n\n* design \\& maintain the architectural framework for ai/ml controls, ensuring alignment with enterprise risk \\& compliance requirements.\n* establish standardized control patterns for data protection, access management, content filtering, prompt governance, and safeguard monitoring.\n* evaluate \\& implement tools for explainability, interpretability, red\u2011teaming, bias detection, drift monitoring, and auditability.\n* partner with risk, compliance, model risk management, and infosec to harmonize ai architectural controls with enterprise governance frameworks.\n\n**assessment of ai control capabilities**\n\n* **conduct formal assessments of ai platforms, tools, and frameworks to determine their control maturity, gaps, and alignment with enterprise standards.**\n* evaluate vendor ai capabilities (e.g., model apis, llm platforms, vector databases, ai orchestration tools) against security, privacy, and operational control requirements.\n* lead control readiness evaluations for new ai solutions, including rag pipelines, agents, llm components, and mlops/llmops platforms.\n* develop structured assessment criteria for ai guardrails such as:\n* data governance controls\n* model monitoring \\& explainability tooling\n* hallucination mitigation\n* prompt/response filtering\n* access and identity management\n* logging, auditability, \\& model lineage\n* provide recommendations, risk mitigation strategies, and architectural guidance based on assessment outcomes.\n\n**responsible ai, model controls \\& lifecycle governance**\n\n* embed responsible ai principles (explainability, fairness, transparency, robustness) into all ai architectural designs.\n* architect the full model lifecycle with built\\-in controls, including documentation, validation, approvals, testing, monitoring, and retirement.\n* develop controlled environments for training and deploying models, including versioning, lineage, reproducibility, isolation, and audit trails.\n* implement continuous monitoring frameworks for compliance, drift, bias, hallucinations, and performance degradation.\n\n**leadership \\& influence**\n\n* serve as the enterprise expert for ai controls, helping shape policy, standards, and long\\-term architecture strategy.\n* influence and guide senior leaders, engineers, and data scientists toward controlled, compliant ai adoption.\n\n**required qualification:**\n\n* **10\\+ years** in data architecture, ml/ai engineering, or enterprise architecture roles.\n* demonstrated experience **assessing ai control capabilities**, including evaluating vendors, platforms, and internal systems.\n* deep expertise in ai/ml governance, risk management, and responsible ai frameworks.\n* strong knowledge of enterprise data architecture, governance, lineage, metadata, and privacy controls.\n* hands\u2011on experience with modern data platforms (databricks, snowflake, azure data lake, synapse, etc.).\n* proficiency with ai infrastructure (vector databases, llm orchestration platforms, embeddings pipelines).\n* strong programming and data engineering skills (python, sql, spark).\n* experience architecting secure cloud\\-native solutions (azure preferred).\n\n**desired qualifications:**\n\n* experience in regulated industries (financial services, healthcare, insurance).\n* prior involvement with model risk governance or ai ethics programs.\n* certifications in cloud architecture, data engineering, or ai governance.\n* experience designing rag architectures, llmops workflows, and genai guardrail frameworks.\n\n*core competencies*\n\n* ai controls design and assessment\n* enterprise data and ai architecture\n* risk\\-based decision making and technology governance\n* clear communication with executives, auditors, and technical teams\n* leadership and ability to influence cross\\-functional groups\n\n*success measures*\n\n* enterprise\\-wide adoption of consistent and effective ai controls\n* measurable reduction in ai\\-related risks\n* strengthened auditability, transparency, and compliance of ai systems\n* delivery of scalable, secure, and responsible ai architectures\n* strong partnership with risk, compliance, legal, infosec, and engineering teams\n\n**skills:**\n\n* architecture\n* business analytics\n* critical thinking\n* data management\n* data visualization\n* consulting\n* data modeling\n* process design\n* regulatory compliance\n* risk management\n* cloud solutions\n* collaboration\n* data governance\n* talent development\n* technical strategy development\n\n**shift:**\n\n\n1st shift (united states of america)**hours per week:**\n\n\n40",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Intern the Otsuka Way 2026 - GenAI Engineer Intern",
        "company": "Otsuka",
        "location": "Princeton, NJ, US USA",
        "posted_at": "2026-02-24",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6e718096f3c0913a",
        "description": "otsuka\u2019s internship program, intern the otsuka way (intow), offers students a unique opportunity to gain hands\\-on experience and develop critical skills while contributing to meaningful projects. interns will work alongside experienced professionals, learn about the complexities of the pharmaceutical industry, and gain insights into potential career paths. by combining practical experience with professional development, our program prepares interns for future success. our internship program also includes instructor led trainings, a leadership engagement series, as well as insightful lunch and learns.\napplication deadline: candidates are encouraged to apply within 10 business days of the posting date. the position may remain open past this window until a suitable candidate is selected.\ninterviews and offers: february through march 2026\\.\nduration of internship: june 1, 2026, through august 10, 2026\\.  \n\njob description\nthe genai engineer intern will support the development and maturation of otsuka\u2019s ai platform, contributing to the creation, enhancement, and evaluation of ai agents and genai engineering components. this role offers hands\\-on exposure to cuttingedge generative ai technologies, including agentic workflows, llm testing, mcp (model context protocol) integrations, and scalable evaluation frameworks. the intern will play a key role in advancing safe, reliable, and enterprisegrade genai practices across the organization.  \n\nthis internship is ideal for students passionate about ai engineering, llm evaluation, and the emerging discipline of agentic ai systems, offering deep technical training and real\\-world application.  \n\nkey responsibilities:* build or enhance 1\u20132 ai agents/tools for the ai platform and develop corresponding usage documentation.\n* design and document a repeatable testing approach for genai components, including prompt robustness testing, evaluation harnesses, regression frameworks, and safety/guardrail validation.\n* prototype an mcp\\-based integration demonstrating standardized tool connectivity, evaluation checkpoints, and scalable agent\\-to\\-tool interactions.\n* collaborate with engineering team members to support platform improvements and contribute to genai engineering best practices.\n* assist with documentation, versioning, and git workflows to ensure clarity and reproducibility across engineering teams.\n* participate in experimentation, benchmarking, and iterative design of genai components and agent behaviors.\n\nminimum:* undergraduate or graduate student graduating in the next 12\\-18 months pursuing a degree in computer science, data science, or a related technical field.\n* basic proficiency in python, with familiarity using apis and git.\n* interest in or foundational knowledge of genai testing techniques, tool\u2011calling workflows, or agentic ai design patterns.\n* curiosity and enthusiasm for llms and hands\\-on experimentation with advanced ai tools.\n* authorization to work in the united states for the duration of the internship.\n* can commit to full duration of internship program.\n* authorization to work in the united states for the duration of the internship\n\nadditional knowledge, skills, and abilities:* familiarity with:\n* llm prompting, prompt engineering, or instruction following models\n* agent frameworks such as langchain, langgraph, or mcp\n* vector stores/rag architectures (e.g., pinecone, chroma, faiss)\n* cloud environments such as azure or aws\n* observability, logging, or evaluation frameworks for genai\n* strong technical writing skills and ability to document engineering workflows clearly and effectively.\n* demonstrates behaviors aligned with the otsuka corporate brand including perseverance, unconventional thinking, and humility.\n* demonstrate ability to work in a team environment.\n* demonstrate ability to think creatively to solve complex problems.\n* exhibits eagerness to learn and ability to learn quickly.\n  \n\ncompetencies  \n\naccountability for results \\- stay focused on key strategic objectives, be accountable for high standards of performance, and take an active role in leading change.  \n\nstrategic thinking \\& problem solving \\- make decisions considering the long\\-term impact to customers, patients, employees, and the business.  \n\npatient \\& customer centricity \\- maintain an ongoing focus on the needs of our customers and/or key stakeholders.  \n\nimpactful communication \\- communicate with logic, clarity, and respect. influence at all levels to achieve the best results for otsuka.  \n\nrespectful collaboration \\- seek and value others\u2019 perspectives and strive for diverse partnerships to enhance work toward common goals.  \n\nempowered development \\- play an active role in professional development as a business imperative.\nminimum $22\\.00 \\- maximum $28\\.00: the range shown represents a typical pay range or starting pay for individuals who are hired to perform the role in the united states. other elements may be used to determine actual pay such as the candidate\u2019s educational background and specific skills. typically, actual pay will be positioned within the established range rather than at its minimum or maximum. this information is provided to applicants in accordance with states and local laws.\napplication deadline: this will be posted for a minimum of 5 business days.\ndisclaimer:  \n\nthis job description is intended to describe the general nature and level of the work being performed by the people assigned to this position. it is not intended to include every job duty and responsibility specific to the position. otsuka reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.  \n\notsuka is an equal opportunity employer. all qualified applicants are encouraged to apply and will be given consideration for employment without regard to race, color, sex, gender identity or gender expression, sexual orientation, age, disability, religion, national origin, veteran status, marital status, or any other legally protected characteristic.  \n\nif you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation, if you are unable or limited in your ability to apply to this job opening as a result of your disability. you can request reasonable accommodations by contacting accommodation request.\nstatement regarding job recruiting fraud scams\nat otsuka we take security and protection of your personal information very seriously. please be aware individuals may approach you and falsely present themselves as our employees or representatives. they may use this false pretense to try to gain access to your personal information or acquire money from you by offering fictitious employment opportunities purportedly on our behalf.\nplease understand, otsuka will never ask for financial information of any kind or for payment of money during the job application process. we do not require any financial, credit card or bank account information and/or any payment of any kind to be considered for employment. we will also not offer you money to buy equipment, software, or for any other purpose during the job application process. if you are being asked to pay or offered money for equipment fees or some other application processing fee, even if claimed you will be reimbursed, this is not otsuka. these claims are fraudulent and you are strongly advised to exercise caution when you receive such an offer of employment.\notsuka will also never ask you to download a third\\-party application in order to communicate about a legitimate job opportunity. scammers may also send offers or claims from a fake email address or from yahoo, gmail, hotmail, etc, and not from an official otsuka email address. please take extra caution while examining such an email address, as the scammers may misspell an official otsuka email address and use a slightly modified version duplicating letters.\nto ensure that you are communicating about a legitimate job opportunity at otsuka, please only deal directly with otsuka through its official otsuka career website https://vhr\\-otsuka.wd1\\.myworkdayjobs.com/en\\-us/external.\notsuka will not be held liable or responsible for any claims, losses, damages or expenses resulting from job recruiting scams. if you suspect a position is fraudulent, please contact otsuka\u2019s call center at: 800\\-363\\-5670\\. if you believe you are the victim of fraud resulting from a job recruiting scam, please contact the fbi through the internet crime complaint center at: https://www.ic3\\.gov, or your local authorities.\notsuka america pharmaceutical inc., otsuka pharmaceutical development \\& commercialization, inc., and otsuka precision health, inc. (\u201cotsuka\u201d) does not accept unsolicited assistance from search firms for employment opportunities. all cvs/resumes submitted by search firms to any otsuka employee directly or through otsuka\u2019s application portal without a valid written search agreement in place for the position will be considered otsuka\u2019s sole property. no fee will be paid if a candidate is hired by otsuka as a result of an agency referral where no pre\\-existing agreement is in place. where agency agreements are in place, introductions are position specific. please, no phone calls or emails.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Developer",
        "company": "nan",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "RAG",
            "LLaMA",
            "Pinecone",
            "CI/CD",
            "Kafka",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=376268bed953895a",
        "description": "**ai developer / agentic ai engineer**\n\n**role summary**\n\n\nwe are building an agentic ai platform to transform commercial banking customer service. the ai developer will design, build, and operate llm\\-powered agents that interpret inbound servicing requests (e.g., email / case intake), retrieve grounded knowledge, and execute approved workflows through secure tool/api integrations \u2013 with enterprise\\-grade controls, observability, and human\\-in\\-the\\-loop patterns.\n\n\nthis role sits within a cross\\-functional team with product, operations, technology, and risk partners and focuses on delivering production\\-ready agentic ai capabilities for regulated financial services\n\n**responsibilities**\n\n\nagentic ai solution development\n\n* build and enhance llm/agent orchestration (planner/supervisor patterns, tool\\-using agents, routing, guardrails).\n* implement intent classification information extraction validation and decision logic for servicing workflows\n* developed tool calling integrations to downstream systems (crm, workflow engine, core banking services, case management)\n* implement human\\-in\\-the\\-loop workflows (review, approval, escalation, override) based on confidence/risk thresholds\n**knowledge and grounding (rag)**\n\n* design and implement retrieval\\-augmented generation (rag) for policy procedure grounding and resolution guidance\n* build knowledge ingestion pipelines with refresh/versioning\n* improve answer quality via chunking strategies, embeddings re ranking and context management\n\nquality, safety and evaluation\n\n* define and run evaluation frameworks: golden datasets, scenario tests, regression tests, and automated scoring.\n* reduce hallucinations and risk by implementing prompt policies, constraints, structured outputs, and verification steps.\n* partner with risk slash compliance to ensure traceability, audit logs, explain ability requirements are met.\n\nproduction readiness and operations\n\n* implement observability for agents (latency, cost, tool failures, drift, quality signals, escalation rates).\n* support ci/cd for agent prompts and configurations (versioning, approvals, rollback).\n* collaborate with platform and security teams on secrets management, access controls, pii protections, and safe deployments.\n\nrequired qualifications\n\n* 4\\+ years of software engineering experience or equivalent with strong cs fundamentals\n* hands\\-on experience building with llms and modern ai app stack (agents, rag, tool/function calling).\n* strong proficiency in python and building back\\-end services/apis.\n* experience with at least one: langchain / langgraph, llamalndex, semantic kernel or equivalent frameworks.\n* experience with vector databases and search (e.g., pinecone, weaviate, milvus, opensearch/elastic, pgvector)\n* experience deploying services in cloud environments (aws/azure/gcp) with basic devops practices\n* strong understanding of security and privacy principles (pii handling, least privilege, audit logging)\n\npreferred qualifications\n\n* experience in financial services or other regulated domains (risk controls, compliance audit readiness)\n* experience integrating with enterprise workflows (e.g., servicenow, custom workflow engines, bpm/rpa)\n* familiarity with model evaluation approaches (llm\\-as\\-judge, rubric scoring, retrieval evals, offline/online testing)\n* experience with messaging/eventing (kafka/sqs), email ingestion pipelines, and document processing\n* exposure to mrm concerns and governance (model cards, risk assessments, validation processes)\n\n **kaleidoscope, an infosys company, is an equal opportunity employer, and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. Software Engineer, Enterprise Technology",
        "company": "Rivian",
        "location": "Palo Alto, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Python",
            "R",
            "Java",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=29c79134461f90b8",
        "description": "about us:\n\nrivian is on a mission to keep the world adventurous forever. this goes for the emissions\\-free electric adventure vehicles we build, and the curious, courageous souls we seek to attract.  \n\nas a company, we constantly challenge what\u2019s possible, never simply accepting what has always been done. we reframe old problems, seek new solutions and operate comfortably in areas that are unknown. our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations.\nrole summary:\n\nour enterprise technology team powers the digital backbone behind supply chain, manufacturing, and operations.\n\n\nwe are seeking a senior software engineer to design and build scalable, intelligent enterprise platforms that enable operational excellence across planning, production, logistics, and supplier ecosystems. this role is ideal for engineers who combine strong fundamentals with a passion for innovation and ai\\-driven solutions.\n\n\nresponsibilities:\ndesign, development, and delivery of enterprise applications for supply chain and manufacturing operations.  \n\n* \n\napply and champion strong software engineering principles and best practices, including architecture, testing, security, and performance.  \n* \n\ndesign scalable, cloud\\-native, and highly reliable systems.  \n* \n\nwork collaboratively with product, operations, data, and cross\\-functional engineering teams.  \n* \n\nlead with a next\\-generation, ai\\-first mindset when designing workflows, automation, and decision\\-support systems.  \n* \n\nown complex technical initiatives from concept through production.  \n* \n\nparticipate in architecture reviews, code reviews, and technical roadmap planning.  \n* \n\nintegrate and optimize systems across erp, logistics, planning, and supplier platforms.  \n* \n* ensure high availability, observability, and operational excellence.\n\n\nqualifications:\nbachelor\u2019s degree in computer science, engineering, or equivalent experience.  \n\n* \n\n4\\+ years of professional software engineering experience.  \n* \n\nstrong foundation in software engineering principles and practices, including:  \n* \n\nsystem design and distributed architecture  \n* + \n\t\n\tapi and service development  \n\t+ \n\t\n\tautomated testing and ci/cd pipelines  \n\t+ \n\t\n\tsecure coding and compliance  \n\t+ \n\t\n\tperformance and scalability optimization  \n\t+\n* proficiency in one or more modern languages (java, c\\#, python, go, typescript, etc.).\n* experience in building modern web applications using frameworks (react, angular, next js).\n* cloud and container experience (aws, azure, gcp, docker, kubernetes).\n* experience with event\\-driven systems, streaming platforms, and data pipelines.\n* experience building enterprise\\-scale or mission\\-critical systems.\n\nproven ability to collaborate effectively across teams.  \n* \n* strong communication and technical documentation skills.\n\n\npay disclosure:\n\nthe salary range for this role is $146,900 \\- $183,600 for california based applicants and $123,400 \\- $154,200 for illinois \\& georgia based applicants. this is the lowest to highest salary we in good faith believe we would pay for this role at the time of this posting. an employee\u2019s position within the salary range will be based on several factors including, but not limited to, specific competencies, relevant education, qualifications, certifications, experience, skills, geographic location, shift, and organizational needs.  \n\nthe successful candidate may be eligible for annual performance bonus and equity awards.  \n\nwe offer a comprehensive package of benefits for full\\-time and part\\-time employees, their spouse or domestic partner, and children up to age 26, including but not limited to paid vacation, paid sick leave, and a competitive portfolio of insurance benefits including life, medical, dental, vision, short\\-term disability insurance, and long\\-term disability insurance to eligible employees. you may also have the opportunity to participate in rivian\u2019s 401(k) plan and employee stock purchase program if you meet certain eligibility requirements. full\\-time employee coverage is effective on their first day of employment. part\\-time employee coverage is effective the first of the month following 90 days of employment. more information about benefits is available at rivianbenefits.com.  \n\nyou can apply for this role through careers.rivian.com (or through internal\\-careers\\-rivian.icims.com if you are a current employee). this job is not expected to be closed any sooner than 3/12/26\ncompany statements:\n#### **equal opportunity**\n\n\nrivian is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. all qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender, gender expression, gender identity, genetic information or characteristics, physical or mental disability, marital/domestic partner status, age, military/veteran status, medical condition, or any other characteristic protected by law.  \n\nrivian is committed to ensuring that our hiring process is accessible for persons with disabilities. if you have a disability or limitation, such as those covered by the americans with disabilities act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@rivian.com.#### **candidate data privacy**\n\n\nrivian may collect, use and disclose your personal information or personal data (within the meaning of the applicable data protection laws) when you apply for employment and/or participate in our recruitment processes (\u201ccandidate personal data\u201d). this data includes contact, demographic, communications, educational, professional, employment, social media/website, network/device, recruiting system usage/interaction, security and preference information. rivian may use your candidate personal data for the purposes of (i) tracking interactions with our recruiting system; (ii) carrying out, analyzing and improving our application and recruitment process, including assessing you and your application and conducting employment, background and reference checks; (iii) establishing an employment relationship or entering into an employment contract with you; (iv) complying with our legal, regulatory and corporate governance obligations; (v) recordkeeping; (vi) ensuring network and information security and preventing fraud; and (vii) as otherwise required or permitted by applicable law.  \n\nrivian may share your candidate personal data with (i) internal personnel who have a need to know such information in order to perform their duties, including individuals on our people team, finance, legal, and the team(s) with the position(s) for which you are applying; (ii) rivian affiliates; and (iii) rivian\u2019s service providers, including providers of background checks, staffing services, and cloud services.  \n\nrivian may transfer or store internationally your candidate personal data, including to or in the united states, canada, the united kingdom, and the european union and in the cloud, and this data may be subject to the laws and accessible to the courts, law enforcement and national security authorities of such jurisdictions. **please note that we are currently not accepting applications from third party application services.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Applied AI & NLP Solutions Specialist",
        "company": "Nuvance Health",
        "location": "Danbury, CT, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Redshift",
            "Git",
            "Redshift",
            "Python",
            "SQL",
            "R",
            "Java",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5b93687bc353da28",
        "description": "**description**\n\n\nat nuvance health, we enjoy the benefits of a two\\-state system as we cultivate an inclusive culture where everyone feels welcomed, respected and supported. together, we are a team of 15,000\\+ strong hearts and open minds. if you share our values of connected, personal, agile and imaginative, we invite you to discover what's possible for you and your career.\n\n**summary:**\n\n\nthe dmao team plays a crucial role in nuvance health's digital transformation by ensuring the quality, integrity, and governance of data assets. they work closely with data engineers, business stakeholders, and it professionals to establish and maintain data standards, policies, and processes. the applied artificial intelligence (ai) / natural language processing (nlp) solutions specialist will partner with business and operational leaders to provide an impact by leveraging ai and analytical tools, strategic thinking, and hypothesis\\-based analysis on machine learning (ml)\\-based projects. the applied ai / nlp solutions specialist is responsible for developing ontologies, knowledge graphs, rules or other symbolic models, for insights into specific business processes or domains. additionally, they will develop and optimize ai\\-generated text prompts to feed into ai platforms to ensure accuracy. further, they will support senior leadership by creating business insights and analyses to aid in the decision\\-making process.,\n\n**responsibilities:**\n\n\n1\\. creates and implements data models and ontologies to support knowledge representation by organizing large volumes of data into knowledge structures. develops and maintains documentation for knowledge graphs as well as knowledge engineering processes and systems.\u00ef\u00bf\u00bd\n\n\n2\\. tests and evaluates new models to identify areas of improvement and optimization.\n\n\n3\\. supports the integration of knowledge systems into existing organizational infrastructure while ensuring seamless data flow.\n\n\n4\\. collaborates with cross\\-functional teams within the organization to gather and translate requirements into effective knowledge frameworks; acts as a translator between data engineers and domain experts.\n\n\n5\\. researches and implements cutting\\-edge techniques and tools in machine learning/deep learning/artificial intelligence to make data analysis more efficient.\n\n\n6\\. conducts audits of knowledge resources to maintain accuracy and relevance.\n\n\n7\\. applies advanced analytics and data visualization tools to tell the story behind the numbers, all while supporting data\\-driven decision making. finds creative solutions to problems when limited information is available.\n\n\n8\\. stays current with industry trends, making recommendations of new technologies and solutions that deliver strategic business value and reduce cost. supports innovation to drive improvements and optimize performance.\n\n\n9\\. remains updated with the latest methodologies, tools, and industry trends in machine learning and artificial intelligence.\n\n\n10\\. maintains and models nuvance health values.\u00ef\u00bf\u00bd\n\n\n11\\. demonstrates regular, reliable and predictable attendance.\n\n\n12\\. performs other duties as required.\n\n\neducation: bachelor's lvl dgr\n\n **education skills experience**  \n\n* bachelor's degree or equivalent experience, with a minimum of 7 years of experience in data analytics, data science or business intelligence. master's degree preferred.\n* 5\\+ years of strategic analysis designed to inform product or business decision making.\n* direct experience working with genai, llm fine\\-tuning, knowledge graphs, and related tools and techniques. strong understanding of ai, its potential roles in solving business problems, and the future trajectory of generative ai models.\n* proficiency with common ai and ml applications, data mining and statistic tools, scripting, and programming experience such as python, c\\+\\+, javascript, r, sas, excel, sql, matlab, spss desired.\n* advanced understanding of information models, knowledge representation and reasoning.\n* proficiency in cloud data platforms, data sciences, and application development.\n* experience with semantic web technologies and linked data principles is preferred.\n* aws cloud/redshift experience preferred.\n* familiarity with data modeling, data transformations and querying.\n\n**minimum knowledge, skills and abilities requirements:**\n\n* excellent communication skills, with the ability to translate complex concepts to non\\-technical audiences.\n* excellent problem\\-solving skills and an analytical mindset.\n* willingness and ability to learn new technologies on the job.\n* working knowledge of emr systems including but not limited to system wide hospital or ambulatory information systems.\n* applied knowledge of project management delivery, capital and operational budgeting.\n* ability to partner with executive stakeholders and customers.\n\n\nworking conditions:\n\n\nmanual: little or no manual skills/motor coord \\& finger dexterity\n\n\noccupational: little or no potential for occupational risk\n\n\nphysical effort: sedentary/light effort. may exert up to 10 lbs. force\n\n\nphysical environment: generally pleasant working conditions\n\n\ncompany: nuvance health\n\n\norg unit: 1782\n\n\ndepartment: data mgmt analytics org\n\n\nexempt: yes\n\n\nsalary range: $45\\.93 \\- $85\\.29 hourly\n\n  \n\nwith strong hearts and open minds, we're pushing past boundaries and challenging the expected, all in the name of possibility. we are neighbors caring for neighbors, working together as partners in health to improve the lives of the people we serve. if you share our passion for the health of our communities, advance your career with nuvance health!",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer",
        "company": "Ford Motor Company",
        "location": "Dearborn, MI, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Data Lake",
            "CI/CD",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a63b04da76cdce4e",
        "description": "we are the movers of the world and the makers of the future. we get up every day, roll up our sleeves and build a better world \\- together. at ford, we\u2019re all a part of something bigger than ourselves. are you ready to change the way the world moves?  \n\n\n\ndo you believe data tells the real story? we do! redefining mobility requires quality data, metrics and analytics, as well as insightful interpreters and analysts. that's where **global data insight \\& analytics** makes an impact. we advise leadership on business conditions, customer needs and the competitive landscape. with our support, key decision makers can act in meaningful, positive ways. join us and use your data expertise and analytical skills to drive evidence\\-based, timely decision making.\n\n\n**in this position...**\n\nwe are seeking a talented and driven **data engineer** to join our team. in this role, you will be instrumental in building and maintaining the foundational data infrastructure that powers our analytics and machine learning initiatives. you will design, develop, and optimize robust data pipelines, ensuring that high\\-quality data is accessible, reliable, and scalable across the organization.  \n\n\n\n**what you'll do...**\n\n* design, develop, and maintain scalable etl/elt pipelines to ingest data from a variety of internal and external sources.\n* architect and optimize data storage solutions (data warehouses and data lakes) to ensure high performance and cost\\-efficiency.\n* ensure data integrity and quality by implementing automated testing, monitoring, and validation frameworks within the data lifecycle.\n* collaborate closely with data scientists and product teams to understand their data requirements and provide \"model\\-ready\" datasets.\n* build and maintain api integrations and microservices to facilitate seamless data movement between systems.\n* implement automation and ci/cd practices for data infrastructure to improve deployment speed and system reliability.\n* troubleshoot and resolve complex data\\-related issues, optimizing slow\\-running queries and bottlenecks in the data flow.\n* stay up\\-to\\-date with the latest advancements in data engineering, cloud architecture, and distributed computing technologies.\n\n**you'll have...**\n\n* master\u2019s degree in computer science, software engineering, information systems, or a closely related quantitative field.\n* demonstrated ability to design and build production\\-grade data pipelines.\n* strong experience with data modeling, database design, and storage optimization (sql and nosql).\n* expertise in at least one major programming language used in data engineering (e.g., python).\n* proficiency in writing complex, highly optimized sql queries for large\\-scale data analysis.\n* experience with workflow orchestration tools.\n* familiarity with distributed computing frameworks.\nproven ability to translate business requirements into technical data architecture.  \n* \n\nyou may not check every box, or your experience may look a little different from what we've outlined, but if you think you can bring value to ford motor company, we encourage you to apply!  \n\n\n\nas an established global company, we offer the benefit of choice. you can choose what your ford future will look like: will your story span the globe, or keep you close to home? will your career be a deep dive into what you love, or a series of new teams and new skills? will you be a leader, a changemaker, a technical expert, a culture builder\u2026or all of the above? no matter what you choose, we offer a work life that works for you, including:\n\n\n* immediate medical, dental, vision and prescription drug coverage\n\t+ flexible family care days, paid parental leave, new parent ramp\\-up programs, subsidized back\\-up child care and more\n\t+ family building benefits including adoption and surrogacy expense reimbursement, fertility treatments, and more\n\t+ vehicle discount program for employees and family members and management leases\n\t+ tuition assistance\n\t+ established and active employee resource groups\n\t+ paid time off for individual and team community service\n\t+ a generous schedule of paid holidays, including the week between christmas and new year\u2019s day\n\tpaid time off and the option to purchase additional vacation time.  \n\t+\n\nthis position is a salary grade 5 \u2013 salary grade 8 and ranges from $66,660\\- $190,500\\.\n\n\nfor more information on salary and benefits, click here: new hire benefits\n\n\n**visa sponsorship is available for this position.**\n\ncandidates for positions with ford motor company must be legally authorized to work in the united states. verification of employment eligibility will be required at the time of hire.\n\n\nwe are an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. in the united states, if you need a reasonable accommodation for the online application process due to a disability, please call 1\\-888\\-336\\-0660\\.\n\n\nthis position is hybrid. candidates who are in commuting distance to a ford hub location may be required to be onsite four or more days per week. **\\#li\\-hybrid** \n\n**\\#li\\-mg1**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist, Data Analytics & Reporting (N92)",
        "company": "Heluna Health",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "CI/CD",
            "Tableau",
            "Power BI",
            "Python",
            "R",
            "Optimization",
            "Bayesian",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=62a449ec34f9069e",
        "description": "**salary range: $9,333\\.00 \\- $12,576\\.46 per month**\n\n\n**summary**\n\n\n\nthe los angeles county department of homeless services and housing (hsh)consolidates our countywide response to homelessness. the driving force behind hsh is increasing accountability and transparency, improving care for people experiencing or at risk of homelessness, and streamlining collaboration with partners including services providers, the county\u2019s 88 cities, and unincorporated areas to deliver high\\-quality, life\\-saving care. staff schedules are based on business need and may include the option of a hybrid work schedule where employees work remotely and from the office.\n\n\n\nthe program analytics team carries out mission\\-critical data science and analytics projects in support of programs that provide housing and social services to the most vulnerable people in los angeles county. this role offers an opportunity for impact across program areas spanning the los angeles county homelessness system, including but not limited to outreach and response to unsheltered people, interim housing and temporary shelters, permanent supportive housing and case management services that sustain people in housing, and benefits entitlement programs.\n\n\n\nthis position supports the associate director of program analytics in building data science products that help those program teams make better decisions, including by building dashboards, predictive models, and supporting internal program evaluation. the role also includes helping to build out metrics for systemwide performance improvement, including building and maintaining indicators that track the performance of the homelessness system and of individual service providers within that system, to ensure taxpayer dollars are having the maximum possible impact on housing and supporting people experiencing homelessness.\n\n\n\nthe senior data scientist works closely with a team of data analysts and data scientists, providing supervision as needed. those analysts are tasked with providing critical analytical and reporting support to hsh programs, ensuring data\\-driven insights inform decision\\-making and enhance program effectiveness. key responsibilities include supervising the development and automation of reports and dashboards, improving the quality and accessibility of program data, and ensuring analytics and reporting efforts are aligned with strategic priorities. the role ensures that analytics and reporting efforts align with strategic priorities, enhances the quality and accessibility of program data, and fosters cross\\-team collaboration to meet organizational goals.\n\n\n**essential functions**\n\n\n* oversees recurring reporting workflows and leads the automation of these processes, including revisions to improve data quality and accuracy and increase analytic value.\n* supports the associate director of program analytics in delivering mission\\-critical data science projects across the los angeles county homelessness system.\n* collaborates with division, departmental, and countywide stakeholders to solicit, define, and manage highly complex data science projects from conception through implementation; ensures that projects result in actionable insights and recommendations that can be used to support business decisions and allocate resources; and provides ongoing consultation during program implementation of recommendations as necessary to ensure program fidelity.\n* provides technical oversight to small teams of data scientists and data analysts as necessary to ensure project success.\n* leads the discovery process with departmental stakeholders to document business requirements and frame business problems so that appropriate corresponding data science techniques and products can be identified and deployed.\n* collaborates with other department subject matter experts to understand, identify, and select available and relevant sources of data for use cases, including internal, external, structured, and unstructured data sources; and provides guidance to other department data experts in the selection and transformation of such sources.\n* supports collection, integration, and retention requirements for large sets of structured and unstructured data from disparate sources; acts as a resource for senior data engineers and architects in the selection and design of systems for use by the data science team.\n* reviews proposals by data scientists and analysts regarding selection and improvement of data science tools and ensures that the team\u2019s tools and systems are well documented.\n* conducts and coordinates statistical inference of data and conveys findings and conclusions orally, in writing, visually, in presentations, and by developing interactive tools as appropriate to communicate effectively with a wide range of audiences, including technical and nontechnical staff, stakeholders, and members of the public.\n* works with program staff to understand the implications of analyses and to ensure that findings are actionable and support data\\-driven program, policy, and operational decision\\-making.\n* assists in implementing recommended business process changes in ways retain fidelity to best practices identified through the analysis and recognize the operational realities underlying existing business processes; provides ongoing consultation as recommended changes and new programs grow and evolve, including recommending modifications to adopted goals and metrics.\n* coordinates with various functional teams to develop and implement products, services, tools, or business process changes resulting from the analysis.\n* creates and validates advanced analytic products (e.g., recommender engines, auto classification algorithms, predictive scoring, geo\\-spatial clustering, nlp classifier, etc.) and places them in production; works with nontechnical program managers and staff to understand analytic products and to integrate them into departmental programs.\n* recommends ongoing improvements to methods and algorithms that lead to findings, including new information.\n* creates and monitors performance of automated\\-detection systems and provides business metrics for overall project to show improvements initially and over multiple iterations. provides ongoing tracking and monitoring of performance of decision systems and statistical models and troubleshoots and implements enhancements and fixes to systems as needed.\n* develops and maintains a high level of expertise in statistical programming languages and packages and other software tools; provides training and support to data scientists and data analysts as necessary; and ensures the quality of code and programming logic generated by the data science team.\n\n\n**job qualifications**\n\n\n\noption i:two (2\\) years of experience carrying out complex data science projects that included independently developing and applying methods to identify, collect, process, structure, and analyze data using statistical prediction, inference, and optimization to support data\\-driven program design and management, at a level equivalent of data scientist.\n\n\n\noption ii: a bachelor\u2019s degree from an accredited college or university in a field of applied research such as data science, machine learning, mathematics, statistics, business analytics, psychology, or public health that included 12 semester or 18 quarter units of coursework in data science, predictive analytics, quantitative research methods, or statistical analysis \\-and\\- six (6\\) years of experience, including two (2\\) years in a lead capacity, applying and overseeing the application of machine learning, predictive analytics, data management, and hypothesis\\-driven data analysis to make actionable recommendations to support program, policy, and operational decision\\-making. a master\u2019s or doctoral degree from an accredited college or university in a field of applied research such data science, machine learning, mathematics, statistics, business analytics, psychology, or public health may substitute for up to two (2\\) years of experience.\n\n\n**certificates/licenses/clearances**\n\n\n* a valid california class c driver license or the ability to utilize an alternative method of transportation when needed to carry out job\\-related essential functions.\n* successful clearance of the live scan process with the county of los angeles.\n\n\n**other skills, knowledge, and abilities**\n\n\n* required:\n\t+ advanced experience with using code for analytics and statistical programming (r or python preferred)\n\t+ advanced experience using statistical models and / or machine learning methods for prediction and causal inference, including model validation, sensitivity checks, robustness checks, and so on\n\t+ thorough understanding of data governance and privacy protection when working with sensitive data\n\t+ ability to translate business requirements into technical data science projects and to communicate results in plain language to technical and nontechnical audiences\n\t+ proven ability to own and manage projects, including development of timelines and ability to unblock when things get stuck\n\t+ proficiency with modern data visualization methods (e.g., r and python plot libraries and cloud\\-based dashboarding tools (e.g., power bi, tableau, etc.)\n\t+ familiarity with modern dataops / devops practices such as version control, code review, ci/cd patterns, and job orchestration\n\t+ designing data models, metrics, and other analytic logic used for performance monitoring and operational reporting\n* preferred:\n\t+ experience validating, deploying, and monitoring statistical or machine learning models in applied settings\n\t+ understanding of quasi\\-experimental and experimental causal inference methods\n\t+ deep statistical knowledge (e.g., nonparametric hypothesis testing, bayesian modeling)\n\t+ understanding of health insurance potability and accountability act\n\t+ proficiency with dashboarding and automating reporting pipelines\n\t+ comfort working within agile project management frameworks, including iterative development, sprint planning, and backlog management\n\t+ experience working in distributed computing frameworks such as spark\n\t+ understanding of etl/elt pipelines, data models, and feature engineering workflows in cloud\\-based environments\n\t+ deep understanding of dataops / devops practices listed above\n\n**physical demands**\n\n\n**stand:** frequently\n\n\n**walk:** frequently\n\n\n**sit:** frequently\n\n\n**reach outward:** occasionally\n\n\n**reach above shoulder:** occasionally\n\n\n**climb, crawl, kneel, bend:** occasionally\n\n\n**lift / carry:** occasionally \\- up to 15 lbs\n\n\n**push/pull:** occasionally \\- up to 15 lbs\n\n\n**see:** constantly\n\n\n**taste/ smell:** not applicable\n\n  \n\n\n**not applicable \\=** not required for essential functions\n\n\n**occasionally \\=** (0 \\- 2 hrs/day)\n\n\n**frequently \\=** (2 \\- 5 hrs/day)\n\n\n**constantly \\=** (5\\+ hrs/day)\n\n\n**work environment**\n\n\n\ngeneral office setting, indoors temperature controlled\n\n\n\nheluna health is an affirmative action, equal opportunity employer that encourages minorities, women, veterans, and disabled to apply.\n\n\n**eeoc statement**\n\n\n\nit is the policy of heluna health to provide equal employment opportunities to all employees and applicants, without regard to age (40 and over), national origin or ancestry, race, color, religion, sex, gender, sexual orientation, pregnancy or perceived pregnancy, reproductive health decision making, physical or mental disability, medical condition (including cancer or a record or history of cancer), aids or hiv, genetic information or characteristics, veteran status or military service.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer",
        "company": "DocuSign",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "MongoDB",
            "NoSQL",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=76acaea9885b5df4",
        "description": "**company overview**\n--------------------\n\n  \n\ndocusign brings agreements to life. over 1\\.5 million customers and more than a billion people in over 180 countries use docusign solutions to accelerate the process of doing business and simplify people\u2019s lives. with intelligent agreement management, docusign unleashes business\\-critical data that is trapped inside of documents. until now, these were disconnected from business systems of record, costing businesses time, money, and opportunity. using docusign\u2019s intelligent agreement management platform, companies can create, commit, and manage agreements with solutions created by the \\#1 company in e\\-signature and contract lifecycle management (clm).  \n\n  \n\n**what you'll do**\n------------------\n\n  \n\nwe are building a cutting\\-edge app ecosystem for docusign that will empower developers to seamlessly integrate and extend docusign\u2019s features and applications by implementing custom apis. you\u2019ll contribute to building and maintaining the extensibility platform that empowers developers to integrate with, and extend, docusign\u2019s features through custom apis. in this role, you will implement reliable, scalable, and secure features for the ecosystem, working alongside experienced engineers and collaborating with cross\\-functional teams.\n\n  \n\nyou will collaborate closely with cross\\-functional teams across the organization, contributing to both platform architecture and feature development. your work will impact multiple engineering and product teams as you contribute to the development of the platform\u2019s foundational features. additionally, you will participate in inner\\-team collaboration by engaging in code reviews, assisting in architectural discussions, and contributing to incident response processes.\n\n  \n\nthis position is an individual contributor role reporting to the sr. manager, engineering.\n\n **responsibility**\n\n* contribute to the development and maintenance of docusign's app ecosystem, enabling integration with core docusign features via apis\n* implement solutions using azure technologies, kubernetes, terraform, and modern software development practices, with guidance from senior engineers\n* assist in designing secure and scalable systems, including queueing architectures and storage (nosql and sql), to meet ecosystem needs\n* work closely with team members and stakeholders to deliver features that support easy creation and integration of external apps\n* participate in code reviews, share feedback, and learn best practices in software development and devops\n* collaborate on internal processes, such as workflow optimization and process automation, to improve team efficiency\n* contribute to incident response and troubleshooting efforts, ensuring platform reliability\n\n  \n\n  \n\n**job designation**\n-------------------\n\n **hybrid:**\n\n\nemployee divides their time between in\\-office and remote work. access to an office location is required. (frequency: minimum 2 days per week; may vary by team but will be weekly in\\-office expectation)  \n\npositions at docusign are assigned a job designation of either in office, hybrid or remote and are specific to the role/job. preferred job designations are not guaranteed when changing positions within docusign. docusign reserves the right to change a position's job designation depending on business needs and as permitted by local law.\n\n  \n\n  \n\n**what you bring**\n------------------\n\n **basic**\n\n* 5\\+ years of professional software development experience, ideally with cloud platforms and distributed systems\n* experience developing applications using nodejs/typescript or related technologies\n* experience with azure services, ci/cd pipelines (e.g., azure devops), kubernetes, and terraform\n* experience with queueing systems, nosql databases (such as cosmosdb or mongodb), and sql databases (such as azure sql)\n\n**preferred**\n\n* bachelor\u2019s degree in computer science, engineering, or related field (or equivalent experience)\n* ability to work effectively on a collaborative engineering team, actively communicating and sharing knowledge\n* a continuous learner, interested in evolving your skills and contributing innovative ideas to the project\n* strong problem\\-solving skills with a desire to learn and apply best practices for scalability, reliability, and security\n* experience in developer ecosystems or extensibility frameworks\n* exposure to code reviews and collaborative development tools\n\n  \n\n  \n\n**wage transparency**\n---------------------\n\n  \n\npay for this position is based on a number of factors including geographic location and may vary depending on job\\-related knowledge, skills, and experience.  \n\nbased on applicable legislation, the below details pay ranges in the following locations:  \n\nwashington, maryland, new jersey and new york (including nyc metro area): $133,800\\.00 \\- $197,750\\.00 base salary  \n\nthis role is also eligible for the following:* bonus: sales personnel are eligible for variable incentive pay dependent on their achievement of pre\\-established sales goals. non\\-sales roles are eligible for a company bonus plan, which is calculated as a percentage of eligible wages and dependent on company performance.\n* stock: this role is eligible to receive restricted stock units (rsus).\n\n  \n\nglobal benefits\n\n\nprovide options for the following:* paid time off: earned time off, as well as paid company holidays based on region\n* paid parental leave: take up to six months off with your child after birth, adoption or foster care placement\n* full health benefits plans: options for 100% employer paid and minimum employee contribution health plans from day one of employment\n* retirement plans: select retirement and pension programs with potential for employer contributions\n* learning and development: options for coaching, online courses and education reimbursements\n* compassionate care leave: paid time off following the loss of a loved one and other life\\-changing events\n\n  \n\n\n  \n\n**life at docusign**\n--------------------\n\n **working here**\n\n\ndocusign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. you can count on us to listen, be honest, and try our best to do what\u2019s right, every day. at docusign, everything is equal.  \n\nwe each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. and for that, you\u2019ll be loved by us, our customers, and the world in which we live. **accommodation**\n\n\ndocusign is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. if you need such an accommodation, or a religious accommodation, during the application process, please contact us at accommodations@docusign.com.  \n\nif you experience any issues, concerns, or technical difficulties during the application process please get in touch with our talent organization at taops@docusign.com for assistance.\n  \n### **our global benefits**\n\n\n#### **paid time off**\n\n\ntake time to unwind with earned days off, plus paid company holidays based on your region.\n#### **paid parental leave**\n\n\ntake up to six months off with your child after birth, adoption or foster care placement.\n#### **full health benefits**\n\n\noptions for 100% employer\\-paid health plans from day one of employment.\n#### **retirement plans**\n\n\nselect retirement and pension programs with potential for employer contributions.\n#### **learning \\& development**\n\n\ngrow your career with coaching, online courses and education reimbursements.\n#### **compassionate care leave**\n\n\npaid time off following the loss of a loved one and other life\\-changing events.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Cloud Solution Architect",
        "company": "nan",
        "location": "Richmond, VA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Terraform",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1800f4b145462834",
        "description": "* 5099583\n* richmond, virginia, united states\n* dept behavioral health/develop\n* information technology\n* full\\-time (salaried)\n* closing at: mar 9 2026 \\- 23:55 edt\n* dept behavioral health/develop\n  \n\n**title:** cloud solution architect\n\n\n**state role title:** info technology specialist iii\n\n\n**hiring range:** up to $165,000\n\n\n**pay band:** 6\n\n\n**agency:** dept behavioral health/develop\n\n\n**location:** central office\n\n\n**agency website:** www.dbhds.virginia.gov\n\n\n**recruitment type:** general public \\- g\n\n\n**job duties**\n\n\n\nthe department of behavioral health and developmental services (dbhds) is seeking an experienced it professional to fill the cloud solution architect position. as part of the enterprise architecture team, this position is responsible for designing scalable, reusable, and secure application architectures across the organization, with a strong focus on aws solution architecture, while supporting workloads on azure and oci as well.\n  \n\n  \n\nkey responsibilities include:  \n\n* designing and implementing scalable, resilient, secure cloud\\-native applications on aws aligned with well\\-architected framework principles.\n* leading application modernization and cloud migration initiatives, embedding performance, reliability, scalability, and cost optimization into all designs.\n* producing detailed architecture documents, diagrams, and technical specifications to support solution delivery and governance.\n* providing architectural guidance for azure deployments and validating/supporting workloads running on oci.\n* promoting consistent cross\\-cloud architecture patterns across networking, identity, devsecops, security, observability, workload portability, and multi\\-cloud resilience strategies.\n* contributing reusable reference architectures, blueprints, accelerators, and standardized design patterns to reduce duplication across teams.\n* developing and maintaining shared frameworks for logging, monitoring, security, ci/cd, api governance, and system integration.\n* building and standardizing infrastructure as code (iac) modules and automation templates.\n* actively participating in solution design workshops, proof\\-of\\-concepts, and providing hands\\-on guidance during development and deployment.\n* conducting architecture reviews, code and design validations, and troubleshooting complex application and integration issues.\n* ensuring solution alignment with enterprise architecture principles, target\\-state roadmaps, compliance standards, and security, regulatory, and risk management policies.\n* participating in architecture review boards and governance processes while maintaining complete and accurate architecture documentation.\n* collaborating with product, engineering, devops, security, and business stakeholders to translate business requirements into scalable, sustainable technical solutions and providing consultative support across business units.\n* demonstrating strong analytical and systems thinking, deep technical credibility, a reusability\\-first platform mindset, effective collaboration and influencing skills, and the ability to balance governance with delivery agility\u2014driving outcomes such as scalable secure solutions, reusable pattern adoption, reduced time\\-to\\-market, improved reliability/performance/cost efficiency, and reduced technical debt.\n\n**minimum qualifications**\n\n\n* it experience, with experience delivering solution and application architectures.\n* experience designing and implementing aws\\-based solutions, with experience in azure and exposure to oci.\n* demonstrated experience in cloud\\-native application design, modernization, and leveraging native cloud services for solution engineering across cloud and on\\-premises environments.\n* ability to translate business requirements into fully tested, scalable workflows by designing, implementing, and maintaining technology assets using approved tools.\n* knowledge of aws services including compute, storage, networking, serverless, containers, and iam, along with cloud networking concepts (vpcs, subnets, security groups).\n* experience in microservices, event\\-driven architecture, api\\-first design, integration patterns, and api development (e.g., aws api gateway).\n* experience with serverless platforms (e.g., aws lambda), containerization (docker), and orchestration (eks, kubernetes).\n* proficiency in infrastructure as code (terraform, cloudformation, arm/bicep) to automate provisioning and infrastructure management.\n* understanding of ci/cd pipelines, devsecops practices, version control, and collaboration tools (e.g., jenkins, aws codepipeline).\n* experience implementing observability frameworks including logging, monitoring, tracing, alerts, dashboards, performance tuning, and cloud cost optimization.\n* scripting and automation skills (e.g., python, node.js), business process automation capabilities, and commitment to security\\-by\\-design and zero\\-trust principles.\n* excellent stakeholder communication, collaboration, end\\-user training and support, continuous learning and mentoring mindset, and adherence to organizational policies and procedures.\n\n\n**additional considerations**\n\n\n* experience showcased in cloud architecture and software engineering skills (ex: api development, algorithm design), scripting (preferably python/nodejs), serverless computing, containerization and orchestration, devops for pipelines.\n* experience building products in a modern cloud ecosystem is crucial.\n* associate/professional aws certified solutions architect\n* professional aws certified devops engineer\n* microsoft azure solutions architect\n* togaf or equivalent enterprise architecture certification\n\n\n**special instructions**\n\n\n\nyou will be provided a confirmation of receipt when your application and/or r\u00e9sum\u00e9 is submitted successfully. please refer to \u201cyour application\u201d in your account to check the status of your application for this position.\n\n\n\nthis position is eligible, however not guaranteed, for telework opportunities; availability, hours, and duration of telework shall be approved as outlined in the commonwealth telework policy.\n  \n\n  \n\nfor consideration, interested applicants must apply by completing the online application and/or submit a resume. however, emailed, faxed, and hand\\-delivered applications and/or resumes will not be accepted. applications must be submitted by 11:55 p.m., on the listed closing date. reasonable accommodations are available to persons with disabilities during application and/or interview processes per the americans with disabilities act.\n  \n\n  \n\ndbhds welcomes all applicants authorized to work in the u.s. for more information on how to seek this authorization, please refer to working in the united states or contact the u.s. citizenship and immigration services office directly.\n\n\n**contact information**\n\n\n\nname: shakiera miles\n\n\n\nphone: n/a\n\n\n\nemail: shakiera.miles@dbhds.virginia.gov \\- inquiries only/no submissions, to include resumes.\n\n  \n\n\n\nin support of the commonwealth\u2019s commitment to inclusion, we are encouraging individuals with disabilities to apply through the commonwealth alternative hiring process. to be considered for this opportunity, applicants will need to provide their ahp letter (formerly cod) provided by the department for aging \\& rehabilitative services (dars), or the department for the blind \\& vision impaired (dbvi). service\\-connected veterans are encouraged to answer veteran status questions and submit their disability documentation, if applicable, to dars/dbvi to get their ahp letter. requesting an ahp letter can be found at ahp letter or by calling dars at 800\\-552\\-5019\\.\n\n\n\nnote**:** applicants who received a certificate of disability from dars or dbvi dated between april 1, 2022\\- february 29, 2024, can still use that cod as applicable documentation for the alternative hiring process.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Engineering Systems & Tools Engineer",
        "company": "eBay",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Git",
            "R",
            "Java",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e9610a75feeeb729",
        "description": "at ebay, we're more than a global ecommerce leader \u2014 we\u2019re changing the way the world shops and sells. our platform empowers millions of buyers and sellers in more than 190 markets around the world. we\u2019re committed to pushing boundaries and leaving our mark as we reinvent the future of ecommerce for enthusiasts.\n\n\nour customers are our compass, authenticity thrives, bold ideas are welcome, and everyone can bring their unique selves to work \u2014 every day. we're in this together, sustaining the future of our customers, our company, and our planet.\n\n\njoin a team of passionate thinkers, innovators, and dreamers \u2014 and help us connect people and build communities to create economic opportunity for all.\n\n### **about the team**\n\n**the** **engineering systems tools** **team at ebay builds and operates the internal tooling and infrastructure that supports thousands of developers across the company. our mission is to enhance developer efficiency through automation, self\\-service platforms, and seamless provisioning.**\n\n**we support and extend core platforms including** **jira, confluence, github enterprise, artifactory, sonarqube, gradle****, and** **airtable****, while building and maintaining custom applications tailored to internal engineering workflows.**\n\n**on the infrastructure side, we operate across** **multi\\-cloud environments (gcp, aws, azure)****, and are actively investing in** **container orchestration (kubernetes)****, infrastructure\\-as\\-code, and service automation to modernize our stack. we own the full lifecycle of our tools\u2014from development and deployment to monitoring and operations\u2014and collaborate closely with teams across the company to deliver resilient, scalable solutions.**\n\n### **responsibilities/what you\u2019ll do**\n\n* **administer and operate scm and build tooling platforms, with primary focus on artifactory, github enterprise, sonarqube, and gradle.**\n\n* **act as a primary support contact for internal engineering teams, troubleshooting issues related to source control, builds, artifacts, and code quality.**\n\n* **manage user access, permissions, repositories, organizations, and integrations across scm and ci/cd tooling.**\n\n* **support and optimize gradle\\-based build systems, dependency management, and artifact publishing workflows.**\n\n* **operate and maintain artifactory repositories, including retention policies, replication, performance tuning, and storage optimization.**\n\n* **design, build, and maintain** **full\\-stack internal tooling** **and platforms that improve engineering productivity and automation.**\n* **develop and support** **restful apis****, microservices, and front\\-end interfaces that integrate with both third\\-party and custom\\-built tools.**\n* **operate and scale** **cloud\\-based infrastructure** **across gcp, aws, and azure; contribute to infrastructure provisioning, configuration management, and monitoring.**\n* **implement and support** **kubernetes\\-based containerization workflows****, including service deployment and resource optimization.**\n* **automate infrastructure and operations using tools like** **terraform, helm, and ci/cd pipelines****.**\n* **own the full lifecycle of systems\u2014from requirements gathering to deployment and post\\-release support.**\n* **troubleshoot performance and reliability issues across the stack and work on platform hardening.**\n* **collaborate with product owners, site reliability engineers, and developers to continuously improve platform usability and reliability.**\n* **contribute to architectural decisions, technical design reviews, and code reviews; mentor junior team members where applicable.**\n\n### **requirements/preferred qualifications**\n\n* **5\\+ years** **of experience in software engineering and platform operations in a large\\-scale or enterprise environment.**\n\n* **5\\+ years of experience supporting or administering scm, ci/cd, or developer productivity platforms in an enterprise environment.**\n\n* **strong hands\\-on experience with github enterprise administration and git\\-based workflows.**\n\n* **proven experience administering artifactory (repository management, access control, integrations, and troubleshooting).**\n\n* **experience with sonarqube administration, code quality metrics, and ci integrations.**\n\n* **solid understanding of gradle builds, dependency management, and enterprise build optimization.**\n\n* **experience providing customer\\-facing technical support to internal engineering teams.**\n\n* **strong programming skills in** **java** **(preferred), with experience in** **spring boot** **or similar frameworks. bonus for front\\-end experience (react, angular, or similar).**\n* **experience designing, deploying, and managing applications in** **kubernetes** **and** **dockerized** **environments.**\n* **solid background in** **multi\\-cloud architecture** **and automation across** **gcp, aws, or azure****.**\n* **familiarity with internal developer platforms and tools like** **jira, github, artifactory, sonarqube****, and related systems.**\n* **comfortable with** **monitoring/logging stacks** **(e.g., prometheus, grafana, elk, or similar).**\n* **experience with** **infrastructure\\-as\\-code** **using tools like terraform, ansible, or similar.**\n* **strong debugging and problem\\-solving skills across distributed systems, apis, and infrastructure layers.**\n* **ability to translate platform needs into maintainable, performant code and configuration.**\n\n **excellent communication skills with the ability to work across engineering and infrastructure domains.**\n\n**additional details**\n\n\nthe base pay range for this position is expected in the range below:\n\n\n$118,800 \\- $205,600\nbase pay offered may vary depending on multiple individualized factors, including location, skills, and experience. the total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as pto and parental leave). details of participation in these benefit plans will be provided if an employee receives an offer of employment.\n\n\nif hired, employees will be in an \u201cat\\-will position\u201d and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors.\n\n\nebay is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. if you have a need that requires accommodation, please contact us at talent@ebay.com. we will make every effort to respond to your request for accommodation as soon as possible. view our accessibility statement to learn more about ebay's commitment to ensuring digital accessibility for people with disabilities. it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability.\n\n\nwe use cookies to enhance your experience and may use ai tools for administrative tasks in the hiring process. to learn how we handle your personal data and use ai responsibly, please visit our talent privacy notice, privacy center, and ai hiring guidelines.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Robotics Infrastructure Software Intern",
        "company": "Anyware Robotics",
        "location": "Fremont, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "S3",
            "EC2",
            "Docker",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f5eb15092385e430",
        "description": "the robotics infrastructure software intern will support the development and maintenance of the core infrastructure that powers our robotics platform, including build systems, containerized environments, ci/cd pipelines, and cloud integrations. this role focuses on improving reliability, scalability, and developer productivity across distributed ros2\\-based systems running on ubuntu. the intern will work closely with robotics and platform engineers to automate workflows, troubleshoot system issues, and help build a robust foundation for deploying and operating robot fleets.\n### **about anyware robotics**\n\n\nanyware robotics deploys multi\\-purpose mobile robots, equipped with physical ai, to safely and autonomously handle strenuous and repetitive operational tasks. pixmo, the company\u2019s robot, is designed to autonomously handle boxes for trailer unloading, palletizing, case picking, and trailer loading. pixmo is powered by anywareos, a physical ai software that enables advanced perception and object handling in dynamic, unstructured environments. pixmo's combination of intelligent software and flexible hardware helps distribution centers and 3pls reduce labor costs, enhance worker safety, and maintain productivity during labor shortages to deliver a clear return on investment (roi). we\u2019re a small team with big goals, building robots that make work safer and smarter. if you like solving real problems, deploying robots into production, and seeing your ideas come to life, you\u2019ll fit right in.\n### **description**\n\n\n**responsibilities**\n--------------------\n\n* develop and maintain robotics infrastructure that enables reliable build, test, deployment, and monitoring of distributed robot software systems.\n* design and manage containerized development and runtime environments using docker and docker compose.\n* build and maintain internal tooling to improve developer productivity, including local dev environments, reproducible builds, and automation scripts.\n* support and improve ci/cd pipelines for robotics software, including build validation, automated testing, and artifact management.\n* assist in managing cloud\\-based infrastructure for simulation, data processing, and fleet support.\n* improve system observability, logging, and debugging workflows across robotics services.\n* troubleshoot infrastructure, networking, and container\\-related issues across development machines and cloud environments.\n* contribute to documentation and best practices for infrastructure, deployment, and developer onboarding.\n* work closely with robotics software engineers to ensure infrastructure scales with system complexity and fleet growth.\n\n**qualifications**\n------------------\n\n### **required**\n\n* pursuing a bachelor\u2019s or master\u2019s degree in computer science, robotics, electrical engineering, or a related technical field.\n* strong experience working in **ubuntu/linux** environments.\n* proficiency in **python** (automation, scripting, tooling) and working knowledge of **c\\+\\+**.\n* hands\\-on experience with **ros2**, including building and launching distributed systems.\n* solid experience with **docker** and **docker compose** for multi\\-container systems.\n* experience using **vscode**, including remote development and container\\-based workflows.\n* familiarity with git\\-based workflows and collaborative development practices.\n* strong debugging skills across system, networking, and container layers.\n* interest in infrastructure, devops, and platform engineering in robotics environments.\n\n### **nice to have**\n\n* experience with infrastructure as code tools such as **terraform**.\n* familiarity with **aws** (ec2, networking, iam, s3, or container services).\n* experience building ci/cd pipelines using **github actions** or similar systems.\n* exposure to distributed systems, networking fundamentals, or observability tooling.\n* experience supporting production systems or robotics fleets.\n\n  \n\n\n#### **salary**\n\n\n$25 \\- $35 per hour",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Full Stack Software Engineer",
        "company": "Sharetec",
        "location": "US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "PostgreSQL",
            "MongoDB",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3be640f527ac8228",
        "description": "sharetec is looking for a **senior full stack software engineer** to join our team!\n\n\nat sharetec, we believe in a people\\-first business. our mission is to make millions of people\u2019s lives easier by developing innovative core banking and lending software solutions for credit unions and their members. our customers rely on us for powerful financial technology that enhances operations across accounting, lending, member services, and online banking and they count on us to be a caring, trusted partner in their success.\n\n\nas a **senior full stack software engineer** at sharetec, you\u2019ll play a key role in building the technology that powers credit unions across the country. you\u2019ll work hands\\-on across the full stack \u2014 from backend services and data architecture to front\\-end applications \u2014 to deliver reliable, scalable, and user\\-friendly software that helps our clients better serve their members. you\u2019ll collaborate closely with product managers, qa, and fellow engineers to design creative solutions, improve system performance, and enhance how credit unions operate every day. this is a great opportunity for someone who enjoys solving complex problems, mentoring others, and making a meaningful impact through technology that truly matters.\n\n\nthis is a full\\-time, exempt, remote position open to candidates residing in any u.s. state, with the exception of california. the starting salary for this position is $110,000 \\- $130,000 based on experience.\n\n**essential duties and responsibilities**:\n\n**technical leadership**\n\n\narchitect scalable, secure, and maintainable systems across backend (node.js), frontend (react), and database layers. lead design discussions, code reviews, and engineering best\\-practice initiatives. evaluate new technologies like node.js frameworks (express) and react tools and guide the adoption of tools that improve developer productivity and system performance. define and document architecture patterns, design principles, and shared libraries for reuse across teams. partner with devops to design efficient cicd pipelines, deployment automation, and observability tooling\n\n**software development**\n\n\nmentor and coach mid\\-level and junior engineers to foster career growth and skill development. partner with product managers and stakeholders to translate business needs into technical solutions. provide feedback and guidance during design and implementation reviews. promote a culture of continuous learning, clear communication, and shared ownership across the team.\n\n**collaboration \\& mentorship**\n\n\ncomplete assigned development and project tasks by due dates while maintaining a high standard of quality. work collaboratively with engineers, product managers, and qa across departments to deliver cohesive solutions. act as a technical resource and subject matter expert in your assigned product domain. contribute to process improvements, documentation, and internal tooling to enhance engineering efficiency.\n\n**general duties and responsibilities****:**\n\n* complete assigned development and project tasks by due dates while maintaining a high standard of quality.\n* work collaboratively with engineers, product managers, and qa across departments to deliver cohesive solutions.\n* act as a technical resource and subject matter expert in your assigned product domain.\n* contribute to process improvements, documentation, and internal tooling to enhance engineering efficiency.\n\n**what success looks like****:**\n\n**in your first 30 days:**\n\n* build an understanding of sharetec\u2019s architecture, domain, and codebase.\n* complete onboarding and deploy your first contributions with mentorship.\n* participate in planning and architecture discussions, bringing forward improvement ideas.\n\n**by 60 days:**\n\n* lead a small\\-to\\-medium\\-size feature from design to production.\n* build strong cross\\-functional relationships with other engineers, qa, and product managers.\n* begin mentoring mid\\-level engineers, providing code review and technical guidance.\n\n**by 90 days and beyond:**\n\n* serve as a technical owner for a significant domain or service.\n* drive architectural improvements and influence long\\-term technology direction.\n* enable your team to ship high\\-quality, maintainable code faster and more confidently.\n\n**qualified candidates should have**:\n\n**required qualifications**\n\n* 6 years of professional software development experience, including technical leadership on complex projects.\n* deep proficiency with node.js (e.g., express) for backend platforms.\n* advanced experience with react for modern front\\-end technologies.\n* strong database design and data architecture expertise (postgresql, sql server, or mongodb).\n* experience integrating apis and third\\-party systems securely and efficiently, with focus on restful design and performance.\n* familiarity with implementing ai into products (e.g., integrating ml models via apis).\n* proven ability to modernize legacy applications into modular node.js/react designs.\n* familiarity with devops tooling, cicd pipelines, and cloud services.\n* strong understanding of software architecture, design patterns, and performance optimization.\n* experience with mcp (model context protocol) for ai\\-tool integrations and normalizing data to standards like fdx.\n* familiarity with service bus integrations (tibco, mulesoft, azure service bus, mq).\n* excellent communication skills for articulating complex technical ideas to varied audiences.\n* bachelors degree in computer science, software engineering, or equivalent experience.\n\n**preferred qualifications**\n\n* hands\\-on building mcp servers/clients in node.js/react for fintech ai, plus fdx/service bus.\n* experience leading small teams or serving as a technical lead on feature squads.\n* experience designing distributed systems or microservice architectures with node.js and apis.\n* hands\\-on experience with containers and orchestration (docker, kubernetes).\n* security\\-first mindset and experience implementing secure coding practices in react/node apps.\n* experience implementing ai/ml in fintech products (e.g., fraud detection, personalization).\n* prior experience in the financial services or fintech industries.\n* familiarity with soc 2 or other compliance frameworks.\n\n**why sharetec:**\n\n\nacquired by evergreen financial technology group (eftg) in late 2020, sharetec is now marching towards rapid growth and expansion into new markets. we are a team of highly focused and dedicated individuals who stop at nothing to achieve success no matter how great or small the challenge; we are also a unique bunch of people that love to work and play together. we do our best to make fun a basic part of every day.\n\n\nsharetec offers a robust benefits package, including competitive salaries, medical, dental, vision, life and disability coverage, paid time off (pto), paid holidays \\- including your birthday off!, $1,000 employee referral program, 401(k) and 401(k) matching. we like to put the fun in the funds with department and company outings like paid food trucks, baseball games, and bowling, along with virtual team\\-building activities such as escape rooms, trivia, and other company\\-wide events.\n\n**sharetec is an equal opportunity employer.**\n\n*keywords:* *full stack software engineer, senior software engineer, angular developer, angular 12\\+, typescript,* *rxjs, angular ui development, progress* *openedge,* *openedge* *developer, progress abl, 4gl development, legacy modernization, core banking software, credit union technology, financial services technology, fintech development, api integration, ci/cd, devops, microservices, c\\#, .net, sql server, postgresql, mongodb, backend development, enterprise software, fintech careers, credit union software, banking technology*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "IT Integration Engineer",
        "company": "Tungsten Automation",
        "location": "US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Data Lake",
            "CI/CD",
            "Git",
            "Snowflake",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=bb56bd3754295d02",
        "description": "about the role\n\n\n\nwe\u2019re seeking a hands\\-on integration engineer with experience in boomi and informatica to design, build, and operate scalable integrations across enterprise applications, data platforms, and external partners. you\u2019ll be responsible for end\\-to\\-end integration delivery\u2014from requirements and mappings to deployment and monitoring\u2014supporting mission\\-critical business processes and digital transformation initiatives.\n\n  \n\n\n\nkey responsibilities\n\n\n* architect, design, and implement integrations using boomi atomsphere (processes, atoms/molecules, connectors, maps) and informatica (iics, powercenter, idmc).\n\n\n* develop api\\-based, event\\-driven, and batch integrations; design reusable components and canonical data models.\n\n\n* create and maintain data mappings, transformations, and orchestrations across erp, crm, hcm, data lakes/warehouses, and third\\-party systems.\n\n\n* implement etl pipelines, ingestion frameworks with incremental loads.\n\n\n* configure and manage boomi atoms/molecules (on\\-prem/cloud), environments, and deployment pipelines.\n\n\n* administer informatica repositories, connections, secure agents, schedules, and runtime infrastructure.\n\n\n* monitor and optimize integration performance; implement alerting, observability, and error handling patterns.\n\n\n* enforce api security, secrets management, oauth 2\\.0/jwt, and data protection (pii, pci as applicable).\n\n\n* ensure compliance with data governance and audit requirements; maintain lineage and metadata.\n\n\n* drive integration sdlc: version control, code reviews, ci/cd, automated testing (unit/integration/contract tests).\n\n\n* produce high\\-quality technical documentation: design specs, mapping docs, and support playbooks.\n\n\n* collaborate with product, data, security, and application teams to translate business requirements into technical designs.\n\n\n* troubleshoot production issues; perform root cause analysis and implement remediation/prevention.\n\n\n* identify opportunities for process automation, reusability, and standardization of integration patterns.\n\n\n* evaluate and recommend platform features, connectors, and best practices, mentor team members.\n\n\n* leverages generative ai and automation tools to enhance business processes, analyze trends, support decision\\-making, and improve employee or customer experiences.\n\n  \n\n\n**about our platform**\n----------------------\n\n\n\ntungsten automation's intelligent automation software platform helps government agencies transform information\\-intensive business processes, reduce manual work and errors, minimize costs, and improve customer engagement. we combine generative ai, knowledge management, intelligent document processing, process orchestration, mobility and engagement, and analytics to ease implementations and deliver dramatic results that mitigate compliance risk and increase competitiveness, growth and profitability\u2014particularly crucial for highly regulated industries facing complex compliance requirements. no other software vendor offers a platform of complementary technologies integrated into a scalable, manageable software platform, positioning us to grow and dominate the process automation space.\n\n  \n\n\n**while the job description describes what is anticipated as the requirements of the position, the job requirements are subject to change based upon any changing needs and requirements of the business.**\n\n  \n\n  \n\n  \n\nrequired experience  \n\nrequired qualifications\n\n\n* 3\\-5 years of demonstrable experience in enterprise integrations utilizing boomi and informatica (iics/powercenter/idmc).\n\n\n* strong knowledge of rest/soap apis, web services, json/xml/xsd, swagger/openapi\n\n\n* proficiency in etl/elt, data modeling, and working with sql and nosql data stores (e.g., postgres, sql server, snowflake).\n\n\n* experience with secure agent management, environment promotion, and multi\\-tenant integration patterns.\n\n\n* familiarity with iam, oauth, sso/saml, and secrets management (e.g., bitwarden).\n\n\n* excellent problem\\-solving, communication, and stakeholder engagement skills.\n\n\n* experience using ai tools to analyze data, design workflows, optimize processes, or support strategic initiatives; ability to evaluate ai outputs for accuracy and bias.\n\n\n* ability to design prompts and workflows for ai tools\n\n\n* experience testing, validating, and refining ai\\-generated outputs\n\n\n* understanding of responsible ai use, ethics, and data governance\n\n  \n\n\n\npreferred qualifications\n\n\n* experience with idmc services (cloud data integration, api management, data quality, mdm/360\\).\n\n\n* knowledge of event\\-driven architectures, cdc, and streaming pipelines\n\n\n* background in erp/crm/hcm integrations (netsuite, salesforce, concur).\n\n\n* certifications (nice to have): boomi integration architect, informatica professional/practitioner\n\n\n* build integrations from detailed designs; own component testing and deployments.\n\n\n* manage secure agents/atoms; monitor jobs and address incidents.\n\n\n* contribute to documentation and reusability libraries.\n\n  \n\n\n***the base salary range for this role, across the us, is $118,000 \\- $139,000\\. your actual base pay within this range will be determined by your work location as well as skills, qualifications, experience, and relevant education/training. the range provided reflects only the base salary for the role and does not include benefits.***\n\n\n***tungsten automation corporation, inc. is an equal opportunity employer m/f/disability/vets***",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Machine Learning Engineer IV",
        "company": "Panasonic",
        "location": "Irvine, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Python",
            "R",
            "Java",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=00b52d63f45b1e0a",
        "description": "overview:\n\nour new global headquarters is conveniently located in irvine, ca near john wayne airport in the park place development. for our onsite and hybrid employees you will be able to enjoy amenities such as access to many restaurants and shops, running trails, a fitness deck, outdoor seating, dry cleaning, car wash, free garage parking, car charging stations, shuttle service for train commuters, outdoor games like bocce, horseshoes, gaming tables, pickle ball, and basketball. for more information on park place visit www.parkplaceirvine.com\n\n **who we are:**\n\never wonder who brings the entertainment to your flights? panasonic avionics corporation is \\#1 in the industry for delivering inflight products such as movies, games, wifi, and now bluetooth headphone connectivity!\n\n\nhow exciting would it be to be a part of the innovation that goes into creating technology that delights millions of people in an industry that\u2019s here to stay! with our company\u2019s history spanning over 40 years, you will have stability, career growth opportunities, and will work with the brightest minds in the industry. and we are committed to a diverse and inclusive culture that will help our organization thrive! we seek diversity in many areas such as background, culture, gender, ways of thinking, skills and more.\n\n\nif you want to learn more about us visit us at www.panasonic.aero  \n\nand for a full listing of open job opportunities go to www.panasonic.aero/join\\-us/\n\n **the avionics way:**\n\n\nthe avionics way captures the essence of what makes our employees special, and by extension, our organization. it\u2019s comprised of 5 core identifiers \\- pioneers, problem solvers, accountable, collaborative, and customer focused \\- that together form a robust, working framework for our continued success. beyond a philosophy, the avionics way is our guide to daily practices. it provides a clear, actionable model to help employees, at all levels, realize their ambitions through outsized contributions to their teams and the company. this model clearly defines expectations for performance at all levels of the organization, as well as establishes specific actions and modes of working that reinforce our philosophy on the path to achieving our goals.\n\n\nresponsibilities:\n\nleads machine learning projects, providing technical guidance and mentorship to junior and mid\\-level engineers. design, develop, and implement data\\-driven solutions, leveraging machine learning techniques to enhance our in\\-flight entertainment \\& connectivity (ifec) systems. collaborate with cross\\-functional teams to drive innovation and elevate the passenger experience through advanced technological solutions.\n\n **what you will be doing:**\n\n  \n\ntechnical leadership\n\n* lead the design and implementation of complex, robust, scalable, and high\\-performance data\\-driven systems, integrating machine learning algorithms to improve content delivery, personalization, and connectivity services.\n\n\nalgorithm development \\& implementation\n\n* develop and deploy machine learning models, predictive analytics, and data\\-driven strategies to optimize user experiences and enhance ifec functionalities.\n* develop machine learning frameworks, tools, and technologies. they often have expertise in specialized areas such as distributed computing, deep learning, or natural language processing. they may also have contributed to open\\-source projects or developed custom tools and libraries.\n\n\nmachine learning infrastructure \\& engineering\n\n* plays a key role in architectural decisions related to machine learning infrastructure and deployment pipelines.\n* represent the team in discussions with stakeholders, articulating technical concepts and recommending strategical technical direction.\n* designs scalable and efficient machine learning pipelines, leveraging cloud\\-based platforms and distributed computing frameworks.\n\n\nexperimentation \\& innovation\n\n* explore emerging trends and technologies in machine learning, conduct research, and propose innovative approaches to improve our ifec offerings. publish technical papers and submit patents.\n\n\ncross\\-functional collaboration\n\n* work collaboratively with data scientists, software engineers, product managers, and internal and external stakeholders, effectively communicating technical concepts and insights to drive project alignment and success.\n\n\nperformance optimization\n\n* continuously monitor and optimize the performance of machine learning models, identifying areas for improvement and implementing enhancements to achieve optimal results.\n\n\nmentorship \\& knowledge sharing\n\n* mentor engineers, provide technical guidance, and contribute to the development of a culture of excellence and continuous learning within the team.\n\n\nthe salary range of $138,000 \u2013 $231,000 annually is just one component of panasonic\u2019s total package. the final offer amount may vary based on factors including but not limited to individual\u2019s knowledge, skills, experience, and location.\n\n\nqualifications:\n**what we are looking for:**\n\n* bachelor\u2019s degree in computer science, data science, software engineering, or a related field. advanced degree preferred.\n* 8\\+ years of experience in software engineering, with a focus on data\\-driven applications, machine learning, or ai, preferably in the aerospace or tech industry.\n* demonstrated proficiency in programming languages such as python, java, or similar, along with expertise in machine learning frameworks (e.g., tensorflow, pytorch) and data processing technologies.\n* demonstrated leadership skills with the ability to lead technical initiatives, collaborate effectively across teams, and drive projects to successful completion.\n* excellent problem\\-solving abilities, critical thinking, and a passion for innovation and technology.\n* ability to write technical publications preferred.\n\n**what we offer:**\n\n  \n\nat panasonic avionics corporation we realize the most important aspects in leading our industry are the bright minds behind everything we do. we are proud to offer our employees a highly competitive, comprehensive and flexible benefits program.\n\n* paid time off: exempt salaried employees receive unlimited pto. this means that there is no fixed number, range, or limit to the amount of personal and vacation days that may be taken for exempt employees. non\\-exempt hourly employees accrue 14 vacation days per year \\+ 7 sick days \\+ 3 personal days. accrual rate increases with tenure. all employees receive 11 company paid holidays per year plus a paid company\\-wide shut down in the u.s. between christmas and new year.\n* insurance: medical insurance offerings from aetna and kaiser (ca \\&hi). options for employee only, employee \\+ spouse/domestic partner, employee \\+ children, or family. dental ppo and dmo options \\& vision insurance through eyemed or vsp.\n* 401k with 50% match on up to 8% contribution, full vested from day 1\\.\n* other offerings include: wellness program, counseling services, fsa \\& hsa, life insurance for employee, spouse and child, ad\\&d insurance, long\\-term and short\\-term disability, critical illness insurance, accident insurance, legal assistance, pet insurance, identity theft protection, dependent care flsa, education assistance, commuter program, employee purchase program, service award program.\n\npanasonic is proud to be an equal opportunity/affirmative action employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, and any other characteristic protected by law or company policy. all qualified individuals are required to perform the essential functions of the job with or without reasonable accommodation. pre\\-employment drug testing is required for safety sensitive positions or as may otherwise be required by contract or law. due to the high volume of responses, we will only be able to respond to candidates of interest. all candidates must have valid authorization to work in the u.s. thank you for your interest in panasonic avionics corporation.\n\n  \n\n\\#li\\-ws1\n\n\nsalary range: $138,000 \\- $221,000 annually",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Machine Learning Engineer",
        "company": "Avita Care Solutions",
        "location": "US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "CI/CD",
            "Git",
            "Databricks",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2c48dba4ca99290f",
        "description": "overview:\n\nthe **machine learning engineer** will have deep expertise in healthcare and pharmacy data. the focus of the role is on designing scalable, reliable, and compliant ml solutions that support improved patient outcomes, optimize pharmacy operations, and enable real\\-time, data\\-driven decision\\-making across the organization. **about the role:**\n\n* design, build, deploy, and maintain production\\-grade machine learning models and pipelines using healthcare and pharmacy data (claims, ehrs, prescription data, formulary data, etc.)\n* develop robust end\\-to\\-end ml systems, including data ingestion, feature engineering, model training, validation, deployment, monitoring, and retraining\n* productionize predictive models related to medication adherence, utilization forecasting, cost optimization, and patient outcomes\n* collaborate closely with data scientists, pharmacy experts, clinicians, and engineering teams to translate business and clinical requirements into scalable ml solutions\n* implement mlops best practices, including ci/cd for ml, model versioning, experiment tracking, performance monitoring, and automated retraining\n* optimize model performance, reliability, and latency for batch and/or real\\-time inference use cases\n* ensure all ml systems comply with healthcare regulations (e.g., hipaa) and internal data governance, security, and audit requirements\n* contribute to ml architecture decisions, tooling selection, and platform improvements within the azure ecosystem\n* document ml systems and communicate technical designs and tradeoffs clearly to both technical and non\\-technical stakeholders\n\n**about you:**\n\n* bachelor\u2019s or master\u2019s degree in computer science, machine learning, data science, engineering, or a related field; or equivalent practical experience\n* 3 years \\+ of experience building and deploying machine learning systems in healthcare or pharmacy domains\n* strong proficiency in python for machine learning and software development\n* solid experience with sql and working with large\\-scale relational and cloud\\-based data stores\n* hands\\-on experience implementing and operationalizing machine learning models in production environments\n* experience with azure cloud services for ml workloads\n* familiarity with databricks and distributed data processing frameworks\n* experience with ml lifecycle tools for experimentation, deployment, and monitoring\n* subject matter expert mindset with ability to work independently and work collaboratively to achieve high\u2011quality outcomes\n* demonstrates compassion in supporting patients, partners, and team members\n* strong interpersonal communication skills to collaborate effectively across cross functional teams\n* applies resourcefulness to solve challenges effectively\n\n  \n\nat avita care solutions, compassionate care is at the heart of everything we do. **join avita and get inspired to*****be the care*** **that unlocks the full potential of health for all.**we\u2019re committed to promoting health equity by providing comprehensive, integrated access to pharmacy services, clinical care delivery and digital health through our individualized and culturally competent lgbtq\\+, hiv, prep, and sexual wellness care. avita is a proud member of the u.s. business action to end hiv and has been recognized multiple times by the human rights campaign foundation as a best place to work for lgbtq\\+ equality.\navita is an **equal opportunity employer** dedicated to building a diverse, inclusive, and authentic workplace. we welcome everyone by recruiting, hiring, and promoting individuals without regard to their sex, race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, veteran status, or any other status protected by applicable law. *avita also provides reasonable accommodation for qualified individuals with disabilities in accordance with the americans with disabilities act (ada) and any other state or local laws.* *pay range: $140,000\\-$185,000 annually, based on credentials and geographic location.* **avita care solutions offers a comprehensive benefits package**:\nhealthcare benefits (medical, dental, vision) for eligible team members and their families, along with additional company paid and voluntary benefit offerings.\nsix company paid holidays and three personal floating holidays, paid time off (pto), paid leaves \\- two weeks paid parental leave, bereavement, sick leave, time to vote and jury duty, award recognition program, professional learning and development opportunities.\ncompany paid benefits \u2013 basic life and ad\\&d, maven and health care advocate work/life balance program, health/dependent flexible spending.\nvoluntary benefits \u2013 long and short\\-term disability, pet insurance, legal, accident, hospital indemnity, critical illness, whole and supplementary life insurance, identity theft protection, 401(k) retirement savings plan with company match.\n*all benefits are subject to the applicable plan terms.*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Cloud Engineer",
        "company": "GSK",
        "location": "Philadelphia, PA, US USA",
        "posted_at": "2025-12-18",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7ed37c67470dc51b",
        "description": "**site name:** upper providence, cambridge 300 technology square, philadelphia walnut street  \n\n**posted date:** feb 23 2026  \n\nthe onyx research data platform organization represents a major investment by gsk r\\&d and digital \\& tech, designed to deliver a step\\-change in our ability to leverage data, knowledge, and prediction to find new medicines. we are a full\\-stack shop consisting of product and portfolio leadership, data engineering, infrastructure and devops, data / metadata / knowledge platforms, and ai/ml and analysis platforms, all geared toward:\n\n* building a next\\-generation data experience for gsk\u2019s scientists, engineers, and decision\\-makers, increasing productivity and reducing time spent on \u201cdata mechanics\u201d\n* providing best\\-in\\-class ai/ml and data analysis environments to accelerate our predictive capabilities and attract top\\-tier talent\n* aggressively engineering our data at scale to unlock the value of our combined data assets and predictions in real\\-time\n\n\na senior cloud engineer is a leading technical contributor who can consistently take a loosely defined business or technical requirement, architect and build it to a well\\-defined specification, and execute on it at a high level. they have a strong focus on metrics, both for the impact of their work and for its inner workings / operations. they have a strong focus on metrics, both for the impact of their work and for its inner workings / operations.\n\n\na senior cloud engineer should be deeply familiar with the tools of their specialization and of their customers, and engaged with the open source community surrounding them \u2013 potentially, even to the level of contributing pull requests.\n\n**key responsibilities include:**\n=================================\n\n* design and deploy scalable and reliable cloud infrastructure solutions using google cloud platform (gcp) services.\n* configure and optimise network architecture, including vpcs, subnets, firewalls, and load balancers, to ensure high availability, security, and performance.\n* collaborate with cross\\-functional teams to understand business requirements and translate them into scalable and efficient cloud infrastructure designs.\n* provides input into the roadmaps of teams representing upstream dependencies to help improve the overall program of work\n* automate infrastructure provisioning and deployment processes using infrastructure as code (iac) tools.\n* stay updated with the latest trends and best practices in cloud infrastructure and contribute to the continuous improvement of our cloud infrastructure architecture.\n* optimise resource utilisation and cost management while maintaining compliance standards.\n* provide technical guidance and mentorship to junior team members, fostering a collaborative and knowledge\\-sharing culture within the organisation.\n* troubleshoot and resolve infrastructure\\-related issues and provide technical support to internal teams.\n\n**why you?**\n============\n\n**basic qualifications:**\n=========================\n\n\nwe are looking for professionals with these required skills to achieve our goals:\n\n* bachelor\u2019s degree in computer science, software engineering or related field\n* 5 plus years of cloud experience (e.g., google cloud, aws or azure)\n* 5 plus years of experience working with ci tools (e.g. github actions, jenkins or azure pipelines)\n* 5 plus years of experience with rust, python, typescript or go\n* 3 plus years of experience with container orchestration (e.g. kubernetes, open shift, docker)\n* 3 plus years of experience with tools such as terraform for infrastructure as code\n\n**preferred qualifications:**\n=============================\n\n\nif you have the following characteristics, it would be a plus:\n\n* deep expertise in modern software development tools / ways of working (e.g. git/github, devops tools, metrics / monitoring, \u2026)\n* demonstrated excellence with agile software development environments using tools like jira and confluence\n* deep familiarity with the tools, techniques, etc relevant to their specialization area, including engagement with the open source community (and potentially making contributions to such tools)\n* experience with cloud security platforms, identifying vulnerabilities and risks (i.e. wiz, trivy, aqua, or prisma cloud)\n* deep expertise of networking concepts, technologies, and designs used with large\\-scale deployments on kubernetes.\n\n\n\\#gsk\\-li \\#r\\&dtechproject\n\n\n\\#gskonyx\n\n* if you are based in cambridge, ma; waltham, ma; rockville, md; or san francisco, ca, the annual base salary for new hires in this position ranges $154,275 to $257,125\\.\n\n  \n\nthe us salary ranges take into account a number of factors including work location within the us market, the candidate\u2019s skills, experience, education level and the market rate for the role. in addition, this position offers an annual bonus and eligibility to participate in our share based long term incentive program which is dependent on the level of the role. available benefits include health care and other insurance benefits (for employee and family), retirement benefits, paid holidays, vacation, and paid caregiver/parental and medical leave.  \n\n  \n\nif salary ranges are not displayed in the job posting for a specific country, the relevant compensation will be discussed during the recruitment process.\nplease visit gsk us benefits summary to learn more about the comprehensive benefits program gsk offers us employees.\n\n\n**why gsk?**\n\n**uniting science, technology and talent to get ahead of disease together.**\n\n\ngsk is a global biopharma company with a purpose to unite science, technology and talent to get ahead of disease together. we aim to positively impact the health of 2\\.5 billion people by the end of the decade, as a successful, growing company where people can thrive. we get ahead of disease by preventing and treating it with innovation in specialty medicines and vaccines. we focus on four therapeutic areas: respiratory, immunology and inflammation; oncology; hiv; and infectious diseases \u2013 to impact health at scale.\n\n\npeople and patients around the world count on the medicines and vaccines we make, so we\u2019re committed to creating an environment where our people can thrive and focus on what matters most. our culture of being ambitious for patients, accountable for impact and doing the right thing is the foundation for how, together, we deliver for patients, shareholders and our people.\n\n\nshould you require any adjustments to our process to assist you in demonstrating your strengths and capabilities contact us at hr.americassc\\-cs@gsk.com where you can also request a call.\n\n\nplease note should your inquiry not relate to adjustments, we will not be able to support you through these channels. however, we have created a recruitment faq guide. click the link where you will find answers to multiple questions we receive\n\n\ngsk is an equal opportunity employer. this ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, genetic information (including family medical history), military service or any basis prohibited under federal, state or local law.\n\n**important notice to employment businesses/ agencies**\n\n\ngsk does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. all employment businesses/agencies are required to contact gsk's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to gsk. the obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and gsk. in the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of gsk. gsk shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n\n\nplease note that if you are a us licensed healthcare professional or healthcare professional as defined by the laws of the state issuing your license, gsk may be required to capture and report expenses gsk incurs, on your behalf, in the event you are afforded an interview for employment. this capture of applicable transfers of value is necessary to ensure gsk\u2019s compliance to all federal and state us transparency requirements. for more information, please visit the centers for medicare and medicaid services (cms) website at https://openpaymentsdata.cms.gov/",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist",
        "company": "Ascendion",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "FAISS",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Git",
            "Hadoop",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=24526c3c91ea4416",
        "description": "**about ascendion**\n===================\n\nascendion is a full\\-service digital engineering solutions company. we make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. headquartered in new jersey, our workforce of 6,000\\+ ascenders delivers solutions from around the globe. ascendion is built differently to engineer the next.\n\n\n**ascendion \\| engineering to elevate life**\n\nwe have a culture built on opportunity, inclusion, and a spirit of partnership. come, change the world with us:\n\n\n* build the coolest tech for the world\u2019s leading brands\n* solve complex problems \u2013 and learn new skills\n* experience the power of transforming digital engineering for fortune 500 clients\n* master your craft with leading training programs and hands\\-on experience\n\nexperience a community of change makers!\n\n\njoin a culture of high\\-performing innovators with endless ideas and a passion for tech. our culture is the fabric of our company, and it is what makes us unique and diverse. the way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at ascendion.  \n\n\n\n  \n\n**about the role**\n------------------\n\n### **job title: data scientist**\n\n**job description:**\n\nwe are seeking a highly skilled data scientist with a strong background in **e\\-commerce search** technologies to join our team on an immediate basis. the ideal candidate will have experience working with **learn to rank (ltr) models, vector search, and retrieval algorithms** to drive relevance and performance in large\\-scale search systems.\n\n\n**required skills \\& experience:**\n\n* proven experience in e\\-commerce or similar domains focused on search and ranking.\n* strong expertise in **learn to rank** , **vector similarity search** , and **retrieval models** .\n* python and libraries such as scikit\\-learn, xgboost, tensorflow, or pytorch.\n* experience with **search engines (e.g., elasticsearch, solr, vespa) and ann frameworks (e.g., faiss, annoy, milvus).**\n* solid understanding of information retrieval concepts, relevance metrics, and evaluation methods.\n* familiarity with large\\-scale data processing (e.g., spark, hadoop) is a plus.\n\n**key responsibilities:**\n\n* design, develop, and implement search ranking models using learn to rank approaches.\n* apply vector search techniques to improve relevance and personalization in search results.\n* develop scalable data pipelines to support retrieval algorithms and feature engineering.\n* collaborate with engineering, product, and ux teams to improve search experience.\n* analyze large\\-scale behavioral and clickstream data to inform algorithmic decisions.\n* rapidly prototype and iterate on models with measurable business impact.\n\n**preferred qualifications:**\n\n* master's or phd in computer science, machine learning, data science, or a related field.\n* experience working in fast\\-paced environments with quick turnaround timelines.\n\n**hiring timeline:**  \n\nwe are aiming to conduct interviews next week and onboard immediately after. quick response and availability are highly appreciated..\n\n\n### **location**\n\nremote in united states or canada  \n\n\n\n**salary and other compensation:** the annual \\[salary/hourly rate] for this position is between \\[$170,000 \u2013 180,000 annually] / \\[$70 \\-75 per hour]. factors which may affect pay within this range may include geography/market, skills, education, experience and other qualifications of the successful candidate.\n\n\n**benefits:** the company offers the following benefits for this position, subject to applicable eligibility requirements: \\[medical insurance] \\[dental insurance] \\[vision insurance] \\[401(k) retirement plan] \\[life insurance] \\[long\\-term disability insurance] \\[short\\-term disability insurance] \\[paid parking/public transportation]\n\n\n**want to change the world? let us know.**\n------------------------------------------\n\ntell us about your experiences, education, and ambitions. bring your knowledge, unique viewpoint, and creativity to the table. let\u2019s talk!\n\n\n  \n#### **preferred skills**\n\ndata scientist\n* learn to rank\n* vector search\n* elastic search\n#### **job details**\n\n###### **job id**\n\n331792\n\n\n###### **job requirements**\n\ndata scientist\n\n\n###### **location**\n\nchicago, illinois, us\n\n\n###### **recruiter**\n\ndarpan\n\n\n###### **email**\n\ndarpan.barot@ascendion.com\n\n\n#### **about ascendion**\n\n###### **ascendion is a full\\-service digital engineering solutions company. we make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees.**\n\nour engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. headquartered in new jersey, our workforce of 6,000\\+ ascenders delivers solutions from around the globe. ascendion is built differently to engineer the next.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Data Scientist",
        "company": "The Hartford",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Matplotlib",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b9aa74a73bfb8b29",
        "description": "data scientist \\- gd08ae\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nstep into the future with the hartford as an ai data scientist, where ai isn't just a buzzword\u2014it's central to our strategy. join a team that's pioneering ai\\-driven solutions in corporate functions, transforming how we operate and make decisions. as part of our ai corporate function team, you'll tackle advanced use cases and shape the development of ai capabilities across the corporate functions. here, your work will have a tangible impact, driving innovation and operational excellence with cutting\\-edge ai tools. if you're looking for a role where your expertise is not only valued but essential to our success, the hartford is the place for you.\n\n  \n\nwe are looking for candidates with strong fundamental skills, who are passionate about keeping up with the quick evolution of ai technologies.\n\n\nthis role can have a hybrid or remote work schedule. candidates who live near one of our locations(hartford, ct, charlotte, nc, columbus, oh, chicago, il) will have the expectation of working in an office 3 days a week (tuesday through thursday).\n\n **responsibilities \\& qualifications**\n\n* **deep understanding of the fundamentals of ml, ai, and gen ai**: our ai systems combine predictive models, rule\\-based decisions, and interactions with llms. your grasp of theoretical underpinnings and practical implications will help you match solutions to problems and handle non\\-standard implementations.\n* **software development**: we deploy ai systems in production, so you need to write production\\-grade code in python. this includes object\\-oriented programming, unit testing, version control using git, and familiarity with (or curiosity about) design patterns and best practices in software architecture.\n* **problem solving**: we develop first\\-time solutions to unique challenges, requiring strong analytic skills, resourcefulness, experience in crafting ai solutions, and judgment about the feasibility of different approaches.\n* **experimentation**: we guide the development of ai solutions by evidence and measure performance of specific steps and end\\-to\\-end solutions. you will need to design pragmatic experiments with solid methodology and meaningful metrics.\n* **data processing and analysis**: while our focus is on unstructured data with ai, we still regularly handle tabular datasets, requiring mastery of the traditional data science toolkit (e.g., pandas, sql, excel, matplotlib).\n* **communication and collaboration**: we partner on projects with other teams and serve as an ai expertise source across the enterprise. a collaborative approach and effective communication are key to your success.\n* **additional qualifications:**\n\t+ professional experience with gen ai using llms\n\t+ experience working in agile\n\t+ experience with cloud architecture\n\n **qualifications:**\n\n* master\u2019s or phd in computer science, artificial intelligence, machine learning, engineering or a related field.\n* 3\\+ years of industry experience in machine learning or data science and with 1\\+ years focused on genai.\n* proven experience in statistical modeling and building machine learning algorithms in python\n* proficiency in python and ml frameworks such as pytorch, tensorflow, etc.\n* proven experience in genai tools such as vertex ai/google agent development kit, langchain/langgraph, rag frameworks, huggingface, openai apis, etc.\n* experience with cloud platforms such as gcp, aws, azure, etc.\n* strong analytical, problem solving and debugging skills.\n* excellent communication and collaboration skills, with the ability to explain complex technical concepts to non\\-technical stakeholders across the enterprise.\n\n  \n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position**\n\n **compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$90,160 \\- $135,240\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Data Scientist",
        "company": "The Hartford",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Matplotlib",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=840834a0721f9428",
        "description": "data scientist \\- gd08ae\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nstep into the future with the hartford as an ai data scientist, where ai isn't just a buzzword\u2014it's central to our strategy. join a team that's pioneering ai\\-driven solutions in corporate functions, transforming how we operate and make decisions. as part of our ai corporate function team, you'll tackle advanced use cases and shape the development of ai capabilities across the corporate functions. here, your work will have a tangible impact, driving innovation and operational excellence with cutting\\-edge ai tools. if you're looking for a role where your expertise is not only valued but essential to our success, the hartford is the place for you.\n\n  \n\nwe are looking for candidates with strong fundamental skills, who are passionate about keeping up with the quick evolution of ai technologies.\n\n\nthis role can have a hybrid or remote work schedule. candidates who live near one of our locations(hartford, ct, charlotte, nc, columbus, oh, chicago, il) will have the expectation of working in an office 3 days a week (tuesday through thursday).\n\n **responsibilities \\& qualifications**\n\n* **deep understanding of the fundamentals of ml, ai, and gen ai**: our ai systems combine predictive models, rule\\-based decisions, and interactions with llms. your grasp of theoretical underpinnings and practical implications will help you match solutions to problems and handle non\\-standard implementations.\n* **software development**: we deploy ai systems in production, so you need to write production\\-grade code in python. this includes object\\-oriented programming, unit testing, version control using git, and familiarity with (or curiosity about) design patterns and best practices in software architecture.\n* **problem solving**: we develop first\\-time solutions to unique challenges, requiring strong analytic skills, resourcefulness, experience in crafting ai solutions, and judgment about the feasibility of different approaches.\n* **experimentation**: we guide the development of ai solutions by evidence and measure performance of specific steps and end\\-to\\-end solutions. you will need to design pragmatic experiments with solid methodology and meaningful metrics.\n* **data processing and analysis**: while our focus is on unstructured data with ai, we still regularly handle tabular datasets, requiring mastery of the traditional data science toolkit (e.g., pandas, sql, excel, matplotlib).\n* **communication and collaboration**: we partner on projects with other teams and serve as an ai expertise source across the enterprise. a collaborative approach and effective communication are key to your success.\n* **additional qualifications:**\n\t+ professional experience with gen ai using llms\n\t+ experience working in agile\n\t+ experience with cloud architecture\n\n **qualifications:**\n\n* master\u2019s or phd in computer science, artificial intelligence, machine learning, engineering or a related field.\n* 3\\+ years of industry experience in machine learning or data science and with 1\\+ years focused on genai.\n* proven experience in statistical modeling and building machine learning algorithms in python\n* proficiency in python and ml frameworks such as pytorch, tensorflow, etc.\n* proven experience in genai tools such as vertex ai/google agent development kit, langchain/langgraph, rag frameworks, huggingface, openai apis, etc.\n* experience with cloud platforms such as gcp, aws, azure, etc.\n* strong analytical, problem solving and debugging skills.\n* excellent communication and collaboration skills, with the ability to explain complex technical concepts to non\\-technical stakeholders across the enterprise.\n\n  \n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position**\n\n **compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$90,160 \\- $135,240\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Data Scientist",
        "company": "The Hartford",
        "location": "Hartford, CT, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Matplotlib",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c990c19acf0c31b5",
        "description": "data scientist \\- gd08ae\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nstep into the future with the hartford as an ai data scientist, where ai isn't just a buzzword\u2014it's central to our strategy. join a team that's pioneering ai\\-driven solutions in corporate functions, transforming how we operate and make decisions. as part of our ai corporate function team, you'll tackle advanced use cases and shape the development of ai capabilities across the corporate functions. here, your work will have a tangible impact, driving innovation and operational excellence with cutting\\-edge ai tools. if you're looking for a role where your expertise is not only valued but essential to our success, the hartford is the place for you.\n\n  \n\nwe are looking for candidates with strong fundamental skills, who are passionate about keeping up with the quick evolution of ai technologies.\n\n\nthis role can have a hybrid or remote work schedule. candidates who live near one of our locations(hartford, ct, charlotte, nc, columbus, oh, chicago, il) will have the expectation of working in an office 3 days a week (tuesday through thursday).\n\n **responsibilities \\& qualifications**\n\n* **deep understanding of the fundamentals of ml, ai, and gen ai**: our ai systems combine predictive models, rule\\-based decisions, and interactions with llms. your grasp of theoretical underpinnings and practical implications will help you match solutions to problems and handle non\\-standard implementations.\n* **software development**: we deploy ai systems in production, so you need to write production\\-grade code in python. this includes object\\-oriented programming, unit testing, version control using git, and familiarity with (or curiosity about) design patterns and best practices in software architecture.\n* **problem solving**: we develop first\\-time solutions to unique challenges, requiring strong analytic skills, resourcefulness, experience in crafting ai solutions, and judgment about the feasibility of different approaches.\n* **experimentation**: we guide the development of ai solutions by evidence and measure performance of specific steps and end\\-to\\-end solutions. you will need to design pragmatic experiments with solid methodology and meaningful metrics.\n* **data processing and analysis**: while our focus is on unstructured data with ai, we still regularly handle tabular datasets, requiring mastery of the traditional data science toolkit (e.g., pandas, sql, excel, matplotlib).\n* **communication and collaboration**: we partner on projects with other teams and serve as an ai expertise source across the enterprise. a collaborative approach and effective communication are key to your success.\n* **additional qualifications:**\n\t+ professional experience with gen ai using llms\n\t+ experience working in agile\n\t+ experience with cloud architecture\n\n **qualifications:**\n\n* master\u2019s or phd in computer science, artificial intelligence, machine learning, engineering or a related field.\n* 3\\+ years of industry experience in machine learning or data science and with 1\\+ years focused on genai.\n* proven experience in statistical modeling and building machine learning algorithms in python\n* proficiency in python and ml frameworks such as pytorch, tensorflow, etc.\n* proven experience in genai tools such as vertex ai/google agent development kit, langchain/langgraph, rag frameworks, huggingface, openai apis, etc.\n* experience with cloud platforms such as gcp, aws, azure, etc.\n* strong analytical, problem solving and debugging skills.\n* excellent communication and collaboration skills, with the ability to explain complex technical concepts to non\\-technical stakeholders across the enterprise.\n\n  \n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position**\n\n **compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$90,160 \\- $135,240\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Data Scientist",
        "company": "The Hartford",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Matplotlib",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=63286a781e5a666e",
        "description": "data scientist \\- gd08ae\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nstep into the future with the hartford as an ai data scientist, where ai isn't just a buzzword\u2014it's central to our strategy. join a team that's pioneering ai\\-driven solutions in corporate functions, transforming how we operate and make decisions. as part of our ai corporate function team, you'll tackle advanced use cases and shape the development of ai capabilities across the corporate functions. here, your work will have a tangible impact, driving innovation and operational excellence with cutting\\-edge ai tools. if you're looking for a role where your expertise is not only valued but essential to our success, the hartford is the place for you.\n\n  \n\nwe are looking for candidates with strong fundamental skills, who are passionate about keeping up with the quick evolution of ai technologies.\n\n\nthis role can have a hybrid or remote work schedule. candidates who live near one of our locations(hartford, ct, charlotte, nc, columbus, oh, chicago, il) will have the expectation of working in an office 3 days a week (tuesday through thursday).\n\n **responsibilities \\& qualifications**\n\n* **deep understanding of the fundamentals of ml, ai, and gen ai**: our ai systems combine predictive models, rule\\-based decisions, and interactions with llms. your grasp of theoretical underpinnings and practical implications will help you match solutions to problems and handle non\\-standard implementations.\n* **software development**: we deploy ai systems in production, so you need to write production\\-grade code in python. this includes object\\-oriented programming, unit testing, version control using git, and familiarity with (or curiosity about) design patterns and best practices in software architecture.\n* **problem solving**: we develop first\\-time solutions to unique challenges, requiring strong analytic skills, resourcefulness, experience in crafting ai solutions, and judgment about the feasibility of different approaches.\n* **experimentation**: we guide the development of ai solutions by evidence and measure performance of specific steps and end\\-to\\-end solutions. you will need to design pragmatic experiments with solid methodology and meaningful metrics.\n* **data processing and analysis**: while our focus is on unstructured data with ai, we still regularly handle tabular datasets, requiring mastery of the traditional data science toolkit (e.g., pandas, sql, excel, matplotlib).\n* **communication and collaboration**: we partner on projects with other teams and serve as an ai expertise source across the enterprise. a collaborative approach and effective communication are key to your success.\n* **additional qualifications:**\n\t+ professional experience with gen ai using llms\n\t+ experience working in agile\n\t+ experience with cloud architecture\n\n **qualifications:**\n\n* master\u2019s or phd in computer science, artificial intelligence, machine learning, engineering or a related field.\n* 3\\+ years of industry experience in machine learning or data science and with 1\\+ years focused on genai.\n* proven experience in statistical modeling and building machine learning algorithms in python\n* proficiency in python and ml frameworks such as pytorch, tensorflow, etc.\n* proven experience in genai tools such as vertex ai/google agent development kit, langchain/langgraph, rag frameworks, huggingface, openai apis, etc.\n* experience with cloud platforms such as gcp, aws, azure, etc.\n* strong analytical, problem solving and debugging skills.\n* excellent communication and collaboration skills, with the ability to explain complex technical concepts to non\\-technical stakeholders across the enterprise.\n\n  \n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position**\n\n **compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$90,160 \\- $135,240\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Data Engineer",
        "company": "AdventHealth Corporate",
        "location": "Altamonte Springs, FL, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Snowflake",
            "Kafka",
            "Hadoop",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Java",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=49fe852730ed983b",
        "description": "**our promise to you:**\n\n\njoining adventhealth is about being part of something bigger. it\u2019s about belonging to a community that believes in the wholeness of each person, and serves to uplift others in body, mind and spirit. adventhealth is a place where you can thrive professionally, and grow spiritually, by extending the healing ministry of christ. where you will be valued for who you are and the unique experiences you bring to our purpose\\-minded team. all while understanding that **together** we are even better.\n\n**all the benefits and perks you need for you and your family:**\n\n* benefits from day one: medical, dental, vision insurance, life insurance, disability insurance\n* paid time off from day one\n* 403\\-b retirement plan\n* 4 weeks 100% paid parental leave\n* career development\n* whole person well\\-being resources\n* mental health resources and support\n* pet benefits\n\n**schedule:**\n\nfull time**shift:**\n\n\nday (united states of america)**address:**\n\n\n900 hope way**city:**\n\n\naltamonte springs**state:**\n\n\nflorida**postal code:**\n\n\n32714**job description:**\n\n\nprovide training, mentorship, and guidance to team members on technical data skills to support ongoing learning and capability development. assembles large, complex data sets from operational and cloud data sources to meet business requirements. optimizes and tunes sql query performance in collaboration with enterprise data services and cloud data teams. assists stakeholders with data\\-related technical issues using various project management methodologies. develops statistical analytics solutions to identify patterns and problems using advanced statistical and bi platforms. translates business logic from multiple transactional emr databases into conformed consumer datasets. performs end\\-to\\-end data validation to ensure data accuracy. develops reusable developer dataset and developer data model processes for standardization. understands and applies multiple data modeling approaches to optimize data delivery and automate processes. provides ad\\-hoc data analysis and support to business functions. designs and develops workflows in alteryx for bi reporting tools to migrate datasets into the cloud data platform. extracts and blends data from various crm sources into meaningful datasets for solutioning. develop, implement, and enforce standard operating procedures (sops) for data management to ensure consistency, quality, and compliance across analytics operations. provide training, mentorship, and guidance to team members on technical data skills to support ongoing learning and capability development. ensure the productivity, stability, and uptime of data solutions supporting revenue cycle analytics, proactively identifying and addressing performance issues. other duties as assigned.**knowledge, skills, and abilities:**  \n\n* advanced knowledge of sql, database technologies, and data warehouse design philosophies \\[required]\n* advanced knowledge of self\\-service analytic/etl tools, specifically alteryx designer and bi tools \\[required]\n* successful history of manipulating, processing, and extracting value from large disconnected datasets \\[required]\n* advanced experience with microsoft office suite with emphasis on excel \\[required]\n* ability to effectively prioritize and execute tasks \\[required]\n* strong analytical, problem\\-solving, and troubleshooting abilities \\[required]\n* ability to follow detailed procedures and ensure accuracy in documentation and data \\[required]\n* knowledge of using predictive analytics tools such as r, python, or sas \\[preferred]\n* working knowledge of clinical processes and terminology \\[preferred]\n* competency of bi tools (qlikview, tableau, powerbi) \\[preferred]\n* competency of cloud\\-based computing and data integration services (ms azure, data factory, snowflake) \\[preferred]\n* analytical techniques such as optimization process, predictive models, and text analytics/ nlp \\[preferred]\n* strong understanding of current developments and trends in information technology in one or more of the following domains: data warehousing, etl/elt, data integration, data modeling / database design \\[required]\n* ability to work in sql, oracle sql, and db2 \\[required]\n* dimensional modeling / star schema \\[required]\n* solid and soft skills, verbal and written communication, and the ability to translate complex technical topics into understandable terminology are necessary \\[required]\n* strong organizational skills, with the ability to focus detailed concentrated efforts on multiple projects and re\\-establish priorities as necessary \\[required]\n* ability to effectively respond to time\\-sensitive issues and meet deadlines \\[required]\n* initiative and ability to work independently while thriving in a setting requiring collaboration and teamwork for maximum efficiency and effectiveness \\[required]\n* strong attention to detail that ensures follow through on all initiatives implemented and projects taken \\[required]\n* mastery of desktop applications, including ms office (word, powerpoint, excel, and access) and crystal reports \\[required]\n* team leadership/facilitating skills \\[required]\n* methodical approach to problem\\-solving with excellent analytical skills \\[required]\n* ability and knowledge to understand departmental goals and analyze data to ensure data integrity \\[required]\n* ability to read, think and comprehend technical manual(s), bylaws, regulations, and instructions \\[required]\n* knowledge of plan\\-do\\-study\\-act (pdsa) methodology \\[required]\n* knowledge of organ procurement and transplant center activities \\[required]\n* knowledge of regulatory agencies \\[required]\n* this position must have the proven ability to lead specific functions of large projects and initiatives \\[required]\n* this position builds trust among colleagues, must be a continuous learner, and focus on delivering \\[required]\n* understanding of transplant regulations and operations \\[required]\n* understanding of multi\\-specialty private practice operations \\[preferred]\n* knowledge of quality management systems (lean, six sigma, pdsa, etc.) \\[preferred]\n\n  \n\n**education:**  \n\n* bachelor's \\[required]\n\n  \n\n**field of study:**  \n\n* in computer science, management information systems, business analytics, mathematics or similar analytical field\n\n  \n\n**work experience:**  \n\n* 2\\+ experience building and managing data pipelines using snowflake, with familiarity in azure data factory or similar cloud\\-based data integration tools. \\[preferred]\n* 2\\+ hands\\-on data modeling experience, including integrating multiple data sources and subject areas; preferably within revenue cycle or other complex technical environments required. \\[preferred]\n* 3\\+ years of related experience with big data specific sql workbench and real\\-time applications (hadoop impala, hue) \\[required]\n* 4\\+ years experience in a data engineering or similar role with extensive experience in sql scripting as well as experience with object oriented/function scripting languages (pyton, java, c\\+\\+) \\[required]\n* 6\\+ years of experience in working with analytics with masters degree \\[preferred]\n* experience building and optimizing big data pipelines, architectures, and datasets. \\[required]\n* experience in building, publishing and managing reports/dashboards leveraging qlikview or any other bi tool (ex. tableau) \\[preferred]\n* experience with big data tools: hadoop, spark, kafka, etc. \\[required]\n* experience with extracting and blending data from salesforce objects (soql) \\[required]\n* experience with hiveql or big data \\[preferred]\n\n  \n\n**additional information:**  \n\n* n/a\n\n  \n\n**licenses and certifications:**  \n\n* n/a\n\n  \n\n**physical requirements:** *(please click the link below to view work requirements)*  \n\nphysical requirements \\- https://tinyurl.com/23km2677**pay range:**\n\n\n$83,699\\.48 \\- $155,693\\.55*this facility is an equal opportunity employer and complies with federal, state and local anti\\-discrimination laws, regulations and ordinances.*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Full Stack Engineer",
        "company": "Divya Stores",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Gemini",
            "Copilot",
            "FastAPI",
            "AKS",
            "CI/CD",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=56b238b0f6fd6cb7",
        "description": "**roles and responsibilities:**\n\n* 15\\+ years of experience.\n* we are seeking a highly autonomous full\u2011stack engineer focused on building and deploying generative ai applications and ai agents.\n* this individual will work across frontend and backend, design and integrate llm\u2011powered features, and leverage azure cloud services\u2014including vector search\u2014along with modern multi\u2011cloud ai platforms such as vertex ai and openai.\n* technical skills \\& responsibilities\n* full\\-stack \\& cloud engineering:\n* strong experience with modern frontend \\+ backend stacks (e.g., react, node.js, python, typescript, fastapi, etc.)\n* understands api design, modular architecture, and secure data flows llm \\& genai integration\n* hands\\-on experience calling llms via api, including prompt orchestration and function calling\n* skilled at indexing and retrieving data from vector databases (e.g., azure ai search hybrid \\+ vector search)\n* experience integrating and fine\u2011tuning models across:\n* azure openai (gpt\u20114/4o)\n* google vertex ai (gemini models, embeddings, vector search)\n* openai platform apis\n* ability to design intelligent workflows powered by llms, embeddings, and retrieval pipelines\n* ai agent frameworks\n* experience building ai agents, including:\n* attaching skills/plugins exposed as apis\n* using function calling / openai tools paradigm\n* familiarity with semantic kernel and azure ai agents framework\n* knowledge of mcp (model context protocol) to expose internal systems and apis as agent capabilities\n* devops / ci\\-cd / azure proficient with git and repository branching strategies\n* builds ci/cd pipelines and deploys workloads to azure:\n* azure web apps / azure container apps / aks / acr.\n* monitoring, observability, and strong operational mindset.\n* ai\\-augmented development.\n* proficient with ai coding assistants (cursor, github copilot, etc.) to maximize autonomy and delivery velocity.\n* behavioural \\& delivery expectations.\n* operates independently, takes ownership from concept to deployment.\n* problem\\-solver with strong communication skills.\n* thrives in fast\\-paced environments and continuously learns.\n* thinks proactively about reliability, performance, and security.\n\njob type: contract\n\npay: up to $50\\.00 per hour\n\nexpected hours: 40 per week\n\nwork location: in person",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer, Financial Systems",
        "company": "Opendoor",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Docker",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Snowflake",
            "Quicksight",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c304968f71791ba5",
        "description": "**software engineer, financial systems**\n========================================\n\n\n**team**: it, security, and corporate engineering  \n\n**location**: seattle, washington\n\n\n**about the role**\n------------------\n\n\n\nat opendoor our goal is to build the biggest, most trusted housing platform and set a new standard for how people move. we've combined our deep, proprietary data and operational expertise with the power of artificial intelligence to make online home selling and buying radically simple.\n\n  \n\n\n\nwe are eliminating the manual work that slows down operations. we\u2019re building ai\\-driven automation that closes the books faster, eliminates repetitive workflows, and gives finance teams real\\-time insights without human intervention. you\u2019ll architect the systems that power thousands of real estate transactions monthly with audit\\-ready precision while enabling finance to move fast and operate efficiently.\n\n\n**what you'll do**\n------------------\n\n\n* build intelligent automation that eliminates manual financial workflows, from journal entries to reconciliations to month\\-end close processes, using ai agents and workflow orchestration.\n* design and own integrations connecting enterprise resource planning (erp), spend management, banking, and data warehouse systems that power real\\-time financial operations.\n* partner with finance, procurement, accounting, accounts payable and capital markets stakeholders to understand their workflows, identify automation opportunities, and deliver software solutions that increase their speed and efficiency.\n* implement observability, monitoring, and reliability practices for production financial data systems that process millions of transactions daily.\n* apply software engineering best practices (ci/cd, infrastructure as code, automated testing, code review) to financial systems traditionally managed through manual configuration.\n\n**tech stack**\n--------------\n\n\n* **financial platforms**: netsuite (erp), ramp (spend management), bank apis\n* **data \\& analytics**: snowflake, dbt, airflow/dagster, quicksight\n* **languages**: python, go, sql, typescript/node.js\n* **infrastructure**: aws, terraform, docker, github actions\n* **ai \\& automation**: llms (claude, openai), agent frameworks, workflow orchestration\n\n  \n\n\n**what you'll need**\n--------------------\n\n\n* deep conviction that ai and automation should eliminate manual work humans shouldn't be doing anyway. you're excited to eliminate manual work by replacing spreadsheets and close processes with automated systems.\n* 8\\+ years of software engineering experience with strong skills in python, go, or similar languages, including api design, systems integration, and automation.\n* experience with modern data platforms (snowflake or similar), sql, and building reliable data pipelines.\n* strong experience with cloud platforms (aws preferred) including ci/cd, infrastructure as code, and running production services.\n* proven problem\\-solving ability to understand complex business workflows, identify automation opportunities, and design elegant technical solutions\n* humility and genuine curiosity. you're as excited to automate journal entries as you are to architect distributed systems, and you see eliminating mundane work as deeply meaningful engineering. you genuinely enjoy understanding user needs, partnering with non\\-technical stakeholders, and building software that makes their work easier.\n\n**bonus points for**\n--------------------\n\n\n* experience with financial/erp systems (netsuite, oracle, sap, workday) or fintech platforms.\n* understanding of financial concepts (general ledger, accounts payable, month\\-end close, sarbanes\\-oxley controls).\n* track record building ai\\-powered automation, workflow orchestration, or internal tools.\n* m.s. in computer science.\n\n**compensation**\n----------------\n\n\n\nthe base pay range for this position is $198,000 \\- $271,000 usd annually, plus rsus and bonuses. pay within this range varies by work location and may also depend on your qualifications, job\\-related knowledge, skills, and experience. we also offer a comprehensive package of benefits including unlimited pto, medical/dental/vision insurance, life insurance, and 401(k) to eligible employees.\n\n  \n\n\n\n\\#li\\-ro\n\n\nat opendoor our mission is to tilt the world in favor of homeowners and those who aim to become one. homeownership matters. it's how people build wealth, stability, and community. it's how families put down roots, how neighborhoods strengthen, how the future gets built. we're building the modern system of homeownership giving people the freedom to buy and sell on their own terms. we\u2019ve built an end\\-to\\-end online experience that has already helped thousands of people and we\u2019re just getting started.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer (AI Enablement)",
        "company": "Redwood Logistics",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "AKS",
            "CI/CD",
            "Git",
            "Snowflake",
            "Kafka",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9f0a71f110d4bcaf",
        "description": "**senior software engineer (ai enablement)**\n\n\nreports to: **lead software engineer**\n\n\nlocation: **remote**\n\n\nenvironment: **remote**\n\n\nstatus: **exempt**\n\n\nrecognized by gartner in their modern 4pl market guide, redwood logistics is at the forefront of industry innovation. our cutting\\-edge supply chain technology pairs with the expertise of our brilliant minds to empower logistics execution across north america and mexico.\n\n\nleveraging a comprehensive range of services, data\\-centric network solutions, and a seamlessly integrated platform, we have established our prominence as a key player in the mid\\-market segment within the freight tech industry.\n\n\nwhether you\u2019re just starting your career or are an established professional looking for your next opportunity, redwood inspires innovation across teams to provide transformative solutions for our customers.\n\n\n**position description:**\n\n\nas a **senior software engineer** with **redwood logistics**, you will play a central role in designing and building the next generation of ai\\-enabled products that power our brokerage and managed services platforms. this role focuses on scalable feature development, ai integration, and platform evolution. you will collaborate closely with redwood\u2019s ai product team to deliver intelligent systems, agentic workflows, and data\\-driven automation capabilities that redefine digital logistics operations.\n\n\n**responsibilities:**\n\n\n* lead the design and development of new platform capabilities across redwood\u2019s technology ecosystem.\n* partner with the ai product and data science teams to integrate predictive, generative, and agentic models directly into product workflows.\n* architect scalable, high\\-performance systems leveraging microservices, apis, and event\\-driven patterns to support ai\\-enabled decisioning and automation.\n* own full\\-stack development across .net core, c\\#, and modern javascript frameworks (react preferred), ensuring maintainable, production\\-grade solutions.\n* guide technical design reviews, ensuring solutions align with redwood\u2019s long\\-term architectural direction and platform modernization roadmap.\n* collaborate with cloud and infrastructure teams to optimize performance, reliability, and scalability within azure and containerized environments.\n* implement automated testing, ci/cd pipelines, and observability frameworks to support rapid, high\\-quality releases.\n* mentor engineers in advanced coding practices, architectural thinking, and ai integration techniques.\n* contribute to the definition of redwood\u2019s ai platform architecture, shaping reusable service patterns, data orchestration flows, and model\\-serving frameworks.\n* stay current with emerging ai frameworks, developer tooling, and best practices, driving innovation and continuous improvement.\n\n\n**qualifications:**\n\n\n* bachelor\u2019s degree in computer science or equivalent professional experience.\n* 5\\+ years of professional experience in software engineering, with a focus on scalable, distributed, or data\\-intensive systems.\n* proven experience building and deploying cloud\\-native applications using .net core, c\\#, and modern javascript/typescript frameworks (react, angular, or vue).\n* strong understanding of ai/ml integration patterns, apis, and data pipelines, with practical experience collaborating with data science or ml engineering teams.\n* expertise in azure cloud services, including container orchestration (aks), serverless computing, and event\\-driven design.\n* experience developing restful and graphql apis with attention to performance, resilience, and data integrity.\n* solid understanding of ci/cd pipelines, test automation, and secure deployment workflows.\n* strong command of relational and non\\-relational databases (sql, cosmos db, postgres, or snowflake preferred).\n* excellent communication and collaboration skills, with the ability to work cross\\-functionally across engineering, product, and ai teams.\n* experience developing agentic or ai\\-driven applications, leveraging llms or orchestration frameworks.\n* familiarity with event streaming (kafka, event hubs) and data pipeline orchestration tools (airflow, data factory).\n* preferred experience in logistics, brokerage, or supply chain technology.\n* knowledge of domain\\-driven design and service mesh architectures.\n* proven success in influencing platform\\-level design and scaling ai capabilities into production applications.\n\n\n**rewards:**\n\n\n* culture of unlimited growth with new positions and departments created regularly to support our growing customer base\n* paid parental leave policies\n* medical, dental, vision and 401k plans (with match)\n* flexible\\-spending, mass transit and dependent care plans\n* health savings account, with company contribution\n* short\\-term, long\\-term and life insurance policies subsidized by company\n* cell phone discounts; reduced fees for health and fitness clubs\n* additional benefits including pet insurance, accident care and more\n* competitive referral bonus program\n* competitive pto plans, with extra floating holiday and time off to volunteer\n* fundraising and volunteer opportunities to give back to our local, national and international communities\n\n\n**work schedule:**\n\n\nthis position is full\\-time and remote or remote monday through friday from 8:00 am to 5:00 pm with an hour break, but flexibility is available based on coverage.\n\n\n**compensation range:**\n\n\nsalary range:\n\n\n$130,000 \\- $150,000\n\n\nthis position is eligible to earn annual incentives based on individual and company performance.\n\n\n*the estimated pay range reflects an anticipated range for this position. the actual base salary offered will depend on a variety of factors, including the qualifications of the individual applicant for the position, years of relevant experience, specific and unique skills, level of education attained, certifications or other professional licenses held, and the geographical location in which the applicant lives and/or which they will be performing the job.*\n\n\nredwood is an equal opportunity employer. employment decisions at the company are based on individual merit, qualifications, abilities, and the company\u2019s needs and resources. the company does not discriminate in recruiting, hiring, compensation, promotions, discipline, termination or any other aspect of employment on the basis of an individual\u2019s actual or perceived race, color, creed, religion, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, national origin, ancestry, citizenship status, age, disability, marital status, military service or status, genetic information, arrest and conviction record, credit history, or any other basis protected by applicable law.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer",
        "company": "NetSPI",
        "location": "Minneapolis, MN, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "S3",
            "Docker",
            "GitHub Actions",
            "Git",
            "PostgreSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=64c43e0c9ccd4fc4",
        "description": "netspi\u00ae pioneered penetration testing as a service (ptaas) and leads the industry in modern pentesting. combining world\\-class security professionals with ai and automation, netspi delivers clarity, speed, and scale across 50\\+ pentest types, attack surface management, and vulnerability prioritization. the netspi platform streamlines workflows and accelerates remediation, enabling our experts to focus on deep dive testing that uncovers vulnerabilities others miss. trusted by the top 10 u.s. banks and fortune 500 companies worldwide, netspi has been driving security innovation since 2001\\.\n\n\n\nnetspi is on an exciting growth journey as we disrupt and improve the proactive security market. we are looking for individuals with a collaborative, innovative, and customer\\-first mindset to join our team. learn more about our award\\-winning workplace culture and get to know our a\\-team at www.netspi.com/careers.\n\n\n\nwe\u2019re looking for a **software engineer** to help build and evolve a modern cybersecurity saas platform. this is an early\\-to\\-mid level role for engineers who want to grow building secure, scalable, and well\\-tested systems. we value those who understand that quality, reliability, and ownership are part of the job.\n\n\n\nyou\u2019ll work on end\\-to\\-end features across the stack, collaborate closely with product, and contribute to a system that must perform reliably under real\\-world conditions. our customers depend on our platform, which means engineering discipline, thoughtful design, and operational awareness are foundational to how we work.\n\n\n\nif you think in systems, enjoy and thrive in a collaborative environment, communicate clearly, and are motivated by building durable solutions then we\u2019d love to meet you!\n\n  \n\n\n**responsibilities:**\n\n\n* design and implement features across the stack (backend services, apis, frontend), owning those features from implementation through production support\n* write clean, well\\-tested, maintainable code\n* contribute to technical design discussions and code reviews\n* debug issues across distributed systems and production environments\n* continuously improve code quality, automation, and engineering practices\n\n  \n\n\n**our general tech stack:**\n\n\n* **languages:** java, python, go, c\\#, typescript (java,\n* **infrastructure:** aws (ecs, eks, fargate, lambda, s3, rds aurora)\n* **data:** postgresql\n* **build/deploy:** github actions\n* **platform task orchestration:** temporal\n\n\n**you\u2019ll thrive here if you:**\n\n\n* take ownership and follow through on commitments\n* care about code quality, performance, and testing\n* are comfortable learning new parts of the stack and enjoy learning and evolving technically, professionally, and personally\n* communicate clearly and collaborate well, valuing candor, care, respectful disagreement as a path to better outcomes\n* believe diverse perspectives make teams and products stronger\n* prefer disciplines execution over heroics\n* approach systems thinking with rigor, curiosity, and a bias toward root\\-cause understanding\n* take initiative\n\n  \n\n\n**minimum qualifications:**\n\n\n* bachelor\u2019s degree in computer science, software engineering, or a related field\n* working proficiency in one or more languages from our stack and relational databases\n* strong problem\\-solving and communication skills\n* experience building or consuming restful apis\n* familiarity with containerization and cloud\\-based architectures (docker, aws)\n* understand testing fundamentals (unit/integration)\n\n  \n\n\n**preferred qualifications:**\n\n\n* graduate degree in a related technical field\n* background or certifications in cybersecurity or penetration testing\n* experience with agents, thick clients, air\\-gapped environments, on\\-premises deployments, and/or ai/ml",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Gen AI Application Developer - LangGraph, Bedrock, AWS (ONSITE)",
        "company": "Cognizant Technology Solutions",
        "location": "Detroit, MI, US USA",
        "posted_at": "2026-02-18",
        "score": 11.1,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "Generative AI",
            "LangChain",
            "RAG",
            "FAISS",
            "Pinecone",
            "CI/CD",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=472f11358fc2e3f7",
        "description": "**senior gen ai application developer** **\\-** **langgraph, bedrock, aws** **(onsite)**\n\n\nwe are looking to build a team of agentic ai / generative ai engineers passionate about building agentic workflows for business process transformation in banking domain. we are seeking a highly skilled and innovative **senior developer** with deep expertise in **generative ai**, **aws cloud services**, and **modern front\\-end development**. this role is ideal for someone passionate about building intelligent, scalable applications using cutting\\-edge ai frameworks and cloud\\-native technologies.\n\n\n**key responsibilities**\n\n\n* design, develop, and deploy ai\\-driven applications using **generative ai frameworks** such as **langchain** and **langgraph**.\n* integrate and optimize **amazon bedrock** and **vector databases** for semantic search and retrieval\\-augmented generation (rag) workflows.\n* build and maintain scalable backend services using **aws lambda**, **step functions**, **rds**, **eventbridge**, and **eks**.\n* develop responsive and user\\-friendly web portals using **reactjs**.\n* collaborate with cross\\-functional teams including data scientists, devops, and product managers to deliver high\\-impact solutions.\n* implement best practices for code quality, performance, and security in cloud\\-native environments.\n\n**required skills \\& qualifications**\n\n\n* proven experience with **generative ai** technologies and frameworks such as **langchain, langgraph, and aws bedrock.**\n* strong understanding of **vector databases** (e.g., pgvector, pinecone, faiss, weaviate).\n* hands\\-on expertise in **aws services**: **lambda, step functions, rds, eventbridge, and eks.**\n* experience in front\\-end development using **reactjs**.\n* strong problem\\-solving skills and ability to work in a fast\\-paced, agile environment.\n* excellent communication and collaboration skills.\n\n**preferred qualifications**\n\n\n* experience with retrieval\\-augmented generation (rag) pipelines.\n* familiarity with ci/cd pipelines and infrastructure\\-as\\-code tools.\n* exposure to mlops and ai model deployment strategies.\n\n**cognizant will only consider applicants for this position who are legally authorized to work in the united states without requiring company sponsorship now or at any time in the future.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist",
        "company": "Guild Mortgage Company LLC",
        "location": "US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "Data Lake",
            "Hadoop",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b3b9c468aaf3a5d0",
        "description": "**guild mortgage company**, closing loans and opening doors since 1960\\. as a mortgage banking firm we are dedicated to serving the home owner/buyer. our goal is to provide affordable home financing for our customers, utilizing the best terms available while providing a level of professionalism and service unsurpassed in the lending industry.\n\n\n**position summary**\n\n\n\nthe senior data scientist is a key technical position and plays an important role in the organization by leading and performing number of activities related to the company\u2019s information technology functions. the sr. data scientist is responsible for leading the development and deployment of innovative and impactful data products and services that solve various business problems and create value for the company and its customers. the role requires a strong background in machine learning, statistics, programming, and data engineering, as well as excellent communication and collaboration skills. the sr. data scientist works closely with other data scientists, engineers, product managers, and stakeholders to understand the business needs, define the data requirements, design and implement the data pipelines and workflows, apply and evaluate the machine learning models, and deliver and communicate the data analysis results and findings. the sr. data scientist also mentors and coaches\u2019 junior data scientists and engineers and stays updated with the latest research and trends in data science and related fields.\n\n\n**compensation**\n\n\n\nthis role is an exempt position with a targeted salary range of $124,000 to $170,000 annually.\n\n\n\ncompensation at guild is influenced by a wide array of factors including but not limited to local and federal minimum wage requirements, education, level of experience, and applicant\u2019s geographical location.\n\n\n**essential functions**\n\n\n* lead the development and deployment of sophisticated analytics models to predict, quantify, and interpret complex data related to mortgage lending. utilize machine learning, statistical analysis, and other advanced techniques to improve decision\\-making processes, risk assessment, and customer segmentation.\n* work closely with business units to identify opportunities for leveraging company data to drive business solutions. innovate and implement new modeling techniques and algorithms for predictive analytics and data mining act as a thought leader, staying abreast of the latest industry trends and technologies in data science.\n* applying advanced machine learning techniques and algorithms to analyze large and complex data sets, such as supervised and unsupervised learning, deep learning, natural language processing, computer vision, recommender systems, anomaly detection, etc.\n* ensure the accuracy and integrity of data used for analysis. implement data collection systems and other strategies that optimize statistical efficiency and data quality. collaborate with data engineering teams to improve data collection, storage, processing, and security practices.\n* partner with it, software development, and business teams to translate business needs into technical specifications. design and build data\\-driven products and services that enhance the customer experience and contribute to the bottom facilitate the integration of analytics and machine learning capabilities into the company\u2019s operations and decision\\-making processes.\n* develop and present clear, comprehensive reports and visualizations for both technical and non\\-technical audiences. communicate complex analytical results and insights in a manner that is easily understandable, facilitating informed decision\\-making across the company.\n* provide guidance and mentorship to junior data scientists and lead project teams, ensuring the timely and successful completion of projects. foster a culture of innovation, continuous learning, and collaboration within the team.\n* identify, track, and monitor trends and avoidable technology\\-related errors; work across functions to develop complex solutions, improvements, and stop\\-gaps.\n* focus on the continual improvement of policies, procedures, and processes falling under the scope of authority.\n* use expertise to resolve high level issues that cannot be solved by teammates.\n* stay abreast of latest technology trends and participate in high\\-level decisions impacting the direction of the information technology function.\n* partner with the devops team to setup scalable mlops pipelines.\n* perform other duties as assigned.\n\n\n**qualifications**\n\n\n* bachelor\u2019s degree in statistics, computer science, data science, or related quantitative field is required.\n* master's degree directly related to the position or equivalent, preferred.\n* minimum five years' experience in data science related role(s) and two of those years spent in a senior level role(s) required. a combination of education and related experience may be considered in lieu of experience requirements.\n* expertise in statistical software (e.g., r, sas), programming languages (e.g., python, sql), and big data technologies (e.g., hadoop, spark). strong experience with machine learning libraries (e.g., scikit\\-learn, tensorflow) and data visualization tools (e.g., tableau, powerbi).\n* proven experience with data lake and warehouse best practices and leading products in the marketplace.\n* experience collecting structured, semi\\-structured and unstructured data in various popular formats and sourced from internal core systems as well as 3rd partner providers such as google analytics, facebook insights, zillow, corelogic, mls data, public records, and property data.\n* exceptional analytical and quantitative problem\\-solving skills. ability to work with complex datasets and extract meaningful strong understanding of statistical analysis, predictive modeling, and machine learning algorithms.\n* demonstrated ability to lead and manage projects and strong mentoring and coaching skills to nurture talent within the team. ability to inspire and drive innovation.\n* strong strategic thinking and planning ability to align data science activities with business objectives. proven capability to work under pressure and adapt to changing business environments.\n* ability to prioritize multiple tasks in a deadline\\-driven environment, strong sense of urgency and responsiveness.\n* strong detail orientation and highly organized with proven ability to lead effectively and drive results in a matrixed management environment.\n* ability to think critically, including the ability to evaluate facts and data to draw conclusions, determine the downstream impact of decisions and associated risks.\n* self\\-starter with the demonstrated ability to learn/adapt to new technologies and techniques.\n* excellent verbal and written communication skills required.\n* highly organized and detail\\-oriented; ability to work in a fast\\-paced, metrics\\-driven environment required.\n* proficiency in microsoft office suite, word, excel, wiki, collaborative cloud\\-based programs, and third\\-party software applications required.\n* commitment to company values.\n* customer service \\- proactive attention to each person\n* integrity \\- do and say what's right\n* respect \\- treat others with dignity\n* collaboration \\- listen and work together\n* learning \\- seek knowledge and strive for improvement\n* excellence \u2013 deliver the unexpected\n\n\n**requirements**\n\n\n\nphysical: work is primarily sedentary; mobility in an office setting.\n\n  \n\n\nmanual dexterity: frequent use of computer keyboard and mouse.\n\n  \n\n\naudio/visual: ability to accurately interpret sounds and associated meanings at a volume consistent with interpersonal conversation. regularly required to accurately perceive, distinguish and interpret information received visually and through audio, e.g., words, numbers and other data broadcasted aloud/viewed on a screen, as well as print and other media.\n\n  \n\n\nenvironmental: office environment \u2013 moderate noise, no substantial exposure to adverse environmental conditions.\n\n  \n\n\ntravel: 5% or less\n\n  \n\n\nmental: learn new tasks, remember processes, maintain focus, complete tasks independently, and make timely decisions in the context of a workflow. this role requires effective adaptation to workplace stressors, including customer service complaints, security responsibilities, and competing priorities. must be able to adhere to process protocol. must be able to apply established protocols in a timely manner.\n\n  \n\n\nschedules: work is primarily performed during the business week, monday \\- friday\n\n\n\n  \n\n**supervision**\n\n\n* **job scope:** responsible for understanding the department/functional area objectives and goals and how own job contributes to achievement of these goals; may recommend changes and enhancements based on analysis and evaluation of circumstances\n* **complexity:** general precedents may exist for most problems; conducts independent research/analysis to identify the appropriate approach\n* **impact:** decisions and actions primarily impact own work with limited impact on peers in their area, contributes as team member rather than leader\n* **interaction/supervision:** acts as a mentor/guide to less experienced professional contributor staff in a similar role; works independently and only under general direction; guided by professional standards, desired outcomes, and project plan specifications\n\n\nguild offers a pleasant work environment, competitive compensation and excellent benefits package; including medical, dental, vision, life insurance, ad\\&d, ltd and 401(k) with employer match.\n\n\n\nguild mortgage company is an equal opportunity employer.\n\n\n\nreq\\#: senio017874",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Associate Data Scientist, New College Grad - 2026",
        "company": "Visa",
        "location": "Foster City, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "Git",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=45d3814a2a03d341",
        "description": "**company description**  \n\nvisa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.\n\n\nwhen you join visa, you join a culture of purpose and belonging \u2013 where your growth is priority, your identity is embraced, and the work you do matters. we believe that economies that include everyone everywhere, uplift everyone everywhere. your work will have a direct impact on billions of people around the world \u2013 helping unlock financial access to enable the future of money movement.\n\n**join visa: a network working for everyone.**\n\n **job description**  \n\nthe global data science team at visa leverages our rich data \\- spanning over 3 billion card accounts and 100 billion transactions per year\\- and other third\\-party data sources to solve meaningful business problems.\n\n\nwe are seeking an associate data scientist to support the data science efforts for visa\u2019s global cross\\-broder team. the role is part of the global data office and will work closely with senior data scientists and business partners in day\\-to\\-day operations. this associate will contribute in executing an analytics agenda that delivers insights and ai\\-powered solutions to improve visa\u2019s products and services.\n\n**essential functions:**\n\n**hands\\-on data science \\& innovation**\n\n* contribute to the development and deployment of analytics and machine learning models, supporting use cases from data exploration through validation and implementation under guidance from senior team members.\n* apply generative ai techniques (e.g., prompt engineering, llm\u2011based text analysis, summarization, and classification) to enhance data analysis, insight generation, and internal workflows.\n* leverage large\u2011scale datasets using sql, python, r, hive, or sas, combining traditional statistical methods with ml and genai\u2011assisted approaches to uncover trends and actionable insights.\n* use ai\u2011powered development tools (e.g., coding assistants, automl, and notebook automation) to accelerate experimentation, improve code quality, and increase productivity.\n* build and maintain bi dashboards and reports, and support user adoption through documentation, walkthroughs, and guidance on best practices for bi usage.\n* develop intuitive visualizations and dashboards to communicate insights and model outputs to technical and non\\-technical audiences.\n* support model deployment and monitoring efforts, collaborating with data engineering teams and following established mlops, data governance, and responsible ai guidelines.\n\n**business partnership \\& strategy**\n\n* partner with product, marketing, operations, and finance teams to understand business questions and translate them into analytical tasks.\n* assist in framing business problems into analytical approaches, contributing to data\u2011driven solutions that inform product and operational decisions.\n* present insights and recommendations using structured storytelling, clearly explaining assumptions, limitations, and potential business impact.\n* support prioritization of analytics initiatives by considering business value, data availability, and technical feasibility in collaboration with senior team members.\n\n**cross\\-functional collaboration**\n\n* communicate technical findings in simple, actionable terms to non\u2011technical partners and stakeholders.\n* help drive adoption of analytics solutions by validating results, documenting methodologies, and demonstrating how insights address real business needs.\n* collaborate closely with cross\u2011functional teams to iterate on analyses and improve solutions based on feedback.\n\n  \n\n**qualifications** **basic qualifications**\n\n* bachelor's or master's degree in computer science, computer engineering, cis/mis, cybersecurity, statistics, business or a related field, graduating may 2025 \\- august 2026\\.\n\n**preferred qualifications**\n\n* 2\\+ years of experience in data analysis, quantitative modeling, or data driven decision making in an academic or professional setting.\n* proficiency in sql and python for data analysis and modeling.\n* experience extracting, transforming, aggregating, and analyzing large datasets using sql, python, r, and spark, including exploratory data analysis and feature engineering.\n* hands\u2011on experience using generative ai or ai\u2011assisted tools (e.g., llms, coding assistants, automl) to support data analysis, insight generation, or workflow efficiency.\n* applied experience with generative ai techniques, such as prompt engineering, text summarization, classification, or llm assisted analysis, through coursework, projects, or professional work.\n* familiarity with responsible ai considerations, including data privacy, bias awareness, and model limitations.\n* solid foundation in statistics and machine learning, including regression, classification, and model evaluation techniques.\n* hands on experience building descriptive and predictive models using machine learning libraries and tools such as scikit learn, jupyter notebooks, python, r, and/or sas.\n* exposure to data mining and statistical modeling techniques, including regression, clustering, decision trees, and related methods.\n* experience in building and maintaining bi solutions using tools like tableau, power bi, or similar platforms, including metrics definitions, semantic layers, data quality validation, and user support.\n* effective communication and collaboration skills, with the ability to explain data driven insights clearly to business stakeholders and translate analysis into actionable recommendations for technical and non\\-technical audiences.\n* experience organizing and managing analytical work using productivity tools such as excel, word, powerpoint, and collaboration platforms (e.g., teams).\n* exposure to financial services, payments, credit cards, or merchant analytics is a plus but not required.\n\n  \n\n**additional information** **u.s. applicants only:** the estimated salary for a new hire into this position is $120,000 which may include potential sales incentive payments (if applicable). salary may vary depending on job\\-related factors which may include knowledge, skills, experience, and location. in addition, this position may be eligible for bonus and equity. visa has a comprehensive benefits package for which this position may be eligible that includes medical, dental, vision, 401 (k), fsa/hsa, life insurance, paid time off, and wellness program.\n\n**work authorization:** permanent authorization to work in the u.s. is a precondition of employment for this position. visa will not sponsor applicants for work visas in connection with this position. future sponsorship will not be considered.\n\n**work hours:** varies upon the needs of the department\n\n**this is a hybrid position**: expectation of days in office will be confirmed by your hiring manager.\n\n**travel requirements:** this position requires travel 5\\-10% of the time.\n\n**mental/physical requirements:** this position will be performed in an office setting. the position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.\n\n\nvisa is an eeo employer. qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. visa will also consider for employment qualified applicants with criminal histories in a manner consistent with eeoc guidelines and applicable local law.\n\n\nvisa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of article 49 of the san francisco police code.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Engineer (Python/Spark)",
        "company": "Take-Two Interactive Software, Inc.",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "S3",
            "Docker",
            "Jenkins",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=98762eeaa2f684be",
        "description": "**who we are**\n--------------\n\n\n\nheadquartered in new york city, take\\-two interactive software, inc. is a leading developer, publisher, and marketer of interactive entertainment for consumers around the globe. we develop and publish products principally through rockstar games, 2k, and zynga. our strategy is to create hit entertainment experiences, delivered on every platform relevant to our audience through a variety of sound business models. our pillars \\- creativity, innovation, and efficiency \\- guide us as we strive to create the highest quality, most captivating experiences for our consumers. the company's common stock is publicly traded on nasdaq under the symbol ttwo. for more corporate and product information please visit our website at http://www.take2games.com.\n\n\n\nwhile our offices (physical and virtual) are casual and inviting, we are deeply committed to our core tenets of creativity, innovation and efficiency, and individual and team development opportunities. our industry and business are continually evolving and fast\\-paced, providing numerous opportunities to learn and hone your skills. we work hard, but we also like to have fun, and believe that we provide a great place to come to work each day to pursue your passions.\n\n\n**the challenge**\n-----------------\n\n\n\nwe are looking for a dynamic data engineer to join a team involved in designing, building, and maintaining reliable and scalable data solutions.\n\n\n\nthe ideal candidate is a strong python developer who has some experience in front\\-end technologies like javascript, css3 and/or html5\\. they are a self\\-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoy working in a fast\\-paced environment.\n\n\n**what you'll take on**\n-----------------------\n\n\n* develop and maintain scalable data pipelines and build out new api integrations to support continuing increases in data volume and complexity.\n* work with business teams and analysts to understand complex business rules and implement these for deriving insights.\n* implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\n* perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.\n* provide thought leadership and collaborate with other team members to continue to scale our architecture to evolve for the needs of tomorrow.\n* develop and support continuous integrations build and deployment processes using jenkins, docker, git, etc.\n* define and implement monitoring and alerting policies for data solutions.\n\n\n**what you bring**\n------------------\n\n\n* 3\\+ years of hands\\-on experience in python.\n* 3\\+ years of hands\\-on experience in using advanced sql queries and writing/optimizing highly efficient sql queries.\n* 2\\+ years of working in aws environment.\n* experience with git.\n* experience with docker.\n* comfortable working with business customers to collect requirements and gain a deep understanding of varied business domains.\n* knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.\n* bachelor's degree or equivalent in an engineering or technical field such as computer science, information systems, statistics, engineering, or similar.\n\n\n**great to have**\n-----------------\n\n\n* experience integrating with 3rd party apis is highly desirable.\n\n\n**what we offer you**\n---------------------\n\n\n* **great company culture**. ranked as one of the most creative and innovative places to work, creativity, innovation, efficiency, diversity and philanthropy are among the core tenets of our organization and are integral drivers of our continued success.\n* **growth**: as a global entertainment company, we pride ourselves on creating environments where employees are encouraged to be themselves, inquisitive, collaborative and to grow within and around the company.\n* **work hard, play hard.** our employees bond, blow\\-off steam, and flex some creative muscles \u2013 through corporate boot camp classes, company parties, game release events, monthly socials, and team challenges.\n* **benefits**. medical (hsa \\& fsa), dental, vision, 401(k) with company match, employee stock purchase plan, commuter benefits, in\\-house wellness program, broad learning \\& development opportunities, a charitable giving platform with company match and more!\n* **perks**. fitness allowance, employee discount programs, free games \\& events and stocked pantries.\n\n  \n\n\n*please be aware that take\\-two does not conduct job interviews or make job offers over third\\-party messaging apps such as telegram, whatsapp, or others. take\\-two also does not engage in any financial exchanges during the recruitment or onboarding process, and the company will never ask a candidate for their personal or financial information over an app or other unofficial chat channel. any attempt to do so may be the result of a scam or phishing exercise. take\\-two's in\\-house recruitment team will only contact individuals through their official company email addresses (i.e., via a take2games.com email domain). if you need to report an issue or otherwise have questions, please contact careers@take2games.com*\n\n\n*as an equal opportunity employer, take\\-two interactive software, inc. (\"take\\-two\") is committed to fostering and celebrating the diverse thoughts, cultures, and backgrounds of its talent, partners, and communities throughout its organization. consistent with this commitment, take\\-two does not discriminate or retaliate against any employee or job applicant because of their race, color, religion, sex (including pregnancy, sexual orientation, and gender identity), national origin, age, disability, and genetic information (including family medical history), or on the basis of any other trait protected by applicable law. if you need to report a concern or have questions regarding take\\-two's equal opportunity commitment, please contact careers@take2games.com.*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Analytics Engineer, Sales Analytics (GTM Data Engineering)",
        "company": "Klaviyo",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "S3",
            "EC2",
            "Terraform",
            "Git",
            "Snowflake",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=721d4b934257a77e",
        "description": "*at klaviyo, we value the unique backgrounds, experiences and perspectives each klaviyo (we call ourselves klaviyos) brings to our workplace each and every day. we believe everyone deserves a fair shot at success and appreciate the experiences each person brings beyond the traditional job requirements. if you're a close but not exact match with the description, we hope you'll still consider applying. want to learn more about life at klaviyo? visit* *klaviyo.com/careers* *to see how we empower creators to own their own destiny.*\n\n**about the team and role**\n---------------------------\n\n\n\ndata is at the heart of every decision made at klaviyo, and we're looking for a business intelligence data engineer to join our go to market (gtm) team supporting sales analytics. this domain of data aims to improve the experience of sales operations and analytics. secondary to this, the role will support ancillary functions of the partnerships organization and collaborate with the cs\\&s organization. this role sits in data engineering as part of the gtm team, which is part of a hub and spoke model of analytics engineering at klaviyo.\n\n\n\nyou'll build and steward the source of truth for sales operational data so people leaders and analysts can answer questions quickly and confidently and turn those insights into a more incentivized, higher\\-performing organization. you will directly support the sales analytics teams at klaviyo, working cross functionally with systems, deal ops,, audits, planning, and business intelligence.\n\n\n\nyou will be an independent self\\-serving, embedded partner to all of gtm leadership where sales subject matter expertise is required, capable of translating ambiguous requirements into stable data products. you'll be supported by the broader data engineering organization's standards, tooling, and review practices.\n\n\n**how you'll make a difference**\n--------------------------------\n\n\n* deliver sales specific data modeling that drives better operational experience, is sox compliant, and provides detailed insight into the day to day of operation as well as forecasting. maintain and stand up curated, documented marts that make it easy for analytics to operate within a structured governance mandate that enables the sales operations team to work fast and focus on their organizations with the safety of production ready data.\n* own the pipelines \\& models end\u2011to\u2011end. build and maintain reliable integrations from core sales systems (e.g., crms/rop/erps), model them in dbt, and publish governed marts and reverse\u2011etls to operational destinations where they create value.\n* create attainment views with compensation and people analytics. partner with analysts to build holistic views of the sales lifecycle. examples of focus are quicker cadences to booking and dynamic reconciliation processes\n* raise the bar on data reliability and governance. instrument monitoring and alerting, tests (freshness/volume/constraints), and documentation so the sales data ecosystem is discoverable, auditable, and self\\-serveable.\n* operate as a trusted partner to leadership. work directly with operations and sales leadership to scope problems, clarify trade\u2011offs, and communicate technical concepts in exec\u2011ready language.\n\n**what you'll do (responsibilities)**\n-------------------------------------\n\n\n* integrations \\& ingestion: own secure ingestion from rop/erp/performance systems into snowflake; define slas/slos; implement monitoring \\& alerting for each feed.\n* modeling \\& marts: design dimensional/entity models (dbt) for employees, positions, org structure, performance history, forecasting, and pipeline movement; publish curated marts with strong contracts and lineage.\n* reverse etl: operationalize high\u2011value models to downstream tools and workflows using reverse\u2011etl patterns to close the loop between insight and action.\n* quality \\& governance: implement tests (unit/integration, schema/freshness), multi\\-layered validation frameworks that routinely validate data integrity, data policies (masking, purpose\u2011based access), and documentation that enable safe self\u2011service across the analytics community.\n* repository stewardship: maintain the analytics codebase (dbt repo), perform code reviews, and ensure modular, reusable patterns the broader team can adopt.\n* stakeholder partnership: run an intake \\& engagement model with revenue and sales operations/ analytics (primary), hris/people tech (security/integration), finance (plan/comp interfaces), and bi/platform teams (shared standards).\n\n**who you are (qualifications)**\n--------------------------------\n\n\n* 3\u20135\\+ years in analytics/data engineering with production elt in snowflake \\+ dbt \\+ sql; python for orchestration/utilities.\n* strong demonstration of sales operations\n* demonstrated independence partnering with senior, non\u2011technical leaders; able to translate open\u2011ended needs into scalable data products.\n* proven experience implementing tests, monitoring, and documentation that keep pipelines healthy and reporting trustworthy.\n* experience building data integrations and reverse\u2011etl pipelines that support business operations.\n* airflow (orchestration) and fivetran/workato (elt/integration).\n* familiarity with data privacy controls (masking/rls) in people data.\n* aws experience (s3/ec2/lambda) and iac/terraform.\n\n**the tools you'll use**\n------------------------\n\n\n\nsnowflake, dbt, airflow, fivetran, workato, python, aws, tableau/looker/thoughtspot; publishing via reverse\u2011etl where appropriate.\n\n**get to know klaviyo**\n\n\n\nwe're klaviyo (pronounced clay\\-vee\\-oh). we empower creators to own their destiny by making first\\-party data accessible and actionable like never before. we see limitless potential for the technology we're developing to nurture personalized experiences in ecommerce and beyond. to reach our goals, we need our own crew of remarkable creators\u2014ambitious and collaborative teammates who stay focused on our north star: delighting our customers. if you're ready to do the best work of your career, where you'll be welcomed as your whole self from day one and supported with generous benefits, we hope you'll join us.\n\n\n*ai fluency at klaviyo includes responsible use of ai (including privacy, security, bias awareness, and human\\-in\\-the\\-loop). we provide accommodations as needed.*\n\n\n*by participating in klaviyo's interview process, you acknowledge that you have read, understood, and will adhere to our* *guidelines for using ai in the klaviyo interview process**. for more information about how we process your personal data, see our* *job applicant privacy notice**.*\n\n  \n\n*klaviyo is committed to a policy of equal opportunity and non\\-discrimination. we do not discriminate on the basis of race, ethnicity, citizenship, national origin, color, religion or religious creed, age, sex (including pregnancy), gender identity, sexual orientation, physical or mental disability, veteran or active military status, marital status, criminal record, genetics, retaliation, sexual harassment or any other characteristic protected by applicable law.*\n\n\n\n*important notice: our company takes the security and privacy of job applicants very seriously. we will never ask for payment, bank details, or personal financial information as part of the application process. all our legitimate job postings can be found on our official career site. please be cautious of job offers that come from non\\-company email addresses (@klaviyo.com), instant messaging platforms, or unsolicited calls.*\n\n**by clicking \"submit application\" you consent to klaviyo processing your personal data in accordance with our job applicant privacy notice. if you do not wish for klaviyo to process your personal data, please do not submit an application.***you can find our job applicant privacy notice* *here* *and* *here* *(fr).*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Digital Analytics Customer Experience (CX) Engineer",
        "company": "Sherwin-Williams",
        "location": "Cleveland, OH, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Git",
            "Snowflake",
            "Databricks",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala",
            "A/B Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=12fcbbc0dd03080c",
        "description": "we are seeking a highly analytical and technically skilled senior digital analytics customer experience (cx) engineer to join our global digital analytics customer experience (cx) team, supporting the performance coatings group\u2019s digital initiatives. this role is responsible for designing, implementing, and optimizing digital data collection strategies across websites, applications, and digital marketing channels.\n\n\nthe ideal candidate brings deep experience in digital analytics, strong data engineering and data governance expertise, and experience in data science and ai. this candidate is expected to use these skill sets to help elevate customer experiences, build a foundation for personalization and digital marketing activation, and drive business growth.\n\n\nas a senior member of the team, this person will play a critical leadership role in ensuring data quality, standardization, and connectivity across digital platforms, while advancing our measurement framework, analytics automation, and predictive insights capabilities.\n\n**additional information:**\n\n**travel required: 10%**\n\n**this role is not remote/hybrid. this role will report to sherwin\\-williams global headquarters located at 1 sherwin way, cleveland, oh 44113\\.**\n\n\njob duties include contact with other employees and access confidential and proprietary information and/or other items of value, and such access may be supervised or unsupervised. the company therefore has determined that a review of criminal history is necessary to protect the business and its operations and reputation and is necessary to protect the safety of the company\u2019s staff, employees, and business relationships.\n\n\nat sherwin\\-williams, our purpose is to inspire and improve the world by coloring and protecting what matters. our paints, coatings and innovative solutions make the places and spaces in our world brighter and stronger. your skills, talent and passion make it possible to live this purpose, and for customers and our business to achieve great results. sherwin\\-williams is a place that takes its stability, growth and momentum and translates it to possibility for our people. our people are behind the strength of our success, and we invest and support you in:\n\n\n\nlife \u2026 with rewards, benefits and the flexibility to enhance your health and well\\-being\n  \n\ncareer \u2026 with opportunities to learn, develop new skills and grow your contribution\n  \n\nconnection \u2026 with an inclusive team and commitment to our own and broader communities\n  \n\nit's all here for you... let's create your **possible**\n\n\n\nat sherwin\\-williams, part of our mission is to help our employees and their families live healthier, save smarter and feel better. this starts with a wide range of world\\-class benefits designed for you. from retirement to health care, from total well\\-being to your daily commute\u2014it matters to us. a general description of benefits offered can be found at http://www.myswbenefits.com/. click on \u201ccandidates\u201d to view benefit offerings that you may be eligible for if you are hired as a sherwin\\-williams employee.\n\n\n\ncompensation decisions are dependent on the facts and circumstances of each case and will impact where actual compensation may fall within the stated wage range. the wage range listed for this role takes into account the wide range of factors considered in making compensation decisions including skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. the wage range, other compensation, and benefits information listed is accurate as of the date of this posting. the company reserves the right to modify this information at any time, with or without notice, subject to applicable law.\n\n\n\nqualified applicants with arrest or conviction records will be considered for employment in accordance with applicable federal, state, and local laws including with the los angeles county fair chance ordinance for employers and the california fair chance act where applicable.\n\n\n\nsherwin\\-williams is proud to be an equal employment opportunity employer. all qualified candidates will receive consideration for employment and will not be discriminated against based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, pregnancy, genetic information, creed, marital status or any other consideration prohibited by law or by contract.\n\n\n\nas a vevraa federal contractor, sherwin\\-williams requests state and local employment services delivery systems to provide priority referral of protected veterans.\n\n\n\nplease be aware, sherwin\\-williams recruiting team members will never request a candidate to provide a payment, ask for financial information, or sensitive personal information like national identification numbers, date of birth, or bank account numbers during the application process.\n\n**enable personalized digital customer experiences through data collection \\& connectivity** **digital data collection, tagging, and implementation*** partner with digital analysts and it to enable insights generation for marketing, product, and digital teams through reliable, well\u2011structured data and tagging strategies (adobe analytics, google analytics)\n* partner closely with development teams, conduct thorough qa and user acceptance testing (uat) for analytics tags, data layer updates, and new digital customer experiences.\n* lead digital tagging strategy, implementation, and validation using enterprise tag management systems (e.g., adobe tags, google tag manager).\n* own and evolve the solution design reference (sdr) documentation, ensuring alignment with business requirements and best\u2011practice analytics architecture.\n* provide technical guidance on digital data layer design, event tracking, custom dimensions/variables, and tagging governance.\n**data\\-driven customer experience personalization and segmentation*** support a/b and multivariate testing with adobe target by establishing proper tagging, data validation, and measurement frameworks.\n* build audience segments for digital marketing activation and personalization in adobe or other digital platforms.\n**data engineering, data science, and ai enablement*** connect digital platforms and data by designing and maintaining scalable, automated data pipelines from digital marketing, analytics, and business intelligence platforms (e.g., tableau)\n* perform exploratory data analysis (eda) using python, r, or sql to support segmentation, clustering, and predictive modeling.\n* assist in the development and deployment of machine learning models (e.g., churn prediction, propensity scoring, anomaly detection).\n**digital data governance \\& quality*** champion enterprise data governance and quality standards across digital analytics platforms\n* identify data gaps, inconsistencies, and duplication; drive improvements to increase data completeness, accuracy, and scalability.\n* maintain and enhance naming conventions, taxonomies, standards, and data dictionaries for analytics assets.\n* contribute to data hygiene routines including anomaly detection, validation processes, and automated quality checks.\n**cross\u2011functional collaboration with customer experience teams*** serve as a trusted technical advisor to marketing, ecommerce, digital products, and it teams.\n* communicate complex data concepts and implementation details to both technical and non\u2011technical audiences with clarity.\n* provide mentorship and guidance to junior analysts and engineers, fostering growth and knowledge sharing across the team.\n**minimum requirements:**\n\n* must be at least eighteen (18\\) years of age\n* must be legally authorized to work in the country of employment without needing sponsorship for employment work visa status now or in the future (e.g., opt, cpt, h1b, eb\\-1,etc.).\n* must have a bachelors degree or higher\n* have at least two (2\\) years of experience in leadership role(s), supervising others or leading teams\n* must have five (5\\) years of experience in digital analytics engineering, analytics implementation, web measurement, or marketing data engineering roles.\n* must have strong hands\\-on expertise with digital analytics platforms such as adobe analytics, google analytics 4, or similar enterprise solutions.\n* must have advanced proficiency with tag management systems (adobe tags, google tag manager).\n* must have experience designing and managing data layers, sdrs, and tagging governance frameworks.\n* must be proficient in sql for data extraction, transformation, and automation.\n* must have working knowledge of python or r\n* must be familiar with cloud and data engineering environments such as snowflake, databricks, azure, or gcp.\n* must have experience with tableau, power bi, or other bi platforms\n* must understand a/b testing platforms and experimentation (e.g., adobe target)\n\n**preferred qualifications:**\n\n* have a bachelor's degree or higher in business or marketing analytics, data science, statistics, mathematics, computer science\n* have at least three (3\\) years of experience in leadership role(s), supervising others or leading teams\n* have seven (7\\) years of experience in digital analytics engineering, analytics implementation, web measurement, or marketing data engineering roles.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist - Agentic AI",
        "company": "Allstate Insurance",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "CI/CD",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2ae05a4ee446a6a0",
        "description": "at allstate, great things happen when our people work together to protect families and their belongings from life\u2019s uncertainties. and for more than 90 years, our innovative drive has kept us a step ahead of our customers\u2019 evolving needs. from advocating for seat belts, air bags and graduated driving laws, to being an industry leader in pricing sophistication, telematics, and, more recently, device and identity protection.\n\n**job description**\n\n\nas a senior data scientist in allstate\u2019s ai risk, governance and research area, you\u2019ll be responsible for the creation of comprehensive data\\-driven agentic solutions, from data collection, model/agent development to deployment and ongoing support. your experience designing and implementing ai agents, cloud\\-based ai solutions and maintaining scalable ai systems will help drive success. you\u2019ll collaborate across teams to establish best practices and reusable components, ensuring consistent and high\\-quality execution of agents .  \n\n  \n\nas a team member, you\u2019ll actively participate in the development and deployment of machine learning, ai solutions, with a focus on observability, security, and efficient llm utilization.### **key responsibilities:**\n\n* identifies llms, programming languages, and tools that can bring efficiencies or needed techniques to the team. identifies new areas of data, research and models and develops data sources that can solve business problems.\n* utilizes effective project planning techniques to break down ai/predictive modeling and/or development tasks into manageable subtasks, and ensure deadlines are kept. executes various analytical and modeling projects while collaborating with peers, as needed.\n* uses best practices to develop complex statistical, machine learning techniques to build models that address business needs and to improve the accuracy of our data and data\\-driven decisions\n* reviews, evaluates, communicates, and makes recommendation on appropriateness of ai/modeling techniques and results to team, leadership, and stakeholders to ensure these are well understood and incorporated into business processes.\n* develops and executes communication strategy, keeping stakeholders informed and influencing business partners and senior leaders on the effectiveness of machine learning/predictive modeling to solve business problems.\n* collaborates with outside business units and works on data and complex business problems to improve the effectiveness of business decisions and business results through designing, building, and partnering to implement machine learning/predictive models.\n* effectively understand the business' problems and requirements to identify the optimal modeling approach.\n* develops frameworks/prototypes that integrate data and machine learning/predictive modeling to make business decisions.\n\n### **functional experience:**\n\n* experience in building and optimizing cloud\\-based machine learning systems, with proficiency in at least one major cloud provider.\n* experience with infrastructure as code (iac) frameworks to provision and manage cloud resources\n* strong understanding of ci/cd pipelines, containerization (docker), observability tools, and cloud security practices.\n* familiarity with modern data science tools and libraries (e.g., python, r, sql, tensorflow, pytorch).\n* aws bedrock / azure foundry experience\n* ai agent development using at least one framework (e.g., azure af, aws strands, google adk, langgraph, openai agents sdk)\n* nice to have: prior risk management or ai governance experience\n\n### **experience:**\n\n* 3 or more years of experience (preferred)\n\n\n\\#li\\-te1\n\n**skills**\n\n\nai agents, ai governance, communication, computer programming, data analytics, data science, machine learning (ml), product roadmapping, python (programming language), tensorflow**compensation**\n\n\ncompensation offered for this role is 100,000\\.00 \\- 170,500\\.00 annually and is based on experience and qualifications.\nthe candidate(s) offered this position will be required to submit to a background investigation.\n\n**joining our team isn\u2019t just a job \u2014 it\u2019s an opportunity. one that takes your skills and pushes them to the next level. one that encourages you to challenge the status quo. one where you can shape the future of protection while supporting causes that mean the most to you. joining our team means being part of something bigger \u2013 a winning team making a meaningful impact.**\n\n\nallstate generally does not sponsor individuals for employment\\-based visas for this position.\n\n\neffective july 1, 2014, under indiana house enrolled act (hea) 1242, it is against public policy of the state of indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the united states, a member of the indiana national guard or a member of a reserve component.\n\n\nfor jobs in san francisco, please click \u201chere\u201d for information regarding the san francisco fair chance ordinance.\n\n  \n\nfor jobs in los angeles, please click \u201chere\u201d for information regarding the los angeles fair chance initiative for hiring ordinance.\n\n\nto view the \u201ceeo know your rights\u201d poster click \u201chere\u201d. this poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the office of federal contract compliance programs.\n\n\nto view the fmla poster, click \u201chere\u201d. this poster summarizing the major provisions of the family and medical leave act (fmla) and telling employees how to file a complaint.\n\n\nit is the company\u2019s policy to employ the best qualified individuals available for all jobs. therefore, any discriminatory action taken on account of an employee\u2019s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. this policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Solutions Architect",
        "company": "Rochester Regional Health",
        "location": "Rochester, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Kubernetes",
            "CI/CD",
            "Snowflake",
            "NoSQL",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a1635116397db4b4",
        "description": "**job title:** ai solutions architect  \n\n**department:** emerging technology  \n\n**location:** riedman campus,100 kings highway south, rochester, new york 14617\\.hybrid (onsite \\& remote)\n\n**hours per week:** 40  \n\n**schedule:** full\\-time, m\\-f (9 am \\- 5 pm), on\\-call will be required as team and platform mature\n\n**position summary:**  \n\narchitect, design, and guide delivery of ai solutions as part of rochester regional health\u2019s enterprise ai initiative, managing and leveraging platforms such as microsoft copilot and microsoft foundry, while also being familiar with capabilities from epic, workday, snowflake and others to accelerate adoption and governance. champion best practices in architecture, risk management, and program oversight, while mentoring team members and aligning technical strategy with organizational goals. ensure all implementations are secure, reliable, ethically responsible, and compliant with healthcare regulations (hipaa). partner with data, engineering, security, and clinical teams to move from use\\-case discovery to production deployment and scale, delivering measurable value in improved care quality, operational efficiency, and patient experience.\n\n**key responsibilities:**\n\n* architecture \\& design: define target architectures for data, model, and application layers; select tooling (mlops, vector stores, model management, observability) and integration patterns with ehr/enterprise systems.\n* use\u2011case delivery: lead end\u2011to\u2011end solutioning for priority use\u2011cases (e.g., it support, clinical documentation assistance, patient safety and quality); author reference implementations and reusable components.\n* security, privacy \\& compliance: implement phi safeguards, role\u2011based access, model governance, prompt/content filtering, audit trails; align with hipaa and internal policies.\n* reliability \\& monitoring: establish model risk controls (drift, bias, hallucination mitigation), human\u2011in\u2011the\u2011loop checkpoints, and performance slas.\n* standards \\& enablement: publish architecture standards and patterns; mentor engineers and analysts; contribute to ai intake, feasibility, and tco assessments.\n* vendor/platform evaluation: assess ai platforms and partners; drive technical due diligence and integration plans; optimize cost/performance.\n* stakeholder engagement: translate clinical/operational needs into technical solutions; communicate tradeoffs, roadmaps, and outcome metrics.\n\n**preferred qualifications:**\n\n* graduate degree in cs/ai/ml\n* certifications (e.g., azure/aws architect, snowflake, kubernetes).\n* experience with llm ecosystems (prompt engineering, retrieval\u2011augmented generation, guardrails), ai agents, agentic platforms, and/or nlp in healthcare.\n* familiarity with ehr platforms and standards (fhir, hl7\\), and clinical safety review processes.\n* familiarity with or contributions to open\u2011source ai tooling or internal reusable frameworks.\n* proven ability to implement solutions in regulated environments; strong grasp of governance, security controls, and healthcare interoperability.\n* excellent systems thinking, documentation, and stakeholder communication skills.\n* hands\u2011on experience with cloud (azure/aws/gcp), data platforms (sql/nosql, lakehouse), mlops (ci/cd for models, feature stores), and secure api integration.\n\n**required qualifications:**\n\n* bachelor\u2019s degree in computer science, data science, engineering, or related field.\n* 5\\+ years in solutions architecture/engineering with enterprise production deployments.\n\n**licensure/certifications:**  \n\n* none required.\n **education:**\n\n**licenses /** **certifications:**\n\n**physical requirements:**\n\n\nl \\- light work \\- exerting up to 20 pounds of force occasionally, and/or up to 10 pounds of force frequently, and/or a negligible amount of force constantly; requires occasional walking, standing or squatting.**for disease specific care programs refer to the program specific requirements of the department for further specifications on experience and educational expectations, including continuing education requirements.**\n\n**any physical requirements reported by a prospective employee and/or employee\u2019s physician or delegate will be considered for accommodations.**\n\n**pay range:**\n\n\n$120,000\\.00 \\- $145,000\\.00**city:**\n\n\nrochester**postal code:**\n\n\n14617***the listed base pay range is a good faith representation of current potential base pay for a successful full time applicant. it may be modified in the future and eligible for additional pay components. pay is determined by factors including experience, relevant qualifications, specialty, internal equity, location, and contracts.***\n\n**rochester regional health is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity or expression, national origin, age, disability, predisposing genetic characteristics, marital or familial status, military or veteran status, citizenship or immigration status, or any other characteristic protected by federal, state, or local law.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "DevSecOps Engineering Intern",
        "company": "Enlyte",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "CI/CD",
            "Jenkins",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c1e5927d938f5c3b",
        "description": "company overview:\n\nat enlyte, we combine innovative technology, clinical expertise, and human compassion to help people recover after workplace injuries or auto accidents. we support their journey back to health and wellness through our industry\\-leading solutions and services. whether you're supporting a fortune 500 client or a local business, developing cutting\\-edge technology, or providing clinical services you'll work alongside dedicated professionals who share your commitment to excellence and make a meaningful impact. join us in fueling our mission to protect dreams and restore lives, while building your career in an environment that values collaboration, innovation, and personal growth. **be part of a team that makes a real difference.**\njob description :\n\nare you looking for an exciting career that will give you cutting\\-edge, hands\\-on experience with a company that puts its employees first? look no further than an intern position at enlyte. as an intern, you will learn cutting edge tools in our devsecops and cloud ops engineering team. you will help us manage our aws environment and automate everything. you will be using ai, jenkins, iac, python, ansible, terraform, aws automation and others. you will work side by side with other departments to automate software saving time and money while elevating our technology foundation. you will build continuous integration and deployment (ci/cd) solutions, grow your professional network, and be a part of an amazing team. learn the latest development tools in the devsecops and cloud ops space while working at an exceptional company and improving productivity and satisfaction for your colleagues. an internship at enlyte is the start on your path to future success. check us out!  \n\nthe devsecops engineer intern will work with their team to design and implement cloud platform solutions, currently focused in aws but always expanding. we are looking for cs/it majors that are familiar with programming, have a curiosity about desire to explore new ideas, have taken interest in technology and have the grit to expand their skillset. we are responsible for the following in our team.* aws cloud ops using iac and monitoring tools (using many aws native tools including lambda)\n* devsecops security tools such as soanqube, sonatype, sigsci, burpsuite and others\n* cicd (automated builds and deploys)\n* shift left security (we embed security tools into our automated process)\n* finops (designing, monitoring and improving an optimized cloud environment)\n\n**what you will be doing*** leveraging ai to elevate the team\u2019s abilities and capacity\n* leveraging your existing programming language skills and incorporating them into our suite of cloud ops and devsecops tools and platforms\n* learn something new\n* + get hands on experience that positions you for new aws certifications.\n\t+ learn a new suite of tools from our expert team\n* show your grit by digging into new technical areas for you and possibly for the team\n* work with team to design, plan and implement cloud platform solutions primarily focused in aws\n* gain a solid understanding of ci/cd tools, automation solutions and overall optimizing technical efficiency\n* automate operational procedures including configuration management, deployments, password management\n* implement appropriate monitoring, alerting and measurements to ensure the health of our environment\n\n\nqualifications:\n**minimum requirements:*** use of ai for personal, academic and personal technical growth.\n* pursuing a bachelor\u2019s degree in it, software engineering, computer science, or in a related field\n* graduating summer or fall 2026\n* gpa of 3\\.3 or higher\n* ability to commit 40 hours per week for 12 weeks during the summer of 2026\n\n**desired skills and experience:*** any aws exposure, lab experience or certifications\n* ai experience\n* awareness of ci/cd tools such as jenkins, git, tfs, docker\n* familiarity with scripting tools such as python, perl, powershell, ansible, or others\n* familiarity with aws iac coding and terraform\n* experience with languages like java and .net are a big plus\n* attentive to department needs as demonstrated by rapid and high\\-quality responsiveness to requests\n* excellent interpersonal skills; able to maintain solid rapport with team members and employees in other departments\n* excellent oral and written communications skills\n* willingness to ask for help and the grit to follow through on your own\n\n**learn more about our summer internship program:****https://careers.enlyte.com/internship**\nbenefits:\n\ncompensation depends on the applicable us geographic market. the expected base pay for this position is $35 per hour, and will be based on a number of additional factors including skills, experience, and education.  \n\n*the company is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, gender identity, sexual orientation, age, status as a protected veteran, among other things, or status as a qualified individual with disability.*\n\n  \n\ndon\u2019t meet every single requirement? studies have shown that women and underrepresented minorities are less likely to apply to jobs unless they meet every single qualification. we are dedicated to building a diverse, inclusive, and authentic workplace, so if you\u2019re excited about this role but your past experience doesn\u2019t align perfectly with every qualification in the job description, we encourage you to apply anyway. you may be just the right candidate for this or other roles.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Applied Research Scientist, LLM Evaluation & Post-Training",
        "company": "Innodata",
        "location": "Ridgefield Park, NJ, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "Hugging Face",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=69285f2f7c852f92",
        "description": "**who we are:**\n\n\ninnodata (nasdaq: inod) is a leading data engineering company. with more than 2,000 customers and operations in 13 cities around the world, we are the ai technology solutions provider\\-of\\-choice to 4 out of 5 of the world\u2019s biggest technology companies, as well as leading companies across financial services, insurance, technology, law, and medicine.\n\n\nby combining advanced machine learning and artificial intelligence (ml/ai) technologies, a global workforce of subject matter experts, and a high\\-security infrastructure, we\u2019re helping usher in the promise of clean and optimized digital data to all industries. innodata offers a powerful combination of both digital data solutions and easy\\-to\\-use, high\\-quality platforms.\n\n\nour global workforce includes over 3,000 employees in the united states, canada, united kingdom, the philippines, india, sri lanka, israel and germany. we\u2019re poised for a period of explosive growth over the next few years.\n\n**position summary:**\n\n\ninnodata is expanding its genai research capability to advance state\\-of\\-the\\-art evaluation and post\\-training methods for llm and multimodal systems. as an applied research scientist, llm evaluation \\& post\\-training, you will lead research and experimentation on how evaluation design, measurement strategies, and feedback signals influence model improvement.\n\n\nthis role is ideal for a technically rigorous researcher who is deeply fluent in modern llm evaluation and post\\-training, and who can turn research insight into practical methods for customer solutions and internal platform innovation. you will work across human\\-in\\-the\\-loop and ai\\-augmented workflows, partnering with language data scientists and ai/ml research engineers to design and validate evaluation frameworks that drive measurable model gains.\n\n\nthe ideal candidate combines strong experimental and statistical judgment with hands\\-on technical ability and can engage as a peer with research and engineering stakeholders at leading ai companies.\n\n**who we\u2019re looking for:**\n\nyou have at least 5\\+ years of relevant experience (including graduate research) in applied ml research, research science, or advanced ml experimentation, with significant experience in llm evaluation, benchmarking, alignment, or post\\-training. you have a track record of designing high\\-quality experiments, interpreting results rigorously, and translating findings into practical improvements.\n\n\nyou are comfortable working across research and product/customer contexts. you can identify important methodological questions, build a research agenda, and collaborate with engineers and data experts to execute. you understand that evaluation is not only about metrics, but about measurement validity, robustness, stress testing, and alignment to real\\-world usage.\n\n\nyou are excited by frontier challenges including long\\-context, cross\\-modal, and dynamic multi\\-turn evaluations, and by the opportunity to build new benchmark datasets and evaluation frameworks that become strategic assets for innodata and its customers.\n\n\nyou bring an implementation\\-minded approach to experimentation and are comfortable collaborating closely with engineers to productionize methods and research outputs when appropriate.\n\n**tell me more:**\n\n\nas an applied research scientist, llm evaluation \\& post\\-training, you will help define the next generation of evaluation\\-driven model improvement workflows. you will study how different evaluation approaches (human, automated, hybrid) shape model selection and post\\-training outcomes, and you will design experiments that produce credible, actionable conclusions.\n\n\nyour work may include designing benchmark datasets, developing evaluation taxonomies and protocols, defining metrics and scoring methodologies, analyzing failure modes, and testing how changes in evaluation setup affect downstream fine\\-tuning results. you will also support customer engagements by bringing scientific rigor to evaluation strategy, methodology review, and technical recommendations.\n\n\nthis is a highly collaborative role that sits at the intersection of research, engineering, and language/data operations.\n\n**responsibilities:**\n\n* define and execute a research agenda focused on llm evaluation and post\\-training, especially evaluation\\-driven model improvement\n* design rigorous experiments to study how evaluation methodologies impact fine\\-tuning and post\\-training outcomes\n* develop and validate evaluation frameworks for llm and multimodal systems, including:\n\t+ benchmark/task design\n\t+ scoring methods\n\t+ judge/model\\-assisted evaluation\n\t+ human evaluation protocols\n\t+ robustness/stress testing\n* lead research on advanced evaluation domains, including long\\-context, cross\\-modal, and dynamic multi\\-turn evaluations\n* study the effectiveness and limitations of existing evaluation techniques, and propose improved methodologies with clear validity and scalability tradeoffs\n* analyze model behavior and failure patterns; generate actionable recommendations for model improvement and evaluation redesign\n* collaborate with ai/ml research engineers to translate research methods into scalable evaluation and post\\-training pipelines\n* collaborate with language data scientists to integrate human\\-in\\-the\\-loop and synthetic data/evaluation strategies into research programs\n* engage with customer technical stakeholders to understand evaluation goals, review methodologies, and provide expert recommendations\n* contribute to internal benchmark datasets, evaluation frameworks, and reusable research assets\n* produce high\\-quality technical documentation, internal research reports, and client\\-facing materials explaining methods, results, assumptions, and limitations\n* contribute to thought leadership and best practices in llm evaluation, post\\-training, and genai quality measurement\n* ms/phd in computer science, machine learning, statistics, applied mathematics, ai, or a related quantitative scientific field (phd strongly preferred)\n* 5\\+ years of relevant experience in applied research / research science in ml/ai, with substantial work in llms or foundation models\n* demonstrated experience with llm evaluation, benchmarking, alignment, post\\-training, or model quality research\n* strong foundation in experimental design, statistical analysis, and scientific reasoning for ml systems\n* strong coding skills in python for research experimentation and analysis (e.g., data processing, evaluation pipelines, statistical analysis, visualization)\n* experience working with modern ml tooling/frameworks (e.g., pytorch, hugging face, jax/tensorflow as applicable) sufficient to design and execute model/evaluation experiments\n* ability to evaluate and compare human and automated evaluation methods, including tradeoffs in cost, reliability, validity, and scalability\n* experience designing evaluation studies and protocols that are reproducible across datasets, model versions, and evaluation runs\n* ability to collaborate directly with technical stakeholders including research scientists, ml engineers, data scientists, and customer technical counterparts\n* strong communication skills and ability to present nuanced technical conclusions, assumptions, and limitations clearly\n\n**technical skills**\n\n\nevaluation science \\& benchmarking\n\n* experience designing benchmark datasets, test suites, or evaluation frameworks for language or multimodal models\n* deep understanding of metric design, scoring reliability, and measurement validity\n* experience with human evaluation methods and quality assurance considerations (e.g., rubric design, inter\\-rater reliability, adjudication frameworks)\n\n\nllm / post\\-training\n\n* understanding of post\\-training methods and how training objectives interact with evaluation outcomes\n* ability to reason about model behavior, failure modes, and tradeoffs across tasks/domains\n* familiarity with alignment and robustness considerations in model evaluation\n\n\nquantitative analysis\n\n* strong statistical analysis skills (sampling, uncertainty, significance testing where appropriate, error analysis, metric interpretation)\n* ability to synthesize complex experimental findings into actionable recommendations\n\n**preferred skills**\n\n* hands\\-on experience running or supporting fine\\-tuning/post\\-training experiments (sft, preference optimization, rlhf/rlaif\\-style workflows)\n* experience with multimodal evaluation (e.g., text\\-image, audio, video)\n* experience with long\\-context benchmarking/evaluation and real\\-world context management challenges\n* experience designing multi\\-turn, interactive, or agentic evaluation protocols\n* published research and/or open\\-source benchmark contributions in llm evaluation, post\\-training, alignment, or related areas\n* experience in customer\\-facing applied research, technical consulting, or cross\\-functional product/research collaborations\n* familiarity with safety, trustworthiness, and governance considerations in genai evaluation\n\n**how this role partners with the team**\n\n\nthis role works closely with:\n\n* language data scientists, who bring deep expertise in language data, human evaluation workflows, multilingual/multimodal process design, and data quality operations\n* ai/ml research engineers, who implement scalable training/evaluation systems and connect research methods to production\\-grade pipelines\n* business and customer teams, who rely on innodata for expert consultation and credible, technically rigorous genai solutions\n* internal r\\&d and platform teams, to transform research outputs into reusable frameworks, benchmarks, and differentiated offerings\n\n*please be aware of recruitment scams involving individuals or organizations falsely claiming to represent employers. innodata will never ask for payment, banking details, or sensitive personal information during the application process. to learn more on how to recognize job scams, please visit the federal trade commission\u2019s guide at* https://consumer.ftc.gov/articles/job\\-scams.\n\n*if you believe you\u2019ve been targeted by a recruitment scam, please report it to innodata at* verifyjoboffer@innodata.com *and consider reporting it to the ftc at* reportfraud.ftc.gov*.*\n\n\n\\#li\\-ns1",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "SAP iXp Intern - AI Machine Learning",
        "company": "SAP",
        "location": "Palo Alto, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "R",
            "Scala",
            "A/B Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=97577c68592f5b4a",
        "description": "**we help the world run better**  \n\nat sap, we keep it simple: you bring your best to us, and we'll bring out the best in you. we're builders touching over 20 industries and 80% of global commerce, and we need your unique talents to help shape what's next. the work is challenging \u2013 but it matters. you'll find a place where you can be yourself, prioritize your wellbeing, and truly belong. what's in it for you? constant learning, skill growth, great benefits, and a team that wants you to grow and succeed. **what you\u2019ll build**\n\n\nthe sap internship experience program is sap\u2019s global, strategic, paid internship program that provides university students with opportunities to find purpose in their careers. this is more than an internship, it\u2019s the foundation for a career built on connection, creativity, and impact.\n\n\nposition title: sap ixp intern \\- ai machine learning\n\n\nlocation: palo alto, ca (in\\-person)\n\n\nexpected start date: june 2026\n\n\ncontract duration: 6 months (extendable)\n\n\nworking hours: 20\\-40 hours/week\n\n  \n\nas an ai/ml intern, you will work alongside senior data scientists and engineers to build, optimize, and deploy intelligent systems. you won\u2019t just be watching from the sidelines; you will be responsible for exploring messy, real\\-world datasets, training models, and helping integrate ai features into our core products.\n\n* data engineering: collect, clean, and preprocess large\\-scale structured and unstructured datasets to ensure they are \"model\\-ready.\"\n* model development: assist in designing and implementing ml algorithms (supervised, unsupervised, and deep learning) using frameworks like pytorch or tensorflow.\n* experimentation: conduct a/b testing, hyperparameter tuning, and error analysis to improve model precision and recall.\n* genai \\& llms: work with large language models (llms), including prompt engineering, fine\\-tuning, and implementing rag (retrieval\\-augmented generation) pipelines.\n* mlops: help maintain and monitor model performance in production; assist in containerizing models using docker for deployment.\n* research \\& documentation: stay current with the latest ai research (e.g., via arxiv) and document your experimental results for the wider engineering team.\n\n **what you\u2019ll bring**\n\n\nwe\u2019re looking for someone who takes initiative, perseveres, and stay curious. you like to work on meaningful innovative projects and are energized by lifelong learning.\n\n* education: currently pursuing a bs, ms, or phd in computer science, data science, mathematics, or a related quantitative field.\n* eligibility: must be currently enrolled, or recently graduated (start date must be within 6 months of graduation date) from a coding academy/bootcamp, apprenticeship, associate, bachelor\u2019s, master\u2019s or jd/phd program\n* problem\\-solving: a proven ability to break down complex problems and think through edge cases.\n* curiosity: a strong desire to learn new libraries and stay updated on the fast\\-moving ai landscape.\n\n **where you belong**\n\n\nbe part of sap next gen, a global community for students, universities, schools and educational partners, who are passionate about innovation and technology.\n\n* culture of collaboration: partner with experienced sap colleagues and expert mentors who will support your growth. grow professionally through personalized mentoring, coaching, and career development support.\n* project\\-driven experience: kickstart your career with hands\\-on learning experience, making an impact from day one by contributing to meaningful projects that help the world run better. you\u2019ll have endless learning resources at your fingertips and gain future\\-ready skills from a variety of virtual, in\\-person, and hybrid learning sessions, cultivated just for you, and aligned with our learning approach.\n* gain visibility: build relationships with leaders and peers across teams and functions. showcase your ideas, skills, and creativity in a global, fast\\-paced environment. open doors for future career opportunities within sap and beyond.\n\n **meet your team \u2013 what do we do?**\n\n\nsap business technology platform (btp), btp fabric \\- business services will be responsible for developing a consistent, scalable, and high\\-quality reusable business capabilities and services. these foundational constructs standardize essential elements \u2014 business entities, workflows, data models, and common reusable apis/services \u2014 so that developers can focus on creating value through innovation and extensions in a highly productive way rather than reinventing basic frameworks. by fostering consistency and offering a shared approach for abap and non\\-abap based application development on top of btp, we will enable ecosystems of developers, partners, and customers a unified foundation to deliver tailored, interoperable solutions that address all business needs.\n\n  \n\nwant to see what life at sap feels like? check out the life at sap youtube channel\n\n\nfollow @lifeatsap on instagram and don't miss anything about our experiences worldwide!\n\n\n\\#lifeatsap \\#sapnextgen\n\n**bring out your best**  \n\nsap innovations help more than four hundred thousand customers worldwide work together more efficiently and use business insight more effectively. originally known for leadership in enterprise resource planning (erp) software, sap has evolved to become a market leader in end\\-to\\-end business application software and related services for database, analytics, intelligent technologies, and experience management. as a cloud company with two hundred million users and more than one hundred thousand employees worldwide, we are purpose\\-driven and future\\-focused, with a highly collaborative team ethic and commitment to personal development. whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. at sap, you can bring out your best.\n  \n\n  \n\n**we win with inclusion**  \n\nsap\u2019s culture of inclusion, focus on health and well\\-being, and flexible working models help ensure that everyone \u2013 regardless of background \u2013 feels included and can run at their best. at sap, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. we ultimately believe in unleashing all talent and creating a better world.  \n\n  \n\nsap is committed to the values of equal employment opportunity and provides accessibility accommodations to applicants with physical and/or mental disabilities. if you are interested in applying for employment with sap and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e\\-mail with your request to recruiting operations team: careers@sap.com.  \n\n  \n\nfor sap employees: only permanent roles are eligible for the sap employee referral program, according to the eligibility rules set in the sap referral policy. specific conditions may apply for roles in vocational training.  \n\nqualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability. **compensation range transparency**: sap believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating sap\u2019s commitment to pay equity. sap provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. the targeted combined range for this position is 15 \\- 62(usd) usd. the actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. any sap variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance. please reference this link for a summary of sap benefits and eligibility requirements: sap north america benefits. **ai usage in the recruitment process**\nfor information on the responsible use of ai in our recruitment process, please refer to our guidelines for ethical usage of ai in the recruiting process.\nplease note that any violation of these guidelines may result in disqualification from the hiring process.  \n\n  \n\nrequisition id: 444149 \\| work area: software\\-design and development \\| expected travel: 0 \\- 10% \\| career status: student \\| employment type: limited full time \\| additional locations: \\#li\\-hybrid",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer, Platform",
        "company": "Savvy Wealth",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ec747612a8efc3b4",
        "description": "**about savvy wealth:**\n-----------------------\n\n\nwealth management is a $545 billion industry in the us, yet remains archaic and inefficient with low technology penetration. 75% of financial advisors don\u2019t offer digital communication beyond email, and 62% still build financial plans manually in excel. this leads to a poor client experience and results in financial advisors spending over 70% of their time on non\\-client facing, manual work.\n\n\nsavvy is changing that. we\u2019re building the most advisor\\-centric platform in wealth management: a digital\\-first solution that modernizes human financial advice. advisors who partner with savvy tap into ai\\-powered software, automated sales and marketing, and seamless back office workflows to scale faster and spend more time with clients.\n\n\nwe\u2019ve raised over $105m to date from thrive capital, index ventures, canvas ventures, mark casady (former lpl financial ceo), and other top\\-tier investors. our team is made up of repeat founders and operators who\u2019ve helped build airbnb, square, brex, carta, facebook, $200b\\+ rias, and more.\n\n\nsavvy is at a pivotal point in its growth trajectory, having established strong product\\-market fit in providing a modern platform to financial advisors. we\u2019ve surpassed $2\\.2 billion in aum in less than three years, grown 600%\\+ in the last 18 months, and are entering the next phase of the company which involves rapid expansion of our product offering and continued revenue growth. come help us scale!\n\n**responsibilities:**\n\n* own and improve ci/cd pipelines to enable confident, low\\-friction deployments that work consistently across services and environments\n* build and maintain local and cloud development environments so engineers can spin up, test, and debug changes with minimal setup or surprises\n* standardize alerting, metrics, and logging across systems to enable quick diagnosis and reduce firefighting time\n* systematically identify and resolve developer experience pain points: slow tests, flaky builds, unclear failures, and missing documentation\n* introduce strategic automation (e.g., test selection, release validation, infrastructure checks) where it clearly improves velocity or safety\n* leverage ai to accelerate engineering workflows, from code review and test generation to infrastructure management\n* partner with product engineering teams to drive adoption of platform improvements through trust and collaboration, not mandates\n\n**must have:**\n\n* 5\\+ years of software engineering experience, with meaningful time spent on infrastructure, platform, or developer experience work\n* strong proficiency with cloud infrastructure (aws preferred), containerization (docker), and infrastructure\\-as\\-code tools (terraform)\n* hands\\-on experience building and maintaining ci/cd pipelines (github actions preferred)\n* solid understanding of observability and monitoring tools (datadog preferred) for metrics, logging, alerting, and distributed tracing\n* proficiency in scripting and automation (ruby, python, bash, or go)\n* experience with web application backends (ruby on rails preferred) and frontend build tooling (pnpm, turborepo, eslint, jest/vitest, playwright)\n* systems thinking with pragmatism. you design reliable systems that scale for current and near\\-term needs without over\\-engineering\n* strong communication skills with the ability to drive adoption through mentorship and partnership\n\n**nice to have:**\n\n* experience migrating workloads between cloud providers or paas platforms (e.g., heroku to aws)\n* familiarity with kubernetes or container orchestration at scale\n* fintech domain knowledge including financial services, regulatory compliance, or financial data integrations\n* experience with macos development toolchains and apple silicon environments\n* track record of measurably improving developer productivity or deployment velocity\n\n**benefits:**\n-------------\n\n* competitive salary and equity package\n* unlimited pto \\+ paid company holidays\n* access to holistic medical, dental, and vision plans\n* company 401(k), commuter, and hsa/fsa plans\n* nyc office in the heart of manhattan\n* lunch and snacks provided in the office\n* access to virtual mental health care (spring health), vision related benefits (xp health), and health concierge (rightway) to help you find the right care\n* access to counseling for stress management, dependent care, nutrition, fitness, legal, and financial issues (guardian worklifematters eap)\n\n\ncompensation range: $165k \\- $226k",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Solutions Architect",
        "company": "Rochester Regional Health",
        "location": "Rochester, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Kubernetes",
            "CI/CD",
            "Snowflake",
            "NoSQL",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=900759ff64f50aec",
        "description": "**job title:** ai solutions architect  \n\n**department:** emerging technology  \n\n**location:** riedman campus,100 kings highway south, rochester, new york 14617\\.hybrid (onsite \\& remote)\n\n**hours per week:** 40  \n\n**schedule:** full\\-time, m\\-f (9 am \\- 5 pm), on\\-call will be required as team and platform mature\n\n**position summary:**  \n\narchitect, design, and guide delivery of ai solutions as part of rochester regional health\u2019s enterprise ai initiative, managing and leveraging platforms such as microsoft copilot and microsoft foundry, while also being familiar with capabilities from epic, workday, snowflake and others to accelerate adoption and governance. champion best practices in architecture, risk management, and program oversight, while mentoring team members and aligning technical strategy with organizational goals. ensure all implementations are secure, reliable, ethically responsible, and compliant with healthcare regulations (hipaa). partner with data, engineering, security, and clinical teams to move from use\\-case discovery to production deployment and scale, delivering measurable value in improved care quality, operational efficiency, and patient experience.\n\n**key responsibilities:**\n\n* architecture \\& design: define target architectures for data, model, and application layers; select tooling (mlops, vector stores, model management, observability) and integration patterns with ehr/enterprise systems.\n* use\u2011case delivery: lead end\u2011to\u2011end solutioning for priority use\u2011cases (e.g., it support, clinical documentation assistance, patient safety and quality); author reference implementations and reusable components.\n* security, privacy \\& compliance: implement phi safeguards, role\u2011based access, model governance, prompt/content filtering, audit trails; align with hipaa and internal policies.\n* reliability \\& monitoring: establish model risk controls (drift, bias, hallucination mitigation), human\u2011in\u2011the\u2011loop checkpoints, and performance slas.\n* standards \\& enablement: publish architecture standards and patterns; mentor engineers and analysts; contribute to ai intake, feasibility, and tco assessments.\n* vendor/platform evaluation: assess ai platforms and partners; drive technical due diligence and integration plans; optimize cost/performance.\n* stakeholder engagement: translate clinical/operational needs into technical solutions; communicate tradeoffs, roadmaps, and outcome metrics.\n\n**preferred qualifications:**\n\n* graduate degree in cs/ai/ml\n* certifications (e.g., azure/aws architect, snowflake, kubernetes).\n* experience with llm ecosystems (prompt engineering, retrieval\u2011augmented generation, guardrails), ai agents, agentic platforms, and/or nlp in healthcare.\n* familiarity with ehr platforms and standards (fhir, hl7\\), and clinical safety review processes.\n* familiarity with or contributions to open\u2011source ai tooling or internal reusable frameworks.\n* proven ability to implement solutions in regulated environments; strong grasp of governance, security controls, and healthcare interoperability.\n* excellent systems thinking, documentation, and stakeholder communication skills.\n* hands\u2011on experience with cloud (azure/aws/gcp), data platforms (sql/nosql, lakehouse), mlops (ci/cd for models, feature stores), and secure api integration.\n\n**required qualifications:**\n\n* bachelor\u2019s degree in computer science, data science, engineering, or related field.\n* 5\\+ years in solutions architecture/engineering with enterprise production deployments.\n\n**licensure/certifications:**  \n\n* none required.\n **education:**\n\n**licenses /** **certifications:**\n\n**physical requirements:**\n\n\nl \\- light work \\- exerting up to 20 pounds of force occasionally, and/or up to 10 pounds of force frequently, and/or a negligible amount of force constantly; requires occasional walking, standing or squatting.**for disease specific care programs refer to the program specific requirements of the department for further specifications on experience and educational expectations, including continuing education requirements.**\n\n**any physical requirements reported by a prospective employee and/or employee\u2019s physician or delegate will be considered for accommodations.**\n\n**pay range:**\n\n\n$120,000\\.00 \\- $145,000\\.00**city:**\n\n\nrochester**postal code:**\n\n\n14617***the listed base pay range is a good faith representation of current potential base pay for a successful full time applicant. it may be modified in the future and eligible for additional pay components. pay is determined by factors including experience, relevant qualifications, specialty, internal equity, location, and contracts.***\n\n**rochester regional health is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity or expression, national origin, age, disability, predisposing genetic characteristics, marital or familial status, military or veteran status, citizenship or immigration status, or any other characteristic protected by federal, state, or local law.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Solutions Architect",
        "company": "Rochester Regional Health",
        "location": "Rochester, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Kubernetes",
            "CI/CD",
            "Snowflake",
            "NoSQL",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=adcb498baa44ef4d",
        "description": "**job title:** ai solutions architect  \n\n**department:** emerging technology  \n\n**location:** riedman campus,100 kings highway south, rochester, new york 14617\\.hybrid (onsite \\& remote)\n\n**hours per week:** 40  \n\n**schedule:** full\\-time, m\\-f (9 am \\- 5 pm), on\\-call will be required as team and platform mature\n\n**position summary:**  \n\narchitect, design, and guide delivery of ai solutions as part of rochester regional health\u2019s enterprise ai initiative, managing and leveraging platforms such as microsoft copilot and microsoft foundry, while also being familiar with capabilities from epic, workday, snowflake and others to accelerate adoption and governance. champion best practices in architecture, risk management, and program oversight, while mentoring team members and aligning technical strategy with organizational goals. ensure all implementations are secure, reliable, ethically responsible, and compliant with healthcare regulations (hipaa). partner with data, engineering, security, and clinical teams to move from use\\-case discovery to production deployment and scale, delivering measurable value in improved care quality, operational efficiency, and patient experience.\n\n**key responsibilities:**\n\n* architecture \\& design: define target architectures for data, model, and application layers; select tooling (mlops, vector stores, model management, observability) and integration patterns with ehr/enterprise systems.\n* use\u2011case delivery: lead end\u2011to\u2011end solutioning for priority use\u2011cases (e.g., it support, clinical documentation assistance, patient safety and quality); author reference implementations and reusable components.\n* security, privacy \\& compliance: implement phi safeguards, role\u2011based access, model governance, prompt/content filtering, audit trails; align with hipaa and internal policies.\n* reliability \\& monitoring: establish model risk controls (drift, bias, hallucination mitigation), human\u2011in\u2011the\u2011loop checkpoints, and performance slas.\n* standards \\& enablement: publish architecture standards and patterns; mentor engineers and analysts; contribute to ai intake, feasibility, and tco assessments.\n* vendor/platform evaluation: assess ai platforms and partners; drive technical due diligence and integration plans; optimize cost/performance.\n* stakeholder engagement: translate clinical/operational needs into technical solutions; communicate tradeoffs, roadmaps, and outcome metrics.\n\n**preferred qualifications:**\n\n* graduate degree in cs/ai/ml\n* certifications (e.g., azure/aws architect, snowflake, kubernetes).\n* experience with llm ecosystems (prompt engineering, retrieval\u2011augmented generation, guardrails), ai agents, agentic platforms, and/or nlp in healthcare.\n* familiarity with ehr platforms and standards (fhir, hl7\\), and clinical safety review processes.\n* familiarity with or contributions to open\u2011source ai tooling or internal reusable frameworks.\n* proven ability to implement solutions in regulated environments; strong grasp of governance, security controls, and healthcare interoperability.\n* excellent systems thinking, documentation, and stakeholder communication skills.\n* hands\u2011on experience with cloud (azure/aws/gcp), data platforms (sql/nosql, lakehouse), mlops (ci/cd for models, feature stores), and secure api integration.\n\n**required qualifications:**\n\n* bachelor\u2019s degree in computer science, data science, engineering, or related field.\n* 5\\+ years in solutions architecture/engineering with enterprise production deployments.\n\n**licensure/certifications:**  \n\n* none required.\n **education:**\n\n**licenses /** **certifications:**\n\n**physical requirements:**\n\n\nl \\- light work \\- exerting up to 20 pounds of force occasionally, and/or up to 10 pounds of force frequently, and/or a negligible amount of force constantly; requires occasional walking, standing or squatting.**for disease specific care programs refer to the program specific requirements of the department for further specifications on experience and educational expectations, including continuing education requirements.**\n\n**any physical requirements reported by a prospective employee and/or employee\u2019s physician or delegate will be considered for accommodations.**\n\n**pay range:**\n\n\n$120,000\\.00 \\- $145,000\\.00**city:**\n\n\nrochester**postal code:**\n\n\n14617***the listed base pay range is a good faith representation of current potential base pay for a successful full time applicant. it may be modified in the future and eligible for additional pay components. pay is determined by factors including experience, relevant qualifications, specialty, internal equity, location, and contracts.***\n\n**rochester regional health is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex (including pregnancy, childbirth, and related medical conditions), sexual orientation, gender identity or expression, national origin, age, disability, predisposing genetic characteristics, marital or familial status, military or veteran status, citizenship or immigration status, or any other characteristic protected by federal, state, or local law.**",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data & AI Senior Engineer - 90405343 - Washington",
        "company": "Amtrak",
        "location": "Washington, DC, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "CI/CD",
            "Git",
            "Databricks",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a277c17b8403d9b6",
        "description": "**your success is a train ride away!**\n\n\nas we move america\u2019s workforce toward the future, amtrak connects businesses and communities across the country. we employ more than 20,000 diverse, energetic professionals in a variety of career fields throughout the united states. the safety of our passengers, our employees, the public and our operating environment is our priority, and the success of our railroad is due to our employees.\n\n **are you ready to join our team?**\n\n\nour values of \u2018do the right thing, excel together and put customers first\u2019 are at the heart of what matters most to us, and our core capabilities, \u2018building trust, accountability, effective communication, customer focus, and proactive safety \\& security\u2019 are what every employee needs to know and do to be most impactful at amtrak. by living the amtrak values, focusing on our capabilities, and actively embracing and fostering diverse ideas, backgrounds, and perspectives, together we will honor our past and make amtrak a company of the future.\n\n**job summary**\n---------------\n\n\nthe data \\& ai senior engineer advances amtrak\u2019s mission to make trusted, high\\-quality, and intelligent data broadly accessible for decision\\-making, automation, and innovation. this role is responsible for leading the design and delivery of complex data and ai solutions that power amtrak\u2019s analytics, operational systems, and digital experiences.\n\n\nsenior engineers apply deep technical expertise to architect and optimize pipelines, integrations, and data services. they ensure solutions meet performance, security, and governance standards while mentoring junior engineers and influencing design decisions across teams. the senior data \\& ai engineer acts as both a hands\\-on developer and a technical leader, helping to scale amtrak\u2019s data and ai capabilities through standardization, reusability, and continuous improvement.\n\n**essential functions**\n-----------------------\n\n* lead the design and evolution of scalable data and ai platform capabilities across databricks (lakehouse), sap data environments (e.g., datasphere/s/4\\), and virtualization layers (e.g., denodo) to deliver a unified, governed, self\\-service ecosystem.\n* provide hands\\-on technical guidance in coding, api\\-first integrations, model lifecycle management, pipeline development, and integration patterns.  \n\narchitect and enforce engineering standards across ingestion, transformation, feature engineering, model deployment, and governance\\-as\\-code controls embedded directly into pipelines.\n* mentor junior and mid\\-level engineers, fostering skill development, collaboration, and engineering excellence.\n* partner with architects, product owners, and governance leads to align solutions with amtrak\u2019s enterprise data strategy and roadmap.\n* design and implement reusable platform accelerators including apis, templates, and feature engineering patterns that enable self\\-service analytics and ai across domains.\n**minimum qualifications**\n--------------------------\n\n* education: bachelor\u2019s degree in computer science, data engineering, or a related technical field; equivalent experience may be considered.\n* experience: 4\u20136 years of experience in data engineering, software development, or data architecture.\n**preferred qualifications**\n----------------------------\n\n* experience in a leadership capacity within a scaled agile environment or data platform modernization initiative.\n* familiarity with enterprise data governance, metadata management, and security standards.\n* exposure to mlops, cloud data platforms, and automation tools that accelerate delivery.\n* experience implementing production\\-grade mlops pipelines, including model versioning, ci/cd, monitoring, and governance controls embedded into data and ai workflows.\n* prior mentorship or technical leadership of multi\\-functional project teams.\n**knowledge, skills, and abilities**\n------------------------------------\n\n* advanced proficiency in python and sql with deep experience in distributed data processing (e.g., spark) and modern lakehouse architectures (databricks strongly preferred), including integration of sap data platforms and virtualization technologies into enterprise\\-scale solutions.\n* strong understanding of data quality, observability, and performance optimization.\n* demonstrated ability to lead teams through technical challenges while remaining hands\\-on in design and coding.\n* experience working within agile product teams, coordinating across multiple stakeholders.\n* strong understanding of api\\-first and event\\-driven architecture patterns, including secure service\\-to\\-service communication and role\\-based access control (rbac).\n* experience embedding data quality validation, schema enforcement, lineage tracking, and policy\\-as\\-code controls directly into pipelines and ai workflows.\n* excellent communication, collaboration, and problem\\-solving abilities.\n* experience building reusable cost effective, feature stores, semantic layers, or internal platform services that enable self\\-service analytics and ai.\n  \n\nthe salary/hourly range is $86,500\\.00 \u2013 $111,996\\.00\\. pay is based on several factors including but not limited to education, work experience, certifications, etc. depending on an employee\u2019s assigned worksite or location, amtrak may consider a geo\\-pay differential to be applied to the employee\u2019s base salary. amtrak may offer additional incentive and pay programs to recognize and reward our employees, including a short\\-term incentive bonus based upon factors such as individual and company performance that is commensurate with the level of the position and/or long\\-term incentive plan compensation. in addition to your salary, amtrak offers a comprehensive benefit package that includes health, dental, and vision plans; health savings accounts; wellness programs; flexible spending accounts; 401k retirement plan with employer match; life insurance; short and long term disability insurance; paid time off; back\\-up care; adoption assistance; surrogacy assistance; reimbursement of education expenses; public service loan forgiveness eligibility; railroad retirement sickness and retirement benefits; and rail pass privileges. learn more about our benefits offerings here.\n\n **requisition id:**166077\n\n**work arrangement:**06\\-onsite 4/5 days click here for more information about work arrangements at amtrak.\n  \n\n**relocation offered:**no\n  \n\n**travel requirements:**up to 25%\n\n**you power our progress through your performance.**\n\n\nwe want your work at amtrak to be more than a job. we want your career at amtrak to be a fulfilling experience where you find challenging work, rewarding opportunities, respect among colleagues, and attractive compensation. amtrak maintains a culture that values high performance and recognizes individual employee contributions.\n\n  \n\namtrak is committed to a safe workplace free of drugs and alcohol. all amtrak positions requires a pre\\-employment background check that includes prior employment verification, a criminal history check and a pre\\-employment drug screen.\n\n\ncandidates who test positive for marijuana will be disqualified, regardless of any state or local statute, ordinance, regulation, or other law that legalizes or decriminalizes the use or possession of marijuana, whether for medical, recreational, or other use. amtrak's pre\\-employment drug testing program is administered in accordance with dot regulations and applicable law.\n\n  \n\nin accordance with dot regulations (49 cfr \u00a7 40\\.25\\), amtrak is required to obtain prior drug and alcohol testing records for applicants/employees intending to perform safety\\-sensitive duties for covered department of transportation positions. if an applicant/employee refuses to provide written consent for amtrak to obtain these records, the individual will not be permitted to perform safety\\-sensitive functions.\n\n  \n\nin accordance with federal law governing security checks of covered individuals for providers of public transportation (title 6 u.s.c. \u00a71143\\), amtrak is required to screen applicants for any permanent or interim disqualifying criminal offenses.\n\n  \n\nnote that any education requirement listed above may be deemed satisfied if you have an equivalent combination of education, training and experience.\n\n  \n\namtrak is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race/color, to include traits historically associated with race, including but not limited to, hair texture and hairstyles such as braids, locks and twists, religion, sex (including pregnancy, childbirth and related conditions, such as lactation), national origin/ethnicity, disability (intellectual, mental and physical), veteran status, marital status, ancestry, sexual orientation, gender identity and gender expression, genetic information, citizenship or any other personal characteristics protected by law.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data & AI Senior Engineer - 90405345 - Washington",
        "company": "Amtrak",
        "location": "Washington, DC, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "CI/CD",
            "Git",
            "Databricks",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7088e497e81755f8",
        "description": "**your success is a train ride away!**\n\n\nas we move america\u2019s workforce toward the future, amtrak connects businesses and communities across the country. we employ more than 20,000 diverse, energetic professionals in a variety of career fields throughout the united states. the safety of our passengers, our employees, the public and our operating environment is our priority, and the success of our railroad is due to our employees.\n\n **are you ready to join our team?**\n\n\nour values of \u2018do the right thing, excel together and put customers first\u2019 are at the heart of what matters most to us, and our core capabilities, \u2018building trust, accountability, effective communication, customer focus, and proactive safety \\& security\u2019 are what every employee needs to know and do to be most impactful at amtrak. by living the amtrak values, focusing on our capabilities, and actively embracing and fostering diverse ideas, backgrounds, and perspectives, together we will honor our past and make amtrak a company of the future.\n\n **job summary:**\n\n  \n\nthe data \\& ai senior engineer advances amtrak\u2019s mission to make trusted, high\\-quality, and intelligent data broadly accessible for decision\\-making, automation, and innovation. this role is responsible for leading the design and delivery of complex data and ai solutions that power amtrak\u2019s analytics, operational systems, and digital experiences.\n\n\nsenior engineers apply deep technical expertise to architect and optimize pipelines, integrations, and data services. they ensure solutions meet performance, security, and governance standards while mentoring junior engineers and influencing design decisions across teams. the senior data \\& ai engineer acts as both a hands\\-on developer and a technical leader, helping to scale amtrak\u2019s data and ai capabilities through standardization, reusability, and continuous improvement.\n\n **essential functions:**\n\n* lead the design and evolution of scalable data and ai platform capabilities across databricks (lakehouse), sap data environments (e.g., datasphere/s/4\\), and virtualization layers (e.g., denodo) to deliver a unified, governed, self\\-service ecosystem.\n* provide hands\\-on technical guidance in coding, api\\-first integrations, model lifecycle management, pipeline development, and integration patterns.  \n\narchitect and enforce engineering standards across ingestion, transformation, feature engineering, model deployment, and governance\\-as\\-code controls embedded directly into pipelines.\n* mentor junior and mid\\-level engineers, fostering skill development, collaboration, and engineering excellence.\n* partner with architects, product owners, and governance leads to align solutions with amtrak\u2019s enterprise data strategy and roadmap.\n* design and implement reusable platform accelerators including apis, templates, and feature engineering patterns that enable self\\-service analytics and ai across domains.\n\n**minimum qualifications:**\n\n* education: bachelor\u2019s degree in computer science, data engineering, or a related technical field; equivalent experience may be considered.\n* experience: 4\u20136 years of experience in data engineering, software development, or data architecture.\n\n **preferred qualifications:**\n\n* experience in a leadership capacity within a scaled agile environment or data platform modernization initiative.\n* familiarity with enterprise data governance, metadata management, and security standards.\n* exposure to mlops, cloud data platforms, and automation tools that accelerate delivery.\n* experience implementing production\\-grade mlops pipelines, including model versioning, ci/cd, monitoring, and governance controls embedded into data and ai workflows.\n* prior mentorship or technical leadership of multi\\-functional project teams.\n\n**required knowledge, skills and abilities:**\n\n* advanced proficiency in python and sql with deep experience in distributed data processing (e.g., spark) and modern lakehouse architectures (databricks strongly preferred), including integration of sap data platforms and virtualization technologies into enterprise\\-scale solutions.\n* strong understanding of data quality, observability, and performance optimization.\n* demonstrated ability to lead teams through technical challenges while remaining hands\\-on in design and coding.\n* experience working within agile product teams, coordinating across multiple stakeholders.\n* strong understanding of api\\-first and event\\-driven architecture patterns, including secure service\\-to\\-service communication and role\\-based access control (rbac).\n* experience embedding data quality validation, schema enforcement, lineage tracking, and policy\\-as\\-code controls directly into pipelines and ai workflows.\n* excellent communication, collaboration, and problem\\-solving abilities.\n* experience building reusable cost effective, feature stores, semantic layers, or internal platform services that enable self\\-service analytics and ai.\n\n\nthe salary/hourly range is $86,500\\.00 \u2013 $111,996\\.00\\. pay is based on several factors including but not limited to education, work experience, certifications, etc. depending on an employee\u2019s assigned worksite or location, amtrak may consider a geo\\-pay differential to be applied to the employee\u2019s base salary. amtrak may offer additional incentive and pay programs to recognize and reward our employees, including a short\\-term incentive bonus based upon factors such as individual and company performance that is commensurate with the level of the position and/or long\\-term incentive plan compensation. in addition to your salary, amtrak offers a comprehensive benefit package that includes health, dental, and vision plans; health savings accounts; wellness programs; flexible spending accounts; 401k retirement plan with employer match; life insurance; short and long term disability insurance; paid time off; back\\-up care; adoption assistance; surrogacy assistance; reimbursement of education expenses; public service loan forgiveness eligibility; railroad retirement sickness and retirement benefits; and rail pass privileges. learn more about our benefits offerings here.\n\n **requisition id:**166067\n\n**work arrangement:**06\\-onsite 4/5 days click here for more information about work arrangements at amtrak.\n  \n\n**relocation offered:**no\n  \n\n**travel requirements:**up to 25%\n\n**you power our progress through your performance.**\n\n\nwe want your work at amtrak to be more than a job. we want your career at amtrak to be a fulfilling experience where you find challenging work, rewarding opportunities, respect among colleagues, and attractive compensation. amtrak maintains a culture that values high performance and recognizes individual employee contributions.\n\n  \n\namtrak is committed to a safe workplace free of drugs and alcohol. all amtrak positions requires a pre\\-employment background check that includes prior employment verification, a criminal history check and a pre\\-employment drug screen.\n\n\ncandidates who test positive for marijuana will be disqualified, regardless of any state or local statute, ordinance, regulation, or other law that legalizes or decriminalizes the use or possession of marijuana, whether for medical, recreational, or other use. amtrak's pre\\-employment drug testing program is administered in accordance with dot regulations and applicable law.\n\n  \n\nin accordance with dot regulations (49 cfr \u00a7 40\\.25\\), amtrak is required to obtain prior drug and alcohol testing records for applicants/employees intending to perform safety\\-sensitive duties for covered department of transportation positions. if an applicant/employee refuses to provide written consent for amtrak to obtain these records, the individual will not be permitted to perform safety\\-sensitive functions.\n\n  \n\nin accordance with federal law governing security checks of covered individuals for providers of public transportation (title 6 u.s.c. \u00a71143\\), amtrak is required to screen applicants for any permanent or interim disqualifying criminal offenses.\n\n  \n\nnote that any education requirement listed above may be deemed satisfied if you have an equivalent combination of education, training and experience.\n\n  \n\namtrak is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race/color, to include traits historically associated with race, including but not limited to, hair texture and hairstyles such as braids, locks and twists, religion, sex (including pregnancy, childbirth and related conditions, such as lactation), national origin/ethnicity, disability (intellectual, mental and physical), veteran status, marital status, ancestry, sexual orientation, gender identity and gender expression, genetic information, citizenship or any other personal characteristics protected by law.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Marketing Engineer",
        "company": "nan",
        "location": "Norfolk, VA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Git",
            "NoSQL",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e7ead4acbbeffb28",
        "description": "**marketing engineer**\n\n\nhealthcare is increasingly unaffordable for many americans. for those who can afford it, they are in a health insurance system that has become more confusing, restrictive, and lower value with each passing year. here at **weshare** our mission is to bring better healthcare to america at a better price. we offer consumers a member\\-to\\-member health sharing program that is much more cost effective than standard health insurance while providing access to over 1\\.2 million physicians across the country. come join us on this important journey to create the next generation of healthcare!\n\n\nweshare is a rapidly growing faith\\-based nonprofit that strives to do good while delivering great and affordable healthcare. the company is led by senior executives with extensive background in both for\\-profit and not\\-for\\-profit enterprises. if you have a bias for action, enjoy challenges, and love creating impact in a massive industry, weshare might be the place for you!\n\n **about this role**\n\n\nthe full stack engineer on the marketing team will play a critical role in building, managing, and optimizing front\\-end and back\\-end technology that drives our digital acquisition, analytics, and customer engagement efforts. this role blends strong engineering capability with an understanding of marketing systems, data flows, and performance\\-driven experimentation. the ideal candidate is a strong versatile builder who can translate marketing needs into scalable technical solutions, move seamlessly between front\\-end and back\\-end work, and thrive in a fast\\-paced, cross\\-functional environment.  \n\n\n\n  \n\n**key responsibilities**\n\n* design, build, and maintain full\\-stack applications that support marketing initiatives, including landing pages, internal tools, integrations, and automation workflows\n* partner closely with brand, product, and analytics teams to translate business requirements into technical solutions\n* develop andoptimizeapis, data pipelines, and integrations across marketing platforms (cms,salesforce, marketing automation, analytics tools, ad platforms, etc.)\n* build high\\-performance, responsive web experiences that support acquisition campaigns and a/b/multi\\-variatetesting\n* implement tracking, tagging, and data collection frameworks to ensureaccuratemeasurement across digital channels\n* own technical qa for marketing launches, ensuring reliability, scalability, and data integrity\n* collaborate with designers and marketers to rapidly prototype and iterate onnew ideas\n* maintain and improve internal dashboards, reporting tools, and data visualizations used by the marketing team\n* ensure best practices in code quality, documentation,security, and performance\n* evaluatenew technologiesand tools that can enhance marketing efficiency, automation, and personalization\n **required qualifications**\n\n* bachelor\u2019s degree in computer science, engineering, or a related technical field\n* 7\\+years of experience as a full stack engineer or software developer\n* strong proficiency in front\\-end frameworks(ionic, vue, or similar)and back\\-end technologies(python,node.js, orc\\+)\n* experience working with databases (sql and nosql) and building scalable apis\n* familiarity with marketing technology platforms(marketing cloud andsalesforce)\n* strong understanding of web analytics, tracking, andgoogle tag manager\n* experience integrating with third\\-party apis, ad platforms, and data tools\n* ability to work cross\\-functionally with non\\-technical stakeholders and translate needs into technical requirements\n* comfortable working in a face\\-paced environment with shifting priorities\n* strong problem\\-solving skills and a proactive, ownership\\-driven mindset\n\n**preferred qualifications*** experience leveraging ai tools (e.g., anthropic) to accelerate workflows, including building, testing, and optimizing processes\n* experience supporting digital acquisition, growth marketing, or lifecycle marketing teams\n* background in consumer brands or high\\-growth environment\n* understanding ofdata visualization tools (tableau, power bi, etc.)\n\n **what we offer**\n\n* competitive salary and benefits\n* ability to make important enhancements to the healthcare industry\n* great culture where you work with the founders and key stakeholders in a relaxed, but innovative atmosphere.\n  \n\nuhsm is an equal opportunity employer. our business is fast\\-paced and will continue to evolve. as such, the duties and responsibilities of this role may be changed as directed by the company at any time to promote and support our business needs. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer - Development and Product Support (On-site in Alpharetta, GA)",
        "company": "Global Payments",
        "location": "Alpharetta, GA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "TensorFlow",
            "Hadoop",
            "MySQL",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=be867fbfa52836d2",
        "description": "every day, global payments makes it possible for millions of people to move money between buyers and sellers using our payments solutions for credit, debit, prepaid and merchant services. our worldwide team helps over 3 million companies, more than 1,300 financial institutions and over 600 million cardholders grow with confidence and achieve amazing results. we are driven by our passion for success and we are proud to deliver best\\-in\\-class payment technology and software solutions. join our dynamic team and make your mark on the payments technology landscape of tomorrow.\n\n\n**summary of this role**\n------------------------\n\n\nworks throughout the software development life cycle and performs in a utility capacity to create, design, code, debug, maintain, test, implement and validate applications with a broad understanding of a variety of languages and architectures. analyzes existing applications or formulate logic for new applications, procedures, flowcharting, coding and debugging programs. maintains and utilizes application and programming documents in the development of code. recommends changes in development, maintenance and system standards. creates appropriate deliverables and develops application implementation plans throughout the life cycle in a flexible development environment.\n\n**what part will you play?**\n----------------------------\n\n* develops basic to moderately complex code using front and / or back end programming languages within multiple platforms as needed in collaboration with business and technology teams for internal and external client software solutions. designs, creates, and delivers routine to moderately complex program specifications for code development and support on multiple projects/issues with a wide understanding of the application / database to better align interactions and technologies.\n* analyzes, modifies, and develops moderately complex code/unit testing in order to develop concise application documentation. performs testing and validation requirements for moderately complex code changes. performs corrective measures for moderately complex code deficiencies and escalates alternative proposals.\n* participates in client facing meetings, joint venture discussions, vendor partnership teams to determine solution approaches.\n* provides support to leadership for the design, development and enforcement of business / infrastructure application standards to include associated controls, procedures and monitoring to ensure compliance and accuracy of data. applies a full understanding of procedures, methodology and application standards to include payment card industry (pci) security compliance.\n* conducts and provides basic billable hours and resource estimates on initiatives, projects and issues.\n* assists with on\\-the\\-job training and provides guidance to other software engineers.\n\n**what are we looking for in this role?**\n-----------------------------------------\n\n### **minimum qualifications**\n\n* bs in computer science, information technology, business / management information systems or related field\n* typically minimum of 4 years \\- professional experience in coding, designing, developing and analyzing data. typically has an advanced knowledge and use of one or more front / back end languages / technologies and a moderate understanding of the other corresponding end language / technology from the following but not limited to; two or more modern programming languages used in the enterprise, experience working with various apis, external services, experience with both relational and nosql databases.\n\n### **preferred qualifications**\n\n* bs in computer science, information technology, business / management information systems or related field\n* 6\\+ years professional experience in coding, designing, developing and analyzing data and experience with ibm rational tools\n\n**what are our desired skills and capabilities?**\n-------------------------------------------------\n\n* skills / knowledge \\- a seasoned, experienced professional with a full understanding of area of specialization; resolves a wide range of issues in creative ways. this job is the fully qualified, career\\-oriented, journey\\-level position.\n* job complexity \\- works on problems of diverse scope where analysis of data requires evaluation of identifiable factors. demonstrates good judgment in selecting methods and techniques for obtaining solutions. networks with senior internal and external personnel in own area of expertise.\n* supervision \\- normally receives little instruction on day\\-to\\-day work, general instructions on new assignments.\n\n  \n\noperating systems:\n\n* linux distributions including one or more for the following: ubuntu, centos/rhel, amazon linux\n* microsoft windows\n* z/os\n* tandem/hp\\-nonstop\n\n\ndatabase \\- design, familiarity with ddl and dml for one or more of the following databases oracle, mysql, ms sql server, ims, db2, hadoop  \n\nback\\-end technologies \\- java, python, .net, ruby, mainframe cobol, mainframe assembler  \n\nfront\\-end technologies \\- html, javascript, jquery, cics  \n\nweb frameworks \u2013 web technologies like node.js, react.js, angular, redux  \n\ndevelopment tools \\- eclipse, visual studio, webpack, babel, gulp  \n\nmobile development \u2013 ios, android  \n\nmachine learning \u2013 python, r, matlab, tensorflow, dmtk\n\n***applicants must be authorized to work in the u.s.***\n\n\nmust be a us citizen, gc holder or on h4ead....opt visa not accepted at this time for this role...  \n\n***we are unable to sponsor or take over sponsorship of an employment visa or student visa at this time.***\n\nglobal payments inc. is an equal opportunity employer. global payments provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. if you wish to request reasonable accommodations related to applying for employment or provide feedback about the accessibility of this website, please contact jobs@globalpay.com.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Engineer",
        "company": "Mercury Insurance Company",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Python",
            "R",
            "Java",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4c67041f6f622e96",
        "description": "overview:\n\njoin an amazing team that is consistently recognized for our achievements and culture, including our most recent forbes award of being one of america's best midsize employers for 2025!\n\n **position summary:**\n\n\nwe're seeking an ai engineer who combines strong software engineering skills with specialized expertise across multiple ai and ml domains. as part of our ai/ml engineering team, you'll work with large language models (llms), vision language models (vlms), speech recognition systems, and conversational ai technologies. this role requires someone who can implement document understanding solutions, ocr systems, human\\-in\\-the\\-loop processes, and multimodal ai applications. you'll focus on model selection, evaluation, fine\\-tuning, and deployment while collaborating with the broader engineering team to build systems that leverage both traditional software engineering principles and advanced ai technologies.\n\n  \n\nan in\\-person interview may be required during the hiring process.\n\n  \n\n\ngeo\\-salary information:\n\nstate specific pay scales for this role are as follows:\n$83,670 to $161,875 (ca, nj, ny, wa, hi, ak, md, ct, ri, ma)\n$76,074 to $147,104 (nv, or, az, co, wy, tx, nd, mn, mo, il, wi, fl, ga, mi, oh, va, pa, de, vt, nh, me)\n$68,457 to $132,394 (ut, id, mt, nm, sd, ne, ks, ok, ia, ar, la, ms, al, tn, ky, in, sc, nc, wv)\nthe expected base salary for this position will vary depending on a number of factors, including relevant experience, skills and location.\nresponsibilities:\n**essential job functions:**\n\n* implement and fine\\-tune large language models (llms) for various business applications\n* develop vision language models (vlms) for image understanding and multimodal applications\n* design and build document understanding systems leveraging ocr and advanced text processing\n* create speech recognition and conversational ai solutions\n* recommend and establish human\\-in\\-the\\-loop processes for ai model improvement across modalities\n* build and deploy ai agents and assistants using foundation models\n* integrate multimodal capabilities (text, vision, speech) into unified ai systems\n* evaluate and benchmark foundation models across different domains and tasks\n* implement effective prompt engineering strategies for llms and vlms\n* partner with our platform team for model serving infrastructure and monitoring solutions for production ai systems\n* create and maintain evaluation frameworks and metrics for different ai modalities\n* collaborate with team members to integrate ai capabilities into existing software systems\n* stay current with emerging ai technologies and research advancements\n\n  \n\n\nqualifications:\n**education:**\n\n\nminimum:\n\n* bachelor's degree in computer science, software engineering, or related technical field\n\n\npreferred:\n\n* master\u2019s degree or phd in computer science, software engineering, or related technical field\n\n**experience:**\n\n\nminimum:\n\n* 2\\+ years of software engineering experience\n* 2\\+ years of hands\\-on experience with ai/ml systems\n* experience with large language models and/or vision language models\n* familiarity with at least one of: document understanding, ocr, speech recognition, or conversational ai\n* experience with foundation model evaluation, selection, or fine\\-tuning\n* knowledge of distributed systems concepts\n\n\npreferred:\n\n* 3\\+ years of software engineering experience\n* 3\\+ years of hands\\-on experience implementing ai/ml solutions\n* demonstrated experience across multiple ai domains (language, vision, speech)\n* experience developing document understanding or ocr systems\n* track record of building conversational ai or speech recognition applications\n* experience with model deployment and monitoring in production\n* experience with multimodal ai systems integration\n* familiarity with large language model evaluation benchmarks\n\n**knowledge and skills:**\n\n\nminimum:\n\n* strong programming skills in python and at least one other modern language (e.g., java, go)\n* solid understanding of software design patterns and principles\n* experience with ai/ml frameworks (pytorch, tensorflow, or similar)\n* proficiency with foundation model apis (openai, anthropic, etc.)\n* knowledge of llm architectures and capabilities\n* knowledge and experience with fine\\-tuning foundational models\n* experience with at least one: computer vision frameworks, ocr technologies, speech recognition systems, or conversational ai platforms\n* experience with prompt engineering and optimization for foundation models\n* knowledge of ai evaluation methodologies and metrics\n* ability to work with cloud\\-based ai infrastructure\n* good verbal and written communication abilities\n\n\npreferred:\n\n* experience with multimodal ai systems (text, vision, speech)\n* expertise in document understanding or information extraction\n* skills in building conversational agents or voice assistants\n* experience with computer vision models and image processing\n* knowledge of rag (retrieval augmented generation) systems\n* experience with human\\-in\\-the\\-loop ai systems\n* familiarity with ai alignment and safety practices\n* experience with model deployment and serving infrastructure\n* skills in ai/ml performance optimization techniques\n* experience with containerization and orchestration technologies\n\n\nabout the company:\n***why choose a career at mercury?***\nat mercury, we have been guided by our purpose to help people reduce risk and overcome unexpected events for more than 60 years. we are one team with a common goal to help others. everyone needs insurance and we can\u2019t imagine a world without it.  \n\nour team will encourage you to grow, make time to have fun, and work together to make great things happen. we embrace the strengths and values of each team member. we believe in having diverse perspectives where everyone is included, to serve customers from all walks of life.  \n\nwe care about our people, and we mean it. we reward our talented professionals with a competitive salary, bonus potential, and a variety of benefits to help our team members reach their health, retirement, and professional goals.  \n\nlearn more about us here: https://www.mercuryinsurance.com/about/careers  \n\n\\#li\\-zk1\nperks and benefits:\n***we offer many great benefits, including:**** competitive compensation\n* flexibility to work from anywhere in the united states for most positions\n* paid time off (vacation time, sick time, 9 paid company holidays, volunteer hours)\n* incentive bonus programs (potential for holiday bonus, referral bonus, and performance\\-based bonus)\n* medical, dental, vision, life, and pet insurance\n* 401 (k) retirement savings plan with company match\n* engaging work environment\n* promotional opportunities\n* education assistance\n* professional and personal development opportunities\n* company recognition program\n* health and wellbeing resources, including free mental wellbeing therapy/coaching sessions, child and eldercare resources, and more\n\n\nmercury insurance is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by federal, state, or local law.\npay range: usd $85,479\\.00 \\- usd $157,868\\.00 /yr.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Azure DevOps Engineer",
        "company": "VeeRteq Solutions Inc.",
        "location": "Libertyville, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Kubernetes",
            "AKS",
            "CI/CD",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8bc75fc220b17ab9",
        "description": "**role:** azure devops engineer\n  \n**location:** libertyville, il (hybrid)\n  \n**duration:** contract to hire after 6 months\n  \noverview\n  \nseeking a highly skilled azure devops engineer to support coud modernization and digital transformation initiatives. this role will focus on designing, implementing, and optimizing ci/cd pipelines, infrastructure automation, security controls, and compliance frameworks in a regulated healthcare environment.\n  \nthe ideal candidate will have strong experience in azure cloud, kubernetes, infrastructure as code, devsecops, and gxp\\-compliant environments, with a mindset focused on automation, governance, and operational excellence.\n  \nsuccess criteria\n  \n  \n\n**the candidate will:**  \n\nreduce deployment cycle time\n  \nincrease release reliability\n  \nstrengthen devsecops posture\n  \nimprove cloud cost optimization\n  \nensure audit\\-ready devops processes in a regulated medical device environment\n  \nkey responsibilities\n  \nci/cd \\& devops engineering\n  \ndesign and implement scalable ci/cd pipelines using azure devops\n  \nautomate build, test, deployment, and release processes\n  \nimplement branching strategies and devops best practices\n  \nenable blue/green and canary deployment models\n  \ninfrastructure as code (iac)\n  \n  \n\n**develop and manage infrastructure using:**  \n\nterraform (preferred)\n  \narm templates / bicep\n  \nimplement automated provisioning of azure environments\n  \nmaintain environment consistency across dev, qa, uat, and production\n  \ncloud \\& containerization\n  \n  \n\n**manage and optimize workloads on:**  \n\nazure kubernetes service (aks)\n  \nazure app services\n  \nazure container registry\n  \nimplement container orchestration and lifecycle management\n  \noptimize cloud performance and cost (finops awareness preferred)\n  \nsecurity \\& compliance\n  \nintegrate security scanning tools (sast, dast, container scanning)\n  \nimplement devsecops practices across pipelines\n  \n  \n\n**support compliance with:**  \n\ngxp\n  \nfda 21 cfr part 11\n  \niso 13485 (preferred)\n  \nenable audit logging, traceability, and change control processes\n  \nwork with security and compliance teams to embed governance into devops workflows\n  \nmonitoring \\& observability\n  \n  \n\n**implement monitoring using:**  \n\nazure monitor\n  \napplication insights\n  \nlog analytics\n  \nset up alerting and automated remediation workflows\n  \nimprove reliability and uptime through observability best practices\n  \ncollaboration\n  \n  \n\n**work closely with:**  \n\ncloud architecture teams\n  \nsecurity teams\n  \napplication development teams\n  \ndigital twin / ai modernization initiatives\n  \nparticipate in agile ceremonies and devops transformation initiatives\n  \nrequired qualifications\n  \n5\\+ years of experience in devops engineering\n  \n3\\+ years hands\\-on experience with azure devops\n  \n  \n\n**strong experience with:**  \n\nazure cloud (iaas, paas)\n  \naks (azure kubernetes service)\n  \nterraform or equivalent iac tools\n  \ngit branching strategies\n  \nexperience implementing devsecops pipelines\n  \nstrong scripting skills (powershell, bash, python)\n  \nexperience working in regulated environments (healthcare, pharma, medical devices preferred)\n  \npreferred qualifications\n  \nexperience supporting gxp environments\n  \nexperience in medical device or life sciences domain\n  \n  \n\n**familiarity with:**  \n\nfinops practices\n  \nai/ml pipeline automation\n  \nkubernetes security hardening\n  \nazure certifications (az\\-400 preferred)\n  \nkey competencies\n  \nautomation\\-first mindset\n  \nstrong governance orientation\n  \nability to work in compliance\\-heavy environments\n  \nproactive problem\\-solving\n  \nstrong communication skills for cross\\-functional collaboration",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Software Engineer",
        "company": "The Walt Disney Company",
        "location": "Orlando, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Terraform",
            "Git",
            "NoSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=05e589f79a796c6d",
        "description": "**job id** 10131020 **location** orlando, florida, united states **business** disney experiences **date posted** feb. 23, 2026\n#### **job summary:**\n\n\n**sr software engineer**\n\n\nat disney experiences technology, our team creates world\\-class immersive digital experiences for the company\u2019s premier vacation brands including disney parks \\& resorts worldwide, disney cruise line, aulani, a disney resort \\& spa, and disney vacation club.  \n\n\n\n  \n\nwe are responsible for the end\\-to\\-end guest and cast experience for technology initiatives across the attractions \\& entertainment, food \\& beverage, resorts \\& transportation, photopass, merchandise and operations workforce management lines of business.\n\n\n**role \\& team:**\n\n\nthe senior software engineer will be responsible for development and engineering for large, custom off\\-the\\-shelf application used for various functions in our disney experience\u2019s workforce management portfolio. this role will conceive, design, develop, test, and implement software fixes, enhancements, components, and/or new software systems and applications of moderate to high complexity. they will own design and development, and drive development of components through their own and subordinate engineers\u2019 work. the senior engineer will provide technical guidance and act as a point of escalation and technical expert. they will design and develop highly scalable software systems and applications.\n\n\nthe successful candidate requires technical knowledge and skills that are broad and deep, covering various hardware, software, and platforms. the candidate must be comfortable operating in complex heterogeneous technology environments and have experience with highly integrated and mission critical solutions. they must possess the ability to share and communicate ideas clearly, both orally and in writing, to business sponsors and partners, technical resources, and executives, in clear concise language that is the effective for each respective group.\n\n**what you will do:**\n\n* owns the design and development of software fixes, enhancements, components, and/or new software systems and applications\n* drives development of components through own and subordinate engineers' work.\n* develops technical solutions that meet specifications and that impact future developments.\n* executes assigned component level software development projects and major fixes using new or existing technologies.\n* develops specifications for assigned components, projects or fixes.\n* reviews or writes code.\n* leads programming, testing and debugging of applications or fixes to existing applications.\n* creates protocols, documentation and tools for installation and maintenance.\n* participates in setting the architectural direction for software development projects.\n* designs specific components for assigned projects, developing specifications for each.\n* designs, develops, manages, creates and maintains technical components and templates.\n* able to code against front\\-end technology stack and lead end\\-to\\-end troubleshooting.\n* interacts and coordinates deliverables with other technical groups in the organization.\n* executes assigned component level projects using new or existing technologies\n* designs and develops specifications for assigned projects\n* reviews or troubleshoots and performs testing.\n* participates in conceiving and setting architectural direction for development projects.\n* designs the component tasks of assigned projects, developing specifications for each\n* serves as a high\\-level technical resource and \u201cgo\\-to\u201d person for less experienced developers, providing technical guidance and oversight.\n* leads team members in problem analysis and issue resolution.\n* recommend improvements to processes, technology, and interfaces that improve the effectiveness of the team.\n\n**required qualifications** **\\& skills:**\n\n* 5\\+ years of experience in software engineering using agile methodology\n* expertise in java/jee/.net, restful/soap services, micro services, angular, nodejs, aws cloud, docker, relational and nosql databases\n* strong collaboration and consensus building skills\n* strong time management and prioritization skills\n* proven ability to learn and apply new technologies\n* strong troubleshooting skills and ability to identify root cause without access to code\n\n**preferred qualifications** :\n\n* experience in a development role for a large custom off the shelf application\n* experience with workforce management systems\n* proven experience in big data technologies, machine learning, bot technologies, and data integration between systems and analytics tools\n* database design experience including both relational and nosql such as dynamodb\n* architecting cloud\\-based solutions using managed and serverless technologies as appropriate such as docker, aws ecs, google gke, google app engine, aws lambda, google cloud functions, aws dynamodb, or google firestore\n* experience managing automated deployments using tools such as terraform, cloudformation, serverless framework, or ansible\n* experience with qa and software analytic tools\n\n**required education:**\n\n* bachelor's degree in computer science, engineering, or related field\n\n\n\\#disneytech",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=659d934badec2ca6",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "Hartford, CT, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a05a899b5290b81b",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e0afa21e3480f282",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=49b0ecb3824425f0",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4785f4e219aa24cd",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - Intelligent Document Processing",
        "company": "The Hartford",
        "location": "Danbury, CT, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=84b39ab1c65d6768",
        "description": "sr cloud engineer \\- ie07ne\nwe\u2019re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. working here means having every opportunity to achieve your goals \u2013 and to help others accomplish theirs, too. join our team as we help shape the future.  \n\n\n\nwe are seeking a highly capable senior software engineer to join our intelligent document processing (idp) team. this role is focused on designing, building, and scaling cloud\\-native software systems that power automated document ingestion, classification, and data extraction at enterprise scale.  \n\n  \n\nyou will be a hands\\-on engineer responsible for developing production\\-grade backend services, driving technical design decisions, and ensuring the reliability, performance, and maintainability of a critical platform. this role partners closely with other software engineers, data scientists, data analysts, and architects to deliver high\\-quality solutions that support core business workflows.\n\n\nthis role will have a hybrid work schedule, with the expectation of working in an office (columbus, oh, chicago, il, hartford, ct or charlotte, nc) 3 days a week (tuesday through thursday).\n\n**key responsibilities**\n\n* design, develop, test, and maintain scalable backend services supporting intelligent document processing workflows.\n* build cloud\\-native applications on aws, leveraging managed services to deliver secure, resilient, and highly available systems.\n* develop and optimize automated document workflows, including ingestion, classification, extraction, and validation pipelines.\n* own service\\-level concerns including performance, reliability, observability, and cost efficiency.\n* implement monitoring, logging, alerting, and fault\\-tolerant patterns.\n* collaborate with cross\\-functional partners to translate business needs into well\\-architected software solutions.\n* participate in system design discussions, code reviews, and technical decision\\-making.\n* contribute to ci/cd pipelines and infrastructure\\-as\\-code.\n* author and maintain technical documentation and runbooks.\n* evaluate emerging technologies and contribute to roadmap discussions.\n\n**qualifications**\n\n**required**\n\n* bachelor\u2019s degree in computer science, software engineering, or related field.\n* 5\\+ years of professional software engineering experience.\n* strong programming skills in python and .net or java.\n* experience building cloud\\-native applications on aws.\n* experience with ci/cd and infrastructure\\-as\\-code tools.\n* strong understanding of distributed systems and backend engineering fundamentals.\n\n**preferred**\n\n* experience with intelligent document processing or genai\\-powered document platforms.\n* exposure to ocr, nlp, or machine learning technologies.\n* experience with docker and kubernetes.\n* aws certifications.\n* familiarity with insurance or financial document types.\n* experience with agile methodology.\n\n**candidate must be authorized to work in the us without company sponsorship. the company will not support the stem opt i\\-983 training plan endorsement for this position.**\n\n**compensation**\n\n\nthe listed annualized base pay range is primarily based on analysis of similar positions in the external market. actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. the base pay is just one component of the hartford\u2019s total compensation package for employees. other rewards may include short\\-term or annual bonuses, long\\-term incentives, and on\\-the\\-spot recognition. the annualized base pay range for this role is:\n\n\n$137,200 \\- $205,800\nequal opportunity employer/sex/race/color/veterans/disability/sexual orientation/gender identity or expression/religion/age",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "IT Automation Engineer",
        "company": "First Community Mortgage",
        "location": "Murfreesboro, TN, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "CI/CD",
            "Git",
            "PostgreSQL",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=be3722d7e483ab58",
        "description": "first community mortgage is seeking a curious, driven it automation engineer to join our it engineering \\& operations team. this junior\\-level role is ideal for a computer science enthusiast who thrives on building workflow automations, connecting systems via apis, and turning manual processes into elegant, self\\-running solutions. you will play a hands\\-on role in designing, building, and maintaining automation workflows using n8n and supporting technologies across our mortgage technology stack.\n\n\n\nthis is a high\\-impact role where you will work alongside senior engineers and cross\\-functional stakeholders to modernize how we move data, process documents, and integrate vendor platforms. if you geek out over clean json payloads, well\\-structured webhook pipelines, and watching workflows fire flawlessly in production, this is the role for you.\n\n\n#### **key responsibilities**\n\n* design, build, test, and deploy automation workflows in n8n, connecting internal systems and thirdparty platforms via rest apis and webhooks.\n* develop and maintain integrations with vendor apis including document processing, loan origination, and crm platforms.\n* write supporting python scripts for data transformation, file processing, pdf manipulation, and custom logic nodes.\n* build and manage sql queries and database interactions for workflow data enrichment, reporting, and validation.\n* support azure cloud infrastructure including azure functions, storage accounts, and serverless compute for document processing pipelines.\n* implement error handling, retry logic, and monitoring for production workflows using tools like grafana and loki.\n* collaborate with business stakeholders to translate manual processes into automated workflows with clear documentation.\n* participate in incident response and troubleshooting of production automation failures.\n* maintain version control and documentation standards for all automation assets.\n* contribute to security and compliance initiatives by following secure coding and data handling practices.\n\n#### **required qualifications**\n\n* bachelor's degree in computer science, information technology, or related field (or equivalent handson experience).\n* demonstrated experience with n8n workflow automation (personal projects, internships, or professional experience accepted).\n* solid understanding of restful apis, webhook architecture, http methods, and json/xml data formats.\n* proficiency in python scripting for automation, data transformation, and file processing tasks.\n* working knowledge of sql and relational databases (queries, joins, stored procedures).\n* familiarity with cloud platforms, preferably microsoft azure (functions, blob storage, app services).\n* basic understanding of git version control and collaborative development workflows.\n* strong troubleshooting mindset with the ability to read logs, trace errors, and debug integrations.\n\n#### **preferred qualifications**\n\n* experience with document processing or ocr platforms (e.g., ocrolus, abbyy, or similar).\n* familiarity with mortgage industry systems, loan origination platforms, or financial services technology.\n* experience with monitoring and observability tools such as grafana, loki, or prometheus.\n* exposure to containerization concepts (docker) and ci/cd pipelines.\n* knowledge of queuebased architectures and eventdriven processing patterns.\n\n#### **technology stack you will work with**\n\n\nautomation n8n (primary), azure functions, python scripting\n\n\n\ncloud \\& infrastructure microsoft azure, azure blob storage, app services\n\n\n\nlanguages \\& scripting python, javascript/node.js, sql, bash\n\n\n\napis \\& integration rest apis, webhooks, oauth, json/xml\n\n\n#### **databases sql server, postgresql**\n\n#### **monitoring grafana, loki, azure monitor**\n\n\nversion control git, github / azure devops\n\n\n\ncollaboration microsoft teams, jira, confluence\n\n\n#### **what we offer**\n\n* handson mentorship from experienced engineers in a collaborative, lowego environment.\n* opportunity to own and build automation solutions that directly impact business operations.\n* exposure to cuttingedge ai and document processing technologies in the mortgage industry.\n* flexible work environment with remotefriendly culture.\n* competitive salary, benefits package, and professional development opportunities.\n* a team that genuinely values curiosity, continuous learning, and a passion for technology.\n\n#### **the ideal candidate is someone who...**\n\n* lights up when they hear \"workflow automation\" and has strong opinions about webhook design.\n* has side projects, homelab experiments, or github repos that showcase their love of building things.\n* reads api documentation for fun (or at least doesn't dread it).\n* asks \"can we automate that?\" in every meeting.\n* communicates clearly with both technical and nontechnical colleagues.\n* approaches problems with intellectual humility and a willingness to learn from mistakes.\n\n#### **equal opportunity statement**\n\n\nfirst community mortgage is an equal opportunity employer committed to fostering a diverse and inclusive workplace. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected characteristic.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist (Transmission & Substation Engineering)",
        "company": "ComEd",
        "location": "Oakbrook Terrace, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Hadoop",
            "Dask",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f8825c9ffb07f54f",
        "description": "who we are: we're powering a cleaner, brighter future.  \n\n  \n\nexelon is leading the energy transformation, and we're calling all problem solvers, innovators, community builders and change makers. work with us to deliver solutions that make our diverse cities and communities stronger, healthier and more resilient.  \n\n  \n\nwe're powered by purpose\\-driven people like you who believe in being inclusive and creative, and value safety, innovation, integrity and community service. we are a fortune 200 company, 20,000 colleagues strong serving more than 10\\.7 million customers at six energy companies \\- atlantic city electric (ace), baltimore gas and electric (bge), commonwealth edison (comed), delmarva power \\& light (dpl), peco energy company (peco), and potomac electric power company (pepco).  \n\n  \n\nwe're committed to creating an environment where every person can thrive. our employee experience is grounded in four tenets that guide how we support our people: purposeful careers, growth opportunities, community impact, and support to thrive.  \n\n  \n\nin our relentless pursuit of excellence, we elevate diverse voices, fresh perspectives and bold thinking. and since we know transforming the future of energy is hard work, we provide competitive compensation, incentives, excellent benefits and the opportunity to build a rewarding career.  \n\n  \n\nare you in? primary purpose:\n\nthe data scientist (transmission \\& substation engineering) at comed will apply the scientific method to extract knowledge and insights from data, which may take the form of time\\-series (t\\&s equipment for eg circuit breakers, transformers, relays etc.), structured (relational data stores), and unstructured (text and multi\\-media) data sets. train state of the art algorithmical models, including but not limited to tree\\-based approaches and neural networks, and implement those models into a production environment following the established machine learning approaches. closely collaborate with various internal stakeholders, information architects, data engineers, project/program managers, and other teams to turn data into analytics\\-driven products and inform decision making. this requires understanding business needs, providing and receiving regular feedback, and planning the proper transfer of developed solutions. validate findings with the business by sharing analysis outputs in a way that can be understood by business stakeholders. become a subject matter expert in the areas of artificial intelligence, machine learning, feature engineering, and high\\-performance computing. demonstrate commitment to continuous learning and professional development in technical subject matter. share knowledge with team members, and business stakeholders, and partners. a successful candidate will quickly adopt the team's established working processes and toolkit while growing his/her knowledge of the utilities industry. position may be required to work extended hours for coverage during storms or other energy delivery emergencies.\n\n\nprimary duties:\n* develop key predictive models that lead to delivering a premier customer experience, operating performance improvement, and increased safety best practices\n* analyze data using advanced analytics techniques in support of process improvement efforts using modern analytics frameworks, including but not limited to python, r, scala, or equivalent; spark, hadoop file system and others\n* access and analyze data sourced from various company systems of record. support the development of strategic business, marketing, and program implementation plans.\n* provide expert data and analytics support to multiple business units\n* access and enrich data warehouses across multiple company departments. build, modify, monitor and maintain high\\-performance computing systems.\n\n\njob scope:\n\nsupport business unit strategic planning while providing a strategic view on machine learning technologies. advice and counsel key stakeholders on machine learning findings and recommend courses of action that redirect resources to improve operational performance or assist with overall emerging business issues. provide key stakeholders with machine learning analyses that best positions the company going forward. educate key stakeholders on the organizations advance analytics capabilities through internal presentations, training workshops, and publications.\n\n\nminimum qualifications:\n* bachelor's or master's degree from a leading program in a quantitative discipline. ex: applied mathematics, computer science, finance, operations research, physics, statistics, or related field\n* 2\\-4 years of relevant experience analyzing multi\\-terabyte datasets is required (industry or academia). previous research or professional experience applying advanced analytic techniques to large, complex datasets. minimum of 1\\-2 years of professional experience in a data scientist role.\n* strong knowledge in at least two of the following areas: machine learning, artificial intelligence, statistical modeling, data mining, information retrieval, or data visualization.\n* demonstratable experience in your analytics/statistics/visualization platform of choice, but preferably in the ms azure suite as well as python, sql. experience developing in unix, using big data technologies like spark, dask, etc.\n* experience working within an open source environment and unix\\-based os.\n* ability to translate data analysis and findings into coherent conclusions and actionable recommendations to business partners, practice leaders, and executives. strong oral and written communication skills.\n\n\npreferred qualifications:\n* master's or phd from a leading program in a quantitative discipline\n* prior exposure to data structures pertaining to smart\\-meters, billing, or outage management systems. prior exposure to the utilities or broader energy sector.\n* solid understanding of relevant theories in machine learning, statistics, probability theory, data structures and algorithms, optimization, etc.\n* expert level coding skills (python, r, scala, etc), and experience developing in a unix environment.\n* ability to translate executive and analytics leaders vision and guidance into methods and analytics. strong time management and presentation skills. experience presenting to diverse audiences including presenting to conferences and business symposia.\n\n\nbenefits:\n  \n\n* annual salary will vary based on a candidate\u2019s skills, qualifications, experience, and other factors: $74,400\\.00/yr. \u2013 $102,300\\.00/yr.\n* annual bonus for eligible positions: 10%\n* 401(k) match and annual company contribution\n* medical, dental and vision insurance\n* life and disability insurance\n* generous paid time off options, including vacation, sick time, floating and fixed holidays, maternity leave and bonding/primary caregiver leave or parental leave\n* employee assistance program and resources for mental and emotional support\n* wellbeing programs such as tuition reimbursement, adoption and surrogacy assistance and fitness reimbursement\n* referral bonus program\n* and much more\n\n  \n\nnote: exelon\\-sponsored compensation and benefit programs may vary or not apply based on length of service, job grade, job classification or represented status. eligibility will be determined by the written plan or program documents.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist",
        "company": "Mutual of Omaha",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "CI/CD",
            "Git",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a094b0555a3afdd7",
        "description": "**location:** remote  \n\n**work type:** full time regular  \n\n**job no:** 504555  \n\n**categories:** information technology  \n\n**application closes:** open until filled\n\n\n2026\\-02\\-23  \n\n**data scientist \\- remote**\n\n\n\njoin mutual of omaha data science team who bridges the gap between traditional statistical modeling and advanced ai orchestration. in this role, you will be partner with stakeholders serving as a key designer and builder of agentic ai systems that power analytical workflows or conversational interfaces. we are looking for a problem\\-solver who is as comfortable writing sql queries as they are building autonomous agents that understand the full\\-stack journey.\n\n\n\n**what we can offer you:**\n\n\n* $100,000 to $140,000, eligible for annual bonus, as applicable.\n* 401(k) plan with a 2% company contribution and 6% company match.\n* work\\-life balance with vacation, personal time and paid holidays. see our benefits and perks page for details.\n* applicants for this position must not now, nor at any point in the future, require sponsorship for employment.\n\n  \n\n\n**what you\u2019ll do:**\n\n\n**stakeholder translation \\& technical spec design:** partner closely with business stakeholders to extract core needs and translate them into rigorous technical specifications for ai systems and agent behaviors.\n\n\n**perform advanced data manipulation:** use sql, python, and r to extract and profile data from structured and unstructured sources, ensuring high\\-quality data inputs for both machine learning models and llm agent contexts.\n\n\n**full\\-stack logic integration:** apply a \"full\\-stack\" mindset to ai development. you will ensure a seamless flow of data between the **backend**, the **ai agent**, and the **ui**, while ensuring your code is architected for **automated ci/cd pipelines** and stable production environments.\n\n\n**build \\& orchestrate ai agents:** develop and refine autonomous ai agents using frameworks such as langgraph to power intelligent chatbots, designing stateful, multi\\-step workflows that navigate complex business logic.\n\n\n**design ai systems:** design ai solutions with a \"production\\-first\" logic. while others may handle the final deployment, you are responsible for ensuring your **python\\-based agents** are modular, and secure, leverage aws cloud platform.\n\n\n**build for observability:** develop evaluation frameworks and metrics to monitor the accuracy, reliability, and cost\\-effectiveness of ai agents and traditional statistical models.\n\n\n**r\\&d:** develop, experiment with, and refine autonomous ai agents using frameworks such as langgraph, conducting iterative research and prototyping to evaluate new agent behaviors, orchestration patterns, and stateful workflows before scaling them into production\\-grade systems.\n\n  \n\n\n**what you\u2019ll bring:**\n\n\n**the ds core:** a graduate degree in an analytical field (math, stats, cs, bi etc.) and 2\\+ years of experience in an analytically driven role.\n\n\n**agentic expertise:** 2\\+ years of experience specifically building ai agents and designing complex systems that leverage langgraph or similar orchestration frameworks.\n\n\n**full\\-stack awareness:** a strong understanding of the full\\-stack lifecycle, including how data travels from backend databases through the application logic to a conversational ui.\n\n\n**language proficiency:** strong command of python and sql (required), with familiarity in r or other statistical languages.\n\n\n**aws cloud fluency:** hands\\-on experience building applications within the aws llm ecosystem (bedrock, sagemaker) and a functional understanding of working within an aws lambda environment.\n\n\n**statistical rigor**: a strong background in machine learning and statistical analysis, with the ability to validate agent outputs against business benchmarks.\n\n\n**production perspective:** experience designing solutions with a \"production\\-grade\" mindset\u2014understanding the requirements for reliability, security, and scalability.\n\n  \n\n\n**preferred:**\n\n\n\nexperience with aws bedrock agents and action group integration.\n\n\n\ndemonstrated ability to navigate under\\-defined problems and deliver robust solutions in fast\\-moving environments.\n\n\n\nworking knowledge of generative ai orchestration frameworks, retrieval\\-augmented generation (rag) architectures, and enterprise ai deployment pipelines.\n\n\nwe value diverse experiences, skills, and a passion for innovation. if your background aligns with this opportunity, we encourage you to apply.\n\n\n\nif you have questions about your application or the hiring process, please email careers@mutualofomaha.com. please allow at least one week after applying before requesting a status update.\n\n\n**stay safe from job scams**\n\n\n\nwe only accept applications through our official careers site. legitimate communications will come from an @mutualofomaha.com email address. we will never request sensitive information or extend an offer without conducting interviews. please stay alert and apply securely.\n\n  \n\n\nneed help? email us\n**great place to work**\n-----------------------\n\n\ntogether we achieve greatness. not only is this a core value, but it\u2019s also representative of the kind of place we are \u2014 built by the strength and integrity of our employees. it\u2019s why we\u2019re named a \u201cgreat place to work\u201d.\n\n  \n\n\n**an inclusive culture**\n------------------------\n\n\nsurround yourself with an authentic and inclusive culture. your strengths and differences will be valued and celebrated by a diverse community of co\u2011workers.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI/ML Software Developer",
        "company": "Takeuchi",
        "location": "Pendergrass, GA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "Git",
            "Snowflake",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7a073898e33b883f",
        "description": "**ai/ml software developer**\n\n\ntakeuchi\\-us is looking for a seasoned developer with at least 3 years\u2019 experience in ai and ml to build and grow our ai practice within the business technology group. in this role, you will be responsible for designing, building and deploying machine learning, deep learning, and natural language processing (nlp) applications to automate business processes, analyze large datasets, and create innovative ai\\-powered applications. **this position is on\\-site in pendergrass, georgia.**\n\n\njob responsibilities\n\n\n\n\n* **model development:** designing, training, and testing machine learning models and neural networks for tasks like computer vision and speech recognition.\n* **system integration:** integrating ai algorithms into existing software applications and infrastructure, ensuring scalability and performance.\n* **data preparation:** collecting, cleaning, and preprocessing large datasets to train and fine\\-tune ai models.\n* **collaboration:** working with cross\\-functional teams, to align ai solutions with business goals.\n* **documentation:** documenting ai development processes, model architecture, and testing results.\n\n  \n\nqualifications:\n\n* degree in computer science, data science, ai or related field.\n* strong proficiency in python, java, c\\+\\+\n* experience working with snowflake\n* experience with ai frameworks\n* experience with aws or azure\n* strong proficiency in sql and data processing tools.\n* knowledge of git, docker, kubernetes, and restful apis\n* knowledge of qlik is a plus.\n* knowledge of epicor p21 is a plus.\n* ability to explain complex technical concepts to non\\-technical stakeholders.\n* ability to work effectively in a team\\-oriented, fast\\-paced environment.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. Site Reliability Engineer - SRE",
        "company": "AT&T",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "Kafka",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=bd62853e90f5d50a",
        "description": "**this position requires office presence of a minimum of 5 days per week and is only located in the location(s) posted. no relocation is offered.**\n\n\njoin at\\&t and help shape the future of communications and technology that connect the world. we value innovators who seek to explore the unknown and challenge the status quo. bring your bold ideas and fearless spirit to redefine connectivity and transform how people share stories and experiences. at at\\&t, you won\u2019t just imagine the future\u2014you\u2019ll build it.\n\n\nas a **sr.** **system engineer with a site reliability engineering (sre) focus** you\u2019ll help maintain and improve the reliability, performance, and scalability of mission\\-critical applications for front office (crm) and back office (supply chain/logistics mgmt).\n\n\nwe are seeking a seasoned professional with deep technical (both development customization and maintenance) and functional (hands\\-on expertise in flows \u2013 order to cash, procure to pay, etc.) expertise to support oracle ebs/erp supply chain modules (procurement, order management, inventory, project accounting, receivables, payables, supply chain planning, etc.), oracle fusion cloud applications, additional cots applications such as o9, blue yonder(jda), relex, and integrations to 3rd\u2011party wms/3pl/logistics platforms. the role is accountable for platform stability, incident and problem management, root\\-cause analysis, monitoring/automation, and continuous improvement across supply chain processes and interfaces.\n\n **what you\u2019ll do**\n\n* provide day\\-to\\-day production support for oracle ebs/erp (procurement, om, inventory); resolve incidents/requests within slas and ensure service stability.\n* troubleshoot end\\-to\\-end issues across business workflows, configuration, master/transaction data, batch/concurrent programs, and interface processing using strong analytical and sql skills.\n* support supply chain execution flows including procure\\-to\\-pay, order\\-to\\-cash, receiving, picking/packing/shipping, inventory transactions (transfers, adjustments, reservations, cycle counts), and reconciliation of on\\-hand balances.\n* perform data triage and correction (where approved) and partner with functional leads for process/config changes.\n* monitor and support 3rd\\-party integrations (e.g., asn, receipts, shipment confirmations, inventory updates, returns/rmas).\n* partner with integration/middleware teams and vendors to resolve cross\\-system defects, mapping issues, sequencing/latency problems, and data mismatches.\n* analyze and remediate interface errors in queues/tables/logs; validate reprocessing/replay and prevent duplicates.\n* lead incident response and minimize downtime.\n* build and maintain monitoring, alerts, and dashboards for proactive issue detection.\n* create run books and automate operational tasks to improve efficiency.\n* collaborate with development teams to define and meet non\\-functional requirements (reliability, performance, scalability).\n* conduct blameless postmortems and drive continuous improvements.\n* support release management, capacity planning, and security best practices.\n* provide 24x7 on\\-call support as needed.\n\n**what you\u2019ll bring**  \n\n**technical requirements:**\n\n* 7\\+ years in development, functional and maintenance experience for oracle applications (ebs/fusion \u2013 ar, ap, fa, po, inv, pa, om, planning, etc.) \\- with sre mindset\n* proficiency in sql/plsql, and oracle technologies, java/j2ee, scripting (python, shell), and automation (ai based automation a plus).\n* strong skills with observability tools (dynatrace, appdynamics, splunk, elk, grafana).\n* experience with containerization (docker, kubernetes) and cloud services (azure).\n* experience with middleware / integration technologies \u2013 oracle soa/oic, mulesoft, kafka/jms, edi.\n* excellent problem\\-solving and communication skills.\n* bachelor\u2019s degree in computer science, it, or related field.\n\n**supervisor:** no\n\n\nour senior system engineering , earns between $143,800\\-$215,800 usd annual , not to mention all the other amazing rewards that working at at\\&t offers. individual starting salary within this range may depend on geography, experience, expertise, and education/training.\n\n**joining our team comes with amazing perks and benefits:**\n\n* medical/dental/vision coverage\n* 401(k) plan\n* tuition reimbursement program\n* paid time off and holidays (based on date of hire, at least 23 days of vacation each year and 9 company\\-designated holidays)\n* paid parental leave\n* paid caregiver leave\n* additional sick leave beyond what state and local law require may be available but is unprotected\n* adoption reimbursement\n* disability benefits (short term and long term)\n* life and accidental death insurance\n* supplemental benefit programs: critical illness/accident hospital indemnity/group legal\n* employee assistance programs (eap)\n* extensive employee wellness programs\n* employee discounts up to 50% off on eligible at\\&t mobility plans and accessories,\n* at\\&t internet (and fiber where available) and at\\&t phone.\n\n\n\\#li\\-onsite \u2013 full\\-time office role\\-\n\n\nready to join our team? apply today.\n\n**weekly hours:**\n\n\n40**time type:**\n\n\nregular**location:**\n\n\nalpharetta, georgia, dallas, texas**salary range:**\n\n$128,400\\.00 \\- $215,800\\.00\nit is the policy of at\\&t to provide equal employment opportunity (eeo) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. in addition, at\\&t will provide reasonable accommodations for qualified individuals with disabilities. at\\&t is a fair chance employer and does not initiate a background check until an offer is made.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr. Site Reliability Engineer - SRE",
        "company": "AT&T",
        "location": "Alpharetta, GA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "Kafka",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=326d5680ae095aba",
        "description": "**this position requires office presence of a minimum of 5 days per week and is only located in the location(s) posted. no relocation is offered.**\n\n\njoin at\\&t and help shape the future of communications and technology that connect the world. we value innovators who seek to explore the unknown and challenge the status quo. bring your bold ideas and fearless spirit to redefine connectivity and transform how people share stories and experiences. at at\\&t, you won\u2019t just imagine the future\u2014you\u2019ll build it.\n\n\nas a **sr.** **system engineer with a site reliability engineering (sre) focus** you\u2019ll help maintain and improve the reliability, performance, and scalability of mission\\-critical applications for front office (crm) and back office (supply chain/logistics mgmt).\n\n\nwe are seeking a seasoned professional with deep technical (both development customization and maintenance) and functional (hands\\-on expertise in flows \u2013 order to cash, procure to pay, etc.) expertise to support oracle ebs/erp supply chain modules (procurement, order management, inventory, project accounting, receivables, payables, supply chain planning, etc.), oracle fusion cloud applications, additional cots applications such as o9, blue yonder(jda), relex, and integrations to 3rd\u2011party wms/3pl/logistics platforms. the role is accountable for platform stability, incident and problem management, root\\-cause analysis, monitoring/automation, and continuous improvement across supply chain processes and interfaces.\n\n **what you\u2019ll do**\n\n* provide day\\-to\\-day production support for oracle ebs/erp (procurement, om, inventory); resolve incidents/requests within slas and ensure service stability.\n* troubleshoot end\\-to\\-end issues across business workflows, configuration, master/transaction data, batch/concurrent programs, and interface processing using strong analytical and sql skills.\n* support supply chain execution flows including procure\\-to\\-pay, order\\-to\\-cash, receiving, picking/packing/shipping, inventory transactions (transfers, adjustments, reservations, cycle counts), and reconciliation of on\\-hand balances.\n* perform data triage and correction (where approved) and partner with functional leads for process/config changes.\n* monitor and support 3rd\\-party integrations (e.g., asn, receipts, shipment confirmations, inventory updates, returns/rmas).\n* partner with integration/middleware teams and vendors to resolve cross\\-system defects, mapping issues, sequencing/latency problems, and data mismatches.\n* analyze and remediate interface errors in queues/tables/logs; validate reprocessing/replay and prevent duplicates.\n* lead incident response and minimize downtime.\n* build and maintain monitoring, alerts, and dashboards for proactive issue detection.\n* create run books and automate operational tasks to improve efficiency.\n* collaborate with development teams to define and meet non\\-functional requirements (reliability, performance, scalability).\n* conduct blameless postmortems and drive continuous improvements.\n* support release management, capacity planning, and security best practices.\n* provide 24x7 on\\-call support as needed.\n\n**what you\u2019ll bring**  \n\n**technical requirements:**\n\n* 7\\+ years in development, functional and maintenance experience for oracle applications (ebs/fusion \u2013 ar, ap, fa, po, inv, pa, om, planning, etc.) \\- with sre mindset\n* proficiency in sql/plsql, and oracle technologies, java/j2ee, scripting (python, shell), and automation (ai based automation a plus).\n* strong skills with observability tools (dynatrace, appdynamics, splunk, elk, grafana).\n* experience with containerization (docker, kubernetes) and cloud services (azure).\n* experience with middleware / integration technologies \u2013 oracle soa/oic, mulesoft, kafka/jms, edi.\n* excellent problem\\-solving and communication skills.\n* bachelor\u2019s degree in computer science, it, or related field.\n\n**supervisor:** no\n\n\nour senior system engineering , earns between $143,800\\-$215,800 usd annual , not to mention all the other amazing rewards that working at at\\&t offers. individual starting salary within this range may depend on geography, experience, expertise, and education/training.\n\n**joining our team comes with amazing perks and benefits:**\n\n* medical/dental/vision coverage\n* 401(k) plan\n* tuition reimbursement program\n* paid time off and holidays (based on date of hire, at least 23 days of vacation each year and 9 company\\-designated holidays)\n* paid parental leave\n* paid caregiver leave\n* additional sick leave beyond what state and local law require may be available but is unprotected\n* adoption reimbursement\n* disability benefits (short term and long term)\n* life and accidental death insurance\n* supplemental benefit programs: critical illness/accident hospital indemnity/group legal\n* employee assistance programs (eap)\n* extensive employee wellness programs\n* employee discounts up to 50% off on eligible at\\&t mobility plans and accessories,\n* at\\&t internet (and fiber where available) and at\\&t phone.\n\n\n\\#li\\-onsite \u2013 full\\-time office role\\-\n\n\nready to join our team? apply today.\n\n**weekly hours:**\n\n\n40**time type:**\n\n\nregular**location:**\n\n\nalpharetta, georgia, dallas, texas**salary range:**\n\n$128,400\\.00 \\- $215,800\\.00\nit is the policy of at\\&t to provide equal employment opportunity (eeo) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. in addition, at\\&t will provide reasonable accommodations for qualified individuals with disabilities. at\\&t is a fair chance employer and does not initiate a background check until an offer is made.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Analytics Engineer II",
        "company": "CDW",
        "location": "Remote, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "Data Lake",
            "AKS",
            "Git",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3133b5a21a1473ac",
        "description": "**description**\n\n\nat cdw, we make it happen, together. trust, connection, and commitment are at the heart of how we work together to deliver for our customers. it\u2019s why we\u2019re coworkers, not just employees. coworkers who genuinely believe in supporting our customers and one another. we collectively forge our path forward with a level of commitment that speaks to who we are and where we\u2019re headed. we\u2019re proud to share our story and make amazing happen at cdw.\n\n**job summary**\n\n\nthe analytics engineer ii plays a pivotal role in designing, building, and operationalizing data solutions that power analytics, reporting, and data\u2011driven decision\u2011making across the enterprise. this role blends software engineering rigor with analytics\u2011focused data modeling by developing scalable data pipelines, transforming raw data into consumable datasets, and supporting analytics teams with well\u2011structured, high\u2011quality data assets. the analytics engineer ii partners closely with business and technical teams to improve data models, enhance data accessibility, and automate complex data preparation tasks using modern cloud platforms and engineering best practices.\n\n**what you will do:**\n\n* build and optimize full\u2011stack, cloud\u2011based data systems and pipelines that deliver high\u2011quality, analysis\u2011ready datasets for business and analytics teams\n* interface with other technology teams to build azure data pipelines to integrate with data lake and other enterprise platforms\n* enhance ingestion, transformation, and orchestration workflows aligned with enterprise architecture standards\n* collaborate with other technology teams to help engineer data sets and curate semantic models that support analytics, machine learning, kpi reporting, and business intelligence use cases.\n* partners with business teams to enhance data models supporting business intelligence tools, increase data accessibility, and foster data\\-driven decision\\-making throughout the organization.\n* partner with analysts and data scientists to translate business requirements into scalable engineering solutions that improve data accessibility and reliability.\n* share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal \\& external technology communities, mentoring other members of the engineering community.\n* collaborate with digital product managers and deliver robust cloud\\-based solutions that drive powerful experiences and support enterprise\u2011wide analytics decision\u2011making\n* responsible for using innovative and modern tools, techniques, and architectures to partially or completely automate the most\\-common, repeatable, and tedious data preparation and integration tasks to minimize manual and error\\-prone processes and improve productivity.\n* be curious and knowledgeable about new data management techniques and how to apply them to solve business problems.\n* document data flows, transformations, and modeling logic to promote consistency and trust in delivered datasets.\n\n**what we expect of you:**\n\n* bs degree in computer science, information systems, business analytics, or a related technical field **and** 3 years of data application development experience using azure data factory/fabric data factory or ssis,\n* 5\\+ years of data application development experience using azure data factory/fabric data factory or ssis.\n* experienced researching technical and business questions with minimal oversight.\n* intermediate to advanced sql and/or python skillsets to transform and analyze data, building or supporting data pipelines, and delivering datasets used for reporting or analytics.\n* experience in developing power bi data models and dashboards\n* ability to work independently on moderately complex tasks.\n* ability to effectively communicate and collaborate with business partners and technical teams.\n* experience in agile project management methodologies\n* experience experimenting with ai agents, llm\u2011based tools, or prompt\u2011driven automation to enhance analytics or data workflows\n\n\npay range: $86,000 \\- $119,600 depending on experience and skill set  \n\nbenefits overview: https://cdw.benefit\\-info.com/  \n\nsalary ranges may be subject to geographic differentials\n\n**we make technology work so people can do great things.**\n\n\ncdw is a leading multi\\-brand provider of information technology solutions to business, government, education and healthcare customers in the united states, the united kingdom and canada. a fortune 500 company and member of the s\\&p 500 index, cdw helps its customers to navigate an increasingly complex it market and maximize return on their technology investments. together, we unite. together, we win. together, we thrive.\n\n\ncdw is an equal opportunity employer. all qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status or any other basis prohibited by state and local law.\n\n\ncdw is committed to fostering an equitable, transparent, and respectful hiring process for all applicants. during our application process, cdw\u2019s goal is to get to know you as an applicant and understand your experience, strengths, skills, and qualifications. while ai can help you present yourself more clearly and effectively, the essence of your application should be authentically yours. to learn more, please review cdw's ai applicant notice.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Entry-Level AI / Machine Learning Software Engineer",
        "company": "MTSI",
        "location": "Huntsville, AL, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Git",
            "Matplotlib",
            "Seaborn",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6cd62aed0ccfaf4c",
        "description": "we are seeking an entry\\-level ai / machine learning software engineer to assist in developing data analysis tools and machine learning capabilities for large datasets and streaming data. this role focuses on building foundational skills in python development, data processing, and applied machine learning while working alongside experienced engineers.\nthe ideal candidate is a motivated learner with basic programming experience and an interest in artificial intelligence, data science, or software development.\n**primary responsibilities*** develop and maintain python scripts for data processing and analysis\n* assist in building and testing machine learning models\n* work with structured and unstructured datasets\n* help implement algorithms for classification, prediction, and pattern detection\n* support integration of ai/ml features into existing software systems\n* debug and troubleshoot code and data issues\n* document code, workflows, and findings\n* collaborate with team members to improve tools and performance\n\n\n**required qualifications*** 0\u20132 \\+ years of experience (internships, academic projects, or personal projects acceptable)\n* basic python programming skills\n* understanding of programming fundamentals (variables, loops, functions, data structures)\n* introductory knowledge of machine learning concepts\n* exposure to at least one ml or deep learning framework:\n* pytorch, tensorflow, or similar\n* familiarity with git or other version control tools\n* ability to learn new technologies quickly and work in a team environment\n\n\n**desired / preferred qualifications*** coursework or projects involving data analysis or machine learning\n* familiarity with common model types (e.g., decision trees or neural networks)\n* exposure to data visualization tools (matplotlib, seaborn, etc.)\n* basic understanding of large datasets or streaming data concepts\n* experience using linux or development environments\n* interest in ai topics such as llms, rag, or deep learning\n\n\n**education*** bachelor\u2019s degree (or working toward a degree) in computer science, data science, engineering, mathematics, or related field\n* (equivalent practical experience considered)\n\n\n**nice\\-to\\-know technologies*** linux development environments\n* jupyter notebooks\n* docker or container basics\n* basic command line usage\n\n  \n\n\\#li\\-as1\n  \n\n**perks and benefits**\n======================\n\n* *vacation:*\nnew hires accrue 20 days of pto and 10 holidays per year\n* *health insurance:*\nzero deductible health plans\n* *flexible schedules:*\nflex schedules\n* *professional development:*\nup to $10,000 annual education/training reimbursement\n* *esop:*\nfunded stock ownership plan\n* *401k match\\+:*\n6% 401k match \\+ immediate vesting\n* *bonus program:*\nsemi\\-annual bonus opportunity\n* *mentorship:*\ncareer mentorship programs\n\n### **why is mtsi a great place to work**\n\n* *interesting work:*\nour co\\-workers support some of the most important and critical programs to our national defense and security.\n* *values:*\nour first core value is that employees come first. we challenge our co\\-workers to provide the highest level of support and service, and reward them with some of the best benefits in the industry.\n* *100% employee owned:*\nwe have a stake in each other's success, and the success of our customers. it's also nice to know what's going on across the company; we have company wide town\\-hall meetings three times a year.\n* *great benefits \\- most full\\-time staff are eligible for:*\n* + starting pto accrual of 20 days pto/year \\+ 10 holidays/year\n\t+ flexible schedules\n\t+ 6% 401k match with immediate vesting up to $9k annually\n\t+ semi\\-annual bonus eligibility (july and decemeber)\n\t+ company funded employee stock ownership plan (esop) \\- a separate qualified retirement account\n\t+ up to $10,000 in annual educational reimbursement\n\t+ other company funded benefits, like life and disability insurance\n\t+ optional zero deductible blue cross/blue shield health insurance plan\n* *track record of success:*\nwe have grown every year since our founding in 1993\\.\n\nmodern technology solutions, inc. (mtsi) is a 100% employee\\-owned engineering services and solutions company that provides high\\-demand technical expertise in digital transformation, modeling and simulation, rapid capability development, test and evaluation, artificial intelligence, autonomy, cybersecurity and mission assurance\n\n\nmtsi delivers capabilities to solve problems of global importance. founded in 1993, mtsi today has employees at over 20 offices and field sites worldwide.\n\n\nfor more information about mtsi, please visitwww.mtsi\\-va.com\n\n#### **eeo statement**\n\n\nmtsi embraces nine core values including our first core value of employees come first. consistent with our core values, we are committed to equal opportunity, making decisions without regard to race, color, religion, sex, national origin, age, military/veteran status, disability, or any other characteristics protected by applicable law. mtsi is committed to equal employment opportunity and providing reasonable accommodations to applicants and employees with physical and/or mental disabilities.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Associate Data Engineer",
        "company": "CVS Health",
        "location": "TX, US USA",
        "posted_at": "2026-01-06",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "BigQuery",
            "Dataflow",
            "Apache Airflow",
            "BigQuery",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9291f3f30c723313",
        "description": "we\u2019re building a world of health around every individual \u2014 shaping a more connected, convenient and compassionate health experience. at cvs health\u00ae, you\u2019ll be surrounded by passionate colleagues who care deeply, innovate with purpose, hold ourselves accountable and prioritize safety and quality in everything we do. join us and be part of something bigger \u2013 helping to simplify health care one person, one family and one community at a time.\n\n**position summary**  \n\ncvs health is looking for a hands\\-on, passionate, and driven data engineer with sql, python, and cloud (preferably gcp) experience who wants to join forces with a high impact team making a difference in customers\u2019 lives by reinventing how they connect with the most qualified healthcare providers in their area.  \n\n  \n\nas part of analytics \\& behavior change, you will be embedded in a data engineering team. you will support other analytic and data science partners who are designing and developing applications/pipelines for cvs health across various business units. you will brainstorm with product owners, consultants, data scientists and fellow engineers to build products used to improve the health outcomes of millions of people.  \n\n**required qualifications**\n\n\n1\\+ years' professional experience building data transformation and processing solutions using sql, python or similar programming language  \n\n* 1\\+ years' leveraging multiple tools and programming languages to analyze and manipulate data sets from disparate data sources\n* 1\\+ years' experience on teradata tool set, unix, sql and etl\n* 1\\+ years' programming using python, sql, and/or shell scripting\n\n  \n\n**preferred qualifications**\nexperience with google cloud platform (gcp) using tools like bigquery, cloud composer, dataproc and dataflow.  \n\n* experience building large\\-scale applications and high\\-volume data pipelines\n* working knowledge of orchestration tools, such as apache airflow\n* ability to understand complex systems and solve challenging analytical problems.\n* excellent problem\\-solving skills and critical thinking ability\n* strong collaboration and communication skills within and across teams\n **education**\n\n\nbachelor's degree in computer science, information systems, data engineering, machine learning, or related discipline\n\n**anticipated weekly hours**\n\n\n40**time type**\n\n\nfull time**pay range**\n\n\nthe typical pay range for this role is:\n\n\n$61,800\\.00 \\- $123,600\\.00\nthis pay range represents the base hourly rate or base annual full\\-time salary for all positions in the job grade within which this position falls. the actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. this position is eligible for a cvs health bonus, commission or short\\-term incentive program in addition to the base pay range listed above.\n\n  \n\nour people fuel our future. our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\n**great benefits for great people**\n\n\nwe take pride in our comprehensive and competitive mix of pay and benefits \u2013 investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. in addition to our competitive wages, our great benefits include:\n\n* **affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan**.\n* **no\\-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n* **benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\n\n\nfor more information, visit https://jobs.cvshealth.com/us/en/benefits\n\n\nwe anticipate the application window for this opening will close on: 02/27/2026\nqualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "Irving, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "Kafka",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ef5a7c704ef11e31",
        "description": "**role description** **job title: data engineer with iceberg**  \n\n**work location : irving, texas**  \n\n**job description**\n\n\n* senior data engineer to design and build scalable high performance data platforms for realtime and analytical workloads the role focuses on streaming lakehouse and federated query architectures using apache flink apache iceberg and starburst trino in a cloud environment\n\n\n**key responsibilities**\n\n\n* build and operate real time streaming pipelines using apache flink\n* design and manage lakehouse architectures with apache iceberg\n* enable federated analytics using starburst trino\n* develop and optimize etl pipelines using python scala java\n* integrate data platforms with aws azure gcp services\n* tune performance across pipelines tables and queries\n* ensure data quality governance monitoring and reliability\n* contribute to platform architecture and mentor junior engineers\n\n\n**required qualifications**\n\n\n* 7 years in data engineering and largescale data platforms\n* strong hands\\-on experience with flink iceberg and trino\n* proficiency in python scala or java and strong sql skills\n* experience with cloud data services and distributed systems\n* knowledge of kafka docker kubernetes and data warehousing concepts\n\n\n**preferred**\n\n\n* financial services experience\n* cloud certifications\n* exposure to delta lake hudi and airflow\n\n  \n\n**skills** **mandatory skills :** java, python, scala\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $83,912\\.00 to $128,080\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer",
        "company": "CNH Industrial",
        "location": "Oak Brook, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=0d56bb5a0c74a4a6",
        "description": "job description\nsenior software engineer\nrequisition id: 3744\njob location: oak brook \\- illinois \\- united states, sioux falls \\- south dakota \\- united states\njob family for posting: software engineering\njob type for job posting: full time* \n**about us**\n------------\n\ninnovation. sustainability. productivity. this is how we are breaking new ground in our mission to sustainably advance the noble work of farmers and builders everywhere. with a growing global population and increased demands on resources, our products are instrumental to feeding and sheltering the world.\n\n\nfrom developing products that run on alternative power to productivity\\-enhancing precision tech, we are delivering solutions that benefit people \u2013 and they are possible thanks to people like you. if the opportunity to build your skills as part of a collaborative, global team excites you, you\u2019re in the right place.  \n\n\n\n**grow a career. build a future!**\n\nbe part of this company at the forefront of agriculture and construction, that passionately innovates to drive customer efficiency and success. and we know innovation can\u2019t happen without collaboration. so, everything we do at cnh industrial is about reaching new heights as one team, always delivering for the good of our customers.  \n\n\n\nagdata services sits at the intersection of modern cloud engineering and next\\-generation agricultural technology.  \n\n  \n\n\n\nat cnh industrial, our \"connected\" machines don\u2019t just plant and harvest \u2014 they generate vast amounts of data in real time across millions of acres worldwide. that data travels over cellular networks into the azure cloud, where our team ingests, transforms, and serves it to power customer experiences, research, analytics, and precision agriculture solutions.  \n\n  \n\n\n\nagdata services is composed of two tightly aligned teams:\n\n\n* data ingestion \u2013 responsible for normalizing, validating, enriching, and structuring agronomic data from diverse formats and generations of equipment (including third\\-party platforms).\napi \\& infrastructure \u2013 responsible for exposing this data through low\\-latency, highly scalable, cloud\\-native apis used by front\\-end applications, internal engineering teams, and third party partners.  \n* \n\nwe operate at significant scale using cutting\\-edge technology:\n\n\n* millions of api requests per day\n* cloud\\-native systems running in microsoft azure\n* modern .net (including .net 8 and newer)\n* containerized workloads with docker and kubernetes\n* ci/cd pipelines and code management in gitlab\nobservability and monitoring through tools like azure app insights and datadog  \n* \n\nwe follow agile development practices with short feedback loops, iterative delivery, and a strong culture of ownership. engineers participate in backlog refinement, sprint planning, architectural discussions, and retrospectives \u2014 with real influence over technical direction and prioritization.  \n\n\n\nwe intentionally leverage ai\\-powered tools and automation to enhance engineering productivity, improve code quality, strengthen observability, and accelerate delivery. we continuously evaluate how ai can enhance both our development workflows and the intelligent capabilities of the platforms we build.  \n\n\n\nwe own our architecture end\\-to\\-end \u2014 from design to deployment to production operations. this is a team for engineers who want real technical ownership, real scale, and real\\-world impact.\n\n\n**job purpose**\n---------------\n\nas a **senior software engineer** on the api \\& infrastructure team, you will design and build distributed, cloud\\-native systems that power precision agriculture at global scale.  \n\n  \n\n\n\nthis role is not just limited to implementing tickets. you will:\n\n\n* help to architect highly available, resilient apis serving millions of requests daily\n* design scalable data access patterns across relational, nosql, and object storage systems\n* drive technical direction for cloud\\-native application development in azure\n* contribute to and help evolve our agile engineering practices\n* own services from concept through deployment and production operations\n* mentor engineers while raising the technical bar across the team\nprovide recommendations and ai\\-assisted workflows on tooling to increase productivity and delivery speed  \n* \n\nyou will play a key role in shaping the backbone that connects agricultural machines in the field to customer\\-facing applications and advanced analytics platforms in the cloud.  \n\n\n\n**key responsibilities**\n------------------------\n\n**architecture \\& system design**\n\n* design and implement scalable, highly available restful apis using c\\# and modern .net\n* define and evolve service architectures supporting high throughput and low latency\n* make informed decisions around distributed systems tradeoffs (consistency, availability, scalability)\ncollaborate with internal stakeholders to define reliable data contracts and service boundaries  \n* \n\n**cloud \\& infrastructure ownership**\n\n* build and maintain cloud\\-native services in microsoft azure\n* containerize applications using docker\n* design and maintain ci/cd pipelines in gitlab\nimplement infrastructure patterns that support resiliency, fault tolerance, horizontal scalability, and cost efficiency  \n* \n\n**performance, reliability \\& observability**\n\n* optimize apis for performance under heavy load\u2014 including the seasonal spikes that come when millions of acres of equipment hit the field simultaneously during planting and harvest\n* implement structured logging, tracing, and monitoring using tools such as datadog\n* participate in production support and incident response\ncontinuously improve system reliability and operational maturity  \n* \n\n**engineering excellence**\n\n* write clean, maintainable, well\\-tested code\n* conduct thoughtful code reviews and mentor junior engineers\n* lead or influence technical initiatives across teams\n* contribute to architectural standards and long\\-term technical strategy\n* champion automation, ai\\-assisted development practices, and continuous improvement\n**experience required**\n-----------------------\n\n* experienced engineer with 5\\+ years of career experience, with bachelor\u2019s degree. 3\\+ years experience with master\u2019s degree.\n* or equivalent combination of education and experience.\n* demonstrated leadership abilities.\n* capable of defining appropriate approaches and solutions.\n* furthers knowledge of profession through continued education and/or seeking or providing mentorship.\n**preferred qualifications**\n----------------------------\n\n* strong experience with c\\#, asp.net core, and modern .net\n* experience building cloud\\-native applications (preferably in microsoft azure or aws)\n* experience with git\\-based version control systems (gitlab, azure devops, or similar)\n* experience with relational databases (preferably sql server) and data modeling\n* experience designing and consuming restful apis\n* experience with ci/cd pipelines and devops practices\n* experience with unit testing frameworks (nunit, xunit, mstest, or similar)\n* experience working in an agile team environment\n* familiarity with agricultural technology, telemetry systems, or data\\-intensive platforms is a plus\n**pay transparency**\n--------------------\n\nthe annual salary for this role is $105,750 \\- $141,000 plus any applicable bonus *(actual salaries will vary and will be based on various factors, such as skill, experience and qualification for the role.)*\n\n**what we offer**\n-----------------\n\nat cnh, our people are at the heart of everything we do. that\u2019s why we offer a comprehensive benefits program designed to support your health, well\\-being, and long\\-term success. from competitive compensation to flexible work arrangements and opportunities for continuous development, our benefits reflect our commitment to creating an environment where employees feel supported and empowered\u2014both personally and professionally. we believe that when you\u2019re given the tools to thrive, you can drive meaningful impact. at cnh, you\u2019ll not only find the resources to succeed today\u2014you\u2019ll find the foundation to grow a career and build a future.\n\n  \n\n\n\nus applicants: cnh industrial is an equal opportunity employer. this company considers candidates regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. applicants can learn more about their rights by viewing the federal \"know your rights\" poster here . cnh industrial participates in e\\-verify and will provide the federal government with your form i\\-9 information to confirm that you are authorized to work in the u.s. you can view additional information here .  \n\n  \n\n\n\ncanada applicants: cnh industrial is an equal opportunity employer. this company considers candidates regardless of race, color, religion, sex, sexual orientation, gender identity, nationality, place of origin, disability, marital status, family status, age, or any other ground prohibited by applicable provincial human rights legislation.  \n\n\n\nif you need reasonable accommodation with the application process, please contact us at narecruitingmailbox@cnhind.com .",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist",
        "company": "Partify Inc.",
        "location": "Warren, MI, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fac62c8a9b25af94",
        "description": "**data scientist**\n\n**employment type:** full time  \n**location:** on site in warren, mi  \n**reports to:** chief executive officer\n\n**role overview:**  \npartify inc. is a fast growing ecommerce automotive parts company focused on building scalable, data driven operations. we are seeking a data scientist to lead advanced analytics, forecasting, and statistical modeling initiatives across the organization. this is a highly analytical, hands\\-on role responsible for developing predictive models, building scalable data frameworks, and uncovering insights that directly influence executive decision making. you will work closely with leadership to identify key business drivers, optimize performance, and design models that support revenue growth, operational efficiency, and strategic planning.\n\n**key responsibilities:**  \ndevelop and deploy predictive models to support revenue forecasting, inventory planning, and operational optimization.  \napply advanced statistical techniques, including regression analysis and multivariate modeling, to identify performance drivers.  \ndesign and maintain scalable data models to support advanced analytics.  \nanalyze large datasets to uncover trends, correlations, and growth opportunities.  \nbuild automated reporting and forecasting workflows.  \npartner with executive leadership to define kpis and measurable performance indicators.  \ntranslate complex statistical findings into actionable business strategies.  \nestablish data validation and governance standards to ensure model accuracy and reliability.  \nsupport additional analytics initiatives as needed.\n\n**required qualifications:**  \nminimum of five years of experience in data science, advanced analytics, or statistical modeling roles.  \nstrong proficiency in sql and working with relational databases.  \nadvanced microsoft excel skills, including financial modeling and power query.  \nstrong foundation in statistics (regression, correlation, hypothesis testing, multivariate analysis).  \nexperience building forecasting models in a production business environment.  \nability to translate complex analytical outputs into clear executive level insights.  \nexperience working with large, structured datasets.  \nstrong problem solving skills and attention to detail.  \nability to manage multiple high impact projects in a fast paced environment.\n\n**preferred qualifications:**  \nproficiency in python or similar analytical programming languages.  \nexperience with machine learning techniques and predictive modeling.  \nexperience working in ecommerce, supply chain, or operational analytics.  \nfamiliarity with data visualization tools such as power bi.  \nexperience building scalable analytics frameworks in growing organizations.\n\n**why join partify:**  \nat partify, data is central to how we scale and compete. this role offers the opportunity to directly influence company strategy, build advanced modeling systems, and work alongside executive leadership to drive measurable growth.\n\n**we offer:**  \ncompetitive salary.  \nhmo medical plans.  \ndental and vision coverage.  \n401(k) with company match.  \npaid holidays and sick time.  \nemployee discounts.\n\n**please note:** partify inc. is unable to sponsor employment visas at this time.\n\npay: from $1\\.00 per hour\n\nbenefits:\n\n* 401(k) matching\n* dental insurance\n* health insurance\n* vision insurance\n\nability to commute:\n\n* warren, mi 48088 (required)\n\nwork location: in person",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Software Engineer",
        "company": "Disney Experiences",
        "location": "Orlando, FL, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Terraform",
            "Git",
            "NoSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ad9da3683bfea092",
        "description": "**sr software engineer**\n\n\nat disney experiences technology, our team creates world\\-class immersive digital experiences for the company\u2019s premier vacation brands including disney parks \\& resorts worldwide, disney cruise line, aulani, a disney resort \\& spa, and disney vacation club.  \n\n\n\n  \n\nwe are responsible for the end\\-to\\-end guest and cast experience for technology initiatives across the attractions \\& entertainment, food \\& beverage, resorts \\& transportation, photopass, merchandise and operations workforce management lines of business.\n\n\n**role \\& team:**\n\n\nthe senior software engineer will be responsible for development and engineering for large, custom off\\-the\\-shelf application used for various functions in our disney experience\u2019s workforce management portfolio. this role will conceive, design, develop, test, and implement software fixes, enhancements, components, and/or new software systems and applications of moderate to high complexity. they will own design and development, and drive development of components through their own and subordinate engineers\u2019 work. the senior engineer will provide technical guidance and act as a point of escalation and technical expert. they will design and develop highly scalable software systems and applications.\n\n\nthe successful candidate requires technical knowledge and skills that are broad and deep, covering various hardware, software, and platforms. the candidate must be comfortable operating in complex heterogeneous technology environments and have experience with highly integrated and mission critical solutions. they must possess the ability to share and communicate ideas clearly, both orally and in writing, to business sponsors and partners, technical resources, and executives, in clear concise language that is the effective for each respective group.\n\n**what you will do:**\n\n* owns the design and development of software fixes, enhancements, components, and/or new software systems and applications\n* drives development of components through own and subordinate engineers' work.\n* develops technical solutions that meet specifications and that impact future developments.\n* executes assigned component level software development projects and major fixes using new or existing technologies.\n* develops specifications for assigned components, projects or fixes.\n* reviews or writes code.\n* leads programming, testing and debugging of applications or fixes to existing applications.\n* creates protocols, documentation and tools for installation and maintenance.\n* participates in setting the architectural direction for software development projects.\n* designs specific components for assigned projects, developing specifications for each.\n* designs, develops, manages, creates and maintains technical components and templates.\n* able to code against front\\-end technology stack and lead end\\-to\\-end troubleshooting.\n* interacts and coordinates deliverables with other technical groups in the organization.\n* executes assigned component level projects using new or existing technologies\n* designs and develops specifications for assigned projects\n* reviews or troubleshoots and performs testing.\n* participates in conceiving and setting architectural direction for development projects.\n* designs the component tasks of assigned projects, developing specifications for each\n* serves as a high\\-level technical resource and \u201cgo\\-to\u201d person for less experienced developers, providing technical guidance and oversight.\n* leads team members in problem analysis and issue resolution.\n* recommend improvements to processes, technology, and interfaces that improve the effectiveness of the team.\n\n**required qualifications** **\\& skills:**\n\n* 5\\+ years of experience in software engineering using agile methodology\n* expertise in java/jee/.net, restful/soap services, micro services, angular, nodejs, aws cloud, docker, relational and nosql databases\n* strong collaboration and consensus building skills\n* strong time management and prioritization skills\n* proven ability to learn and apply new technologies\n* strong troubleshooting skills and ability to identify root cause without access to code\n\n**preferred qualifications** :\n\n* experience in a development role for a large custom off the shelf application\n* experience with workforce management systems\n* proven experience in big data technologies, machine learning, bot technologies, and data integration between systems and analytics tools\n* database design experience including both relational and nosql such as dynamodb\n* architecting cloud\\-based solutions using managed and serverless technologies as appropriate such as docker, aws ecs, google gke, google app engine, aws lambda, google cloud functions, aws dynamodb, or google firestore\n* experience managing automated deployments using tools such as terraform, cloudformation, serverless framework, or ansible\n* experience with qa and software analytic tools\n\n**required education:**\n\n* bachelor's degree in computer science, engineering, or related field\n\n\n\\#disneytech",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Software Engineer - ML Platform",
        "company": "Latitude AI",
        "location": "Pittsburgh, PA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "PyTorch",
            "Kubernetes",
            "Terraform",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4388f870668aad41",
        "description": "latitude ai (lat.ai) develops automated driving technologies, including l3, for ford vehicles at scale. we're driven by the opportunity to reimagine what it's like to drive and make travel safer, less stressful, and more enjoyable for everyone.\n\n\n\nwhen you join the latitude team, you'll work alongside leading experts across machine learning and robotics, cloud platforms, mapping, sensors and compute systems, test operations, systems and safety engineering \u2013 ***all dedicated to making a real, positive impact on the driving experience for millions of people.***\n\n\n\nas a ford motor company subsidiary, we operate independently to develop automated driving technology at the speed of a technology startup. latitude is headquartered in pittsburgh with engineering centers in dearborn, mich., and palo alto, calif.\n\n\n**meet the team:**\n\n\n\nthe machine learning platform team is the engine driving all ml initiatives at latitude. we provide the essential services for every ml pipeline, training run, and critical workflow automation across multiple internal teams.\n\n\n\nour flagship product is **bluemind**, our in\\-house platform for orchestrating, executing, and rigorously tracking all ml experiments. every piece of ml training at latitude runs on bluemind, which seamlessly integrates with our high\\-performance computing cluster, dataset versioning tools, and logging/artifact frameworks.\n\n\n\nbeyond ml training, we maintain platforms that orchestrate vital business processes, including simulation execution, metrics calculation, bench test execution, map and labeled data production, vehicle/simulation event triage.\n\n\n\nour platforms utilize a powerful combination of cutting\\-edge off\\-the\\-shelf and robust in\\-house web applications and libraries, all integrated with cloud resources.\n\n\n\nwe are looking for experienced candidates who thrive in a fast\\-paced, collaborative, and distributed environment. if you are a senior engineer who is ready to transform customer needs and innovative ideas into impactful new platform features, we want to hear from you.\n\n\n**what you'll do:**\n\n\n* **drive innovation on the bluemind ml platform:** contribute to the development and enhancement of the in\\-house bluemind platform, a critical tool for ml experiment tracking and execution. this platform supports the construction and execution of complex ml training and inference workflows on an on\\-premise hpc cluster, integrating with other internal and off\\-the\\-shelf services for dataset versioning, data lineage, artifact tracking, checkpointing, logging, and metrics tracking\n* **serve as a trusted partner to ml teams:** regularly engage with and understand the unique needs of diverse ml teams across latitude (including scalable machine learning, sensing, autonomy behavior, state estimation, intelligent systems evaluation, bench testing, and virtual test ecosystem). you will be instrumental in capturing requirements, authoring robust design documents, implementing high\\-quality solutions, validating results, and providing critical support for production services\n* **tackle complex, high\\-impact challenges:** apply your experience and creativity to solve large\\-scale, complex technical problems in a dynamic, fast\\-paced environment, with a direct focus on accelerating latitude's product launch goals and core mission\n* **develop and own full\\-stack features:** design, implement, and enhance a wide range of full\\-stack capabilities, spanning web apis/services, react applications and component libraries, custom and generated client libraries, command\\-line interfaces, kubernetes resources, cloud architecture, and crucial integration solutions for our on\\-premise hpc cluster\n* **collaborate across engineering boundaries:** work closely with essential supporting teams, including internal cloud platform, product, and site reliability teams, as well as external vendors, to ensure the smooth build, deployment, monitoring, and support of our foundational ml platforms\n\n\n**what you'll need to succeed:**\n\n\n* bachelor's degree in computer engineering, computer science, electrical engineering, robotics or a related field and 4\\+ years of relevant experience (or master's degree and 2\\+ years of relevant experience, or phd)\n* 4\\+ years of full\\-stack software engineering experience\n* strong communication, problem solving, task prioritization, and teamwork skills\n* solid architecture and design skills\n* experience coding with python and familiarity with popular python tools and frameworks such as poetry and fast api\n* some knowledge or pytorch, ddp, ray and overall distributed machine learning\n* some knowledge of distributed file systems\n* experience with sql databases\n* experience with cloud infrastructure (e.g.: aws, gcp, terraform, kubernetes)\n* experience with high performance computing environments\n\n\n**nice to have:**\n\n\n* experience with workflow automation frameworks (e.g. airflow, dagster, camunda)\n* experience with java and/or other jvm\\-based languages\n* experience with typescript and/or javascript, node.js, and frameworks (e.g.: react, redux, mui)\n* experience with bazel, gradle, and/or other build tools\n\n\n**what we offer you:**\n\n\n* competitive compensation packages\n* high\\-quality individual and family medical, dental, and vision insurance\n* health savings account with available employer match\n* employer\\-matched 401(k) retirement plan with immediate vesting\n* employer\\-paid group term life insurance and the option to elect voluntary life insurance\n* paid parental leave\n* paid medical leave\n* unlimited vacation\n* 15 paid holidays\n* daily lunches, snacks, and beverages available in all office locations\n* pre\\-tax spending accounts for healthcare and dependent care expenses\n* pre\\-tax commuter benefits\n* monthly wellness stipend\n* adoption/surrogacy support program\n* backup child and elder care program\n* professional development reimbursement\n* employee assistance program\n* discounted programs that include legal services, identity theft protection, pet insurance, and more\n* company and team bonding outlets: employee resource groups, quarterly team activity stipend, and wellness initiatives\n\n\nlearn more about latitude's team, mission and career opportunities at lat.ai!\n\n\n\nthe expected base salary range for this full\\-time position in california is $179,200 \\- $268,800 usd. actual starting pay will be based on job\\-related factors, including exact work location, experience, relevant training and education, and skill level. latitude employees are also eligible to participate in latitude's annual bonus programs, equity compensation, and generous company benefits program, subject to eligibility requirements.\n\n\n\ncandidates for positions with latitude ai must be legally authorized to work in the united states on a permanent basis. verification of employment eligibility will be required at the time of hire. visa sponsorship is available for this position.\n\n\n\nwe are an equal opportunity employer committed to a culturally diverse workforce. all qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Perception Engineer - Data",
        "company": "Forterra",
        "location": "Arlington, VA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e4d12d00e8a3f926",
        "description": "**about forterra**  \n\n  \n\nforterra is a leading provider of autonomous systems for ground\\-based movement in the working world. amongst some of the earliest innovators in the field of driverless technology, forterra is focused on building systems that protect front\\-line soldiers and enable civilian workers in our industrial base. forterra is the go\\-to provider of ground autonomy solutions for the u.s. department of defense, which harnesses the technology for asymmetric warfare in critical conditions.\n\n **about forterra**  \n\n  \n\nforterra is a leading provider of autonomous systems for ground\\-based movement in the working world. amongst some of the earliest innovators in the field of driverless technology, forterra is focused on building systems that protect front\\-line soldiers and enable civilian workers in our industrial base. forterra is the go\\-to provider of ground autonomy solutions for the u.s. department of defense, which harnesses the technology for asymmetric warfare in critical conditions.\n\n **about the role**\n\n  \n\nforterra is seeking an experienced perception data engineer to join our team in supporting the development of state\\-of\\-the\\-art deep learning algorithms for on\\-road and off\\-road applications. the ideal candidate will have a strong background in managing large quantities of visual data from a variety of sensor types, supporting the tools for developing deep learning algorithms for online real\\-time perception and offline data generation in multiple sensor domains, and running on hardware operating in varied environments under real\\-time constraints.\n\n **what you'll do**\n\n* manage and maintain the perception data processing database\n* design, deploy, and operate metaflow pipelines\n* enhance and support the deep learning training pipeline dashboard\n* own the perception data processing unit test architecture and test asset management\n* develop and validate data upload/download tools for third party data annotation\n* support aws infrastructure and platform reliability\\- build, optimize, and distribute production model debians\n\n **what we're looking for:**\n\n *mission\\-focused technical stewardship*\n\n\nwe\u2019re looking for someone who understands the criticality of reliable data infrastructure, reproducible pipelines, and high\\-integrity tooling within autonomy and perception systems. you recognize that our models, datasets, and platforms must be trustworthy, maintainable, and consistently improving to support downstream mission needs.\n\n *deep ownership of complex systems*\n\n\nyou have a demonstrated ability to take full responsibility for multifaceted environments, from aws resources to kubernetes clusters, metaflow pipelines, dashboards, and ci/cd systems. you proactively identify problems, trace them across layers of the stack, and implement durable fixes rather than temporary workarounds.\n\n *architect\u2019s mindset for data and ml workflows*\n\n\nyou think holistically about data ingestion, annotation, anonymization, model optimization, and deployment pipelines. you understand that data quality, reproducibility, and model lineage are foundational, and design processes that uphold these principles across environments.\n\n *collaborative partner \\& cross\\-functional influencer*\n\n\nyou work effectively with engineering, infrastructure, mlops, it, and security partners, navigating govcloud constraints, modifying setups, coordinating dashboard access, or working with upstream owners of tooling. you can guide others through intricate workflows, mentor teammates, and build trust across organizations.\n\n **minimum qualifications:**\n\n* bs in machine learning, computer vision, or robotics, or with equivalent industrial experience.\n* 4\\+ years of similar academic and/or professional working experience.\n* excellent understanding of standard deep learning algorithms for perception.\n* solid understanding of machine learning training/deployment pipelines and their implementation.\n* experience with pytorch or tensorflow, ros, docker, python\n* excellent core software engineering skills: software design, containerization, unit testing, and debugging.\n* experience with delivering production\\-quality software in a continuously integrated environment using test\\-driven development patterns.\n\n **preferred qualifications:**\n\n* experience with deploying deep learning algorithms into latency\\-constrained on\\-road and off\\-road environments, ideally on nvidia jetson devices\n* excellent programming skills in c\\+\\+\n* experience with cloud infrastructure and aws in particular\n\n  \n\nus salary range  \n\n $150,000\u2014$175,000\n\n  \n\nthe salary range for this role is an estimate and is based on a wide variety of compensation factors. the salary offered to candidates will vary based on a variety of factors including (but not limited to) relevant work experience, education, specialized training, critical expertise, training, and more. equity in forterra is included in most of our full\\-time, high\\-demand roles and is therefore considered part of forterra\u2019s overall compensation package. in addition to base salary and equity, forterra offers competitive benefits for full\\-time employees including:\n\n* premium healthcare benefits: three plan options, including an hsa\\-eligible plan, with forterra covering 80% of the plan premium for you and your dependents.\n* basic life/ad\\&d, short and long\\-term disability insurance plans 100% covered by forterra, plus the option to purchase additional life insurance for you and your dependents.\n* extremely generous company holiday calendar including a winter break in december.\n* competitive paid time off (pto) offering 20 days accrued per year.\n* a minimum of 7 weeks fully paid parental leave for birth/adoption.\n* a $9k annual tuition reimbursement or professional development stipend.\n* fully stocked beverage refrigerators with all the celsius your little heart desires.\n* 401(k) retirement savings plan, including traditional, roth 401(k), and after\\-tax deferral with company match up to 4%.\n\n\nyour recruiter will be able to share more information about our salary and benefits offering during the hiring process.\n\n  \n\nforterra is an equal\\-opportunity employer, providing and promoting equal employment opportunity in accordance with local, state, and federal laws. forterrans are unique, talented individuals who are united through a shared passion to deliver autonomous systems that enable national resilience and a robust supply chain. all qualified applications will receive equal consideration for employment.\n\n **the pay range for this role is:**  \n\n150,000 \\- 175,000 usd per year(ava)",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Sr Software Engineer - Remote",
        "company": "Optum",
        "location": "Basking Ridge, NJ, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "Jenkins",
            "PySpark",
            "Hadoop",
            "Python",
            "SQL",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d1aa56bb760b5544",
        "description": "optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. the work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. come make an impact on the communities we serve as you help us advance health optimization on a global scale. join us to start **caring. connecting. growing together.**\n\n  \n\nare you a talented software engineer looking for that next big plunge? do you want to develop software using cutting edge technology in a high\\-impact environment? do you want to work alongside a highly talented team of engineers to solve real\\-world problems? perfect, you are at the right place!\n\n  \n\napply with us, accelerate your career like never before!\n\n  \n\nthe roles in the software engineering job function will cover all primary development activity across all technology functions that ensure we deliver code with high quality for our applications, products and services and to understand customer needs. these roles include, but are not limited to analysis, design, coding, engineering, testing, debugging, standards, methods, tools analysis, documentation, research and development, maintenance, new development, operations and delivery.\n\n  \n\nour mission depends on hiring the best and the brightest. we need high performers and risk\\-takers with curious minds who have compassion for the world around them. from the role standpoint, we are looking at candidates passionate about technology and problem solving, foresighted towards future capabilities with the ability to apply cutting edge technology in new and innovative ways.\n\n  \n\nyou'll enjoy the flexibility to work remotely \\* from anywhere within the u.s. as you take on some tough challenges.\n\n **primary responsibilities:**\n\n* collaborate with team, architects, and product stakeholders to understand the scope and design of a deliverable\n* participate in product support activities as needed by the team\n* understand product architecture, features being built and come up with product improvement ideas and pocs\n* individual contributor for data engineering \\- data pipelines, data modelling and data warehouse\n* designing end to end architecture using azure\n* data integration\n* building artifacts\n\n  \n\nyou'll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n\n**required qualifications:**\n\n* 5\\+ years of experience with azure data processing including azure data factory and azure data bricks\n* 5\\+ years of experience building data pipelines using adf\n* 5\\+ years of experience with sql and complex queries\n* 5\\+ years of experience with programming languages such as python, pyspark\n* 3\\+ years of ai experience\n\n **preferred qualifications:**\n\n* knowledge/experience with containerization \\- docker, kubernetes\n* knowledge/experience with bigdata/hadoop ecosystem \\- spark, hive, hbase, sqoop etc.\n* knowledge/experience using microsoft visio, power point\n* knowledge of agile/scrum\n* proven ability to learn and adapt to new data technologies\n* proven ability to build / deployment automation \\- jenkins\n* proven good problem\\-solving skills\n* proven good communication skills\n* all employees working remotely will be required to adhere to unitedhealth group's telecommuter policy.\n\n  \n\npay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. in addition to your salary, we offer benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). no matter where or when you begin a career with us, you'll find a far\\-reaching choice of benefits and incentives. the salary for this role will range from $91,700 to $163,700 annually based on full\\-time employment. we comply with all minimum wage laws as applicable.\n\n **application deadline:** this will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. job posting may come down early due to volume of applicants.\n\n *at unitedhealth group, our mission is to help people live healthier lives and make the health system work better for everyone. we believe everyone\\-of every race, gender, sexuality, age, location and income\\-deserves the opportunity to live their healthiest life. today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. we are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes \\- an enterprise priority reflected in our mission.*\n\n *unitedhealth group is an equal employment opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.*\n\n *unitedhealth group is a drug\\-free workplace. candidates are required to pass a drug test before beginning employment.*",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer I, Observability",
        "company": "DigitalOcean",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=16af568f8829f63e",
        "description": "dive in and do the best work of your career at digitalocean. journey alongside a strong community of top talent who are relentless in their drive to build the simplest scalable cloud. if you have a growth mindset, naturally like to think big and bold, and are energized by the fast\\-paced environment of a true industry disruptor, you'll find your place here. we value winning together\u2014while learning, having fun, and making a profound difference for the dreamers and builders in the world.\n\n**we want people who care about empowering their peers to understand their services.**\n--------------------------------------------------------------------------------------\n\n\n\nobservability platforms is focused on creating visibility into digitalocean's services and infrastructure. we design, build, and operate the internal logging, metrics, distributed tracing, error reporting, monitoring, and alerting platforms that are depended on to ensure good, reliable experiences for digitalocean's customers. through a mix of open source services and in\\-house developed software we seek to continuously improve the ability of digitalocean engineers and product managers to understand the behavior of their products and services in order to improve the experiences of our customers.\n\n\n**what you'll be doing:**\n-------------------------\n\n\n* integrating and operating open source distributed data systems such as opensearch, victoriametrics, kafka, and clickhouse\n* implementing features to both improve the operability of our services and help design and implement the next generations of metrics and logging at digitalocean.\n* guiding other engineers on how they can best utilize our systems to gain confidence that their services are performing as expected.\n\n**what we'll expect from you:**\n-------------------------------\n\n\n* experience running and using distributed databases such as opensearch, victoriametrics, clickhouse\n* experience with kubernetes not just as an end user, but also an operator\n* experience writing software in golang. experience with the challenges of concurrency and distributed systems is a plus.\n* experience with devops tooling such as terraform, ansible, and github actions (or other relevant ci/cd experience).\n* an ability to work well as part of a geographically distributed team.\n* participation in a 24/7 on call rotation\n* someone who is motivated to learn, teach, grow, explore, and seek out areas we can improve\n* strong communication and customer service skills \\- we provide support to the internal users of the systems we design, build, and operate.\n\n**compensation range:**\n-----------------------\n\n\n* $139,000 \\- $174,000\n\n\n* this is a remote role\n\n\n\njr: 2026\\-7494\n\n\n*\\#li\\-remote*\n\n**why you'll like working for digitalocean**\n--------------------------------------------\n\n\n* **we innovate with purpose.** you'll be a part of a cutting\\-edge technology company with an upward trajectory, who are proud to simplify cloud and ai so builders can spend more time creating software that changes the world. as a member of the team, you will be a shark who thinks big, bold, and scrappy, like an owner with a bias for action and a powerful sense of responsibility for customers, products, employees, and decisions.\n* **we prioritize career development.** at do, you'll do the best work of your career. you will work with some of the smartest and most interesting people in the industry. we are a high\\-performance organization that will always challenge you to think big. our organizational development team will provide you with resources to ensure you keep growing. we provide employees with reimbursement for relevant conferences, training, and education. all employees have access to linkedin learning's 10,000\\+ courses to support their continued growth and development.\n* **we care about your well\\-being.** regardless of your location, we will provide you with a competitive array of benefits to support you from our employee assistance program to local employee meetups to flexible time off policy, to name a few. while the philosophy around our benefits is the same worldwide, specific benefits may vary based on local regulations and preferences.\n* **we reward our employees.** the salary range for this position is based on market data, relevant years of experience, and skills. you may qualify for a bonus in addition to base salary; bonus amounts are determined based on company and individual performance. we also provide equity compensation to eligible employees, including equity grants upon hire and the option to participate in our employee stock purchase program.\n* **digitalocean is an equal\\-opportunity employer.** we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\n\n**application limit:** you may apply to a maximum of 3 positions within any 180\\-day period. this policy promotes better role\\-candidate matching and encourages thoughtful applications where your qualifications align most strongly.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer I, Observability",
        "company": "DigitalOcean",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1dd9190af8f5e921",
        "description": "dive in and do the best work of your career at digitalocean. journey alongside a strong community of top talent who are relentless in their drive to build the simplest scalable cloud. if you have a growth mindset, naturally like to think big and bold, and are energized by the fast\\-paced environment of a true industry disruptor, you'll find your place here. we value winning together\u2014while learning, having fun, and making a profound difference for the dreamers and builders in the world.\n\n**we want people who care about empowering their peers to understand their services.**\n--------------------------------------------------------------------------------------\n\n\n\nobservability platforms is focused on creating visibility into digitalocean's services and infrastructure. we design, build, and operate the internal logging, metrics, distributed tracing, error reporting, monitoring, and alerting platforms that are depended on to ensure good, reliable experiences for digitalocean's customers. through a mix of open source services and in\\-house developed software we seek to continuously improve the ability of digitalocean engineers and product managers to understand the behavior of their products and services in order to improve the experiences of our customers.\n\n\n**what you'll be doing:**\n-------------------------\n\n\n* integrating and operating open source distributed data systems such as opensearch, victoriametrics, kafka, and clickhouse\n* implementing features to both improve the operability of our services and help design and implement the next generations of metrics and logging at digitalocean.\n* guiding other engineers on how they can best utilize our systems to gain confidence that their services are performing as expected.\n\n**what we'll expect from you:**\n-------------------------------\n\n\n* experience running and using distributed databases such as opensearch, victoriametrics, clickhouse\n* experience with kubernetes not just as an end user, but also an operator\n* experience writing software in golang. experience with the challenges of concurrency and distributed systems is a plus.\n* experience with devops tooling such as terraform, ansible, and github actions (or other relevant ci/cd experience).\n* an ability to work well as part of a geographically distributed team.\n* participation in a 24/7 on call rotation\n* someone who is motivated to learn, teach, grow, explore, and seek out areas we can improve\n* strong communication and customer service skills \\- we provide support to the internal users of the systems we design, build, and operate.\n\n**compensation range:**\n-----------------------\n\n\n* $139,000 \\- $174,000\n\n\n* this is a remote role\n\n\n\njr: 2026\\-7494\n\n\n*\\#li\\-remote*\n\n**why you'll like working for digitalocean**\n--------------------------------------------\n\n\n* **we innovate with purpose.** you'll be a part of a cutting\\-edge technology company with an upward trajectory, who are proud to simplify cloud and ai so builders can spend more time creating software that changes the world. as a member of the team, you will be a shark who thinks big, bold, and scrappy, like an owner with a bias for action and a powerful sense of responsibility for customers, products, employees, and decisions.\n* **we prioritize career development.** at do, you'll do the best work of your career. you will work with some of the smartest and most interesting people in the industry. we are a high\\-performance organization that will always challenge you to think big. our organizational development team will provide you with resources to ensure you keep growing. we provide employees with reimbursement for relevant conferences, training, and education. all employees have access to linkedin learning's 10,000\\+ courses to support their continued growth and development.\n* **we care about your well\\-being.** regardless of your location, we will provide you with a competitive array of benefits to support you from our employee assistance program to local employee meetups to flexible time off policy, to name a few. while the philosophy around our benefits is the same worldwide, specific benefits may vary based on local regulations and preferences.\n* **we reward our employees.** the salary range for this position is based on market data, relevant years of experience, and skills. you may qualify for a bonus in addition to base salary; bonus amounts are determined based on company and individual performance. we also provide equity compensation to eligible employees, including equity grants upon hire and the option to participate in our employee stock purchase program.\n* **digitalocean is an equal\\-opportunity employer.** we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\n\n**application limit:** you may apply to a maximum of 3 positions within any 180\\-day period. this policy promotes better role\\-candidate matching and encourages thoughtful applications where your qualifications align most strongly.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer I, Observability",
        "company": "DigitalOcean",
        "location": "Denver, CO, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=faf1fcd9ebafc0c9",
        "description": "dive in and do the best work of your career at digitalocean. journey alongside a strong community of top talent who are relentless in their drive to build the simplest scalable cloud. if you have a growth mindset, naturally like to think big and bold, and are energized by the fast\\-paced environment of a true industry disruptor, you'll find your place here. we value winning together\u2014while learning, having fun, and making a profound difference for the dreamers and builders in the world.\n\n**we want people who care about empowering their peers to understand their services.**\n--------------------------------------------------------------------------------------\n\n\n\nobservability platforms is focused on creating visibility into digitalocean's services and infrastructure. we design, build, and operate the internal logging, metrics, distributed tracing, error reporting, monitoring, and alerting platforms that are depended on to ensure good, reliable experiences for digitalocean's customers. through a mix of open source services and in\\-house developed software we seek to continuously improve the ability of digitalocean engineers and product managers to understand the behavior of their products and services in order to improve the experiences of our customers.\n\n\n**what you'll be doing:**\n-------------------------\n\n\n* integrating and operating open source distributed data systems such as opensearch, victoriametrics, kafka, and clickhouse\n* implementing features to both improve the operability of our services and help design and implement the next generations of metrics and logging at digitalocean.\n* guiding other engineers on how they can best utilize our systems to gain confidence that their services are performing as expected.\n\n**what we'll expect from you:**\n-------------------------------\n\n\n* experience running and using distributed databases such as opensearch, victoriametrics, clickhouse\n* experience with kubernetes not just as an end user, but also an operator\n* experience writing software in golang. experience with the challenges of concurrency and distributed systems is a plus.\n* experience with devops tooling such as terraform, ansible, and github actions (or other relevant ci/cd experience).\n* an ability to work well as part of a geographically distributed team.\n* participation in a 24/7 on call rotation\n* someone who is motivated to learn, teach, grow, explore, and seek out areas we can improve\n* strong communication and customer service skills \\- we provide support to the internal users of the systems we design, build, and operate.\n\n**compensation range:**\n-----------------------\n\n\n* $139,000 \\- $174,000\n\n\n* this is a remote role\n\n\n\njr: 2026\\-7494\n\n\n*\\#li\\-remote*\n\n**why you'll like working for digitalocean**\n--------------------------------------------\n\n\n* **we innovate with purpose.** you'll be a part of a cutting\\-edge technology company with an upward trajectory, who are proud to simplify cloud and ai so builders can spend more time creating software that changes the world. as a member of the team, you will be a shark who thinks big, bold, and scrappy, like an owner with a bias for action and a powerful sense of responsibility for customers, products, employees, and decisions.\n* **we prioritize career development.** at do, you'll do the best work of your career. you will work with some of the smartest and most interesting people in the industry. we are a high\\-performance organization that will always challenge you to think big. our organizational development team will provide you with resources to ensure you keep growing. we provide employees with reimbursement for relevant conferences, training, and education. all employees have access to linkedin learning's 10,000\\+ courses to support their continued growth and development.\n* **we care about your well\\-being.** regardless of your location, we will provide you with a competitive array of benefits to support you from our employee assistance program to local employee meetups to flexible time off policy, to name a few. while the philosophy around our benefits is the same worldwide, specific benefits may vary based on local regulations and preferences.\n* **we reward our employees.** the salary range for this position is based on market data, relevant years of experience, and skills. you may qualify for a bonus in addition to base salary; bonus amounts are determined based on company and individual performance. we also provide equity compensation to eligible employees, including equity grants upon hire and the option to participate in our employee stock purchase program.\n* **digitalocean is an equal\\-opportunity employer.** we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\n\n**application limit:** you may apply to a maximum of 3 positions within any 180\\-day period. this policy promotes better role\\-candidate matching and encourages thoughtful applications where your qualifications align most strongly.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer I, Observability",
        "company": "DigitalOcean",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ec3420072b142500",
        "description": "dive in and do the best work of your career at digitalocean. journey alongside a strong community of top talent who are relentless in their drive to build the simplest scalable cloud. if you have a growth mindset, naturally like to think big and bold, and are energized by the fast\\-paced environment of a true industry disruptor, you'll find your place here. we value winning together\u2014while learning, having fun, and making a profound difference for the dreamers and builders in the world.\n\n**we want people who care about empowering their peers to understand their services.**\n--------------------------------------------------------------------------------------\n\n\n\nobservability platforms is focused on creating visibility into digitalocean's services and infrastructure. we design, build, and operate the internal logging, metrics, distributed tracing, error reporting, monitoring, and alerting platforms that are depended on to ensure good, reliable experiences for digitalocean's customers. through a mix of open source services and in\\-house developed software we seek to continuously improve the ability of digitalocean engineers and product managers to understand the behavior of their products and services in order to improve the experiences of our customers.\n\n\n**what you'll be doing:**\n-------------------------\n\n\n* integrating and operating open source distributed data systems such as opensearch, victoriametrics, kafka, and clickhouse\n* implementing features to both improve the operability of our services and help design and implement the next generations of metrics and logging at digitalocean.\n* guiding other engineers on how they can best utilize our systems to gain confidence that their services are performing as expected.\n\n**what we'll expect from you:**\n-------------------------------\n\n\n* experience running and using distributed databases such as opensearch, victoriametrics, clickhouse\n* experience with kubernetes not just as an end user, but also an operator\n* experience writing software in golang. experience with the challenges of concurrency and distributed systems is a plus.\n* experience with devops tooling such as terraform, ansible, and github actions (or other relevant ci/cd experience).\n* an ability to work well as part of a geographically distributed team.\n* participation in a 24/7 on call rotation\n* someone who is motivated to learn, teach, grow, explore, and seek out areas we can improve\n* strong communication and customer service skills \\- we provide support to the internal users of the systems we design, build, and operate.\n\n**compensation range:**\n-----------------------\n\n\n* $139,000 \\- $174,000\n\n\n* this is a remote role\n\n\n\njr: 2026\\-7494\n\n\n*\\#li\\-remote*\n\n**why you'll like working for digitalocean**\n--------------------------------------------\n\n\n* **we innovate with purpose.** you'll be a part of a cutting\\-edge technology company with an upward trajectory, who are proud to simplify cloud and ai so builders can spend more time creating software that changes the world. as a member of the team, you will be a shark who thinks big, bold, and scrappy, like an owner with a bias for action and a powerful sense of responsibility for customers, products, employees, and decisions.\n* **we prioritize career development.** at do, you'll do the best work of your career. you will work with some of the smartest and most interesting people in the industry. we are a high\\-performance organization that will always challenge you to think big. our organizational development team will provide you with resources to ensure you keep growing. we provide employees with reimbursement for relevant conferences, training, and education. all employees have access to linkedin learning's 10,000\\+ courses to support their continued growth and development.\n* **we care about your well\\-being.** regardless of your location, we will provide you with a competitive array of benefits to support you from our employee assistance program to local employee meetups to flexible time off policy, to name a few. while the philosophy around our benefits is the same worldwide, specific benefits may vary based on local regulations and preferences.\n* **we reward our employees.** the salary range for this position is based on market data, relevant years of experience, and skills. you may qualify for a bonus in addition to base salary; bonus amounts are determined based on company and individual performance. we also provide equity compensation to eligible employees, including equity grants upon hire and the option to participate in our employee stock purchase program.\n* **digitalocean is an equal\\-opportunity employer.** we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\n\n**application limit:** you may apply to a maximum of 3 positions within any 180\\-day period. this policy promotes better role\\-candidate matching and encourages thoughtful applications where your qualifications align most strongly.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Engineer I, Observability",
        "company": "DigitalOcean",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=93d6453a656d5be2",
        "description": "dive in and do the best work of your career at digitalocean. journey alongside a strong community of top talent who are relentless in their drive to build the simplest scalable cloud. if you have a growth mindset, naturally like to think big and bold, and are energized by the fast\\-paced environment of a true industry disruptor, you'll find your place here. we value winning together\u2014while learning, having fun, and making a profound difference for the dreamers and builders in the world.\n\n**we want people who care about empowering their peers to understand their services.**\n--------------------------------------------------------------------------------------\n\n\n\nobservability platforms is focused on creating visibility into digitalocean's services and infrastructure. we design, build, and operate the internal logging, metrics, distributed tracing, error reporting, monitoring, and alerting platforms that are depended on to ensure good, reliable experiences for digitalocean's customers. through a mix of open source services and in\\-house developed software we seek to continuously improve the ability of digitalocean engineers and product managers to understand the behavior of their products and services in order to improve the experiences of our customers.\n\n\n**what you'll be doing:**\n-------------------------\n\n\n* integrating and operating open source distributed data systems such as opensearch, victoriametrics, kafka, and clickhouse\n* implementing features to both improve the operability of our services and help design and implement the next generations of metrics and logging at digitalocean.\n* guiding other engineers on how they can best utilize our systems to gain confidence that their services are performing as expected.\n\n**what we'll expect from you:**\n-------------------------------\n\n\n* experience running and using distributed databases such as opensearch, victoriametrics, clickhouse\n* experience with kubernetes not just as an end user, but also an operator\n* experience writing software in golang. experience with the challenges of concurrency and distributed systems is a plus.\n* experience with devops tooling such as terraform, ansible, and github actions (or other relevant ci/cd experience).\n* an ability to work well as part of a geographically distributed team.\n* participation in a 24/7 on call rotation\n* someone who is motivated to learn, teach, grow, explore, and seek out areas we can improve\n* strong communication and customer service skills \\- we provide support to the internal users of the systems we design, build, and operate.\n\n**compensation range:**\n-----------------------\n\n\n* $139,000 \\- $174,000\n\n\n* this is a remote role\n\n\n\njr: 2026\\-7494\n\n\n*\\#li\\-remote*\n\n**why you'll like working for digitalocean**\n--------------------------------------------\n\n\n* **we innovate with purpose.** you'll be a part of a cutting\\-edge technology company with an upward trajectory, who are proud to simplify cloud and ai so builders can spend more time creating software that changes the world. as a member of the team, you will be a shark who thinks big, bold, and scrappy, like an owner with a bias for action and a powerful sense of responsibility for customers, products, employees, and decisions.\n* **we prioritize career development.** at do, you'll do the best work of your career. you will work with some of the smartest and most interesting people in the industry. we are a high\\-performance organization that will always challenge you to think big. our organizational development team will provide you with resources to ensure you keep growing. we provide employees with reimbursement for relevant conferences, training, and education. all employees have access to linkedin learning's 10,000\\+ courses to support their continued growth and development.\n* **we care about your well\\-being.** regardless of your location, we will provide you with a competitive array of benefits to support you from our employee assistance program to local employee meetups to flexible time off policy, to name a few. while the philosophy around our benefits is the same worldwide, specific benefits may vary based on local regulations and preferences.\n* **we reward our employees.** the salary range for this position is based on market data, relevant years of experience, and skills. you may qualify for a bonus in addition to base salary; bonus amounts are determined based on company and individual performance. we also provide equity compensation to eligible employees, including equity grants upon hire and the option to participate in our employee stock purchase program.\n* **digitalocean is an equal\\-opportunity employer.** we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\n\n**application limit:** you may apply to a maximum of 3 positions within any 180\\-day period. this policy promotes better role\\-candidate matching and encourages thoughtful applications where your qualifications align most strongly.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Engineer III (Software EUP)",
        "company": "Ross Dress For Less",
        "location": "Dublin, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=38362cff165251d5",
        "description": "**our values start with our people, join a team that values you!**\n\n\nbring your talents to ross, our leading off\\-price retail chain with over 2,200 stores, and a strong track record of success and growth. our focus has always been bringing our customers a constant stream of high\\-quality brands and on\\-trend merchandise at extraordinary savings. all while providing a fun and exciting treasure hunt experience.  \n\nas part of our team, you will experience:\n\n* **success.** our winning team pursues excellence while learning and evolving\n* **career growth.** we develop industry leading talent because ross grows when our people grow\n* **teamwork.** we work together to solve the hard problems and find the right solution\n* **our commitment to diversity, equality \\& inclusion, and our community.** we celebrate the backgrounds, identities, and ideas of those who work and shop with us because our differences make us stronger. we strive to be a positive force in our community.\n\n\nour corporate headquarters are in dublin, ca, we have 3 buying offices in key markets in new york city, los angeles, and boston, and 8 distribution centers nationwide. with 2023 revenues of $20\\.4 billion, we are a fortune 500 company who is committed to providing an inclusive work environment with continuous learning opportunities and development for our teams.\n\n**general purpose:**\n  \n\n  \n\nthe engineer iii role focuses on creating intuitive, responsive, and high\\-performing user interfaces using angular and other modern ui frameworks. it involves building and optimizing web\\-based interfaces for complex business workflows, collaborating with backend engineers to ensure seamless integration, implementing reusable components for consistency, and providing ongoing support and enhancements to maintain performance and usability.  \n\n  \n\nthe base salary range for this role is $129,100 \\- $220,600\\. the base salary range is dependent on factors including, but not limited to, experience, skills, qualifications, relevant education, certifications, seniority, and location. the range listed is just one component of the total compensation package for employees. other rewards vary by position and location.  \n\n  \n\n**essential functions:**  \n\n  \n\n* developing and optimizing user interfaces using angular and other modern ui frameworks, ensuring performance, scalability, and maintainability.\n\n  \n\n* implementing micro frontend architectures to enable modular and reusable ui components.\n\n  \n\n* collaborating with backend teams to integrate with microservices\\-based architectures for seamless functionality.\n\n  \n\n* utilizing ux design tools such as figma to translate wireframes and prototypes into high\\-quality, interactive user interfaces.\n\n  \n\n* leveraging ai\\-powered development tools like github copilot and mcp server\\-based integrations to improve developer productivity and code quality.\n\n  \n\n* building and maintaining ci/cd pipelines using standard tools (e.g., jenkins, github actions, azure devops) to automate build, test, and deployment processes.\n\n  \n\n* ensuring adherence to coding standards, accessibility guidelines, and responsive design principles.\n\n  \n\n* providing ongoing support, troubleshooting, and enhancements for existing applications to maintain usability and performance.\n\n  \n\n* working in the retail domain, understanding business workflows and customer\\-facing application needs.\n\n  \n\n* developing azure cloud\\-native applications as well as on\\-premises solutions, ensuring flexibility and scalability across environments.\n\n  \n\n**competencies:**  \n\n  \n\n**people**  \n\n  \n\n* building effective teams\n\n  \n\n* developing talent\n\n  \n\n* collaboration\n\n  \n\n**self**  \n\n  \n\n* leading by example\n\n  \n\n* communicates effectively\n\n  \n\n* ensures accountability and execution\n\n  \n\n* manages conflict\n\n  \n\n**business**  \n\n  \n\n* business acumen\n\n  \n\n* plans, aligns and prioritizes\n\n  \n\n* organizational agility\n\n  \n\n**with particular emphasis on the following specific position\\-related competencies**  \n\n  \n\n* problem solving\n\n  \n\n* time management\n\n  \n\n* dealing with ambiguity\n\n  \n\n* customer focus\n\n  \n\n* written communications\n\n  \n\n* composure\n\n  \n\n* integrity \\& trust\n\n  \n\n* listening\n\n  \n\n**qualifications and special skills required:**  \n\n  \n\n* 10\\+ years of experience in software engineering, with extensive hands\\-on frontend development and at least 3\\+ years in senior or lead roles.\n\n  \n\n* bachelor's degree in computer science, information technology, or related field, or an equivalent combination of education, industry certifications, and relevant experience.\n\n  \n\n* proven ability to design and deliver enterprise\\-grade applications, preferably in the retail domain, with a strong understanding of responsive design, accessibility standards, and performance optimization.\n\n  \n\n* deep expertise in angular and proficiency with other modern ui frameworks such as react and vue.\n\n  \n\n* hands\\-on experience with ux design tools like figma for ui/ux collaboration.\n\n  \n\n* strong knowledge of micro frontend architectures and component\\-based design principles.\n\n  \n\n* proficiency in ci/cd tools and pipelines (jenkins, github actions, azure devops).\n\n  \n\n* ability to leverage ai\\-powered development tools such as github copilot and mcp server\\-based integrations to enhance productivity and code quality.\n\n  \n\n* excellent analytical and troubleshooting skills, with the ability to creatively solve complex technical problems.\n\n  \n\n* strong collaboration skills, capable of working independently and in team settings, mentoring junior developers, and leading technical discussion\n\n  \n\n**preferred**  \n\n  \n\n* experience and certification in azure cloud development.\n\n  \n\n* good understanding of it security principles.\n\n  \n\n* familiarity with monitoring tools such as splunk and dynatrace.\n\n  \n\n**physical requirements/ada:**  \n\n  \n\nthe job requires the ability to work in an office environment, primarily on a computer.\n  \n\nrequires sitting, standing, walking, hearing, talking on the telephone, attending in\\-person meetings, typing, and working with paper/files, etc.\n  \n\nconsistent timeliness and regular attendance.\n  \n\nvision requirements: ability to see information in print and/or electronically.  \n\nthis role requires regular in\\-office presence, including to engage in in\\-person team interaction, meetings and collaboration, client support, mentoring, coaching, and/or feedback. however, this role can perform duties effectively using a combination of in\\-office and remote work. \\#li\\-hybrid  \n\n  \n\n**supervisory responsibilities:**  \n\n  \n\nn/a  \n\n  \n\n**disclaimer**  \n\n  \n\nthis job description is a summary of the primary duties and responsibilities of the job and position. it is not intended to be a comprehensive or all\\-inclusive listing of duties and responsibilities. contents are subject to change at management's discretion.  \n\n  \n\nross is an equal employment opportunity employer. we consider individuals for employment or promotion according to their skills, abilities and experience. we believe that it is an essential part of the company's overall commitment to attract, hire and develop a strong, talented and diverse workforce. ross is committed to complying with all applicable laws prohibiting discrimination based on race, color, religious creed, age, national origin, ancestry, physical, mental or developmental disability, sex (which includes pregnancy, childbirth, breastfeeding and medical conditions related to pregnancy, childbirth or breastfeeding), veteran status, military status, marital or registered domestic partnership status, medical condition (including cancer or genetic characteristics), genetic information, gender, gender identity, gender expression, sexual orientation, as well as any other category protected by federal, state or local laws.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI/GIS Engineer",
        "company": "Alson Services",
        "location": "Grand Rapids, MI, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "PostgreSQL",
            "MongoDB",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6b2577bc005b4c8f",
        "description": "**position summary**\n\nalson services is seeking an ai/gis engineer to design, develop, and deploy ai\\-powered agents and automation tools that transform fiber optic network design workflows. this role sits at the intersection of artificial intelligence, geospatial information systems (gis), and telecommunications engineering. the ideal candidate will build intelligent systems capable of automating ftth/fttx design tasks\u2014from high\\-level market area planning to low\\-level construction\\-ready design outputs\u2014producing industry\\-standard shp files and design packages.\n\nyou will work closely with our design engineering team and business development leadership to create ai agents that dramatically accelerate the fiber design lifecycle, improve accuracy, and scale our capacity to serve isps, utilities, and data center clients nationwide.\n\n**core responsibilities**\n\n**ai agent development \\& automation**\n\n* design, build, and maintain ai agents capable of performing gis\\-based fiber optic design tasks autonomously or semi\\-autonomously\n* develop machine learning models and rule\\-based systems for automated network routing, demand analysis, and construction cost estimation\n* integrate llm\\-based workflows (e.g., claude api, function calling, rag pipelines) with gis tools and fiber design software\n* build feedback loops and quality assurance mechanisms to validate ai\\-generated designs against engineering standards\n\n**gis \\& fiber optic design automation**\n\n* automate high\\-level design (hld) workflows including market area identification, demand aggregation, and service area boundary generation\n* automate low\\-level design (lld) workflows including strand mapping, splice schematics, drop cable routing, and bom generation\n* build tools for automated pole assessment analysis, make\\-ready engineering review, and attachment cataloging\n* develop market area pinning automation for identifying optimal ftth deployment zones based on density, roi, and infrastructure data\n* generate and manage shp file outputs compatible with industry\\-standard gis platforms (arcgis, qgis, autocad map 3d, etc.)\n* integrate with geospatial data sources including parcel data, road centerlines, utility pole databases, and aerial/satellite imagery\n\n**data engineering \\& integration**\n\n* build data pipelines for ingesting, cleaning, and transforming geospatial datasets from multiple sources (county gis, usda, fcc broadband data, etc.)\n* develop apis and integration layers between ai agents and existing design tools, project management systems, and client platforms\n* implement version control and traceability for ai\\-generated design outputs\n\n**collaboration \\& continuous improvement**\n\n* collaborate with field engineers, designers, and project managers to translate manual design workflows into automated processes\n* stay current with advancements in ai/ml, gis technology, and telecommunications design standards\n* document ai agent architectures, design logic, and operational procedures\n* present technical solutions to internal stakeholders and external clients\n\n**required qualifications**\n\n* bachelor\u2019s degree in computer science, gis, geospatial engineering, telecommunications, or a related field (or equivalent experience)\n* 3\\+ years of experience with gis platforms and geospatial data processing (arcgis, qgis, postgis, geopandas, or equivalent)\n* 2\\+ years of experience developing ai/ml solutions, including experience with llms, nlp, or computer vision\n* proficiency in python with strong experience in geospatial libraries (shapely, fiona, geopandas, gdal/ogr, pyqgis)\n* demonstrated ability to generate and manipulate shp files, geojson, kml/kmz, and other geospatial formats programmatically\n* experience building ai agents, agentic workflows, or automation pipelines (e.g., using langchain, claude api, openai function calling, or similar frameworks)\n* understanding of fiber optic network design principles (ftth/fttx architecture, pon/gpon, splitter placement, drop cable design)\n* experience with database systems (postgresql/postgis, mongodb, or similar) for geospatial data management\n* strong problem\\-solving skills with the ability to work independently and manage multiple concurrent projects\n\n**preferred qualifications**\n\n* direct experience in telecom/isp fiber design or osp (outside plant) engineering\n* experience with pole loading analysis software (e.g., katapult, o\\-calc pro, poleforeman)\n* familiarity with nesc (national electrical safety code) standards for pole attachments\n* experience with 3d modeling or point cloud processing for pole and infrastructure assessment\n* knowledge of fcc broadband funding programs (bead, rdof, caf) and associated mapping/reporting requirements\n* experience with cloud platforms (aws, gcp, azure) for deploying scalable geospatial and ai workloads\n* background in computer vision or image recognition for aerial/satellite imagery analysis\n* experience with anthropic claude, mcp (model context protocol), or similar ai orchestration tools\n* familiarity with data center connectivity design and 400g fiber optic solutions\n\n**technical environment**\n\nour team works with a modern stack that includes python, javascript/node.js, postgresql/postgis, arcgis and qgis, cloud\\-based ai services (anthropic claude api), and standard telecom design tools. the successful candidate will have the opportunity to define and shape the ai tooling architecture from the ground up.\n\njob type: full\\-time\n\npay: $21\\.00 \\- $26\\.00 per hour\n\nexpected hours: 40 per week\n\nwork location: in person",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Software Engineer II, Data Services",
        "company": "Fox Corporation",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Kafka",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=717fdee43693f735",
        "description": "overview of the company\nfox corporation\nunder the fox banner, we produce and distribute content through some of the world\u2019s leading and most valued brands, including: fox news media, fox sports, fox entertainment, fox television stations and tubi media group. we empower a diverse range of creators to imagine and develop culturally significant content, while building an organization that thrives on creative ideas, operational expertise and strategic thinking.\njob description\nabout the role\nfox is looking for a software engineer ii to join the data services team\u2014a small, high\\-impact group that builds the systems behind some of the biggest moments in news, weather, elections, and entertainment. this role sits within the data services team and reports to the head of data services.\nas an individual contributor, you will design, develop, and build data service products that support multiple fox business units. you\u2019ll create apis, pipelines, and backend services that scale to millions of users, power real\\-time experiences, and enable emerging technologies such as ai\\-driven tooling. if you\u2019re excited about building resilient infrastructure that supports the events people care about most, this is an opportunity to make meaningful impact at scale.\na snapshot of your responsibilities* design, build, and maintain http and rpc api services using go, python, c\\#, node.js, and typescript\n* build and maintain event\\-driven data pipelines leveraging kafka and aws sqs/sns\n* deploy and manage cloud infrastructure in aws, including eks (kubernetes), lambda, dynamodb, redis, and postgres\n* implement and enhance ci/cd pipelines using github actions and terraform\n* improve system reliability through enhanced test coverage, observability, and alerting\n* collaborate cross\\-functionally with product and engineering teams to refine requirements and deliver iteratively\n* participate in code reviews, technical design discussions, and team ceremonies\n\n\nwhat you will need* 2\\+ years of experience building and maintaining production backend services (go preferred; python a plus)\n* familiarity with the .net development environment, including c\\#, f\\#, and nuget\n* experience working with containers and kubernetes, particularly aws eks\n* working knowledge of postgres, dynamodb, and redis\n* experience with distributed messaging systems such as kafka and aws sqs/sns\n* strong understanding of api design principles, including rest, rpc, authentication, and versioning\n* ci/cd experience using github actions or similar tooling\n* strong problem\\-solving skills and the ability to work effectively in a collaborative, fast\\-paced environment\n* regular, on\\-site attendance at the workplace a minimum of 3 days per week is an essential function of the position. selected candidate must be able to reliably meet this requirement.\n\n\nnice to have, but not a dealbreaker* experience supporting large\\-scale media, streaming, or real\\-time data platforms\n* familiarity with infrastructure as code best practices\n* exposure to ai/ml\\-enabled data systems or tooling\n* experience contributing to platform\\-level engineering or shared services teams\n\n\n\\#ll\\-hybrid\n\\#ll\\-kd1*we are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. we will consider for employment qualified applicants with criminal histories consistent with applicable law.*\npursuant to state and local pay disclosure requirements, the pay rate/range for this role, with final offer amount dependent on education, skills, experience, and location is $114,000\\.00\\-151,000\\.00 annually. this role is also eligible for an annual discretionary bonus, various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents. benefits for union represented employees will be in accordance with the applicable collective bargaining agreement.\nview more detail about fox benefits.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "AI Engineer I",
        "company": "BillGO, Inc.",
        "location": "Fort Collins, CO, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "Prompt Engineering",
            "CI/CD",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=72139005356b854a",
        "description": "**position summary**  \n\nthe primary role of the artificial intelligence engineer 1 is to develop software solutions by studying information needs, conferring with users, and studying systems flow, data usage and work processes. as an ai engineer i, you will be an integral member of the software development team, contributing to the design, development, testing, and maintenance of software products and applications. key factors for this position are not only technical aptitude, but also being a problem solver, continuous learner, and great collaborator. the ideal candidate will also have experience working in a fast\\-paced environment.  \n\nthis role is based onsite at billgo's headquarters located in fort collins, co. there is an opportunity to work remotely on occasion, but will be based in the office with the team the majority of the time.  \n\n**area of focus**  \n\n*all other duties as assigned, plus\u2026*\n\n* software development: collaborate with cross\\-functional teams to design, develop, and deploy software solutions that meet the needs of our fintech products. utilize programming languages, frameworks, and tools to build scalable, secure, and reliable software applications.\n* code review and quality assurance: participate in code reviews to ensure adherence to best practices, maintain code quality, and identify areas for improvement. perform thorough testing, debugging, and troubleshooting to resolve issues promptly.\n* system architecture and design: contribute to the design and architecture of software systems, focusing on scalability, performance, and maintainability. work closely with senior engineers to implement robust and efficient solutions.\n* technology research: stay updated with the latest industry trends, emerging technologies, and best practices in fintech and software development. suggest and evaluate new tools, frameworks, and methodologies to enhance the team's efficiency and product offerings.\n* collaboration and communication: engage in effective communication with team members, stakeholders, and other departments to understand project requirements, provide updates, and ensure alignment on goals and timelines.\n* continuous improvement: demonstrate a commitment to continuous learning and improvement. identify opportunities to streamline processes, enhance productivity, and optimize software performance.\n* security and compliance: ensure that all software developed complies with industry security standards and regulatory requirements. implement necessary security measures to protect sensitive financial data.\n* documentation: create clear and comprehensive technical documentation for developed software, including apis, code documentation, and user guides.\n\n**minimum qualifications**  \n\n***education and work experience***\n\n* bachelor's degree or equivalent experience in computer science, software engineering, or a related field. (master's degree is a plus but not required).\n* demonstrated proficiency with both off\\-the\\-shelf and custom ai tools utilizing llms, agentic ai, mcp, and prompt engineering.\n* demonstrated proficiency in one or more object\\-oriented programming languages such as java, python, c\\#, or javascript/typescript.\n* understanding of rest apis, microservices, and cloud platforms (aws, azure, or gcp).\n* familiarity with databases (sql and nosql) and ci/cd pipelines.\n* familiarity with software development methodologies, version control systems, and project management tools.\n* solid understanding of data structures, algorithms, and software design principles.\n* strong problem\\-solving skills and the ability to troubleshoot complex technical issues.\n* excellent communication skills and the ability to work effectively in a team\\-oriented environment.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Data Scientist",
        "company": "Qode",
        "location": "Basking Ridge, NJ, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "XGBoost",
            "BigQuery",
            "BigQuery",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=31fc8ee741d634cf",
        "description": "**role:** data scientistlocation: basking ridge, njhybrid / remote: hybrid (2\\-3 days)required skills:\n  \n**programming:** python, sql, gcp bigquery, machine learning (xgboost, classification, regression, unsupervised learning), model deployment, feature engineering, model monitoring, llms, api integration\n  \n**visualization:** looker, qlik, dashboard development/maintenance experience.\n  \n**nice to have:** teradata, tableau\n  \nmaintain data products such as nem (network equipment maintenance for wls) and project hubble \\& network almanac (wls revenue and opex dataset). these data products enable multiple use cases ranging from maintenance dollar optimization, capex planning, and energy efficiency. \u2022 maintain and enhance the qlik and tableau dashboards (tableau will eventually be converted to looker or qlik)\n  \n* provide business support (fp\\&a, real estate, cost assurance teams) for data/analytics needs\n* data modelling and insights analytics to manage equipment expenses and identify opportunities for opex reduction for wln operations and p\\&l profiling of wls cell sites.\n* prior telecom exposure will be great.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Data Scientist, Condition Monitoring",
        "company": "Caterpillar",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-24",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "PyTorch",
            "Git",
            "Snowflake",
            "Python",
            "R",
            "Optimization",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fc8e43d1baa05943",
        "description": "**career area:**\n\n\ntechnology, digital and data**job description:**\n\n**your work shapes the world at caterpillar inc.**\n\nwhen you join caterpillar, you're joining a global team who cares not just about the work we do \u2013 but also about each other. we are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. we don't just talk about progress and innovation here \u2013 we make it happen, with our customers, where we work and live. together, we are building a better world, so we can all enjoy living in it.\n\n*cat digital is the digital and technology arm of caterpillar inc., leveraging the latest technologies to build industry leading digital solutions for our customers and dealers. with over 1\\.5 million connected assets worldwide, our teams use data, technology, advanced analytics, telematics, and ai capabilities to help our customers build a better, more sustainable world.*\n\n**job summary:**\n\n\njoin the condition monitoring analytics team of cat digital and take a key role in guiding the development of advanced machine learning systems that assess the health and risk of our machines and components. as a senior contributor, you will help shape the technical direction for transforming telematics, environmental factors, and service history into actionable insights \\- helping customers prevent unplanned downtime and maximize productivity.\n\n **what you will do:**\n\n* directing the data gathering, data mining, and data processing processes in huge volume; creating appropriate data models to support predictive maintenance.\n* exploring, promoting, and implementing semantic data capabilities and advanced modeling paradigms to synthesize equipment insights from unstructured service data.\n* defining requirements and scope of data analyses; presenting and reporting possible business insights to management using data visualization technologies.\n* conducting research on data model optimization and evaluating emerging generative technologies to improve the effectiveness and accuracy of equipment health assessments.\n\n **what you will have:**\n\n* **analytical thinking:** extensive knowledge of techniques and tools that promote effective analysis; ability to determine the root cause of organizational problems and create alternative solutions that resolve these problems.\n* **machine learning:** extensive knowledge of principles, technologies and algorithms of machine learning; ability to develop, implement and deliver related systems, products and services.\n* **programming languages:** extensive knowledge of basic concepts and capabilities applying python (numpy, scipy, pandas, pytorch, etc.) programming; ability to use tools, techniques and platforms to write and modify programming languages.\n* **query and database access tools:** working knowledge of data management systems; ability to use, support and access facilities for searching, extracting and formatting data for further use.\n* **requirements analysis:** working knowledge of tools, methods, and techniques of requirement analysis; ability to elicit, analyze and record required business functionality and non\\-functionality requirements to ensure the success of a system or software development project.\n\n**considerations for top candidates:**\n\n* typically, a master\u2019s or phd degree in applied statistics, data science, business analytics, predictive analytics, business intelligence \\& analytics, mathematics, computer science, engineering (aerospace, electrical, mechanical, computer, industrial, agricultural, etc.), or equivalent technical degree\n* experience with advanced statistical methods such as survival analysis/reliability engineering, statistical process control, bayesian inference, and time series analysis.\n* practical applications of machine learning techniques such as clustering, regression/classification, random forests, and deep learning.\n* experience with model interpretability methods and communicating model behavior to stakeholders.\n* competency in coaching and mentoring junior data scientists in the creation, validation, and application of statistical models as well as in implementing digital solutions.\n* extensive experience applying python (numpy, scipy, pandas, etc) programming to solve business challenges (typically 3\\+ years).\n* practical knowledge of cloud computing and workflow orchestration (e.g., airflow) for building solutions in cloud environments (aws, snowflake, etc.).\n* good communication, interpersonal, and collaboration skills.\n* strong initiative to research and apply modern ai advancements, including generative approaches, to further enhance the evolution of predictive maintenance capabilities.\n\n**additional information:**\n\n\nthis position will have the option to be based out of our chicago, il or peoria, il offices.\n\n\n\\#li\n\n\n\\#bi (used to post on built in chicago)\n\n**what you will get:**\n\n\nworking with a fortune 100 leader, you can build your career on a global scale and take advantage of development opportunities with emerging technologies. we\u2019ve created an inclusive environment for you to explore your passions, make an impact and do the work that really matters. join us.\n\n**about caterpillar**\n\ncaterpillar inc. is the world\u2019s leading manufacturer of construction and mining equipment, off\\-highway diesel and natural gas engines, industrial gas turbines and diesel\\-electric locomotives. for nearly 100 years, we\u2019ve been helping customers build a better, more sustainable world and are committed and contributing to a reduced\\-carbon future. our innovative products and services, backed by our global dealer network, provide exceptional value that helps customers succeed.\n\n**summary pay range:**\n\n\n$112,710\\.00 \\- $183,140\\.00\ncompensation and benefits offered may vary depending on multiple individualized factors, job level, market location, job\\-related knowledge, skills, individual performance and experience. please note that salary is only one component of total compensation at caterpillar.\n\n\n**benefits:**\n\n\nsubject to plan eligibility, terms, and guidelines. this is a summary list of benefits.\n\n* medical, dental, and vision benefits\\*\n* paid time off plan (vacation, holidays, volunteer, etc.)\\*\n* 401(k) savings plans\\*\n* health savings account (hsa)\\*\n* flexible spending accounts (fsas)\\*\n* health lifestyle programs\\*\n* employee assistance program\\*\n* voluntary benefits and employee discounts\\*\n* career development\\*\n* incentive bonus\\*\n* disability benefits\n* life insurance\n* parental leave\n* adoption benefits\n* tuition reimbursement\n\n\n\n\n* these benefits also apply to part\\-time employees\n\n\nvisa sponsorship is not available for this position. this employer is not currently hiring foreign national applicants that require or will require sponsorship tied to a specific employer, such as, h, l, tn, f, j, e, o. as a global company, caterpillar offers many job opportunities outside of the u.s which can be found through our employment website at www.caterpillar.com/careers.**posting dates:**\n\n\nfebruary 24, 2026 \\- march 8, 2026\nany offer of employment is conditioned upon the successful completion of a drug screen.\n\n\ncaterpillar is an equal opportunity employer, including veterans and individuals with disabilities. qualified applicants of any age are encouraged to apply.\n\n\nnot ready to apply? join our talent community.",
        "scrapped_date": "2026-02-24"
    },
    {
        "title": "Senior Systems Engineer, UDS Data Management - West",
        "company": "Dell Technologies",
        "location": "CA, US USA",
        "posted_at": "2026-02-22",
        "score": 24.4,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "LLaMA",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Redshift",
            "BigQuery",
            "Synapse"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6bf94c93449dfa3d",
        "description": "our field sales professionals rely on proactive technical support during the sales process \u2013 and our expert systems engineering team always steps up to the mark. we lead the development and implementation of complex and specialized products, applications, services and solutions. from delivering sales presentations and product demonstrations, to developing detailed installation or system integration plans, we ensure customers get the innovative, relevant, interoperable solutions they need.\n\n\njoin us to do the best work of your career and make a profound social impact as a senior systems engineer on our systems engineering team in **the west region of the us.**\n\n **what you\u2019ll achieve**  \n\nas a senior systems engineer, you will provide pre\\-sales technical support to our field sales teams, helping to define the overall dell technologies solution for our customers using the full range of company products and services.  \n\n  \n\n**you will:**  \n\n* build and lead relationships for highly sophisticated customer accounts\n* conduct customer needs analysis and anticipate requirements beyond existing solution\u2019s scope\n* prepare detailed product specifications to enable the sale of our products and solutions, and deliver impact presentations at customer facilities\n* verify operability of sophisticated product and service configurations within the customer\u2019s environment\n* perform advanced systems integration and provide technical expertise to design and implement the solution\n\n  \n\n**take the first step towards your dream career**  \n\nevery dell technologies team member brings something unique to the table. here\u2019s what we are looking for with this role: **technical skills**\n\n* hands\\-on experience with at least one major cloud data platform (e.g., snowflake, databricks, bigquery, redshift, cloudera, synapse, or similar).\n* strong understanding of data warehousing, data lakes/lakehouse, and etl/elt concepts (staging, modeling, performance tuning, cost/perf tradeoffs).\n* data engineering and integration including unstructured data processing (pdfs, logs, images, text) and transformation into structured/vectorized formats\n* strong sql skills for analytical queries, performance tuning, and data modeling (star/snowflake schemas, dimensional modeling, partitioning, clustering).\n* unstructured data \\& ai/rag: understanding of vector databases (e.g., elasticsearch, milvus, pgvector), embedding models, and rag architectures. familiarity with document processing pipelines, chunking strategies, and semantic search patterns.\n* familiarity with data pipeline and orchestration tools (e.g., airflow, dbt, spark, kafka, cloud\\-native etl tools) and batch vs. streaming patterns.\n* understanding of data governance (catalog, lineage, security, rbac, masking, compliance requirements like gdpr/ccpa).\n* analytics, bi, and data science\n* ability to design and explain analytics solutions end\\-to\\-end: from raw data to dashboards and predictive models.\n* working knowledge of bi tools (e.g., tableau, power bi, looker, qlik) and how to connect, model, and optimize for self\\-service analytics.\n* familiarity with data science and ml workflows (feature engineering, experimentation, model training/deployment, rag pipeline development, prompt engineering) and tools/languages such as python, spark, notebooks, and ml frameworks (e.g., scikit\\-learn, mlflow, tensorflow/pytorch, langchain, llamaindex at a conceptual level).\n\n**consulting skills**\n\n* skilled at asking the right questions to uncover technical requirements, constraints, and business drivers.\n* can translate ambiguous business problems into clear data and analytics use cases.\n* storytelling \\& communication\n* excellent at translating complex technical topics into clear, business\\-oriented narratives for both technical and non\\-technical audiences.\n* comfortable presenting to large groups and senior stakeholders (cio/cdo, heads of data/analytics).\n* demo \\& poc excellence\n* able to build and deliver compelling demonstrations that tell a story around customer data and use cases, not just features.\n* can structure and run pocs with clear success criteria, timelines, and executive readouts to accelerate technical win.\n* competitive positioning\n* understands the broader data \\& ai ecosystem and can articulate differentiation versus other data warehouses, data lake/lakehouse platforms, and analytics tools.\n\n**5\\+ years in a customer\\-facing technical role such as sales engineer, solutions architect, data engineer, analytics consultant, or data scientist with strong commercial exposure.**\n\n**proven experience architecting and delivering data management, analytics, or data science solutions in one or more of the following areas:**\n\n* cloud data warehouse or lakehouse migrations\n* enterprise bi modernization/self\\-service analytics\n* genai and rag implementations for enterprise knowledge management, intelligent document processing, or customer\\-facing ai applications\n* real\\-time or streaming analytics\n* advanced analytics / data science enablement\n* hands\\-on experience with at least one major public cloud (aws, azure, or gcp) and one or more leading data platforms (e.g., snowflake, databricks, cloudera, bigquery, redshift, synapse).\n\n**compensation**  \n\ndell is committed to fair and equitable compensation practices. the total target compensation (ttc) range for this position is $204,850 \\- $265,100 which includes base salary and potential commissions.  \n\n  \n\n**benefits and perks of working at dell technologies**  \n\nyour life. your health. supported by your benefits. you can explore the overall benefits experience that awaits you as a dell technologies team member \u2014 right now at mywellatdell.com\n\n **who we are**\n\n\nwe believe that each of us has the power to make an impact. that\u2019s why we put our team members at the center of everything we do. if you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.  \n\n  \n\ndell technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. join us to build a future that works for everyone because progress takes all of us.  \n\n  \n\ndell technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.\n\n**job id:**r285990",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Systems Engineer, UDS Data Management - Central",
        "company": "Dell Technologies",
        "location": "IL, US USA",
        "posted_at": "2026-02-22",
        "score": 24.4,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "LLaMA",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Redshift",
            "BigQuery",
            "Synapse"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=23fa7ab7f29d61af",
        "description": "our field sales professionals rely on proactive technical support during the sales process \u2013 and our expert systems engineering team always steps up to the mark. we lead the development and implementation of complex and specialized products, applications, services and solutions. from delivering sales presentations and product demonstrations, to developing detailed installation or system integration plans, we ensure customers get the innovative, relevant, interoperable solutions they need.\n\n\njoin us to do the best work of your career and make a profound social impact as a senior systems engineer on our systems engineering team in **the central region of the us.**\n\n **what you\u2019ll achieve**  \n\nas a senior systems engineer, you will provide pre\\-sales technical support to our field sales teams, helping to define the overall dell technologies solution for our customers using the full range of company products and services.  \n\n  \n\n**you will:**  \n\n* build and lead relationships for highly sophisticated customer accounts\n* conduct customer needs analysis and anticipate requirements beyond existing solution\u2019s scope\n* prepare detailed product specifications to enable the sale of our products and solutions, and deliver impact presentations at customer facilities\n* verify operability of sophisticated product and service configurations within the customer\u2019s environment\n* perform advanced systems integration and provide technical expertise to design and implement the solution\n\n  \n\n**take the first step towards your dream career**  \n\nevery dell technologies team member brings something unique to the table. here\u2019s what we are looking for with this role: **technical skills**\n\n* hands\\-on experience with at least one major cloud data platform (e.g., snowflake, databricks, bigquery, redshift, cloudera, synapse, or similar).\n* strong understanding of data warehousing, data lakes/lakehouse, and etl/elt concepts (staging, modeling, performance tuning, cost/perf tradeoffs).\n* data engineering and integration including unstructured data processing (pdfs, logs, images, text) and transformation into structured/vectorized formats\n* strong sql skills for analytical queries, performance tuning, and data modeling (star/snowflake schemas, dimensional modeling, partitioning, clustering).\n* unstructured data \\& ai/rag: understanding of vector databases (e.g., elasticsearch, milvus, pgvector), embedding models, and rag architectures. familiarity with document processing pipelines, chunking strategies, and semantic search patterns.\n* familiarity with data pipeline and orchestration tools (e.g., airflow, dbt, spark, kafka, cloud\\-native etl tools) and batch vs. streaming patterns.\n* understanding of data governance (catalog, lineage, security, rbac, masking, compliance requirements like gdpr/ccpa).\n* analytics, bi, and data science\n* ability to design and explain analytics solutions end\\-to\\-end: from raw data to dashboards and predictive models.\n* working knowledge of bi tools (e.g., tableau, power bi, looker, qlik) and how to connect, model, and optimize for self\\-service analytics.\n* familiarity with data science and ml workflows (feature engineering, experimentation, model training/deployment, rag pipeline development, prompt engineering) and tools/languages such as python, spark, notebooks, and ml frameworks (e.g., scikit\\-learn, mlflow, tensorflow/pytorch, langchain, llamaindex at a conceptual level).\n\n**consulting skills**\n\n* skilled at asking the right questions to uncover technical requirements, constraints, and business drivers.\n* can translate ambiguous business problems into clear data and analytics use cases.\n* storytelling \\& communication\n* excellent at translating complex technical topics into clear, business\\-oriented narratives for both technical and non\\-technical audiences.\n* comfortable presenting to large groups and senior stakeholders (cio/cdo, heads of data/analytics).\n* demo \\& poc excellence\n* able to build and deliver compelling demonstrations that tell a story around customer data and use cases, not just features.\n* can structure and run pocs with clear success criteria, timelines, and executive readouts to accelerate technical win.\n* competitive positioning\n* understands the broader data \\& ai ecosystem and can articulate differentiation versus other data warehouses, data lake/lakehouse platforms, and analytics tools.\n\n**5\\+ years in a customer\\-facing technical role such as sales engineer, solutions architect, data engineer, analytics consultant, or data scientist with strong commercial exposure.**\n\n**proven experience architecting and delivering data management, analytics, or data science solutions in one or more of the following areas:**\n\n* cloud data warehouse or lakehouse migrations\n* enterprise bi modernization/self\\-service analytics\n* genai and rag implementations for enterprise knowledge management, intelligent document processing, or customer\\-facing ai applications\n* real\\-time or streaming analytics\n* advanced analytics / data science enablement\n* hands\\-on experience with at least one major public cloud (aws, azure, or gcp) and one or more leading data platforms (e.g., snowflake, databricks, cloudera, bigquery, redshift, synapse).\n\n**compensation**  \n\ndell is committed to fair and equitable compensation practices. the total target compensation (ttc) range for this position is $204,850 \\- $265,100, which includes base salary and potential commissions.  \n\n  \n\n**benefits and perks of working at dell technologies**  \n\nyour life. your health. supported by your benefits. you can explore the overall benefits experience that awaits you as a dell technologies team member \u2014 right now at mywellatdell.com\n\n **who we are**\n\n\nwe believe that each of us has the power to make an impact. that\u2019s why we put our team members at the center of everything we do. if you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.  \n\n  \n\ndell technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. join us to build a future that works for everyone because progress takes all of us.  \n\n  \n\ndell technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.\n\n**job id:**r285983",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Systems Engineer, UDS Data Management - East",
        "company": "Dell Technologies",
        "location": "MA, US USA",
        "posted_at": "2026-02-22",
        "score": 24.4,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "LLaMA",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Redshift",
            "BigQuery",
            "Synapse"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ff8fbb8c2cb45e2e",
        "description": "our field sales professionals rely on proactive technical support during the sales process \u2013 and our expert systems engineering team always steps up to the mark. we lead the development and implementation of complex and specialized products, applications, services and solutions. from delivering sales presentations and product demonstrations, to developing detailed installation or system integration plans, we ensure customers get the innovative, relevant, interoperable solutions they need.\n\n\njoin us to do the best work of your career and make a profound social impact as a senior systems engineer on our systems engineering team in **the east region of the us.**\n\n **what you\u2019ll achieve**  \n\nas a senior systems engineer, you will provide pre\\-sales technical support to our field sales teams, helping to define the overall dell technologies solution for our customers using the full range of company products and services.  \n\n  \n\n**you will:**  \n\n* build and lead relationships for highly sophisticated customer accounts\n* conduct customer needs analysis and anticipate requirements beyond existing solution\u2019s scope\n* prepare detailed product specifications to enable the sale of our products and solutions, and deliver impact presentations at customer facilities\n* verify operability of sophisticated product and service configurations within the customer\u2019s environment\n* perform advanced systems integration and provide technical expertise to design and implement the solution\n\n  \n\n**take the first step towards your dream career**  \n\nevery dell technologies team member brings something unique to the table. here\u2019s what we are looking for with this role: **technical skills**\n\n* hands\\-on experience with at least one major cloud data platform (e.g., snowflake, databricks, bigquery, redshift, cloudera, synapse, or similar).\n* strong understanding of data warehousing, data lakes/lakehouse, and etl/elt concepts (staging, modeling, performance tuning, cost/perf tradeoffs).\n* data engineering and integration including unstructured data processing (pdfs, logs, images, text) and transformation into structured/vectorized formats\n* strong sql skills for analytical queries, performance tuning, and data modeling (star/snowflake schemas, dimensional modeling, partitioning, clustering).\n* unstructured data \\& ai/rag: understanding of vector databases (e.g., elasticsearch, milvus, pgvector), embedding models, and rag architectures. familiarity with document processing pipelines, chunking strategies, and semantic search patterns.\n* familiarity with data pipeline and orchestration tools (e.g., airflow, dbt, spark, kafka, cloud\\-native etl tools) and batch vs. streaming patterns.\n* understanding of data governance (catalog, lineage, security, rbac, masking, compliance requirements like gdpr/ccpa).\n* analytics, bi, and data science\n* ability to design and explain analytics solutions end\\-to\\-end: from raw data to dashboards and predictive models.\n* working knowledge of bi tools (e.g., tableau, power bi, looker, qlik) and how to connect, model, and optimize for self\\-service analytics.\n* familiarity with data science and ml workflows (feature engineering, experimentation, model training/deployment, rag pipeline development, prompt engineering) and tools/languages such as python, spark, notebooks, and ml frameworks (e.g., scikit\\-learn, mlflow, tensorflow/pytorch, langchain, llamaindex at a conceptual level).\n\n**consulting skills**\n\n* skilled at asking the right questions to uncover technical requirements, constraints, and business drivers.\n* can translate ambiguous business problems into clear data and analytics use cases.\n* storytelling \\& communication\n* excellent at translating complex technical topics into clear, business\\-oriented narratives for both technical and non\\-technical audiences.\n* comfortable presenting to large groups and senior stakeholders (cio/cdo, heads of data/analytics).\n* demo \\& poc excellence\n* able to build and deliver compelling demonstrations that tell a story around customer data and use cases, not just features.\n* can structure and run pocs with clear success criteria, timelines, and executive readouts to accelerate technical win.\n* competitive positioning\n* understands the broader data \\& ai ecosystem and can articulate differentiation versus other data warehouses, data lake/lakehouse platforms, and analytics tools.\n\n**5\\+ years in a customer\\-facing technical role such as sales engineer, solutions architect, data engineer, analytics consultant, or data scientist with strong commercial exposure.**\n\n**proven experience architecting and delivering data management, analytics, or data science solutions in one or more of the following areas:**\n\n* cloud data warehouse or lakehouse migrations\n* enterprise bi modernization/self\\-service analytics\n* genai and rag implementations for enterprise knowledge management, intelligent document processing, or customer\\-facing ai applications\n* real\\-time or streaming analytics\n* advanced analytics / data science enablement\n* hands\\-on experience with at least one major public cloud (aws, azure, or gcp) and one or more leading data platforms (e.g., snowflake, databricks, cloudera, bigquery, redshift, synapse).\n\n**compensation**  \n\ndell is committed to fair and equitable compensation practices. the total target compensation (ttc) range for this position is $204,850 \\- $265,100, which includes base salary and potential commissions.  \n\n  \n\n**benefits and perks of working at dell technologies**  \n\nyour life. your health. supported by your benefits. you can explore the overall benefits experience that awaits you as a dell technologies team member \u2014 right now at mywellatdell.com\n\n **who we are**\n\n\nwe believe that each of us has the power to make an impact. that\u2019s why we put our team members at the center of everything we do. if you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.  \n\n  \n\ndell technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. join us to build a future that works for everyone because progress takes all of us.  \n\n  \n\ndell technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.\n\n**job id:**r285979",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "GenAI Architect",
        "company": "Tata Consultancy Services (TCS)",
        "location": "Edison, NJ, US USA",
        "posted_at": "2026-02-23",
        "score": 24.4,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "RAG",
            "LLaMA",
            "Hugging Face",
            "Pinecone",
            "TensorFlow",
            "PyTorch",
            "S3",
            "Data Lake"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e014fe941f1f22ec",
        "description": "must have technical/functional skills\nmandatory skills:\n1\\. genai application development expertise\n* programming languages: python\n* development tools: langchain, llamaindex, langflow, langgraph, langsmith, flowise\n* techniques: rag techniques\n* databases: vector databases (pinecone, weaviate, qdrant)\n* additional technologies: knowledge graphs, fastapi, streamlit, gradio\n\n2\\. domain model fine\\-tuning capabilities\n* languages \\& libraries: python, data engineering, oss llms (llama2, mixtral, gpt\\-neo, gpt\\-j)\n* tokenization \\& frameworks: tokenizers (sentencepiece, hugging face tokenizers), fine\\-tuning frameworks (hugging face transformers, pytorch lightning)\n* datasets: huggingface datasets, tensorflow datasets\n\n3\\. llmops proficiency\n* infrastructure \\& ci/cd: devops, kubernetes, docker, git, jenkins, gitlab, github actions\n* monitoring \\& management: ray, seldoncore, mlflow, mlserver, triton, bentoml, prometheus, grafana\n\n4\\. data engineering for ai applications\n* data processing \\& management: python, apache spark, apache kafka, aws s3, azure data lake storage (adls), delta lake\n* workflow automation: apache airflow, dbt, apache nifi, fivetran, airbyte, great expectations\n* data catalogs: amundsen, collibra, alation\n\n5\\. ai\\-ready cybersecurity knowledge\n* threat modeling \\& security: ai\\-specific threat modeling tools, secure ml pipeline tools, api security tools\n* monitoring \\& prevention: ai security monitoring tools, prompt injection prevention libraries, adversarial example detection libraries\n\n6\\. genai guardrails and ethics\n* ethics \\& fairness: ai ethics frameworks and tools, bias detection and mitigation tools, fairness metrics libraries\n* privacy \\& security: privacy\\-preserving machine learning libraries, robustness and security tools\n\ntransparency \\& governance: model interpretability libraries, ai governance frameworks and tools\nroles \\& responsibilities\n candidate will be working in ai engineering group which is collectively responsible for product developments and ai infused automations.\ncandidates need to lead the solution design discussion and arrive at best\\-in\\-class architecture by assessing all the pros\nand cons across multiple technologies. build design documentation and implementation.  \n\n\n  \n\n  \n\nsalary range $170000\\-$180000 year\n\ntcs employee benefits summary:\ndiscretionary annual incentive.\ncomprehensive medical coverage: medical \\& health, dental \\& visio n, disability planning \\& insurance, pet insurance plans.\nfamily support: maternal \\& parental leaves.\ninsurance options: auto \\& home insurance, identity theft protection.\nconvenience \\& professional growth: commuter benefits \\& certification \\& amp; training reimbursement.\ntime off: vacation, time off, sick leave \\& holidays.\nlegal \\& financial assistance: legal assistance, 401k plan, performance bonus, college fund, student loan refinancing.\n\n\\#li\\-sp1\n**location**\nedison, nj\n**job function**\ntechnology\n**role**\nsolution architect\n**job id**\n398037\n**desired skills**\npython\n**salary range**\n$170,000\\-$180,000 a year\ndesired candidate profile\n\n\n**qualifications** : bachelor of computer science",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Java Full Stack AI Engineer (Senior Software Engineer)",
        "company": "LTIMindtree",
        "location": "Tampa, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 23.3,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "RAG",
            "LLaMA",
            "Gemini",
            "Copilot",
            "Pinecone",
            "Prompt Engineering",
            "Kinesis",
            "Docker"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8915b5aa273f1b5b",
        "description": "**role description** **title: java full stack \\- ai engineer**\n\n\n**location: tampa, fl**\n\n\n\njoin our pioneering team as a full stack ai engineer and be at the forefront of building the next generation of intelligent ai powered products that redefine user experiences and solve complex business challenges this pivotal role empowers you to own the entire solution stack from intuitive frontend interfaces to robust backend infrastructure and cutting edge agentic ai systems you will play a critical role in designing developing and deploying high impact ai driven solutions driving engineering excellence and influencing our software architecture were seeking a passionate problem solver who thrives in a collaborative agile environment dedicated to crafting innovative solutions and contributing to vibrant technical community.\n\n\n**key responsibilities**\n\n\n* pioneer full stack ai application development design develop and deploy transformative end\\-to\\-end applications leveraging modern web frameworks react next.js fast api, python java with large language models llms at their core\n\n\n* architect agentic ai systems lead the design and implementation of sophisticated autonomous agents capable of multistep reasoning planning and dynamic task execution using frameworks like langchain langgraph or autogen\n\n\n* innovate rag pipeline design architect and optimize production grade retrieval augmented generation rag pipelines focusing on advanced embedding strategies efficient vector database management pinecone milvus chroma and superior retrieval performance\n\n\n* build scalable backend api integrations develop highly performant asynchronous backend services that seamlessly integrate llms external apis and tools ensuring exceptional reliability and ultralow latency\n\n\n* optimize data database management strategically manage structured and unstructured data overseeing both traditional sql nosql databases and advanced vector databases to enable context aware and intelligent ai systems\n\n\n* champion engineering excellence agile delivery drive best practices in code quality unit testing cicd and security act as a strong contributor within an agile software delivery team collaborating to achieve sprint goals and actively participating in the broader technical community and agile scrum processes\n\n\n**required qualifications:**\n\n\n* 5 years of progressive professional software engineering experience with a minimum of 2 years dedicated to building and deploying ai powered solutions\n\n\n* expert proficiency in python java or typescript javascript coupled with a profound understanding of data structures and object oriented principles\n\n\n* extensive hands on experience with large language models llms eg openai gpt anthropic claude gemini llama including finetuning techniques and advanced prompt engineering\n\n\n* proven expertise in designing and implementing rag architectures encompassing embedding strategies vector stores and semantic search\n\n\n* deep experience with microservices designing and implementing restful, graph ql, apis and practical application of containerization technologies docker, kubernetes, openshift\n\n\n* demonstrated experience with modern web stacks react next.js tailwind css javascript typescript etc\n\n\n* practical experience with agentic frameworks langchain, langgraph, autogen crew ai etc including designing implementing and orchestrating multiagent systems\n\n\n* solid understanding and practical application of frameworks such as spring boot fast api express etc\n\n\n* significant experience working within agile and iterative software delivery methodologies safe scrum kanban\n\n\n* proficiency in various database technologies including advanced sql and vector databases oracle postgresql mongodb aurora pinecone pg vector etc\n\n\n* experience with event driven design and architecture eg: sqs sns kinesis kafka spark flink rabbitmq\n\n\n* btech be eng degree or equivalent work experience\n\n\n* preferred qualifications\n\n\n* experience in designing and implementing complex multiagent collaboration architectures\n\n\n* familiarity with ai driven development tools eg: cursor claude copilot to enhance productivity\n\n\n* proven architectural experience in building horizontally scalable highly available highly resilient and low latency applications\n\n\n* experience with both on premise and public cloud infrastructure eg: openshift aws including infrastructure as code tools eg: terraform cloudformation\n\n\n* knowledge of security observability and monitoring tools eg: grafana prometheus splunk elk cloudwatch\n\n\n* prior experience mentoring and providing technical leadership to teams of 5 or more developers\n\n\n* familiarity with job schedulers eg apache workflow autosys cloudwatch\n\n **skills** **mandatory skills :** ai/genai research, java, react, spring\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $79,851\\.00 to $107,720\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI Engineer",
        "company": "Mount Tech",
        "location": "La Jolla, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 21.1,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "RAG",
            "LLaMA",
            "Copilot",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "FastAPI"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ba4cb3da56fe3cf3",
        "description": "**job overview**  \nwe are seeking an applied ai engineer to design, build, and deploy ai applications using large language models (llms), machine learning systems, and intelligent automation. this role focuses on integrating ai models into real\\-world products, building ai agents, and delivering scalable ai solutions that solve business problems. applied ai engineers typically work close to product teams to embed intelligence into applications rather than conduct pure research.\n\n**responsibilities**\n\n* design and build ai features such as chatbots, copilots, recommendation systems, and ai assistants\n* integrate llm apis (openai, anthropic, etc.) into backend and frontend systems\n* develop intelligent workflows using prompt engineering, rag, and agents\n* integrate vector databases (pinecone, pgvector, weaviate, etc.)\n* deploy ai models to cloud platforms (aws, azure, gcp)\n* optimize performance, latency, and cost of ai systems\n* monitor ai systems and ensure reliability in production\n* implement mlops pipelines for continuous deployment\n* automate workflows using ai orchestration frameworks\n* work with product managers and engineers to deliver ai features\n\n**preferred qualifications**\n\n* bachelor\u2019s or master\u2019s in computer science or related field\n* experience building ai products\n* experience deploying ai to production\n* experience with backend frameworks (node.js, fastapi, spring boot)\n* 3\\-5 years experience in software engineering\n\n**required technical skills**\n\nprogramming\n\n* python (required)\n* javascript / typescript\n* rest api development\n\nai / ml tools\n\n* openai api, anthropic, huggingface\n* langchain, llamaindex\n* pytorch, tensorflow (optional but useful)\n\ndatabases\n\n* sql / postgresql\n* vector databases (pinecone, chroma, pgvector)\n\ncloud and deployment\n\n* aws, gcp, or azure\n* docker\n* kubernetes (optional)\n* ci/cd pipelines\n\njob type: full\\-time\n\npay: $120,000\\.00 \\- $150,000\\.27 per year\n\nbenefits:\n\n* 401(k)\n* 401(k) matching\n* dental insurance\n* flexible schedule\n* flexible spending account\n* health insurance\n* health savings account\n* life insurance\n* paid time off\n* parental leave\n* retirement plan\n\nwork location: in person",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Data Engineer",
        "company": "Mariner Wealth Advisors",
        "location": "Overland Park, KS, US USA",
        "posted_at": "2026-02-23",
        "score": 17.8,
        "matched_keywords": [
            "RAG",
            "Cortex",
            "S3",
            "Redshift",
            "FastAPI",
            "CI/CD",
            "Terraform",
            "Git",
            "Snowflake",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=074ebeab3093055c",
        "description": "**job summary:**\n\nwe are seeking candidates for a full\\-time senior data engineer role based out of our overland park, ks headquarters (hybrid, one in\\-office day/wk). the candidate will join a team developing data\\-pipelines, building enterprise data products and ai tools, deploying api and integration platforms, and constructing system integrations for mariner\u2019s ecosystem of enterprise platforms. this position will require developers to weave together etl/elt tools and practices, cloud and legacy databases, apis and enterprise platforms, and automation systems to build scalable business tools, workflows, and products enabling mariner\u2019s front and back\\-office. you will drive our ci/cd and iac strategy, coach junior engineers and participate in code reviews, manage development projects, and enforce software lifecycle practices for the team. as a sr. data engineer, you will be working in conjunction with like\\-minded departments such as data management, business intelligence, operations, trading, compliance, and it. we are searching for positive, experienced, data obsessed engineer with a passion for the industry and a capacity to leverage a variety of technical disciplines to support mariner\u2019s growing portfolio of businesses.\n\n\n**essential duties and responsibilities:**\n\n* developing enterprise data platforms, data extraction, and mastering pipelines\n* systems integrations using python, api development, cloud platforms, and scripting languages\n* developing rest, graphql, and mcp apis in aws using python.\n* cloud architecture and deployments using iac tools\n* ci/cd pipelines and deployment automation\n* developing and maintaining orchestration configurations and scheduling\n* building tools and processes for data validation, testing, alerting, and monitoring\n* developer coaching, guiding pr reviews, and code quality enforcement\n* supporting data\\-operations and critical run\\-the\\-business automations (rotation\\-based)\n\n**required education and experience:**\n\n* a bachelors degree in computer science, engineering, or related discipline\n* eight years (\\&\\#43;) professional experience in software or data\\-engineering\n* five years (\\&\\#43;) experience in python development\n* five years (\\&\\#43;) experience in sql development\n* prior experience with snowflake\n* excellent verbal and written communication skills\n\n**preferred education and experience:**\n\n* experience building and deploying back\\-end services and apis\n* experience building and operating enterprise data pipelines\n* experience in the wealth management or financial services technology and platforms\n* experience managing tactical implementation projects, requirements refinement, and task tracking\n* experience working with stakeholders and subject matter experts on complex development projects\n\n**skills:**\n\n* developing and using cloud database platforms (snowflake, databricks, redshift, rds)\n* data analysis \\& pipelining tools (dbt, sql, python/pandas, snowspark)\n* managing automation\\\\orchestration platforms (windmill, airflow, dagster, activebatch)\n* deploying aws technologies s3\\\\ecs\\\\lambda, terraform and iac deployment tools\n* restful\\\\graphql\\\\mcp api development, integrations, and tooling (strawberry, fastapi, postman)\n* deploying devops and ci/cd processes using github\n* using common data formats and data exchanges (json, csv, odbc, ftp, s3, parquet)\n* enterprise systems such as salesforce, oracle fusion, portfolio management platforms, order\\-management systems, and reporting tools (tableau/streamlit)\n* experience with scripting languages (powershell, bash/sh, etc)\n* genai\\\\llm tooling and agent development (snowflake cortex)\n* leveraging ai enabled code development and agentic workflows (codex, claude)\n\n**supervisory responsibilities:**\n\n* coaching, pr reviews, project management and coordination\n\n**physical demands/requirements:**\n\nthe physical demands described here are representative of those requirements employees must meet to perform the essential functions of this job with or without reasonable accommodations. while performing job functions the employee is regularly required to sit, stand, write, review and type reports, compile data, operate a pc, communicate, listen, and assess information. the employee may move about the office complex, may travel to other office locations and may lift, push, pull or move 10 \\- 15 pounds. visual requirements include distant, close and color vision, and ability to adjust focus.\n\n\n**work environment:**\n\nthis job operates in a professional office environment. this role routinely uses standard office equipment such as laptop computers, photocopiers and smartphones. reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n\n\n**travel requirement:**\n\navailable for 1\\-day a week in\\-office (overland park hq)",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Data Engineer (1043) - DataSF",
        "company": "City and County of San Francisco",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 16.7,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "BigQuery",
            "Kinesis",
            "Terraform",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "PySpark",
            "Kafka"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=012bacb5fe848397",
        "description": "**company description** **department:** datasf  \n\n**job class:** 1043 senior data engineer  \n\n**salary range:** $153,686/year \\- $193,388/year  \n\n**role type:** permanent exempt  \n\n**hours:** full\\-time. the department has a hybrid work schedule.\n\n**about:**  \n\npermanent exempt: this is an exempt position excluded by the charter from the competitive civil service examination process, pursuant to the city and county of san francisco, charter section 10\\.104\\. it is considered \"at will\" and shall serve at the discretion of the appointing officer.  \n\n  \n\napplication opening: **monday, february 23, 2026**  \n\napplication closing: interested candidates are encouraged to apply as soon as possible, as this job announcement will close at any time, but not earlier than **sunday, march 2, 2026 at 11:59pm pst**\n\n**as part of the application process, you must also complete a supplemental questionnaire using this link by the filing deadline:** **https://forms.office.com/g/pgvqyrjydx**\n\n**the office of the city administrator** and its 25\\+ divisions and departments operate core internal and public\\-facing services in san francisco.  \n\n  \n\n**the office of the city administrator\u2019s mission and vision**  \n\nour vision is to lead the nation in public administration and to enable city departments to effectively deliver critical public services. we aim to help the city run better, to connect san francisco residents and constituents to the vital public services they seek, and to create a meaningful and diverse work culture that is the place of choice for people who are invested in a career in public service.\n\n\nwe are committed to ensuring that city services are inclusive, efficient, equitable, and culturally competent for san franciscans of all races, ethnic backgrounds, religions, and sextual orientations. this commitment requires comprehensive review and thorough analysis of existing practices and policies to remove barriers to real inclusion. we are also committed to ensuring that we have a safe, equitable, and inclusive workplace for individuals of all races. this includes creating opportunities for hiring, promotion, training, and development, for all employees, including but not limited to, black, indigenous, and people of color (bipoc).\n\n\nto learn more about our departments, divisions, and programs, visit: https://sf.gov/departments/city\\-administrator\n\n**about datasf:**\n\n\nwant to build robust and scalable data infrastructure to power data\\-driven city services? join the datasf team to empower and expand the use of data in government!\n\n**company description**\n\n\ndatasf\u2019s mission is to transform the way san francisco works with data. we believe that data, when harnessed effectively, can improve transparency, resident engagement, and government performance. we are committed to removing barriers and making it easier for all people to access services and knowledge, and we actively seek team members whose diverse life experiences reflect the residents we serve.\n\n **job description**  \n\ndatasf is seeking a senior data engineer with 3\\+ years of experience to join our growing team. reporting to the principal data engineer, you will be instrumental in designing, building, and maintaining the city's data infrastructure, enabling robust data pipelines and reliable data access for analytical and operational needs. this is an exciting position for someone eager to apply advanced data engineering techniques to complex urban challenges, contributing directly to san francisco's commitment to efficient, equitable, and ethical service delivery.\n\n\nlearn more about datasf\u2019s recent work on our blog. if you are an entrepreneurial and passionate data enthusiast, join our team to improve government through good use of data!\n\n**essential duties include, but are not limited to, the following:**\n\n* **platform administration**: manage our central snowflake data warehouse, including access control, security policies, resource monitoring, performance tuning, and cost optimization. administer our platform with a focus on data democratization and accessibility, while protecting privacy and security.\n* **pipeline developmen**t: build and maintain scalable and resilient pipelines to ingest and structure data from diverse sources. design infrastructure to support both streaming and batch processes, and both structured and unstructured data sources.\n* **infrastructure as code (iac)**: use terraform to define, deploy, and manage data infrastructure, ensuring our pipelines are reproducible, version\\-controlled, and production\\-ready.\n* **best practices \\& innovation**: champion and implement best practices for documentation, data modeling, warehouse architecture, sql optimization, and testing. think creatively to find new ways to improve our data platform's capabilities and efficiency. provide guidance to department partners on data engineering best practices.\n* **collaboration**: work closely with data scientists, analysts, product managers, software engineers, and nontechnical stakeholders in diverse domains to understand data requirements and build solutions that meet their needs.\n* **monitoring \\& support**: proactively monitor the health of the data platform and pipelines, troubleshoot issues, and ensure high standards of data quality and availability.\n\n**desirable qualifications**\n\n* **technical knowledge**\n\t+ hands\\-on experience administering and developing on managed cloud data platforms such as snowflake, bigquery, or databricks.\n\t+ demonstrated expertise in writing advanced, performant sql, and using tools like dbt for sql\\-based data transformation and modeling.\n\t+ strong programming skills in python (with libraries like pandas, pyspark) for data processing and automation.\n\t+ proficiency with an infrastructure as code tool, with a preference for terraform.\n\t+ experience building and deploying data pipelines using orchestration tools like azure data factory, airflow, dagster, or similar technologies.\n\t+ deep understanding of data warehousing concepts, data modeling, and modern elt principles.\n\t+ understanding of data governance, data security, and data privacy principles.\n\t+ experience with real\\-time data streaming technologies (e.g., kafka, kinesis, snowpipe).\n\t+ experience deploying and managing data pipelines for machine learning models.\n* **collaboration and communication skills**\n\t+ strong problem\\-solving skills with ability to design practical and effective data solutions\n\t+ excellent verbal and written communication skills, includingthe ability to explain technical concepts to non\\-technical stakeholders\n\t+ a collaborative mindset with enthusiasm to work across diverse, cross\\-functional teams\n* **mission alignment**\n\t+ commitment to equity, transparency, and ethical data use\n\t+ passion for public service and using data to improve government services\n\t+ empathy for san francisco\u2019s diverse communities and a drive to make data and services and more accessible for sf residents\n\t+ interest or experience in public sector data or social impact work\n\n  \n\n**qualifications** **education**:  \n\nan associate degree in computer science, computer engineering, information systems, or a closely related field from an accredited college or university or its equivalent in terms of total course credits/units \\[i.e., at least sixty (60\\) semester or ninety (90\\) quarter credits/units with a minimum of twenty (20\\) semester or thirty (30\\) quarter credits/units in one of the fields above or a closely\\-related field].\n\n**experience**:  \n\nthree (3\\) years of experience analyzing, installing, configuring, enhancing, and/or maintaining the components of an enterprise network.\n\n**substitution**:  \n\nadditional experience as described above may be substituted for the required degree on a year\\-for\\-year basis (up to a maximum of two (2\\) years). one (1\\) year is equivalent to thirty (30\\) semester units/ forty\\-five (45\\) quarter units with a minimum of 10 semester / 15 quarter units in one of the fields above or a closely related field.\n\n\ncompletion of the 1010 information systems trainee program may be substituted for the required degree.\n\n**verification:**  \n\n**please make sure it is clear in your application exactly how you meet the minimum qualifications.** applicants will be required to submit verification of qualifying education and experience during the recruitment and selection process. for information on how to verify experience and/or education requirements, including verifying foreign education credits or degree equivalency, please visit verification of experience and/or education.\n\n**note:** falsifying one\u2019s education, training, or work experience or attempted deception on the application may result in disqualification for this and future job opportunities with the city and county of san francisco.\n\n*applicants must meet the minimum qualification requirement by the final application deadline unless otherwise noted.*\n\n **additional information** **selection procedures**  \n\nthe selection process will include evaluation of applications in relation to minimum requirements and assessment of candidates\u2019 job\\-related knowledge, skills and abilities. depending on the number of applicants, the department may establish and implement additional screening mechanisms to evaluate candidate qualifications. this typically includes an oral interview and/or a written or performance exercise.\n\n\nif this becomes necessary, only those applicants whose qualifications most closely meet the department needs will be invited to continue in the selection process. applicants meeting the minimum requirements are not guaranteed advancement in the selection process.  \n\n  \n\nto find departments which use this classification, please see: https://sfdhr.org/sites/default/files/documents/forms\\-documents/position\\-counts\\-by\\-job\\-codes\\-and\\-department\\-fy\\-2022\\-23\\.pdf.\n\n**additional information regarding employment with the city and county of san francisco:**\n\n* information about the hiring process\n* conviction history\n* employee benefits overview\n* equal employment opportunity\n* disaster service worker\n* ada accommodation\n* veterans preference\n* right to work\n* copies of application documents\n* diversity statement\n\n**how to apply:**\n\n**applications for city and county of san francisco jobs are only accepted through an online process.** visit https://careers.sf.gov and begin the application process.\n\n* interested candidates are encouraged to apply as soon as possible, as this job announcement will close at any time, but not before **sunday, march 1, 2026** (11:59 pm, pst)\n* select the **\"apply now\"** button and follow instructions on the screen\n* **you must include a cover letter.**\n* **as part of the application process, you must also complete a supplemental questionnaire using this link:** **https://forms.office.com/g/pgvqyrjydx**\n\n\nfor best practices on the application process, please visit apply for jobs in the city and county of san francisco best practices guide. applicants may be contacted by email about this announcement and, therefore, it is their responsibility to ensure that their registered email address is accurate and kept up\\-to\\-date. also, applicants must ensure that email from ccsf is not blocked on their computer by a spam filter. to prevent blocking, applicants should set up their email to accept ccsf mail from the following addresses @sfgov.org, @sfdpw.org, @sfport.com, @flysfo.com, @sfwater.org, @sfdph.org, @asianart.org, @sfmta.com, @sfpl.org, @dcyf.org, @first5sf.org, @famsf.org, @ccsf.edu, @smartalerts.info, and @smartrecruiters.com).\n\n\napplicants will receive a confirmation email that their online application has been received in response to every announcement for which they file. applicants should retain this confirmation email for their records. **failure to receive this email means that the online application was not submitted or received.**\n\n\nall your information will be kept confidential according to eeo guidelines.\n\n**hr analyst information:** if you have any questions regarding this recruitment or application process, please contact connie poon at connie.poon@sfgov.org\n\n**condition of employment:**\n\n\nthe city and county of san francisco encourages women, minorities and persons with disabilities to apply. applicants will be considered regardless of their sex, race, age, religion, color, national origin, ancestry, physical disability, mental disability, medical condition (associated with cancer, a history of cancer, or genetic characteristics), hiv/aids status, genetic information, marital status, sexual orientation, gender, gender identity, gender expression, military and veteran status, or other protected category under the law.\n\n\nthe city and county of san francisco encourages women, minorities and persons with disabilities to apply. applicants will be considered regardless of their sex, race, age, religion, color, national origin, ancestry, physical disability, mental disability, medical condition (associated with cancer, a history of cancer, or genetic characteristics), hiv/aids status, genetic information, marital status, sexual orientation, gender, gender identity, gender expression, military and veteran status, or other protected category under the law.\n\n\nthe city and county of san francisco encourages women, minorities and persons with disabilities to apply. applicants will be considered regardless of their sex, race, age, religion, color, national origin, ancestry, physical disability, mental disability, medical condition (associated with cancer, a history of cancer, or genetic characteristics), hiv/aids status, genetic information, marital status, sexual orientation, gender, gender identity, gender expression, military and veteran status, or other protected category under the law.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Sr Field Engineer",
        "company": "Striim",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "BigQuery",
            "Kubernetes",
            "Git",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Kafka",
            "MySQL",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9fd06b655c3730ee",
        "description": "striim, (pronounced \"stream\" with two i's for integration and intelligence), is a unified data integration and streaming platform that connects clouds, data, and applications with unprecedented speed and simplicity to deliver the right data at the right time. striim is used by enterprise companies to monitor events across any environment, build applications that drive digital transformation, and leverage true real\\-time analytics to provide a superior experience to their customers. at our company, we believe and expect all of our employees to operate as one with unlimited potential and dignity.\n\n\nstriim is hiring a senior field engineer to be a technical leader who will engage with our customers throughout the life cycle of the striim platform. a sr field engineer will partner with a technical account manager to create a guided customer journey through all implementation phases, including new customer onboarding, solution architecture strategy, and technical enablement. this is a highly customer\\-focused role, driving toward operational excellence, continuous learning, and mission\\-critical innovation. this is a fully technical and hands\\-on position.\n\n\n**responsibilities**\n\n\n* onboard new striim customers, delivering learning, solution strategy, technical enablement, and implementation guidance during the deployment process.\n* lead customer technical teams during development phases to implement best practices and innovate enterprise adoption challenges.\n* support the seamless transition from pre\\-sales prototypes to post\\-sales production scale experiences.\n* collaborate with customer success and account management teams to conduct periodic health checks with customers, ensuring operational excellence with striim.\n* be the striim product expert, coach, and trusted technical advisor during deployment.\n* be the in\\-house customer champion, coordinating with striim customer support and stakeholders to drive technical advocacy, product evolution, and technical innovation.\n* proactively engage existing striim customers to realize technology value with further adoption of the striim platform, demonstrating new applications and use cases for striim.\n* increase internal knowledge base by developing and/or maintaining documentation related to striim implementations, best practices, and troubleshooting.\n* track customer issues, collaborate with support on customer\\-training\\-related matters, and work with customer to develop and track product feature requests (pfrs).\n* report on the status of key accounts, including escalations, to the manager and customer success manager (if applicable).\n\n\n**requirements**\n\n\n* a successful candidate must be based 100% remote somewhere in the continental united states\n* 7\\+ years of experience in customer\\-facing technical roles, e.g., sales engineer/pre\\-sales engineer, solutions architect, data engineer, customer success engineer\n* 7\\+ years of experience with database management systems software such as oracle, mysql, microsoft sql server, and sql programming language\n* 7\\+ years of experience programming languages such as java, python, shell scripting\n* 7\\+ years of experience or deep core knowledge of cloud infrastructure and data services such as amazon aws, microsoft azure and google cloud\n* working proficiency and knowledge of kubernetes/gke\n* deep knowledge in working with configuring, and troubleshooting operating systems based on unix/linux and windows\n* experience working with message systems such as kafka and jms\n* experience in working with enterprise data environments and mission critical it systems is desirable\n* knowledge of snowflake, google bigquery or databricks is a plus\n* excellent written, verbal, and presentation communication skills\n* bachelor's degree in information systems, computer science, computer engineering, or information systems\n\n\n**benefits**\n\n\n* competitive salary and pre\\-ipo stock options\n* comprehensive health care plans (medical, dental and vision), including medical and dependent fsa\n* paid time off (vacation, sick \\& public holidays)\n* the chance to contribute to and shape an upbeat, fully engaged culture\n\n\n**compensation**\n\n\n\n$170,000 \\- $190,000 usd on an annualized basis. in addition to base pay, this role offers the opportunity to earn commission\\-based rewards.\n\n\n\napplications will be reviewed on a rolling basis and accepted until the position is filled.\n\n\nour company culture fosters entrepreneurship and nurtures our team members to grow with the company. come join a silicon valley startup focused on delivering a product that's loved by its customers and primed to be a core part of the cloud data stack.\n\n\n\nwe are an equal opportunity employer, and we value diversity at our company.it is in our best interest to continue to foster an environment of diversity, equity, and inclusion to bring the most value to our workforce, customers, and partners. all applicants are considered for employment without attention to race, color, religion, sex, age, marital status, sexual orientation, gender identity, national origin, veteran status, or disability status.\n\n\n\nfor more information on striim's privacy policy, click here.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Associate - Data Engineer - Global Master Data",
        "company": "Eli Lilly",
        "location": "Indianapolis, IN, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Glue",
            "Redshift",
            "Data Lake",
            "CI/CD",
            "Git",
            "Databricks",
            "Redshift",
            "Kafka"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=326b602342da8867",
        "description": "at lilly, we unite caring with discovery to make life better for people around the world. we are a global healthcare leader headquartered in indianapolis, indiana. our employees around the world work to discover and bring life\\-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. we give our best effort to our work, and we put people first. we\u2019re looking for people who are determined to make life better for people around the world.\n\n**what you will do**\n\n\nas a data engineer you will be located in **indianapolis, in** and responsible for designing, developing, and maintaining the data solutions that ensure the availability and quality of data for analysis and/or business transactions. they design and implement efficient data storage, processing and retrieval solutions for datasets and build data pipelines, optimize database designs, and work closely with data scientists, architects, and analysts to ensure data quality and accessibility. data engineers require strong skillsets in data integration, acquisition, cleansing, harmonization, and transforming data. they play a crucial role in transforming raw data into datasets designed for analysis which enable organizations to unlock valuable insights for decision making.\n\n* design, build, and maintain scalable and reliable data pipelines for batch and real\\-time processing.\n* own **incident response and resolution**, including root cause analysis and post\\-mortem reporting for data failures and performance issues.\n* develop and optimize data models, etl/elt workflows, and data integration across multiple systems and platforms.\n* collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.\n* implement data governance, security, and quality standards across data assets.\n* lead end\\-to\\-end data engineering projects and contribute to architectural decisions.\n* design and implement cloud\\-native solutions on aws (preferred) using tools such as aws glue, emr, and databricks. experience with azure or gcp is a plus.\n* promote best practices in coding, testing, and deployment.\n* monitor, troubleshoot, and improve performance and reliability of data infrastructure.\n* automate manual processes and identify opportunities to optimize data workflows and reduce costs.\n\n**how you will succeed:**\n\n* **deliver scalable solutions** by designing robust data pipelines and architectures that meet performance and reliability standards.\n* **collaborate effectively** with cross\\-functional teams to turn business needs into technical outcomes.\n* **lead with expertise**, mentoring peers and driving adoption of best practices in data engineering and cloud technologies.\n* **continuously improve systems** through automation, performance tuning, and proactive issue resolution.\n* **communicate with clarity** to ensure alignment across technical and non\\-technical stakeholders.\n\n**your basic qualifications**\n\n* bachelor\u2019s degree in computer science, information technology, management information systems or similar stem fields\n* at least 2 years of experience in data engineering\n* 1\\+ years of experience using github and ci/cd pipelines for code deployment.\n* proven experience in architecting and building high\\-performance, scalable data pipelines following data lakehouse, data warehouse, and data mart standards.\n* strong expertise in data modelling (both oltp and olap), managing large datasets, and implementing secure, compliant data governance practices.\n* ***qualified applicants must be authorized to work in the united states on a full\\-time basis. lilly will not provide support for or sponsor work authorization or visas for this role, including but not limited to f\\-1 cpt, f\\-1 opt, f\\-1 stem opt, j\\-1, h\\-1b, tn, o\\-1, e\\-3, h\\-1b1, or l\\-1\\.***\n\n***remote candidates will not be considered***\n\n**what you should bring:**\n\n* strong proficiency in **sql and** python.\n* hands\\-on experience with **cloud platforms** (aws, azure, or gcp) and tools like glue, emr, redshift, lambda, or databricks.\n* deep understanding of **etl/elt workflows**, data modelling, and data warehousing concepts.\n* familiarity with **big data and streaming frameworks** (e.g., apache spark, kafka, flink).\n* knowledge of **data governance, security, and quality practices**.\n* working knowledge of databricks for building and optimizing scalable data pipelines and analytics workflows.\n* experience with **ci/cd, version control (git)**, and infrastructure\\-as\\-code tools is a plus.\n* a **problem\\-solving mindset**, attention to detail, and a passion for clean, maintainable code.\n* strong **communication and collaboration** skills to work with both technical and non\\-technical stakeholders.\n* domain experience in healthcare, pharmaceutical (customer master, product master, alignment master, activity, consent), or regulated industries is a plus.\n* partner with and influence vendor resources on solution development to ensure understanding of data and technical direction for solutions as well as delivery\n* aws certified\n* databricks certified\n* familiarity with ai/ml workflows and integrating machine learning models into data pipelines\n* experience in leading a small team of data engineers and providing technical mentorship.\n* ability to collaborate with business stakeholders to translate key business requirements into scalable technical solutions.\n* familiarity with security models and developing solutions on large\\-scale, distributed data systems.\n\n**about the tech at lilly organization:**\n\n\ntech at lilly builds and maintains capabilities using cutting edge technologies like most prominent tech companies. what differentiates tech at lilly is that we create new possibilities through tech to advance our purpose \u2013 creating medicines that make life better for people around the world, like data driven drug discovery and connected clinical trials. we hire the best technology professionals from a variety of backgrounds, so they can bring an assortment of knowledge, skills, and diverse thinking to deliver innovative solutions in every area of the enterprise.\n\n\nlilly is dedicated to helping individuals with disabilities to actively engage in the workforce, ensuring equal opportunities when vying for positions. if you require accommodation to submit a resume for a position at lilly, please complete the accommodation request form (https://careers.lilly.com/us/en/workplace\\-accommodation) for further assistance. please note this is for individuals to request an accommodation as part of the application process and any other correspondence will not receive a response.\n\n\nlilly is proud to be an eeo employer and does not discriminate on the basis of age, race, color, religion, gender identity, sex, gender expression, sexual orientation, genetic information, ancestry, national origin, protected veteran status, disability, or any other legally protected status.\n\n  \n\nour employee resource groups (ergs) offer strong support networks for their members and are open to all employees. our current groups include: africa, middle east, central asia network, black employees at lilly, chinese culture network, japanese international leadership network (jiln), lilly india network, organization of latinx at lilly (ola), pride (lgbtq\\+ allies), veterans leadership network (vln), women\u2019s initiative for leading at lilly (will), enable (for people with disabilities). learn more about all of our groups.\n\n\nactual compensation will depend on a candidate\u2019s education, experience, skills, and geographic location. the anticipated wage for this position is\n\n\n$64,500 \\- $184,800\nfull\\-time equivalent employees also will be eligible for a company bonus (depending, in part, on company and individual performance). in addition, lilly offers a comprehensive benefit program to eligible employees, including eligibility to participate in a company\\-sponsored 401(k); pension; vacation benefits; eligibility for medical, dental, vision and prescription drug benefits; flexible benefits (e.g., healthcare and/or dependent day care flexible spending accounts); life insurance and death benefits; certain time off and leave of absence benefits; and well\\-being benefits (e.g., employee assistance program, fitness benefits, and employee clubs and activities).lilly reserves the right to amend, modify, or terminate its compensation and benefit programs in its sole discretion and lilly\u2019s compensation practices and guidelines will apply regarding the details of any promotion or transfer of lilly employees.\n\n\n\\#wearelilly",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "IT Data Platform Engineer",
        "company": "National Retail Systems, Inc",
        "location": "Lyndhurst, NJ, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Glue",
            "Athena",
            "CI/CD",
            "Git",
            "Snowflake",
            "Power BI",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4da5376ea9ed63ff",
        "description": "**company overview:**\n\n\n\nnrs is a leading provider of transportation \\& supply chain solutions. as a family\\-owned and operated company, nrs has delivered smart logistics solutions to numerous fortune 500 companies spanning over 70 years. whether it\u2019s nrt, keystone, keystone fresh, or keystone capacity, our innovative energy drives us towards new and valuable solutions for our clients, even as we continuously grow and strengthen our network. we are dedicated to creating a culture that empowers the individual and offers our associates the opportunity to apply their unique skill to the challenges facing our clients. in the office, the warehouse, or on the road, it is this commitment to our innovative spirit that unites us in common mission to push boundaries in the logistics industry.\n\n\n**job overview:**\n\nas a member of the data \\& analytics team within the information technology organization, reporting to the manager of data \\& analytics, the data platform engineer is directly responsible for building, operating, and automating internal platforms that abstract infrastructure, tooling, and operational complexity into reliable, self\\-service capabilities for data engineers, analysts, and data scientists. this individual will play a critical role in scaling enterprise data platforms and tools, standardizing ci/cd processes, ml ops and deployment frameworks, monitoring and optimizing systems to ensure performance and reduce costs, as well as streamlining data management activities for our data owners and stewards, such as data quality and metadata management automations within the platform.\n\n\n**why this job\u2019s a big deal:**\n\n\n\nas part of a small data and analytics team you will have the opportunity to perform a wide range of tasks and manage a modern data technology stack including snowflake on aws, fivetran, dbt, github, power bi, and related integrations. you will be the point person for the configuration and operation of all the data platforms and tools, responsible for security administration, cost management, integration, monitoring, resiliency, dataops processes, mlops processes, and automations using python and sql scripts. you\u2019ll also wear multiple hats on the team, backing up team members in their responsibilities including data engineering tasks and helping translate business needs into technical solutions consistent with enterprise architecture best practices. we value ownership and accountability while building our external products to be the best in the world for our customers. we need an innovator that will solve complex technical problems, drive new data initiatives and introduce positive change in a highly engaged, collaborative environment, helping us leverage the value of data for our customers, partners, and employees.\n\n\n**job description:**\n\n\n**lead, manage, and hold accountable**\n\n\n* support and promote the company values, culture, and hr processes\n* establish technical standards, best practices, and architectural guidelines for data and analytics\n* provide thought leadership on emerging technologies, tools, and methodologies in data engineering, platform engineering, and analytics.\n* mentor and develop team members in good data engineering principles and platform features, fostering technical growth and career development.\n* ensure adherence to data governance, security, and compliance policies across all data and platform initiatives.\n* champion a culture of collaboration, innovation, and high\\-quality engineering practices.\n\n\n**data \\& analytics related functions**\n\n\n* architect and maintain enterprise\\-scale data pipelines, ingestion, storage, and transformation processes using fivetran, dbt, and github.\n* implement and enforce data quality, lineage, observability, and certification processes with the platform.\n* integrate metadata from across the platform to support data quality, metadata management, and end\\-to\\-end observability.\n* integrate and automate activities and processes to help data architects and owners manage security approvals, retention, master data, metadata, and reference data policies.\n* oversee deployment and operationalization of analytics, reporting, and machine learning platforms.\n* monitor, support, and troubleshoot issues with data platforms, tools, connections, pipelines, security, and data users.\n* drive adoption of modern data engineering and machine learning practices, including ci/cd, testing, ml ops, and platform automation.\n\n\n**deliver innovative solutions**\n\n\n* identify opportunities to leverage data and analytics for business impact and operational efficiency.\n* design and deliver scalable, reusable data products and platforms that enable self\\-service analytics.\n* prototype and evaluate emerging technologies, frameworks, and tools for adoption in enterprise data platforms.\n* automate manual processes and standardize workflows to reduce friction and improve agility.\n* ensure solutions are secure, compliant, and optimized for performance and cost.\n\n\n**business stakeholder \\& relationship management**\n\n\n* collaborate with business and team to understand data needs, translate requirements, and prioritize initiatives.\n* serve as a trusted advisor for analytics strategy and platform capabilities.\n* communicate progress, challenges, and risks to leadership and business stakeholders in a clear and actionable manner.\n* foster strong partnerships with cross\\-functional teams, including it, finance, and operations.\n* advocate for data\\-driven decision\\-making across the organization.\n* backup teammates in their duties and perform other tasks as assigned.\n\n\n**what success looks like:**\n\n\n* in the first 15 months:\n* decrease data product development time by 25% through automation of manual processes\n* automate platform administration tasks by 80% through integrations and stored procedures\n* decrease data ingestion and compute costs by 20% through optimization and improved ingestion processes\n* automate capture of 10 data quality metrics in snowflake for all data products\n* standardize 7 data quality test package in dbt for data engineers to add to all data products\n* implement end\\-to\\-end monitoring and real\\-time notifications for 90% of failures across fivetran, snowflake, and dbt\n\n\n**qualifications:**\n\n\n* bachelor\u2019s degree in computer science, or a related field, or equivalent practical experience.\n* 5\\+ years of experience in platform engineering, database administration, or related technical roles.\n* 3\\+ years of experience in data engineering\n* 5\\+ years of python.\n* 5\\+ years of sql.\n* 2\\+ years of dbt, with demonstrated ability to design modular, decoupled, and reusable systems using sound software engineering principles.\n* 2\\+ years of aws experience, including lambda, s3, step functions, glue, athena, sns, eventbridge, or other relevant data\\-related services.\n* exposure to machine learning platforms or advanced analytics use cases (preferred).\n* familiarity with power bi tenant administration and security (preferred).\n* experience with jira and agile delivery principles (preferred).\n\n\n**required skills:**\n\n\n* design and operate enterprise\\-scale data platforms (data warehouses, lakehouses, or hybrid architectures); proficiency with snowflake is a plus.\n* build self\\-service platforms, reusable frameworks, or internal developer tools.\n* strong understanding of distributed systems, columnar performance tuning, and scalability considerations.\n* familiarity with security best practices, including iam, secrets management, and least\\-privileged access.\n* conceptual understanding of networking and server infrastructure (enough to integrate with networking/infrastructure teams).\n* excellent written documentation and code commenting.\n* excellent verbal communication and interpersonal skills.\n* understanding of data governance and management principles.\n* continuous learning mindset; stays updated with emerging data analytics technologies and techniques.\n* willingness to travel 10\u201325% depending on remote/onsite work arrangements.\n\n\n***eeo statement:***\n\n\n***nrs is an equal opportunity employer. we do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.***\n\n\n\n\\#nrsind",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist III",
        "company": "Fanatics",
        "location": "US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "Redshift",
            "BigQuery",
            "MLflow",
            "Git",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a21ec81d7d224fd0",
        "description": "**about fanatics markets**\nfanatics markets is the real\\-money prediction and trading app where you can invest in moments you care about. built on a secure platform, we let users predict real\\-world outcomes and trade on events they actually follow \\- from sports and entertainment to political elections and beyond. our mission is to redefine how fans engage with the moments and markets that matter most. we're looking for the right people to help us build the future of prediction markets.\n\n**role overview**\nwe are looking for a data scientist ii to join the fanatics markets data team. this is a high\\-impact role where you will bridge the gap between complex behavioral data and actionable business strategy. our culture is built on high ownership and accountability, and we value fast iteration balanced with strong statistical rigor, clear documentation, and reproducible research. you\u2019ll thrive here if you are business\\-impact\\-driven and enjoy the challenge of connecting betting behavior to long\\-term value signals in a collaborative, innovative environment.\n\n**responsibilities**\n* partner with product and crm teams to design, evaluate, and scale a rigorous experimentation roadmap (a/b testing, hypothesis testing) to improve promotional efficiency and bonus roi.\n* build and maintain production\\-grade models for churn, ltv, segmentation, and cross\\-sell propensity, moving from initial feature engineering to automated monitoring.\n* analyze customer journeys and high\\-frequency betting patterns to improve lifecycle marketing and reduce early\\-lifecycle churn.\n* support live testing and deploy models into production environments using modern orchestration tools (airflow, mlflow).\n* create dashboards and automated insight pipelines for key kpis\n* present clear, data\\-driven recommendations to stakeholders across london, dublin, and u.s. offices.\n* contribute to internal standards for reproducible analysis, version control (git), and model monitoring.\n\n**qualifications required**\n* 5 plus years of experience as a data scientist or quantitative analyst in tech, gaming, or finance.\n* strong experience with python (specifically pandas, numpy, scikit\\-learn) and advanced sql.\n* strong applied statistics and experimental design knowledge (a/b testing, hypothesis testing, causal reasoning).\n* hands\\-on experience with modern data warehouses (snowflake, bigquery, or redshift)\n* familiarity with dbt, uplift modeling, and causal ml techniques\n* working experience within aws or gcp environments\n* ability to translate complex quantitative findings into clear insights for non\\-technical audiences.\n* degree in a quantitative field (statistics, computer science, economics, engineering, or similar).\n\n*preferred*\n* experience in sports betting, igaming, or fintech, particularly working with crm and lifecycle marketing datasets.\n* familiarity with a/b testing at scale, uplift modeling, and causal inference.\n* exposure to ml pipeline orchestration (airflow, databricks, mlflow).\n\n**salary range:** $129,200\u2013 $212,500 usd per year\nthe base salary for this role is based on job\\-related knowledge, skills, and experience and may vary depending on the successful candidate\u2019s geographic location. remote employees may also be eligible for a home office setup stipend. for information about our benefits, please visit https://benefitsatfanatics.com/\n\ndepending on the role, your interview and onboarding experience may include in\\-person components, such as onsite interviews or launching into better: live\u2014a multi\\-day cultural immersion in new york city for full\\-time, non\\-seasonal hires. these sessions are designed to build connection and bring our culture to life, though specific travel and participation requirements will be confirmed based on your role and location. your recruiter will provide clear guidance at each stage of the process.\n\nlaunched in 2021, fanatics betting and gaming is the online and retail sports betting subsidiary of fanatics, a global digital sports platform. the fanatics sportsbook is available to 95% of the addressable online sports bettor market in the u.s. fanatics casino is currently available online in michigan, new jersey, pennsylvania and west virginia. fanatics betting and gaming operates twenty\\-two retail sports betting locations, including the only sportsbook inside an nfl stadium at northwest stadium. fanatics betting and gaming is headquartered in new york with offices in denver, leeds and dublin.\n\n\nfanatics is building a leading global digital sports platform. we ignite the passions of global sports fans and maximize the presence and reach for our hundreds of sports partners globally by offering products and services across fanatics commerce, fanatics collectibles, and fanatics betting \\& gaming, allowing sports fans to buy, collect, and bet. through the fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its sportsbook and igaming platform. fanatics has an established database of over 100 million global sports fans; a global partner network with approximately 900 sports properties, including major national and international professional sports leagues, players associations, teams, colleges, college conferences and retail partners, 2,500 athletes and celebrities, and 200 exclusive athletes; and over 2,000 retail locations, including its lids retail stores. our more than 22,000 employees are committed to relentlessly enhancing the fan experience and delighting sports fans globally.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "DevOps Engineer",
        "company": "kp reddy",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=31ba26cff0eff16a",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a devops engineer to architect, implement, and maintain the cloud infrastructure that powers our innovative aec technology solutions. you'll work closely with our engineering team to build robust ci/cd pipelines, optimize cloud\\-native architectures, and ensure our systems scale reliably to meet the demanding requirements of construction and engineering workflows. this role bridges the gap between development and operations, enabling rapid prototyping while maintaining production\\-grade reliability.\n\n\nthis is an opportunity to shape the infrastructure foundation that enables digital transformation in one of the world's most critical industries, leveraging modern cloud patterns, automation, and observability to deliver real impact.\n\n**key responsibilities**\n========================\n\n* design and maintain infrastructure\\-as\\-code using cdk and cloudformation for aws environments, ensuring reproducible and scalable deployments.\n* build and optimize ci/cd pipelines for containerized microservices written in python, typescript, and go.\n* implement and manage amazon ecs clusters for orchestrating production workloads and development environments.\n* build, configure, and maintain observability stack using industry best practices.\n* manage data infrastructure including rds, s3, and streaming services for aec data pipelines.\n* implement security best practices, including secrets management, iam policies, and network segmentation.\n* collaborate with development teams to optimize application performance and deployment strategies.\n* document infrastructure patterns, runbooks, and operational procedures for team knowledge sharing.\n* participate in on\\-call rotation and incident response, driving root cause analysis and prevention.\n\n**requirements**\n================\n\n* bachelor's degree in computer science, engineering, or related field, or equivalent practical experience.\n* 2\\-4 years of hands\\-on experience with aws services (ec2, ecs, lambda, rds, s3, cloudformation/cdk).\n* strong proficiency with docker and container orchestration using kubernetes.\n* experience with infrastructure\\-as\\-code tools (cloudformation/cdk preferred, terraform acceptable).\n* solid understanding of ci/cd principles and tools (github actions, jenkins, or gitlab ci).\n* strong background in python scripting.\n* experience with observability and monitoring tools (prometheus, grafana, and opentelemetry).\n* strong linux/unix administration skills and networking fundamentals.\n* excellent problem\\-solving abilities and capacity to troubleshoot complex distributed systems.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* knowledge of gitops practices and tools (argocd, flux).\n* experience with data engineering infrastructure (apache kafka, airflow, dbt).\n* familiarity with compliance frameworks and security scanning tools.\n* experience supporting ai/ml workloads and gpu\\-accelerated computing.\n* understanding of aec industry workflows and data formats (ifc, revit, autocad).\n* aws certifications (solutions architect, devops engineer, or similar).\n\n**what you\u2019ll gain**\n====================\n\n* opportunity to build cloud infrastructure that transforms how the built environment is designed and delivered.\n* exposure to cutting\\-edge devops practices and tools in a rapidly evolving technology landscape.\n* mentorship from senior engineers with deep expertise in cloud architecture and aec technology.\n* the chance to work on diverse technical challenges spanning real\\-time data processing, ai/ml pipelines, and enterprise integrations.\n* direct impact on products that influence billion\\-dollar construction projects and critical infrastructure.\n\n**why us?**\n===========\n\n\nfor us, devops isn't just about keeping the lights on\u2014you'll be architecting the foundation for innovation in an industry ripe for transformation. we believe in infrastructure as a product, where automation, observability, and developer experience are first\\-class concerns. our engineers have the autonomy to experiment with emerging technologies while maintaining the reliability our enterprise clients demand. if you're passionate about building resilient systems that enable others to create amazing things, we want to hear from you.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "DevOps Engineer",
        "company": "kp reddy",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=dce2730f22d939ec",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a devops engineer to architect, implement, and maintain the cloud infrastructure that powers our innovative aec technology solutions. you'll work closely with our engineering team to build robust ci/cd pipelines, optimize cloud\\-native architectures, and ensure our systems scale reliably to meet the demanding requirements of construction and engineering workflows. this role bridges the gap between development and operations, enabling rapid prototyping while maintaining production\\-grade reliability.\n\n\nthis is an opportunity to shape the infrastructure foundation that enables digital transformation in one of the world's most critical industries, leveraging modern cloud patterns, automation, and observability to deliver real impact.\n\n**key responsibilities**\n========================\n\n* design and maintain infrastructure\\-as\\-code using cdk and cloudformation for aws environments, ensuring reproducible and scalable deployments.\n* build and optimize ci/cd pipelines for containerized microservices written in python, typescript, and go.\n* implement and manage amazon ecs clusters for orchestrating production workloads and development environments.\n* build, configure, and maintain observability stack using industry best practices.\n* manage data infrastructure including rds, s3, and streaming services for aec data pipelines.\n* implement security best practices, including secrets management, iam policies, and network segmentation.\n* collaborate with development teams to optimize application performance and deployment strategies.\n* document infrastructure patterns, runbooks, and operational procedures for team knowledge sharing.\n* participate in on\\-call rotation and incident response, driving root cause analysis and prevention.\n\n**requirements**\n================\n\n* bachelor's degree in computer science, engineering, or related field, or equivalent practical experience.\n* 2\\-4 years of hands\\-on experience with aws services (ec2, ecs, lambda, rds, s3, cloudformation/cdk).\n* strong proficiency with docker and container orchestration using kubernetes.\n* experience with infrastructure\\-as\\-code tools (cloudformation/cdk preferred, terraform acceptable).\n* solid understanding of ci/cd principles and tools (github actions, jenkins, or gitlab ci).\n* strong background in python scripting.\n* experience with observability and monitoring tools (prometheus, grafana, and opentelemetry).\n* strong linux/unix administration skills and networking fundamentals.\n* excellent problem\\-solving abilities and capacity to troubleshoot complex distributed systems.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* knowledge of gitops practices and tools (argocd, flux).\n* experience with data engineering infrastructure (apache kafka, airflow, dbt).\n* familiarity with compliance frameworks and security scanning tools.\n* experience supporting ai/ml workloads and gpu\\-accelerated computing.\n* understanding of aec industry workflows and data formats (ifc, revit, autocad).\n* aws certifications (solutions architect, devops engineer, or similar).\n\n**what you\u2019ll gain**\n====================\n\n* opportunity to build cloud infrastructure that transforms how the built environment is designed and delivered.\n* exposure to cutting\\-edge devops practices and tools in a rapidly evolving technology landscape.\n* mentorship from senior engineers with deep expertise in cloud architecture and aec technology.\n* the chance to work on diverse technical challenges spanning real\\-time data processing, ai/ml pipelines, and enterprise integrations.\n* direct impact on products that influence billion\\-dollar construction projects and critical infrastructure.\n\n**why us?**\n===========\n\n\nfor us, devops isn't just about keeping the lights on\u2014you'll be architecting the foundation for innovation in an industry ripe for transformation. we believe in infrastructure as a product, where automation, observability, and developer experience are first\\-class concerns. our engineers have the autonomy to experiment with emerging technologies while maintaining the reliability our enterprise clients demand. if you're passionate about building resilient systems that enable others to create amazing things, we want to hear from you.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Chesterfield, MO, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=65d238209415bd35",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Jacksonville, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=08527261a275aa62",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Lewisville, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=dc66fc514f39b781",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Boise, ID, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=003cc1fe2c6f4d82",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ceeb3d3f63ddd02b",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Chesterfield, MO, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6394fdb64160187b",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Lewisville, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4bf2ba0a6a2d2ef7",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Boise, ID, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6c579e60e29085a0",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9566bb3277fc5b3e",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Rogers, AR, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8b988c6d60df206b",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nle groupe acosta est un collectif r\u00e9unissant les agences de d\u00e9tail, de marketing et de services alimentaires les plus fiables : acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services et product connections. ensemble, nous mettons les consommateurs en contact avec les marques qu'ils aiment par le biais de la vente omnicanale, du merchandising, de la promotion des marques et du marketing int\u00e9gr\u00e9.  \n\n\n  \n\nnous sommes conscients que nos collaborateurs sont \u00e0 la base de notre succ\u00e8s. c'est pourquoi nous donnons la priorit\u00e9 \u00e0 leur croissance, \u00e0 leur d\u00e9veloppement et \u00e0 leur bien\\-\u00eatre afin de les aider \u00e0 atteindre leur plein potentiel. gr\u00e2ce \u00e0 des programmes con\u00e7us pour favoriser l'\u00e9quilibre entre vie professionnelle et vie priv\u00e9e, nous offrons des opportunit\u00e9s adapt\u00e9es \u00e0 votre style de vie et \u00e0 vos ambitions, que vous soyez \u00e0 la recherche d'une flexibilit\u00e9 \u00e0 temps partiel ou d'une progression de carri\u00e8re \u00e0 temps plein.  \n\n\n  \n\nle groupe acosta est un employeur qui souscrit au principe de l'\u00e9galit\u00e9 des chances et veille \u00e0 ce que les candidats handicap\u00e9s b\u00e9n\u00e9ficient d'am\u00e9nagements raisonnables. si un am\u00e9nagement raisonnable est n\u00e9cessaire, veuillez contacter askhr@acosta.com. n'oubliez pas d'indiquer \"accommodement pour les candidats\" dans l'objet de votre courriel afin d'acc\u00e9l\u00e9rer le traitement de la demande.  \n\n\n  \n\nle groupe acosta estime en toute bonne foi que le salaire annuel minimum et maximum ou la fourchette de r\u00e9mun\u00e9ration horaire pour cette opportunit\u00e9 sont exacts et raisonnables au moment de la publication.  \n\n\n  \n\nen postulant, vous acceptez notre politique de confidentialit\u00e9 et nos conditions d'utilisation",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Jacksonville, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=40ae88bd702dcb13",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI/Machine Learning Data Engineer",
        "company": "Acosta Group",
        "location": "Rogers, AR, US USA",
        "posted_at": "2026-02-23",
        "score": 15.6,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Copilot",
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e490a596b2e349ab",
        "description": "acosta group is seeking a highly skilled and adaptable ai/ml data engineer to design, develop, deploy, and support ai\\-powered solutions that enhance acosta\u2019s internal operations and decision\\-making capabilities. this role combines deep technical expertise with a strong focus on scalability, reliability, and integration across enterprise platforms such as azure ai, power platform, and copilot studio.\n\n\nthe ideal candidate will have hands\\-on experience with cloud platforms (preferably azure), agentic frameworks (e.g., langchain), large language models (llms), and generative ai techniques including fine\\-tuning and retrieval\\-augmented generation (rag). proficiency in python and sql is essential, along with a solid understanding of vector databases, semantic search, and dimensional data modeling. experience with databricks, serverless deployments (e.g., azure functions), containerization (docker/kubernetes), and ci/cd pipelines using azure devops is highly valued.\n\n\nbeyond technical implementation, the ai engineer will provide second\\- and third\\-level support, mentor business users and citizen developers, and collaborate with cross\\-functional teams to embed ai into business\\-critical workflows. the role also supports acosta\u2019s broader ai initiatives by enabling infrastructure readiness, contributing to the ai center of excellence, and integrating intelligent automation tools across the enterprise.\n\n  \n\nthis is a **hybrid role** based in our office environment in either **jacksonville, fl, lewisville, tx** or **mississauga/toronto, on** . candidates will be expected to work as much as 3 days per week onsite depending on proximity to corporate hubs.\n\n\nessential functions of this position\n\n* design, develop, and deploy ai/ml models and intelligent automation solutions that support internal business operations and strategic initiatives\n* api development \\& integration: ability to build and integrate rest/graphql apis to serve ai/ml models.\n* vectorization \\& embeddings: expertise in vector databases and semantic search.\n* collaborate with cross\\-functional teams to identify, scope, and implement ai use cases using tools such as azure ai, power platform ai builder, and copilot studio\n* integrate ai capabilities into enterprise applications and workflows, including power apps, power automate, and microsoft 365\n* monitor and maintain ai solutions in production, ensuring performance, reliability, and responsible ai practices\n* provide second\\- and third\\-level support for ai\\-related incidents, enhancements, and deployments\n* partner with infrastructure and security teams to ensure readiness and compliance for ai workloads\n* contributes to the development of reusable ai components, templates, and frameworks to accelerate adoption\n* mentor and support business users and citizen developers in building and scaling ai\\-powered solutions\n* participate in the evaluation of new ai tools, platforms, and methodologies to support innovation and continuous improvement\n* document solution architectures, workflows, and best practices to support knowledge sharing and operational continuity\n* support the ai center of excellence by contributing to governance, enablement, and enterprise\\-wide ai strategy\n* maintain and enforce change control processes.\n* assist in developing and maintaining operational standards and best practices.\n* leverage ai and automation tools for proactive monitoring, anomaly detection, and incident response.\n* collaborate with ai solutions engineers and the ai center of excellence to support infrastructure for ai workloads and pilot initiatives.\n* ensure infrastructure readiness for hybrid cloud and ai platforms (e.g., azure ml, vmware private ai, nvidia ai enterprise).\n* perform other duties as required and/or assigned.\n\n\napplicants must be **legally authorized to work in the united states and/or canada** without the current or future need for **employer\\-sponsored work authorization**.\n\n\nminimum education and work experience\n\n* bachelor\u2019s degree in technology industry is preferred or equivalent work experience\n* high school diploma/ ged is required\n* two (2\\) or more years of professional experience in machine learning, data science, or software engineering.\n* microsoft exam ai\\-102: designing and implementing an azure ai solutions (preferred).\n\n\nknowledge, skills, and abilities requirements\n\n* programming: proficiency in python (must) and sql for data manipulation, querying, and automation.\n* data warehousing: solid understanding of data warehouse concepts, including dimensional modeling (fact and dimension tables).\n* agentic frameworks: knowledge of ai agent frameworks and orchestration (e.g., langchain).\n* llms \\& generative ai: strong understanding of large language models, fine\\-tuning, and rag pipelines.\n* knowledge of machine learning fundamentals, including supervised and unsupervised learning, nlp, and generative ai\n* familiarity with mlops practices and tools for model lifecycle management and monitoring\n* understanding of responsible ai principles, including fairness, transparency, and data privacy\n* ability to rapidly learn and apply new ai tools and frameworks in a fast\\-evolving technology landscape\n* experience with cloud\\-based development environments, particularly microsoft azure\n* ability to support ai solutions in production, including troubleshooting, performance tuning, and incident response\n* excellent communication skills (both written and oral) combined with strong interpersonal skills.\n* strong analytical skills and thought processes combined with the ability to be flexible and work analytically in a problem\\-solving environment.\n\n  \n\nphysical requirements\n\n* seeing\n* listening\n\n\nthe incumbent(s) in this position should exhibit the following acosta values:\n\n* people minded \u2013 must show dignity and respect to all people\n* integrity \u2013 must exemplify the highest degree of ethical behavior\n* results oriented \u2013 must show passion, pride and commitment to succeed\n* trust \u2013 must be honest, sincere and confident\n* teamwork \u2013 must build trusting relationships\n* innovation \u2013 must progress through a combination of creativity, common sense and vision\n* balance \u2013 must maintain an optimistic attitude and keep perspective on what is important in life.\n\n\nacosta group is a collective uniting the most trusted retail, marketing, and foodservice agencies\u2014acosta, actionlink, core foodservice, crossmark, mosaic, premium retail services, and product connections. together, we connect consumers with the brands they love through omnichannel selling, merchandising, brand advocacy, and integrated marketing.  \n\n\n  \n\nwe recognize our associates are the foundation of our success. that\u2019s why we prioritize your growth, development, and well\\-being to help you reach your full potential. with programs designed to support a fulfilling work\\-life balance, we offer opportunities that fit your lifestyle and ambitions\u2014whether you\u2019re looking for part\\-time flexibility or full\\-time career advancement.  \n\n\n  \n\nready for a career path that\u2019s as unique as you? discover your path at acosta group!  \n\n\n  \n\nacosta group is an equal opportunity employer and will ensure that applicants with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed, please contact askhr@acosta.com. be sure to include \"applicant accommodation\" in the subject of your email to expedite the request.  \n\n\n  \n\nacosta group believes in good faith that the minimum and maximum annual salary or hourly compensation range for this opportunity is accurate and reasonable at the time of posting.  \n\n  \n\nthe acosta group utilizes e\\-verify for validating the ability to work in the united states for all job candidates. if you want more information on what this entails and your rights as a job applicant, please use the link provided to access information on our use of e\\-verify and your right to work. employer resources (e\\-verify.gov)\n\n  \n\n  \n\nby applying, you agree to our privacy policy and terms and conditions of use.  \n\n\n  \n\n**\\#discoveryourpath**",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior AI Engineer",
        "company": "Texplorers Inc.",
        "location": "Lewisville, TX, US USA",
        "posted_at": "2026-02-22",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "RAG",
            "Hugging Face",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "CI/CD",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9aa5392b1ae4eac1",
        "description": "### **job description**\n\n\nthe senior ai engineer, based in lewisville, tx, will design and architect scalable ai and machine learning solutions that align with enterprise goals while leading the development, training, fine\u2011tuning, and optimization of advanced ml models and large language models for real\u2011world applications.\n\n\n\nlocation: lewisville, tx\n\n\n\nposition type: full\\-time\n\n\n\nhours: 9:00 am to 5:00 pm weekdays (monday \u2013 friday)\n\n\n### **key responsibilities**\n\n\n* design and architect scalable artificial intelligence and machine learning solutions aligned with enterprise business objectives and technical requirements.\n* develop, train, fine\\-tune, evaluate, and optimize machine learning models and large language models (llms) for real\\-world applications.\n* implement advanced generative ai solutions including conversational ai, document processing, summarization, and intelligent automation systems.\n* design and build retrieval\\-augmented generation (rag) pipelines to enhance knowledge\\-based ai systems using structured and unstructured enterprise data.\n* create ai agents and workflow automation solutions to streamline operational processes and improve efficiency.\n* develop end\\-to\\-end ai pipelines covering data ingestion, data preprocessing, feature engineering, model training, evaluation, deployment, and continuous improvement.\n* integrate ai models into enterprise applications using rest apis, microservices architecture, and event\\-driven systems.\n* deploy and maintain ai solutions on cloud platforms such as microsoft azure, aws, or gcp using scalable and resilient architectures.\n* work with vector databases, embeddings, and semantic search technologies to support intelligent retrieval and context\\-aware ai systems.\n* implement mlops best practices including version control, automated testing, continuous integration and deployment, monitoring, and model lifecycle management.\n* optimize model performance, scalability, and resource utilization to ensure efficient production deployments.\n* collaborate with data engineers, software developers, product managers, and business stakeholders to translate requirements into technical ai solutions.\n* perform research and evaluation of emerging ai technologies and frameworks to improve system capabilities and innovation.\n* develop reusable ai components, frameworks, and best practices to support enterprise\\-wide ai adoption.\n* ensure compliance with data governance, security standards, privacy regulations, and ethical ai principles.\n* provide technical leadership, mentorship, and guidance to engineering teams, including code reviews and architecture reviews.\n* troubleshoot complex ai system issues and implement solutions to improve performance and reliability.\n* document ai architectures, workflows, and technical designs to support knowledge sharing and maintainability.\n* participate in strategic planning and contribute to defining ai roadmaps aligned with organizational goals.\n### **technical experience**\n\n\n**minimum degree requirement:** bachelor\u2019s degree in computer science, information technology, software engineering, or a closely related field.\n\n\n**minimum work experience:**\n\n\n\n3 years total professional experience in artificial intelligence, machine learning, or software engineering.\n\n\n**minimum years of experience must include the following:**\n\n\n* 3 years of experience in programming skills in languages such as python, c\\#, java, or similar.\n* hands\\-on experience with machine learning and deep learning frameworks (e.g., tensorflow, pytorch, hugging face, or equivalent).\n* experience developing and deploying ai solutions including natural language processing (nlp), predictive analytics, or generative ai applications.\n* knowledge of large language models (llms), prompt engineering, or ai automation workflows.\n* experience integrating ai models into production systems using apis or microservices architecture.\n* familiarity with cloud platforms such as microsoft azure, aws, or google cloud platform.\n* understanding of data preprocessing, feature engineering, and working with structured and unstructured datasets.\n* knowledge of software development lifecycle practices, version control, and ci/cd concepts.\n* strong analytical, problem\\-solving, and technical documentation skills.\n* ability to collaborate effectively with cross\\-functional teams in an enterprise environment.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer New",
        "company": "Convey",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7f0c38e749fbda51",
        "description": "**why project44?**\n\n\n\nat project44, we believe in better.\n\n\n\nwe challenge the status quo because we know a better supply chain isn\u2019t just possible\u2014it\u2019s essential. better for our customers. better for their business. better for the world. with our decision intelligence platform, *movement*, we\u2019re redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you\u2019re driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a software engineer \\- backend to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world\u2019s logistics networks.\n\n\n**key accountabilities: what you\u2019ll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team\u2019s services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 2\\-4 years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters four days a week. together, we\u2019re building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we\u2019re focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe\u2019re building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we\u2019d love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don\u2019t meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role\u2019s requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world\u2019s leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer New",
        "company": "Convey",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5a7e1e1ef2ff4bf9",
        "description": "**why project44?**\n\n\n\nat project44, **we believe in better.**\n\n\n\nwe challenge the status quo because we know a better supply chain isn\u2019t just possible\u2014it\u2019s essential. better for our customers. better for their business. better for the world. with our **decision intelligence platform,** ***movement*****,** we\u2019re redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you\u2019re driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a **senior software engineer \\- backend** to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world\u2019s logistics networks.\n\n\n**key accountabilities: what you\u2019ll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team\u2019s services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n* mentor and support other engineers through code reviews, pairing, and technical guidance\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 4\\+ years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters four days a week. together, we\u2019re building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we\u2019re focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe\u2019re building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we\u2019d love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don\u2019t meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role\u2019s requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world\u2019s leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Engineer II",
        "company": "Corteva Agriscience",
        "location": "Indianapolis, IN, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "Git",
            "Databricks",
            "PySpark",
            "Kafka"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=84bbb3c9d11dfa7f",
        "description": "corteva agriscience llc seeks a full\\-time data engineer ii based in indianapolis, in. the data engineer ii is responsible for leveraging expertise in data engineering and machine learning to implement advanced data analytics and artificial intelligence (ai) technologies to address scientific and complex business problems and drive innovation for corteva. this position enhances data accessibility and quality, enabling seamless integration of large\\-scale datasets into data products, machine learning models, and artificial intelligence applications. requirements: master\u2019s degree or equivalent in information management, computer science, or a related field and 1 year related agriculture exp. must also have 12 months of exp. with: (1\\) develop etl pipelines using modern data stack (spark, trino, dagster, dbt, iceberg); (2\\) data modelling, storage and processing of life sciences/agriculture data (genomics, satellite imagery, geospatial, biological); (3\\) support and develop powerbi, rshiny, spotfire dashboards that utilize lakehouse data; (4\\) use dagster for data orchestration for scheduling data pipelines and use gitlab for code version control, and docker, kubernetes for deployment; and (5\\) utilize the following technologies: python (pyspark), trino/sparksql, aws (s3, rds, ec2, ecs, eks), azure (data factory, adls, powerbi, databricks), dbt, dagster, docker, kubernetes, kafka, fastapi, and django. will accept experience gained before, during or after master\u2019s program. employer will accept experience gained concurrently. position includes a telecommute benefit within commuting distance to indianapolis, in corteva office, as directed. salary: $117,104 to $141,000/year. please apply online at https://apply.corteva.com/careers/.\n\n\n**benefits \u2013 how we\u2019ll support you:**\n\n* numerous development opportunities offered to build your skills\n* be part of a company with a higher purpose and contribute to making the world a better place\n* health benefits for you and your family on your first day of employment\n* four weeks of paid time off and two weeks of well\\-being pay per year, plus paid holidays\n* excellent parental leave which includes a minimum of 16 weeks for mother and father\n* future planning with our competitive retirement savings plan and tuition reimbursement program\n* \n* check out life at corteva! www.linkedin.com/company/corteva/life\n\n  \n\nare you a good match? apply today! we seek applicants from all backgrounds to ensure we get the best, most creative talent on our team.\n\n\ncorteva agriscience is an equal opportunity employer. we are committed to embracing our differences to enrich lives, advance innovation, and boost company performance. qualified applicants will be considered without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, military or veteran status, pregnancy related conditions (including pregnancy, childbirth, or related medical conditions), disability or any other protected status in accordance with federal, state, or local laws.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer",
        "company": "project44",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2ad7acbb70aad17a",
        "description": "**why project44?**\n\n\n\nat project44, **we believe in better.**\n\n\n\nwe challenge the status quo because we know a better supply chain isn't just possible\u2014it's essential. better for our customers. better for their business. better for the world. with our **decision intelligence platform,** ***movement*****,** we're redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you're driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a **senior software engineer \\- backend** to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world's logistics networks.\n\n\n**key accountabilities: what you'll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team's services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n* mentor and support other engineers through code reviews, pairing, and technical guidance\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 4\\+ years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters **three days a week**. together, we're building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we're focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe're building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we'd love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don't meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role's requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world's leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer",
        "company": "project44",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=166c9d26347adcb3",
        "description": "**why project44?**\n\n\n\nat project44, we believe in better.\n\n\n\nwe challenge the status quo because we know a better supply chain isn't just possible\u2014it's essential. better for our customers. better for their business. better for the world. with our decision intelligence platform, *movement*, we're redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you're driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a software engineer \\- backend to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world's logistics networks.\n\n\n**key accountabilities: what you'll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team's services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 2\\-4 years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters **three days a week**. together, we're building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we're focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe're building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we'd love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don't meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role's requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world's leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Full Stack Magento Developer \u2013 Remote (Canada)",
        "company": "Jarvis Recruitment",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Prompt Engineering",
            "Docker",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "MySQL",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5e6328c1040c5967",
        "description": "**at hire with jarvis, we help organizations of all sizes hire the right people, in the right seats, at the right time.**  \n\n  \n\nwe are hiring a **senior full stack developer** to lead the development of high performance magento 2 and venia pwa storefronts in a fully remote environment. this is an opportunity to own both backend architecture and frontend experience, working at the intersection of scalable commerce systems and modern progressive web applications. if you thrive on solving complex integration challenges and building seamless user experiences that convert, this role was built for you.\n\n\n\nyou will play a key role in shaping a future ready ecommerce ecosystem. from custom magento 2 modules to react powered venia components, your work will directly impact performance, scalability, and user engagement across global storefronts.\n\n\n**what you will be responsible for**  \n\n**web application storefront development and integration**\n\n\n* design and build reusable react components within magento pwa studio and venia.\n* integrate magento 2 and adobe commerce backends using graphql and rest apis.\n* optimize api calls, payload sizes, and rendering performance to deliver fast, responsive storefronts.\n* ensure smooth integrations with pos, erp, and third party systems.\n* make high level architectural decisions and guide technical direction.\n\n\n**magento backend development**\n\n\n* design and develop custom magento 2 modules to support evolving business needs.\n* maintain clean, modular php code using strong oop principles\n* ensure consistent data flow between backend services and the pwa frontend.\n* improve caching strategies, query performance, and overall scalability.\n* troubleshoot and resolve complex integration challenges with minimal downtime.\n\n\n**venia pwa and ui implementation**\n\n\n* develop scalable venia components that integrate with magento page builder and extensions.\n* translate ui and ux designs into responsive, accessible, cross browser interfaces.\n* optimize storefront performance using lighthouse and chrome devtools.\n* implement tracking, monitoring, and error handling for reliability and insights.\n\n\n**quality assurance and best practices**\n\n\n* write clean, maintainable, and testable code aligned with industry standards.\n* participate in and lead code reviews.\n* implement unit and integration tests to ensure application stability.\n* test solutions across linux, windows, and mobile environments.\n\n\n**leadership and innovation**\n\n\n* mentor medior and junior developers\n* promote coding standards and best practices across the team.\n* continuously evaluate new tools and technologies to improve workflows.\n* drive process improvements that increase productivity and code quality.\n\n\n**ai is a hard requirement**  \n\nthis team expects developers who actively integrate ai into their workflow.\n\n\n\nyou should be building and experimenting with:\n\n\n* ai assisted development workflows\n* prompt engineering for code generation and optimization\n* logging and tooling integrations powered by ai\n* chatbots and automation tools\n* rapid prototyping using ai driven approaches\n\n\nif you are not already incorporating ai into your daily development process, this role will not be the right fit. top performers in this team are actively leveraging ai to move faster and build smarter.\n\n\n**what you bring**\n\n\n* minimum 5 years of react.js frontend development experience\n* minimum 5 years of magento 2 or adobe commerce backend development experience\n* strong expertise in javascript es6\\+, react, php 7 and 8, mysql, html, css, ajax\n* deep knowledge of magento rest and graphql apis\n* solid oop principles and scalable architecture practices\n* experience with git branching, merging, and version control best practices\n* adobe commerce professional certification required or willingness to obtain within 6 months\n\n\nstrong english communication skills and the ability to collaborate with international teams are essential.  \n\n  \n\n**nice to have**\n\n\n* experience with aws deployments and scalable infrastructure\n* ci/cd pipelines such as jenkins, github actions, or bitbucket\n* docker and containerized environments\n* redis, sessions, and caching strategies\n* testing libraries such as jest or react testing library\n* experience with service workers and pwa architecture\n\n\n**compensation and work setup**\n\n\n* fully remote position (canada)\n* full time employment\n* competitive salary based on experience and certification level\n* performance based growth opportunities\n* work with an international, collaborative engineering team\n* opportunity to work on complex, large scale commerce systems\n\n\nthis is more than a coding role. it is a chance to influence architecture, mentor others, and build high performance ecommerce systems that scale.  \n\nif you are a senior engineer who combines magento depth, react expertise, and ai driven development practices, we would love to hear from you.\n\n\n\napply today or reach out to hire with jarvis to learn more about this opportunity.\n\n\n\nby applying for this job you consent to hire with jarvis to hold personal data and we may use this to contact you, either through sms or email. you can read our privacy policy at https://hirewithjarvis.com/privacy\\-policy",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer New",
        "company": "Convey",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f5324f4c6c59141c",
        "description": "**why project44?**\n\n\n\nat project44, **we believe in better.**\n\n\n\nwe challenge the status quo because we know a better supply chain isn\u2019t just possible\u2014it\u2019s essential. better for our customers. better for their business. better for the world. with our **decision intelligence platform,** ***movement*****,** we\u2019re redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you\u2019re driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a **senior software engineer \\- backend** to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world\u2019s logistics networks.\n\n\n**key accountabilities: what you\u2019ll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team\u2019s services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n* mentor and support other engineers through code reviews, pairing, and technical guidance\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 4\\+ years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters **three days a week**. together, we\u2019re building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we\u2019re focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe\u2019re building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we\u2019d love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don\u2019t meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role\u2019s requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world\u2019s leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer New",
        "company": "Convey",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=121b212372d04928",
        "description": "**why project44?**\n\n\n\nat project44, we believe in better.\n\n\n\nwe challenge the status quo because we know a better supply chain isn\u2019t just possible\u2014it\u2019s essential. better for our customers. better for their business. better for the world. with our decision intelligence platform, *movement*, we\u2019re redefining how global supply chains operate. by transforming fragmented logistics data into real\\-time, ai\\-powered insights, we empower companies to connect instantly, see clearly, act decisively, and automate intelligently. our supply chain ai enhances visibility, drives smarter execution, and unlocks next\\-gen applications that keep businesses moving forward. headquartered in chicago, il with a 2nd hq in bengaluru, india we are powered by a diverse global team that is tackling the toughest logistics challenges with innovation, urgency, and purpose. if you\u2019re driven to solve meaningful problems, leverage ai to scale rapidly, drive impact daily, and be part of a high\\-performance team \u2013 we should talk.\n\n\n**description:**\n\n\n\nproject44 is looking for a software engineer \\- backend to join our engineering team. you will work in a fast\\-paced agile environment designing, building, and implementing best\\-in\\-class integrations to accelerate how project44 connects to the world\u2019s logistics networks.\n\n\n**key accountabilities: what you\u2019ll work on**\n\n\n* design, build, and operate backend systems that power decision intelligence capabilities, using ai and analytics to help customers make informed, timely, and confident supply chain decisions\n* build services and workflows that reduce manual effort and enable action, empowering customers to collaborate more effectively with carriers and partners across the supply chain\n* own and modernize core platform services, such as identity and access management (iam) and search infrastructure, ensuring they remain secure, scalable, and extensible as new decision intelligence capabilities are introduced\n* transform large\\-scale logistics data into actionable insights and outcomes, supporting use cases such as recommendations, decision support, and automated actions\n* partner closely with product, data, and design to translate ambiguous customer problems into well\\-scoped, scalable technical solutions\n* build and operate systems on google cloud platform (gcp), leveraging managed services to deliver reliable and scalable backend solutions\n* take shared ownership of development, testing, deployment, and operations for the systems owned by your team\n* participate in a team on\\-call rotation, debugging and resolving issues across your team\u2019s services\n* improve observability, monitoring, and operational tooling to ensure system reliability and a strong on\\-call experience\n* contribute to technical design discussions, code reviews, and documentation, helping set a high bar for quality and maintainability within the team\n\n\n**qualities**\n\n\n* strong communication skills, with the ability to clearly explain technical decisions through design docs, discussions, and reviews, and collaborate effectively with both technical and non\\-technical partners\n* a strong analytical approach to problem\\-solving, with comfort navigating ambiguity and making tradeoffs explicit\n* a sense of ownership, curiosity, and resilience \\- comfortable voicing ideas, incorporating feedback, and iterating toward better solutions\n\n\n**qualifications**\n\n\n* 2\\-4 years of professional experience building large\\-scale, cloud\\-based systems\n* strong programming and debugging skills with proficiency in java\n\n\n**experience in:**\n\n\n* designing and building scalable java\\-based microservices\n* designing and consuming apis and web services (rest, openapi/swagger)\n* working with databases, including nosql and/or relational databases (e.g., mongodb, dynamodb, postgres, or similar)\n* working in an agile software development environment\n* using ai\\-assisted development tools for code generation, refactoring, and debugging (e.g., cursor, claude, github copilot, or similar)\n* integrating ai\\-assisted workflows into daily development while maintaining high standards for code quality, security, and correctness\n\n\n**desirable and helpful (but not necessary) are experience in:**\n\n\n* building event\\-driven systems and stream\\-processing pipelines (kafka, kinesis, or similar)\n* containerization and orchestration (docker, kubernetes)\n* designing or working with agentic ai systems, including multi\\-step or autonomous llm\\-powered workflows\n* familiarity with agentic ai frameworks or patterns (e.g., task planning, tool invocation, stateful agents, workflow orchestration)\n* building platform or framework\\-level solutions that enable ai\\-driven automation and decision\\-making\n\n\n**work authorization:** candidates must be authorised to work in the us without current or future employer\\-sponsored visa support.\n\n\n**in\\-office commitment:**our office is where ideas spark, connections thrive, and innovation comes alive. we are looking for candidates who are enthusiastic and committed to joining our team on\\-site, in our beautiful headquarters **three days a week**. together, we\u2019re building something extraordinary\u2014learn, grow, and thrive in our fast\\-paced, transformative environment.\n\n\n**diversity \\& inclusion**\n\n\n\nat project44, we're designing the future of how the world moves and is connected through trade and global supply chains. as we work to deliver a truly world\\-class product and experience, we are also intentionally building teams that reflect the unique communities we serve. we\u2019re focused on creating a company where all team members can bring their authentic selves to work every day.\n\n\n\nwe\u2019re building a company that every one of us at project44 is proud to work for, and our journey of becoming a more diverse, equitable and inclusive organization, where all have a sense of belonging, is shaped through the actions of our leadership, global teams, and individual team members. we are resolute in our belief that each team member has an equal responsibility to mold and uphold our culture.\n\n\n\nproject44 is an equal opportunity employer seeking to enrich our work environment by creating opportunities for individuals of all backgrounds and experiences to thrive. if you share our values and our passion for helping the way the world moves, we\u2019d love to review your application!\n\n\n\nfor any accommodation needed during the hiring process, please email recruiting@project44\\.com. even if you don\u2019t meet 100% of the above job description you should still seriously consider applying. studies show that you can still be considered for a role if you meet just 50% of the role\u2019s requirements.\n\n  \n\n**more about project44**\n\n\n\nsince 2014, project44 has been transforming the way one of the largest, most important global industries does business. as transportation and logistics continue to evolve and customer expectations around delivery become more demanding, industry technology must rise to the occasion. in just a few short years, we have created a digital infrastructure that eliminates the inefficiencies caused by dated technology and manual processes. our advanced visibility platform is used by the world\u2019s leading brands to track shipments, collaborate with supply chain partners, drive operational efficiencies, and create outstanding customer experiences.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer - AI, Building Design",
        "company": "kp reddy",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "PyTorch",
            "YOLO",
            "AWS SageMaker",
            "Azure ML",
            "MLflow",
            "Docker",
            "Kubernetes",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c1a9a570fd3e2cff",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a senior ai engineer to lead the development of automated building design systems and advance the state\\-of\\-the\\-art in ai/ml applications for the built environment. you'll architect and implement generative design algorithms, computer vision systems, and optimization models that fundamentally transform how buildings are\n\n\nconceived, designed, and validated. this role sits at the intersection of deep learning, computational geometry, and architectural design, creating ai systems that augment human creativity while respecting engineering constraints and building codes.\n\n\nthis is a rare opportunity to apply cutting\\-edge ai research to one of humanity's most fundamental challenges: creating better spaces for people to live, work, and thrive.\n\n**key responsibilities**\n========================\n\n* design and implement generative ai models for automated building design, including floor plan generation, facade design, and structural optimization using state\\-of\\-the\\-art architectures (diffusion models, transformers, gans).\n* develop computer vision pipelines for design and drawing analysis using modern frameworks like yolo, sam, and nerf\\-based 3d reconstruction.\n* build graph neural networks and geometric deep learning models for structural analysis and mep (mechanical, electrical, plumbing) system optimization.\n* create reinforcement learning systems for multi\\-objective building optimization (energy efficiency, cost, occupant comfort, sustainability metrics).\n* integrate ai models with industry\\-standard bim tools (revit, rhino/grasshopper) through custom apis and plugins.\n* deploy production ml pipelines using modern mlops practices, including experiment tracking (weights \\& biases, mlflow), model versioning, and a/b testing frameworks.\n* implement physics\\-informed neural networks for building performance simulation and predictive modeling.\n* collaborate with architects and engineers to ensure ai systems produce practical, code\\-compliant, and constructible designs.\n* lead research initiatives and publish findings to establish us as a thought leader in aec ai innovation.\n\n**requirements**\n================\n\n* master's degree or phd in computer science, ai/ml, computational design, or related field (or equivalent industry experience).\n* 3\\-5\\+ years of hands\\-on experience building and deploying ml models in production environments.\n* deep expertise with modern deep learning frameworks (pytorch preferred).\n* strong foundation in computer vision, 3d geometry processing, and spatial reasoning algorithms.\n* experience with generative ai models (vaes, gans, diffusion models, transformers) and their practical applications.\n* proficiency in python and scientific computing libraries (numpy, scipy, scikit\\-learn, open3d, trimesh).\n* experience with cloud ml platforms (aws sagemaker, vertex ai, or azure ml) and distributed training frameworks.\n* understanding of optimization techniques (genetic algorithms, gradient\\-based optimization, constraint satisfaction).\n* strong software engineering practices and experience with containerization (docker) and orchestration (kubernetes).\n* excellent communication skills to translate complex ai concepts to domain experts and stakeholders.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* experience with computational design tools (grasshopper, dynamo) and parametric modeling.\n* familiarity with building information modeling (bim) standards and ifc data schemas.\n* knowledge of graph neural networks (pytorch geometric, dgl) for structural and spatial analysis.\n* experience with physics simulation engines (mujoco, isaac sim) or fea integration.\n* background in multi\\-agent reinforcement learning for complex system optimization.\n* contributions to open\\-source ml projects or published research in relevant venues (neurips, icml, cvpr, or domain\\-specific conferences).\n* experience with point cloud processing and 3d scene understanding (pointnet\\+\\+, dgcnn).\n* understanding of construction workflows and building codes.\n\n**what you\u2019ll gain**\n====================\n\n* the opportunity to define and build ai systems that will reshape a $10 trillion global industry.\n* access to unique datasets and real\\-world problems at the intersection of ai and the built environment.\n* collaboration with leading architects, engineers, and construction professionals who are eager to embrace ai transformation.\n* resources to pursue cutting\\-edge research while maintaining a focus on practical, deployable solutions.\n* mentorship from industry veterans who understand both the technical and business aspects of aec technology.\n* the freedom to experiment with emerging ai architectures and techniques in a high\\-impact domain.\n\n**why us?**\n===========\n\n\nhere, ai engineers aren't building demos\u2014you'll be creating systems that influence how real buildings get designed and built. we believe the aec industry is on the cusp of an ai revolution, and we're positioning ourselves at the forefront. our team has the domain expertise to identify the right problems, the technical depth to solve them, and the industry connections to deploy solutions at scale. if you're passionate about using ai to create a more sustainable, efficient, and beautiful built environment, this is where you can make it happen.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer - AI, Building Design",
        "company": "kp reddy",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "PyTorch",
            "YOLO",
            "AWS SageMaker",
            "Azure ML",
            "MLflow",
            "Docker",
            "Kubernetes",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=30cf3c854ab272eb",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a senior ai engineer to lead the development of automated building design systems and advance the state\\-of\\-the\\-art in ai/ml applications for the built environment. you'll architect and implement generative design algorithms, computer vision systems, and optimization models that fundamentally transform how buildings are\n\n\nconceived, designed, and validated. this role sits at the intersection of deep learning, computational geometry, and architectural design, creating ai systems that augment human creativity while respecting engineering constraints and building codes.\n\n\nthis is a rare opportunity to apply cutting\\-edge ai research to one of humanity's most fundamental challenges: creating better spaces for people to live, work, and thrive.\n\n**key responsibilities**\n========================\n\n* design and implement generative ai models for automated building design, including floor plan generation, facade design, and structural optimization using state\\-of\\-the\\-art architectures (diffusion models, transformers, gans).\n* develop computer vision pipelines for design and drawing analysis using modern frameworks like yolo, sam, and nerf\\-based 3d reconstruction.\n* build graph neural networks and geometric deep learning models for structural analysis and mep (mechanical, electrical, plumbing) system optimization.\n* create reinforcement learning systems for multi\\-objective building optimization (energy efficiency, cost, occupant comfort, sustainability metrics).\n* integrate ai models with industry\\-standard bim tools (revit, rhino/grasshopper) through custom apis and plugins.\n* deploy production ml pipelines using modern mlops practices, including experiment tracking (weights \\& biases, mlflow), model versioning, and a/b testing frameworks.\n* implement physics\\-informed neural networks for building performance simulation and predictive modeling.\n* collaborate with architects and engineers to ensure ai systems produce practical, code\\-compliant, and constructible designs.\n* lead research initiatives and publish findings to establish us as a thought leader in aec ai innovation.\n\n**requirements**\n================\n\n* master's degree or phd in computer science, ai/ml, computational design, or related field (or equivalent industry experience).\n* 3\\-5\\+ years of hands\\-on experience building and deploying ml models in production environments.\n* deep expertise with modern deep learning frameworks (pytorch preferred).\n* strong foundation in computer vision, 3d geometry processing, and spatial reasoning algorithms.\n* experience with generative ai models (vaes, gans, diffusion models, transformers) and their practical applications.\n* proficiency in python and scientific computing libraries (numpy, scipy, scikit\\-learn, open3d, trimesh).\n* experience with cloud ml platforms (aws sagemaker, vertex ai, or azure ml) and distributed training frameworks.\n* understanding of optimization techniques (genetic algorithms, gradient\\-based optimization, constraint satisfaction).\n* strong software engineering practices and experience with containerization (docker) and orchestration (kubernetes).\n* excellent communication skills to translate complex ai concepts to domain experts and stakeholders.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* experience with computational design tools (grasshopper, dynamo) and parametric modeling.\n* familiarity with building information modeling (bim) standards and ifc data schemas.\n* knowledge of graph neural networks (pytorch geometric, dgl) for structural and spatial analysis.\n* experience with physics simulation engines (mujoco, isaac sim) or fea integration.\n* background in multi\\-agent reinforcement learning for complex system optimization.\n* contributions to open\\-source ml projects or published research in relevant venues (neurips, icml, cvpr, or domain\\-specific conferences).\n* experience with point cloud processing and 3d scene understanding (pointnet\\+\\+, dgcnn).\n* understanding of construction workflows and building codes.\n\n**what you\u2019ll gain**\n====================\n\n* the opportunity to define and build ai systems that will reshape a $10 trillion global industry.\n* access to unique datasets and real\\-world problems at the intersection of ai and the built environment.\n* collaboration with leading architects, engineers, and construction professionals who are eager to embrace ai transformation.\n* resources to pursue cutting\\-edge research while maintaining a focus on practical, deployable solutions.\n* mentorship from industry veterans who understand both the technical and business aspects of aec technology.\n* the freedom to experiment with emerging ai architectures and techniques in a high\\-impact domain.\n\n**why us?**\n===========\n\n\nhere, ai engineers aren't building demos\u2014you'll be creating systems that influence how real buildings get designed and built. we believe the aec industry is on the cusp of an ai revolution, and we're positioning ourselves at the forefront. our team has the domain expertise to identify the right problems, the technical depth to solve them, and the industry connections to deploy solutions at scale. if you're passionate about using ai to create a more sustainable, efficient, and beautiful built environment, this is where you can make it happen.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer - AI, Building Design",
        "company": "kp reddy",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "PyTorch",
            "YOLO",
            "AWS SageMaker",
            "Azure ML",
            "MLflow",
            "Docker",
            "Kubernetes",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e50a761ac02df3ba",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a senior ai engineer to lead the development of automated building design systems and advance the state\\-of\\-the\\-art in ai/ml applications for the built environment. you'll architect and implement generative design algorithms, computer vision systems, and optimization models that fundamentally transform how buildings are\n\n\nconceived, designed, and validated. this role sits at the intersection of deep learning, computational geometry, and architectural design, creating ai systems that augment human creativity while respecting engineering constraints and building codes.\n\n\nthis is a rare opportunity to apply cutting\\-edge ai research to one of humanity's most fundamental challenges: creating better spaces for people to live, work, and thrive.\n\n**key responsibilities**\n========================\n\n* design and implement generative ai models for automated building design, including floor plan generation, facade design, and structural optimization using state\\-of\\-the\\-art architectures (diffusion models, transformers, gans).\n* develop computer vision pipelines for design and drawing analysis using modern frameworks like yolo, sam, and nerf\\-based 3d reconstruction.\n* build graph neural networks and geometric deep learning models for structural analysis and mep (mechanical, electrical, plumbing) system optimization.\n* create reinforcement learning systems for multi\\-objective building optimization (energy efficiency, cost, occupant comfort, sustainability metrics).\n* integrate ai models with industry\\-standard bim tools (revit, rhino/grasshopper) through custom apis and plugins.\n* deploy production ml pipelines using modern mlops practices, including experiment tracking (weights \\& biases, mlflow), model versioning, and a/b testing frameworks.\n* implement physics\\-informed neural networks for building performance simulation and predictive modeling.\n* collaborate with architects and engineers to ensure ai systems produce practical, code\\-compliant, and constructible designs.\n* lead research initiatives and publish findings to establish us as a thought leader in aec ai innovation.\n\n**requirements**\n================\n\n* master's degree or phd in computer science, ai/ml, computational design, or related field (or equivalent industry experience).\n* 3\\-5\\+ years of hands\\-on experience building and deploying ml models in production environments.\n* deep expertise with modern deep learning frameworks (pytorch preferred).\n* strong foundation in computer vision, 3d geometry processing, and spatial reasoning algorithms.\n* experience with generative ai models (vaes, gans, diffusion models, transformers) and their practical applications.\n* proficiency in python and scientific computing libraries (numpy, scipy, scikit\\-learn, open3d, trimesh).\n* experience with cloud ml platforms (aws sagemaker, vertex ai, or azure ml) and distributed training frameworks.\n* understanding of optimization techniques (genetic algorithms, gradient\\-based optimization, constraint satisfaction).\n* strong software engineering practices and experience with containerization (docker) and orchestration (kubernetes).\n* excellent communication skills to translate complex ai concepts to domain experts and stakeholders.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* experience with computational design tools (grasshopper, dynamo) and parametric modeling.\n* familiarity with building information modeling (bim) standards and ifc data schemas.\n* knowledge of graph neural networks (pytorch geometric, dgl) for structural and spatial analysis.\n* experience with physics simulation engines (mujoco, isaac sim) or fea integration.\n* background in multi\\-agent reinforcement learning for complex system optimization.\n* contributions to open\\-source ml projects or published research in relevant venues (neurips, icml, cvpr, or domain\\-specific conferences).\n* experience with point cloud processing and 3d scene understanding (pointnet\\+\\+, dgcnn).\n* understanding of construction workflows and building codes.\n\n**what you\u2019ll gain**\n====================\n\n* the opportunity to define and build ai systems that will reshape a $10 trillion global industry.\n* access to unique datasets and real\\-world problems at the intersection of ai and the built environment.\n* collaboration with leading architects, engineers, and construction professionals who are eager to embrace ai transformation.\n* resources to pursue cutting\\-edge research while maintaining a focus on practical, deployable solutions.\n* mentorship from industry veterans who understand both the technical and business aspects of aec technology.\n* the freedom to experiment with emerging ai architectures and techniques in a high\\-impact domain.\n\n**why us?**\n===========\n\n\nhere, ai engineers aren't building demos\u2014you'll be creating systems that influence how real buildings get designed and built. we believe the aec industry is on the cusp of an ai revolution, and we're positioning ourselves at the forefront. our team has the domain expertise to identify the right problems, the technical depth to solve them, and the industry connections to deploy solutions at scale. if you're passionate about using ai to create a more sustainable, efficient, and beautiful built environment, this is where you can make it happen.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer \u2013 CRG (Analyst / Associate)",
        "company": "Goldman Sachs",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "Prompt Engineering",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Kafka",
            "MongoDB",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ca332a980655e235",
        "description": "our impact\n\nthe conflicts resolution group (crg) is a control function with a primary objective to identify and mitigate the firm\u2019s actual or perceived conflicts of interest and associated reputational risks which may result from the firm taking on a new role, responsibility, or position on a potential deal or with a potential counterparty. crg engineering plays a critical role in the development and support of software solutions aligned with the crg objectives for improved risk management for the firm and ensuring a timely turnaround for our revenue divisions.\n\n\nhow you will fulfill your potential\n* partner globally across crg and other engineering teams to design and deliver end\\-to\\-end ai\\-enabled platforms.\n* design, develop, test, deploy, and maintain services with a strong emphasis on ai/genai use cases.\n* build scalable data processing, workflow, and api solutions across large datasets, real\\-time processing, and event\\-driven architectures.\n* collaborate in an agile environment with product, legal, data science, and design partners to prototype ideas and share best practices.\n* demonstrate willingness to learn, support, and modernize legacy systems that underpin critical conflicts resolution business processes.\n* apply modern ai techniques\u2014including llms, rag, and agent\\-based frameworks\u2014to solve business and risk\\-management problems.\n\n\nqualifications\n* strong programming experience in java and python.\n* hands\\-on experience applying llms, prompt engineering, rag, and ai agents to real\\-world problems.\n* strong analytical and problem\\-solving skills, with the ability to translate complex business and regulatory requirements into technical solutions.\n* experience with data stores and search technologies (e.g., sql, mongodb, elastic) and familiarity with distributed or real\\-time systems (e.g., kafka).\n* experience building ai agents, including tool use, planning, and orchestration.\n* experience building and deploying cloud\\-native services on an aws stack, with strong grounding in automated testing, sdlc practices, and ci/cd pipelines.\n* excellent communication and collaboration skills across technical and non\\-technical stakeholders.\n* bachelor\u2019s or master\u2019s degree in computer science, engineering, or a related quantitative field.\n* 0\u20133\\+ years of relevant professional software engineering experience, depending on level.\n* experience in some of the following is desired and can set you apart from other candidates:\n* experience with agent and rag frameworks such as langchain, langgraph, and/or adk.\n* frontend experience with react/angular.\n* familiarity with platforms and tooling such as gitlab and kubernetes.\n* knowledge of financial services or control functions such as legal, compliance, or risk management.\n**we offer best\\-in\\-class benefits**\nhealthcare \\& medical insurance\nwe offer a wide range of health and welfare programs that vary depending on office location. these generally include medical, dental, short\\-term disability, long\\-term disability, life, accidental death, labor accident and business travel accident insurance.\n\n\nholiday \\& vacation policies\nwe offer competitive vacation policies based on employee level and office location. we promote time off from work to recharge by providing generous vacation entitlements and a minimum of three weeks expected vacation usage each year.\n\n\nfinancial wellness \\& retirement\nwe assist employees in saving and planning for retirement, offer financial support for higher education, and provide a number of benefits to help employees prepare for the unexpected. we offer live financial education and content on a variety of topics to address the spectrum of employees\u2019 priorities.\n\n\nhealth services\nwe offer a medical advocacy service for employees and family members facing critical health situations, and counseling and referral services through the employee assistance program (eap). we provide global medical, security and travel assistance and a workplace ergonomics program. we also offer state\\-of\\-the\\-art on\\-site health centers in certain offices.\n\n\nfitness\nto encourage employees to live a healthy and active lifestyle, some of our offices feature on\\-site fitness centers. for eligible employees we typically reimburse fees paid for a fitness club membership or activity (up to a pre\\-approved amount).\n\n\nchild care \\& family care\nwe offer on\\-site child care centers that provide full\\-time and emergency back\\-up care, as well as mother and baby rooms and homework rooms. in every office, we provide advice and counseling services, expectant parent resources and transitional programs for parents returning from parental leave. adoption, surrogacy, egg donation and egg retrieval stipends are also available.\n\n\nbenefits at goldman sachs\nread more about the full suite of class\\-leading benefits our firm has to offer.\n\n\n  \nopportunity overview\ncorporate title\nassociate\noffice location(s)\ndallas\njob function\nsoftware engineering\ndivision\nconflicts resolution group",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer II",
        "company": "Fanatics",
        "location": "US USA",
        "posted_at": "2026-02-23",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Redshift",
            "Kinesis",
            "CI/CD",
            "Git",
            "Redshift",
            "Kafka",
            "MongoDB"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1be8fdec648472af",
        "description": "**overview**\n\n\n\nas **software engineer ii** at fanatics betting \\& gaming (fbg) with a focus on ai and machine learning, you will play a crucial role in our team's ambitious initiative designed to leverage ai and data\\-driven systems to transform customer operations into a market differentiator. you'll architect and enhance sophisticated data infrastructure and pipelines that fuel ai\\-driven insights, real\\-time conversational analytics, predictive troubleshooting, and contribute to the development of intuitive ui screens and tools that empower customer success teams. you will be collaborating closely with data scientists, ml engineers, and cross\\-functional teams, and your work will significantly impact operational efficiency, customer engagement, and innovation within fbg.\n\n\n**responsibilities**\n\n\n* architect, build, and manage robust data pipelines and infrastructure that support large\\-scale ai and machine learning initiatives.\n* develop and optimize integrations between core ai services, including aws bedrock, llms, and internal platforms (salesforce, tableau, slack) for enhanced operational performance.\n* collaborate with applied scientists and ml engineers to refine model deployments, feature engineering, and ensure seamless operationalization of predictive and conversational ai solutions.\n* implement advanced data processing and real\\-time analytics workflows for conversational intelligence, sentiment analysis, and predictive troubleshooting.\n* ensure high\\-quality data management, emphasizing security, compliance, data governance, and maintainability within ai\\-driven environments.\n* drive tooling and infrastructure solutions that facilitate efficient testing, debugging, monitoring, and continuous improvement of ai applications.\n* establish comprehensive observability frameworks (logs, metrics, alerts) to maintain system reliability, performance, and operational insights.\n* participate actively in technical discussions, code reviews, and strategic planning to align ai infrastructure development with business goals.\n\nproto\\-type and develop intuitive, user\\-friendly ui screens and dashboards that enable customer success teams to leverage ai\\-driven insights effectively.  \n* \n\n**required qualifications**\n\n\n* 3 \\-7 years of professional experience focused on data engineering, specifically supporting ai, ml, or nlp\\-driven systems.\n* deep expertise in cloud technologies (aws highly preferred), including hands\\-on experience with aws bedrock, redshift, mongodb, and s3\\.\n* proficiency in java, sprintboot, and experience with orchestration tools (airflow, prefect) for managing complex ai\\-driven workflows.\n* demonstrated experience integrating and operationalizing large language models (llms) and machine learning systems within production environments.\n* strong understanding of microservices, restful api development, and real\\-time data streaming (kafka, kinesis).\n* robust experience with observability tools and ci/cd pipelines, ensuring the reliability and continuous deployment of ai services.\n\nexceptional problem\\-solving abilities, comfortable navigating ambiguity, and adept at collaborating across technical and business\\-focused teams.  \n* \n\n**preferred qualifications**\n\n\n* extensive experience specifically with aws bedrock or equivalent managed ai services.\n* proven track record in building real\\-time ai analytics systems, predictive troubleshooting tools, and advanced nlp\\-driven applications.\n* experience working with salesforce or other related products or a firm understanding of customer success workflows.\n* background in customer\\-centric operations, including ai\\-driven customer service solutions, chatbots, and contact center technologies.\n* familiarity with sentiment analysis, conversational analytics, and predictive modeling in dynamic, consumer\\-facing environments.\n* passionate about ai trends, data engineering best practices, and their practical application in high\\-impact industries such as gaming, sports betting, or customer engagement platforms.\n* experience building front\\-end applications using modern frameworks (react, angular, vue) and familiarity with ui/ux design principles.\n\n\n*ready to build the future of sports betting? if you possess some of these skills but not all of them, we still encourage you to apply!*\n\n  \n\n\n\n***salary range:*** *$137,750 \\- $181, 250 usd per year*\n  \n\n*the base salary for this role is based on job\\-related knowledge, skills, and experience and may vary depending on the successful candidate\u2019s geographic location. for information about our benefits, please visit* *https://benefitsatfanatics.com/*\ndepending on the role, your interview and onboarding experience may include in\\-person components, such as onsite interviews or launching into better: live\u2014a multi\\-day cultural immersion in new york city for full\\-time, non\\-seasonal hires. these sessions are designed to build connection and bring our culture to life, though specific travel and participation requirements will be confirmed based on your role and location. your recruiter will provide clear guidance at each stage of the process.  \n\nlaunched in 2021, fanatics betting and gaming is the online and retail sports betting subsidiary of fanatics, a global digital sports platform. the fanatics sportsbook is available to 95% of the addressable online sports bettor market in the u.s. fanatics casino is currently available online in michigan, new jersey, pennsylvania and west virginia. fanatics betting and gaming operates twenty\\-two retail sports betting locations, including the only sportsbook inside an nfl stadium at northwest stadium. fanatics betting and gaming is headquartered in new york with offices in denver, leeds and dublin.\n\n\nfanatics is building a leading global digital sports platform. we ignite the passions of global sports fans and maximize the presence and reach for our hundreds of sports partners globally by offering products and services across fanatics commerce, fanatics collectibles, and fanatics betting \\& gaming, allowing sports fans to buy, collect, and bet. through the fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its sportsbook and igaming platform. fanatics has an established database of over 100 million global sports fans; a global partner network with approximately 900 sports properties, including major national and international professional sports leagues, players associations, teams, colleges, college conferences and retail partners, 2,500 athletes and celebrities, and 200 exclusive athletes; and over 2,000 retail locations, including its lids retail stores. our more than 22,000 employees are committed to relentlessly enhancing the fan experience and delighting sports fans globally.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Security Engineer \u2013 Cloud & Data Security",
        "company": "Sigma Computing",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "BigQuery",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5d4ac754da67252d",
        "description": "### **about the role**\n\n\n\nwe are hiring a senior, hands\\-on cloud security engineer to secure a large\\-scale, cloud\\-native saas platform. this is an engineering\\-first role for someone who builds security solutions\u2014not just manages tools.\n\n\n\nyou will be a sme for cloud security architecture across platform, iam, network, workload, data, and ai enablement, and partner with engineering, security, and product to implement scalable controls that support business growth. you'll design secure architectures, embed controls into infrastructure\\-as\\-code, and build automated guardrails so teams can move fast without waiting on manual security approvals.\n\n\n\nwe're looking for a builder\\-defender who thrives in complex cloud environments, automates aggressively (\"let the robots do the work\"), and can scale cloud security for a fast\\-moving saas company.\n\n\n### **what you'll do**\n\n\n* **architectural leadership:** partner deeply with infrastructure and engineering teams to embed security into development workflows, leading high\\-level technical discussions to guide security efforts and strategic priorities.\n* **multi\\-cloud engineering:** design, implement, and continuously improve sigma cloud security across **aws, gcp, and azure** environments with architect\\-level technical depth.\n* **threat modeling \\& ir:** conduct cloud threat modeling and demonstrate hands\\-on experience in cloud incident response, including investigating and remediating malicious activity within cloud environments.\n* **identity \\& access:** build iam and privileged access strategy (rbac/abac, federation, least privilege, cross\\-account access), eliminating standing privilege and long\\-lived credentials. develop and enforce iam best practices, including zero\\-trust models and privileged access controls across iaas and saas.\n* **drive cloud data security controls** including classification, encryption/kms, masking/tokenization, access governance, retention/deletion, and exfiltration risk reduction across apis and data pipelines.\n* **develop automated remediation workflows** for recurring cloud misconfigurations, drift, and policy violations to reduce manual effort and response time.\n* **security stack management:** deploy and manage cloud\\-native services (cspm, cnapp, dspm, siem, dlp, waf, kubernetes, and container security).\n* **network defense:** review and apply zero\\-trust principles through strict network segmentation, authentication, and authorization.\n* **automation:** develop sophisticated signatures/rules for cloud security and automate detection and response workflows.\n* **ai :** use ai securely and effectively to scale security practices and improve team efficiency.\n* **continuous evolution:** stay ahead of threats by leveraging intelligence, attack simulation, and red/blue team learnings.\n\n### **what we're looking for**\n\n\n* minimum 7\\+ years in security roles with at least 5\\+ years focused on cloud security engineering,iam, and data security\n* bachelor's or master's degree in computer science, cyber security, or a related field.\n* deep technical expertise in cloud architectures aws/azure/gcp; including iam, networking (vpcs, security groups, privatelink), and native security services is strongly desired.\n* strong infrastructure\\-as\\-code skills\u2014you write terraform professionally, not just read it.\n* advanced understanding and experience with container security, kubernetes, and secure ci/cd pipeline design\n* proven ability to demonstrate incident response experience specifically related to cloud\\-based malicious activity and breach remediation.\n* advanced cloud iam expertise: federation, sso, pam/jit access, service identities, and least privilege design.\n* strong background in cloud network security (segmentation, private connectivity, egress controls, waf).\n* strong proficiency in scripting languages (e.g., python, go, powershell) for automation, data analysis, and security tooling development.\n* strong knowledge of security platforms such as cnapp (wiz), waf (cloudflare), sase (netskope)\n* demonstrated ability to lead cloud/saas architecture reviews and influence senior engineering stakeholders.\n* experience securing data platforms (nice to have) \\- snowflake, databricks, bigquery etc.\n* experience in high\\-growth saas or data platforms organizations (nice\\-to have)\n* prior experience in platform engineering, devsecops or similar (nice\\-to have)\n* **certifications (preferred):** professional\\-level cloud certifications are required, such as:\n* + **aws:** certified security \u2013 specialty or solutions architect \u2013 professional.\n\t+ **gcp:** professional cloud security engineer or professional cloud architect.\n\t+ **azure:** az\\-500 (security technologies) or az\\-305 (solutions architect).\n\n**why sigma?**\n\n\n\nat sigma, security is at the core of our mission. we power insights and innovation for our customers, and protecting their data is our highest priority. as a senior security engineer, you will have the autonomy to shape our security engineering strategy, access to cutting\\-edge technologies, and the opportunity to solve real problems at scale.\n\n\n\njoin us and be part of a security team that values collaboration, innovation, and resilience\u2014while giving you the room to grow, lead, and leave your mark on sigma's security journey.\n\n\n**additional job details**\n\n\n\nthe base salary range for this position is $210k \\- $240k annually.\n\n\n\ncompensation may vary outside of this range depending on a number of factors, including a candidate's qualifications, skills, competencies and experience. base pay is one part of the total package that is provided to compensate and recognize employees for their work at sigma computing. this role is eligible for stock options, as well as a comprehensive benefits package.\n\n#### **about us:**\n\n\n\nsigma is the ai apps and analytics platform connected to the cloud data warehouse. using sigma, business and technical teams can build intelligent, production\\-ready ai apps that accelerate and automate operational workflows. sigma provides a spreadsheet interface, sql and python editors, visual builders, and native ai to help teams turn live data into interactive applications, analysis, reports, and embedded experiences.\n\n\n\nsigma announced its $200m in series d financing in may 2024, to continue transforming bi through its innovations in ai infrastructure, data application development, enterprise\\-wide collaboration, and business user adoption. spark capital and avenir growth capital co\\-led the series d funding round, with additional participation from a group of past investors including snowflake ventures and sutter hill ventures.the series d funding, raised at a valuation 60% higher than the company's series c round three years ago, promises to further accelerate sigma's growth.\n\n\ncome join us!\n\n\n#### **benefits for our full\\-time employees:**\n\n\n* equity\n* generous health benefits\n* flexible time off policy. take the time off you need!\n* paid bonding time for all new parents\n* traditional and roth 401k\n* commuter and fsa benefits\n* lunch program\n* dog friendly office\n\n\nsigma computing is an equal opportunity employer. we are committed to building a smart and strong team regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. we look forward to learning how your experience can enable all of us to grow*.*\n\n\n*note: we have an in\\-office work environment in all our offices in sf, nyc, and london.*\n\n\n**our privacy practices**\n\nwhen you submit a job application on this site, sigma processes your personal data for the purposes of evaluating your candidacy for employment at sigma and as otherwise needed throughout the recruitment and hiring process. please review sigma's candidate privacy notice for more details. please note that your personal data may be transferred to a country other than the one in which it was provided (including to usa, the uk, and canada).\n\n\n**sigma's use of ai**\n\nthis hiring process utilizes artificial intelligence tools to assist in candidate screening and assessment. our ai tools are designed to complement, not replace, human decision\\-making.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Solutions Architect",
        "company": "BakerHostetler",
        "location": "Cleveland, OH, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "Data Lake",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2b9840944141fbe6",
        "description": "we are seeking a solution architect to design and implement enterprise business solutions and data platforms on microsoft azure. in this role, you will work closely with developers, data engineers, and cloud engineers to build and enhance the practice intelligence platform, built on a data lake architecture. you will help ensure solutions are scalable, secure, cost\\-effective, and aligned with organizational standards and industry best practices.\n\n##### **key responsibilities:**\n\n\n##### **application and data solution architecture**\n\n\n* **design and architect enterprise business solutions using microsoft azure services**\n\n\n* integrate azure openai service into applications to support intelligent features and ai\\-driven workflows\n\n\n* manage and optimize azure resources that support solution development, including azure functions, storage accounts, and azure openai deployments\n\n\n* use the scaledsense platform to support resource scaling, cost optimization, and operational insights across application environments\n\n\n* implement infrastructure as code (iac) using terraform to ensure consistent and repeatable deployments\n\n\n* implement continuous integration and continuous delivery (ci/cd) pipelines using devops and github actions\n\n\n* monitor system performance, resource usage, and cost metrics for azure\\-hosted solutions and identify optimization opportunities\n\n\n* create architectural blueprints, technical specifications, and design documentation for custom developed integrations, systems, and business solutions in azure\n\n  \n\n\n**cross\\-functional collaboration**\n\n\n* work closely with developers, data engineers, and cloud engineers to support application development, data integration, and deployment into the firm\u2019s azure environment\n\n\n* provide technical guidance on azure best practices, design patterns, and technology standards to support consistent solution design\n\n\n* partner with cloud engineering to define and enforce azure governance, security controls, and compliance standards\n\n  \n\n**technical skills and experience**\n\n\n* application and compute: azure app service, azure functions, azure openai service, azure kubernetes service (aks)\n\n\n* data and storage: azure sql database, azure data lake storage, azure storage\n\n\n* integration and apis: azure api management, azure data factory\n\n\n* devops and automation: ci/cd using github actions, azure devops, and terraform (iac) for git\\-based repositories\n\n\n* monitoring and management: azure monitor, application insights\n\n\n* security and identity: microsoft entra id (formerly azure active directory), azure key vault, role\\-based access control (rbac)\n\n  \n\n\n**preferred skills, experience, and certifications**\n\n\n* experience with react application architecture, microservices, application programming interface (api) design, and representational state transfer (rest) services\n\n\n* understanding of the software development lifecycle (sdlc), agile methodologies, and devops practices\n\n\n* proven experience designing enterprise\\-scale solutions, preferably within a law firm or professional services environment\n\n\n* microsoft certified: azure solutions architect expert or an equivalent industry\\-recognized certification\n\n\n**required education and experience**\n\n\n* bachelor\u2019s degree in computer science, information technology, or a related field\n\n\n* eight to ten years of hands\\-on experience in solution architecture, system architecture, or engineering roles\n\n\n**collaboration and communication**\n\n\n* build and maintain effective relationships with clients, vendors, and internal stakeholders, including attorneys and support teams\n\n\n* lead discussions, influence decisions, and build consensus across technical and non\\-technical teams\n\n\n* follow established project management methodologies\n\n\n* accurately track and report time spent on project work\n\n\n* manage multiple priorities effectively in a fast\\-paced environment\n\n\n* communicate technical concepts clearly to both technical and non\\-technical audiences\n\n\n* perform essential duties under time constraints, interruptions, and pressure\n\n  \n\n\n**what helps you succeed in this role**\n\n\n* strong attention to detail and accuracy\n\n\n* professional and business\\-focused demeanor\n\n\n* proactive and self\\-motivated\n\n\n* flexible and willing to take on new and challenging responsibilities\n\n  \n\n\n**travel requirement**\n\n\n* minimal travel required\n\n  \n\n\n**about us:**\n\n\n\nbakerhostetler is recognized as one of the leading law firms in the country. with over 1,000 attorneys in 18 offices from coast to coast, bakerhostetler is a great place to work for those seeking professional and personal growth in a collaborative environment. we deliver the highest quality counsel to our clients, who include many of the nation\u2019s largest and most well\\-known companies. bakerhostetler\u2019s values have remained unchanged since our founding more than 100 years ago: dedication to the law, commitment to the highest standard of client service, continuous development of our people, and active participation in the communities in which we work and live.\n\n\n\nbakerhostetler offers a comprehensive and competitive benefits program. specific information is provided during the interview process.\n\n\n* competitive salaries\n* performance bonus program\n* generous time off\n* generous retirement program including 401(k) plan\n* group health, dental, and vision insurance.\n* bhealthy wellness program\n* life insurance\n* voluntary accident insurance \u2013 self and family\n* short and long\\-term disability\n* pre\\-tax benefit programs\n\n\nbaker \\& hostetler llp is an equal opportunity employer.\n\n\n\nplease visit www.bakerlaw.com for more information about our firm.\n\n  \n\n\n**location:**  \n\natlanta, ga; austin, dallas or houston, tx; chicago, il; cincinnati, cleveland or columbus, oh; denver, co; lost angeles, orange county or san francisco, ca; new york, ny; orlando, fl; philadelphia, pa; seattle, wa; washington d.c.; wilmington, de\n\n  \n\n\n**salary range:**\n\nthe salary offered in any location will be determined by a wide range of factors, including, but not limited to, experience level, education/training, geographic region, and relevant skills. associates also participate in a performance\\- and hours\\-based bonus program. the salary range for this role is $150,000\\-160,000/year.\n\n  \n\n\n**application submission information:**\n\n\n\ninterested individuals should apply directly to the posting via our website. please include your resume.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior ML Platform Engineer",
        "company": "NVIDIA",
        "location": "Santa Clara, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "Cortex",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Git",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=062cd6698cf7e079",
        "description": "nvidia is at the forefront of innovations in artificial intelligence, high\\-performance computing, and visualization. our invention\u2014the gpu\u2014functions as the visual cortex of modern computing and is central to groundbreaking applications from generative ai to autonomous vehicles. we are now looking for a ml platform engineer to help accelerate the next era of machine learning innovation.\nin this role, you will architect, build, and scale our high\\-performance ml infrastructure using modern infrastructure\\-as\\-code practices. your primary focus will be on creating reliable, automated platforms that empower scientists and engineers to train and deploy the most advanced ml models on some of the world\u2019s most powerful gpu systems. join our top team and apply your sre and software engineering skills to craft robust, user\\-friendly platforms for seamless ml development.\nwhat you'll be doing:* design, build, and maintain our core ml platform infrastructure as code, primarily using ansible and terraform, ensuring reproducibility and scalability across large\\-scale, distributed gpu clusters.\n* apply sre principles to diagnose, troubleshoot, and resolve complex system issues across the entire stack, ensuring high availability and performance for critical ai workloads.\n* develop robust internal automation and tooling for ml workflow orchestration, resource scheduling, and platform operations, with a strong focus on software engineering best practices.\n* collaborate with ml researchers and applied scientists to understand infrastructure needs and build solutions that streamline their end\\-to\\-end experimentation.\n* evolve and operate our multi\\-cloud and hybrid (on\\-prem \\+ cloud) environments, implementing monitoring, alerting, and incident response protocols.\n* participate in on\\-call rotation to provide support for platform services and infrastructure running critical ml jobs, driving root cause analysis and implementing preventative measures.\n* write high\\-quality, maintainable code (python, go) to contribute to the core orchestration platform and automate manual processes.\n* drive the adoption of modern gpu technologies and ensure smooth integration of next\\-generation hardware into ml pipelines (e.g., gb200, nvlink, etc.).\n\n\nwhat we need to see:* bs/ms in computer science, engineering, or equivalent experience.\n* 5\\+ years in software/platform engineering or sre roles, including 3\\+ years focused on ml infrastructure or distributed compute systems.\n* strong proficiency in infrastructure\\-as\\-code (iac) tools, specifically ansible and terraform, with a proven track record of building and managing production infrastructure.\n* sre\\-oriented mindset with extensive experience in diagnosing system\\-level issues, performance tuning, and ensuring platform reliability.\n* solid understanding of ml workflows and lifecycle\u2014from data preprocessing to deployment.\n* proficiency in operating containerized workloads with kubernetes and docker.\n* strong software engineering skills in languages such as python or go, with a focus on automation, tooling, and writing production\\-grade code.\n* experience with linux systems internals, networking, and performance tuning at scale.\n\n\nways to stand out from the crowd:* experience building or operating ml platforms supporting frameworks like pytorch or tensorflow at scale.\n* deep understanding of distributed training techniques (e.g., data/model parallelism, horovod, nccl).\n* expertise with modern ci/cd methodologies and gitops practices.\n* passion for building developer\\-centric platforms with great ux and strong operational reliability.\n* proven ability to contribute code to complex orchestration or automation platforms.\n\n\nyour base salary will be determined based on your location, experience, and the pay of employees in similar positions. the base salary range is 152,000 usd \\- 241,500 usd for level 3, and 184,000 usd \\- 287,500 usd for level 4\\.\nyou will also be eligible for equity and benefits.\napplications for this job will be accepted at least until february 27, 2026\\.\nthis posting is for an existing vacancy.\nnvidia uses ai tools in its recruiting processes.\nnvidia is committed to fostering a diverse work environment and proud to be an equal opportunity employer. as we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer - Full Stack",
        "company": "kp reddy",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "Docker",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "PostgreSQL",
            "MySQL",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=15451134c42bdf64",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a software engineer \\- full stack to help build, refine, and maintain production\\-grade software that supports aec workflows. you'll work alongside senior engineers to develop robust backend services, create responsive web interfaces, and ensure our systems meet the reliability demands of enterprise clients. this role emphasizes learning, ownership, and steady growth\u2014you'll start by contributing to existing systems and progressively take on more complex challenges as you develop expertise.\n\n\nthis is an ideal opportunity for a recent graduate who wants to apply full\\-stack development skills in a high\\-impact industry context, working with modern cloud infrastructure, ai\\-integrated systems, and real\\-world data pipelines.\n\n**key responsibilities**\n========================\n\n* develop and maintain full\\-stack features using react frontends and python backend services, following established architectural patterns and coding standards.\n* write clean, well\\-tested, and documented code that meets production quality standards for enterprise aec applications.\n* collaborate with senior engineers to troubleshoot, debug, and resolve issues across the stack, contributing to system reliability and performance.\n* build and consume restful apis that integrate with internal services and third\\-party aec tools.\n* participate in code reviews\u2014both giving and receiving constructive feedback to improve code quality and personal growth.\n* contribute to containerized microservices deployed on aws, gaining hands\\-on experience with cloud\\-native development practices.\n* support data pipelines and integrations that process aec project data, documents, and workflows.\n* document technical implementations, contribute to runbooks, and help maintain internal knowledge bases.\n* participate in agile development processes, including sprint planning, standups, and retrospectives.\n\n**requirements**\n================\n\n* bachelor's or master's degree in computer science, software engineering, or a related field (recent graduates welcome).\n* solid foundation in data structures, algorithms, and object\\-oriented programming principles.\n* proficiency in python with experience building backend services or apis.\n* experience with react.js and modern javascript/typescript for frontend development.\n* familiarity with relational databases (postgresql, mysql) and writing sql queries.\n* basic understanding of git workflows and collaborative development practices.\n* exposure to containerization concepts (docker) through coursework, projects, or internships.\n* strong problem\\-solving skills and a willingness to learn in a fast\\-paced environment.\n* excellent communication skills and ability to work effectively in a collaborative team setting.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* internship or project experience with full\\-stack application development.\n* familiarity with at least one major cloud platform (aws, azure, or gcp).\n* experience with testing frameworks and writing unit/integration tests.\n* exposure to ci/cd concepts and tools (github actions, jenkins, or similar).\n* knowledge of graphql or experience designing api contracts.\n* coursework or projects involving distributed systems, databases, or cloud computing.\n* any exposure to the aec industry, cad/bim tools, or construction technology.\n\n**what you\u2019ll gain**\n====================\n\n* a structured path to grow from junior engineer to mid\\-level and beyond, with mentorship from experienced engineers and technical leaders.\n* hands\\-on experience building and maintaining production systems that serve real enterprise clients in the aec industry.\n* exposure to modern development practices including containerization, cloud\\-native architecture, ci/cd pipelines, and observability.\n* the opportunity to work on diverse technical challenges spanning web applications, data processing, and ai\\-integrated systems.\n* a collaborative environment that values learning, experimentation, and pragmatic problem\\-solving.\n* direct impact on products that influence how buildings and infrastructure are designed and delivered.\n\n**why us?**\n===========\n\n\nhere, young engineers aren't relegated to bug fixes and maintenance tickets\u2014you'll be a contributing member of a team building technology that transforms a $10 trillion global industry. we believe in learning by doing, and you'll have real ownership of features and systems from day one. our senior engineers are committed to mentorship, providing guidance while giving you the autonomy to grow. you'll work on problems that matter, using modern tools and practices, in an environment that rewards curiosity and initiative.\n\n\nif you're a recent graduate eager to build production software, learn from experienced engineers, and make an impact in an industry ripe for transformation, we want to hear from you.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer - Full Stack",
        "company": "kp reddy",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "Docker",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "PostgreSQL",
            "MySQL",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a3288093ad8485f4",
        "description": "**about the job**\n=================\n\n\nhere, we partner with forward\\-thinking organizations in the aec (architecture, engineering, and construction) industry to push the boundaries of how the built environment is designed and delivered. as the premier full\\-service management advisory firm exclusively focused on the built world, we combine domain expertise with leading\\-edge technology strategy to solve real problems at scale.\n\n\nwe are seeking a software engineer \\- full stack to help build, refine, and maintain production\\-grade software that supports aec workflows. you'll work alongside senior engineers to develop robust backend services, create responsive web interfaces, and ensure our systems meet the reliability demands of enterprise clients. this role emphasizes learning, ownership, and steady growth\u2014you'll start by contributing to existing systems and progressively take on more complex challenges as you develop expertise.\n\n\nthis is an ideal opportunity for a recent graduate who wants to apply full\\-stack development skills in a high\\-impact industry context, working with modern cloud infrastructure, ai\\-integrated systems, and real\\-world data pipelines.\n\n**key responsibilities**\n========================\n\n* develop and maintain full\\-stack features using react frontends and python backend services, following established architectural patterns and coding standards.\n* write clean, well\\-tested, and documented code that meets production quality standards for enterprise aec applications.\n* collaborate with senior engineers to troubleshoot, debug, and resolve issues across the stack, contributing to system reliability and performance.\n* build and consume restful apis that integrate with internal services and third\\-party aec tools.\n* participate in code reviews\u2014both giving and receiving constructive feedback to improve code quality and personal growth.\n* contribute to containerized microservices deployed on aws, gaining hands\\-on experience with cloud\\-native development practices.\n* support data pipelines and integrations that process aec project data, documents, and workflows.\n* document technical implementations, contribute to runbooks, and help maintain internal knowledge bases.\n* participate in agile development processes, including sprint planning, standups, and retrospectives.\n\n**requirements**\n================\n\n* bachelor's or master's degree in computer science, software engineering, or a related field (recent graduates welcome).\n* solid foundation in data structures, algorithms, and object\\-oriented programming principles.\n* proficiency in python with experience building backend services or apis.\n* experience with react.js and modern javascript/typescript for frontend development.\n* familiarity with relational databases (postgresql, mysql) and writing sql queries.\n* basic understanding of git workflows and collaborative development practices.\n* exposure to containerization concepts (docker) through coursework, projects, or internships.\n* strong problem\\-solving skills and a willingness to learn in a fast\\-paced environment.\n* excellent communication skills and ability to work effectively in a collaborative team setting.\n\n**preferred qualifications (a plus, not a requirement)**\n========================================================\n\n* internship or project experience with full\\-stack application development.\n* familiarity with at least one major cloud platform (aws, azure, or gcp).\n* experience with testing frameworks and writing unit/integration tests.\n* exposure to ci/cd concepts and tools (github actions, jenkins, or similar).\n* knowledge of graphql or experience designing api contracts.\n* coursework or projects involving distributed systems, databases, or cloud computing.\n* any exposure to the aec industry, cad/bim tools, or construction technology.\n\n**what you\u2019ll gain**\n====================\n\n* a structured path to grow from junior engineer to mid\\-level and beyond, with mentorship from experienced engineers and technical leaders.\n* hands\\-on experience building and maintaining production systems that serve real enterprise clients in the aec industry.\n* exposure to modern development practices including containerization, cloud\\-native architecture, ci/cd pipelines, and observability.\n* the opportunity to work on diverse technical challenges spanning web applications, data processing, and ai\\-integrated systems.\n* a collaborative environment that values learning, experimentation, and pragmatic problem\\-solving.\n* direct impact on products that influence how buildings and infrastructure are designed and delivered.\n\n**why us?**\n===========\n\n\nhere, young engineers aren't relegated to bug fixes and maintenance tickets\u2014you'll be a contributing member of a team building technology that transforms a $10 trillion global industry. we believe in learning by doing, and you'll have real ownership of features and systems from day one. our senior engineers are committed to mentorship, providing guidance while giving you the autonomy to grow. you'll work on problems that matter, using modern tools and practices, in an environment that rewards curiosity and initiative.\n\n\nif you're a recent graduate eager to build production software, learn from experienced engineers, and make an impact in an industry ripe for transformation, we want to hear from you.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Engineer - ITS4",
        "company": "State of Minnesota - Minnesota IT Services",
        "location": "Saint Paul, MN, US USA",
        "posted_at": "2026-02-18",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "S3",
            "Glue",
            "Redshift",
            "Data Lake",
            "Git",
            "Redshift",
            "PySpark",
            "NoSQL",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=54c8deff754c16fa",
        "description": "**job details**\n---------------\n\n**working title: data engineer**  \n\n**job class: information technology specialist 4**  \n\n**agency: minnesota it services**\n\n* **job id**: 92079\n* **location**: st. paul\n* **telework eligible**: yes \\#li\\-hybrid\n* **full/part time**: full\\-time\n* **regular/temporary**: limited\n* **who may apply**: open to all qualified job seekers\n* **date posted**: 02/18/2026\n* **closing date**: 02/24/2026\n* **hiring agency/seniority unit**: minnesota it services\n* **division/unit**: minnesota department of health (mdh)\n* **work shift/work hours**: day shift\n* **days of work**: monday \\- friday\n* **travel required**: no\n* **salary range:** $37\\.07 \\- $61\\.14 / hourly; $77,402 \\- $127,660 / annually\n* **classified status**: unclassified\n* **bargaining unit/union**: 214 \\- mn assoc of professional empl/mape\n* **end date**: up to two (2\\) years\n* **flsa status**: exempt \\- professional\n* designated in connect 700 program for applicants with disabilities: no\n\n### **the work you'll do is more than just a job.**\n\n\nat the state of minnesota, employees play a critical role in developing policies, providing essential services, and working to improve the well\\-being and quality of life for all minnesotans. the state of minnesota is committed to equity and inclusion, and invests in employees by providing benefits, support resources, and training and development opportunities.\n\n\njoin the 2,800\\+ professionals of minnesota it services (mnit) who connect minnesotans to services that will improve their lives. this position serves our partners at the minnesota department of health (mdh). the mnit mdh team provides services for over 250\\+ applications and delivers the information technology to drive one of the leading public health systems in the nation. you will support an agency that works to protect, maintain, and improve the health of all minnesotans.\n**job summary**\n---------------\n\n***this position is temporary and includes full state fringe benefits; it is anticipated to last for up to two (2\\) years.***\nin this dynamic, fast\\-paced role, you\u2019ll serve as a technical expert on our data services team at minnesota it services (mnit), partnering with minnesota department of health (mdh) to design, deploy, and maintain data pipelines that leverage the full power of amazon web services (aws). your creativity and innovative thinking is not only appreciated but encouraged in our ever\\-evolving environment.\nwork where you live, contribute to something bigger than yourself, and be challenged every day. come build technology that makes a real difference in minnesotans life, join minnesota it services and the minnesota department of health!\n**this position requires an employee to be onsite at 625 robert street n, st. paul, minnesota at least 50% of the time, with some opportunity to perform work from a telework location.** telework for minnesota it services is available on a limited basis. employees will be required to meet current telework eligibility requirements.\n**qualifications**\n------------------\n\n### **minimum qualifications**\n\n**to qualify, candidates must clearly show all of the following qualifications in their resume. resume tips available** **here****:**\nposition requires a minimum of four (4\\) years of it related experience in amazon web service (aws) or related data technologies.\nexperience must include:* automating aws data pipeline/etl technologies in a production data environment \u2013 s3, glue, lambda.\n* working with a variety of relational data storage options (aws data lake, aws rds, or amazon redshift).\n* writing and interpreting data transformations in python or r.\n* big data processing utilizing pyspark.\n* utilizing version control (svn, git, or other version control system) to build transparent data pipelines.\n* demonstrated ability to communicate clearly and effectively in a manner that facilitates mutual understanding.\n* customer\\-focused approach grounded in active listening, empathy, and solution\\-oriented problem\\-solving to deliver positive user experiences.\n\n\neducation in information technology (it) or an it related field may substitute for experience as follows: a master\u2019s degree for eighteen (18\\) months of experience; a bachelor\u2019s degree for one (1\\) year; and an associate\u2019s degree for six (6\\) months.\n### **preferred qualifications**\n\n* working with non\\-relational data storage options (vector stores or nosql databases) to support machine learning and ai data pipelines.\n* an aws certification such as cloud practitioner or data engineer or solutions architect\n* knowledge of iam identity center and lake formation\n* knowledge of containerization deployments utilizing amazon ecs\n* comfort with troubleshooting data pipelines with little precedence and adjusting to multiple demands, shifting priorities, ambiguity and rapid changes.\n* passion in one or more of the following aspects of data governance: quality, integration, security, lifecycle management, stewardship, metadata management.\n* apply in\\-depth critical thinking and analytic thinking skills to a class of domain\\-specific problems and projects to provide practical assessment and solution generation.\n\n### **additional requirements**\n\n\nit is the policy of minnesota it services that a successful candidate must pass all legally required checks prior to employment which may consist of the following:\n* sema4 records check (applies to current and past state employees only)\n* criminal history check\n* reference check\n* social security and address verification\n* education verification\n* other legally required checks\n\n**minnesota it services does not participate in e\\-verify. minnesota it services will not sponsor applicant for work visas, including f\\-1 stem opt extensions. all applicants must be legally authorized to work in the united states.**\n**application details**\n-----------------------\n\n### **how to apply**\n\nselect \u201capply for job\u201d at the top of this page. if you have questions about applying for jobs, contact the careers help desk at 651\\-259\\-3637 or email careers@state.mn.us. for additional information about the application process, go to http://www.mn.gov/careers.\n\n### **contact**\n\n\nif you have questions about this position, contact jessica lang at jessica.lang@state.mn.us.\n\n### **about minnesota it services**\n\nselect \u201capply for job\u201d at the top of this page. if you have questions about applying for jobs, contact the careers help desk at 651\\-259\\-3637 or email careers@state.mn.us. for additional information about the application process, go to http://www.mn.gov/careers.### \n\n### **contact**\n\n\nif you have questions about this position, contact jessica lang at jessica.lang@state.mn.us.\n**veterans**\n**to be considered for any** **veteran's status****, you must indicate this on your application.**recently separated veterans (rsv): effective july 1, 2009, legislation provides that the top five rsv applicants who apply and meet the qualifications for a vacancy shall be granted an interview. you must:\n* meet all minimum qualifications identified in this posting.\n* meet all of the rsv criteria.\n* submit a copy of your dd\\-214 form by the closing date to: mandie.stevenson@state.mn.us.\n\n**current state employees*** current state employees: please note that employment provisions (including but not limited to seniority and leave accrual) vary among the three branches of minnesota state government. when considering a job with another branch of state government, you are highly encouraged to explore these differences. for assistance, please direct questions to your current or anticipated human resources office.\n\n**about minnesota it services**minnesota it services is the information technology agency for the state of minnesota. mnit partners with agencies, boards, councils, and commissions to deliver secure, reliable technology solutions as we set it strategy, direction, policies, and standards statewide. work for mnit and be part of a nation\\-leading it organization that helps to create an innovative government that works for everyone. our culture promotes collaboration, demands agility, and encourages us to embrace change. be a part of something bigger than yourself, something to be inspired by; come to work for mnit.#### **working together to improve the state we love.**\n\nwhat do minnesota's state employees have in common?\n\n\n* a sense of purpose in their work\n* connection with their coworkers and communities\n* opportunities for personal and professional growth\n\n#### **benefits**\n\nas an employee, you'll have access to one of the most affordable health insurance plans in minnesota, along with other benefits to help you and your family be well.\n\n#### **your benefits may include:**\n\n* paid vacation and sick leave\n* 12 paid holidays each year\n* low\\-cost medical, dental, vision, and prescription drug plans\n\t+ fertility care, including ivf\n\t+ diabetes care\n\t+ dental and orthodontic care for adults and children\n* 6 weeks paid leave for parents of newborn or newly adopted children\n* pension plan that provides income when you retire (after working at least three years)\n* employer paid life insurance to provide support for your family in the event of death\n* short\\-term and long\\-term disability insurance that can provide income if you are unable to work due to illness or injury\n* tax\\-free expense accounts for health, dental, and dependent care\n* resources that provide support and promote physical, emotional, social, and financial well\\-being\n\n#### **support to help you reach your career goals:**\n\n* training, classes, and professional development\n* federal public service loan forgiveness program (some positions may qualify for the public service loan forgiveness program. for more information, visit the federal student aid website at studentaid.gov)\n\n#### **employee assistance program (eap) for work/life support:**\n\n* a voluntary confidential program that helps employees and their families with life challenges that may impact overall health, personal well\\-being, or job performance\n* common sources of stress can be addressed through the eap: mental health, relationship challenges (personal and work), grief and loss, finances, and legal issues\n* daily living/convenience services: chore services, home repair, trip planning, child/elder care\n\n**programs, resources and benefits eligibility varies** based on type of employment, agency, funding availability, union/collective bargaining agreement, location, and length of service with the state of minnesota.\n\n\n**equal opportunity employers**\n\nminnesota state agencies are equal opportunity, affirmative action, and veteran\\-friendly employers. state agencies are committed to creating a workforce that reflects the diversity of the state and strongly encourages persons of color and indigenous communities, members of the lgbtqia2s\\+ community, individuals with disabilities, women, and veterans to apply. the varied experiences and perspectives of employees strengthen the work we do together and our ability to best serve minnesotans.\n\n\nall qualified applicants will receive consideration for employment without regard to race, color, creed, religion, national origin, sex (including pregnancy, childbirth, and disabilities related to pregnancy or childbirth), gender identity, gender expression, marital status, familial status, age, sexual orientation, status regarding public assistance, disability, veteran status or activity in a local human rights commission or any other characteristic protected by law.\n\n**applicants with disabilities**\n\n\nminnesota state agencies make reasonable accommodations to their employees and applicants with disabilities. if you have a disability and need assistance in searching or applying for jobs with the state of minnesota, call the careers help desk at 651\\-259\\-3637 or email careers@state.mn.us and let us know the support you need.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Associate Data Scientist",
        "company": "MetLife",
        "location": "Cary, NC, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "spaCy",
            "Git",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=916958a0f37fc7ab",
        "description": "location(s)\n* posting location: cary, north carolina\n\n  \n\ncity/cities\ncary\ncountry\nunited states\nworking schedule\nfull\\-time\nwork arrangement\nhybrid\nrelocation assistance available\nno\nposted date\n23\\-feb\\-2026\njob id\n15748\n### **description and requirements**\n\n**role value proposition:**\nthe gg\u20118 data scientist is an entry\u2011level role designed for early\u2011career analytical talent. this position supports the full data and analytics lifecycle under close guidance from senior colleagues. the role contributes to production\u2011grade data science solutions while developing strong foundational skills in analytics, machine learning, data preparation, llm\u2011based analysis, and business understanding. the role also provides exposure to modern ai\u2011augmented workflows, including foundation model adaptation and responsible ai practices. **objectives of this role:*** build core analytical and machine learning fundamentals through hands\u2011on project work.\n* support the development, validation, and deployment of analytics and machine learning models.\n* learn to interpret analytical outputs and communicate clear business insights.\n* develop a foundational understanding of llm concepts, prompt engineering, rag workflows, and ai governance.\n **key responsibilities:*** with supervision, participate in basic analytical tasks and project assignments.\n* learn and contribute to the development of simple machine learning solutions.\n* perform exploratory data analysis and feature engineering activities.\n* support preparation of datasets, model development, evaluation, and deployment tasks.\n* under guidance, design, build, validate, and deploy production\u2011grade rule\u2011based and machine learning models using structured and unstructured data.\n* assist with adapting or fine\u2011tuning foundation models and large language models for internal use cases.\n* use ai copilots and ai\u2011augmented tooling to accelerate analysis and validate generated outputs for accuracy.\n* monitor and evaluate model performance and document findings.\n* ensure development follows quality, security, compliance, explainability, and responsible ai guidelines.\n **essential business experience and technical skills:****required:*** bachelor\u2019s degree in mathematics, statistics, operations research, social sciences, computer science, engineering, or related quantitative field.\n* 0\u20132 years of relevant experience in data science, analytics, or related quantitative domains (internships or academic projects acceptable).\n* technical skills in python, statistics, hypothesis testing, feature engineering, and modeling.\n* exposure to machine learning frameworks such as scikit\u2011learn, tensorflow, and/or pytorch.\n* exposure to nlp tools such as spacy, transformers, and/or ocr technologies.\n* strong analytical, communication, and structured problem\u2011solving skills.\n **preferred:*** introductory experience with generative ai concepts, including prompting, embeddings, vector stores, and rag workflows.\n* ability to work with ai copilots and ai\u2011augmented development environments.\n* portfolio of academic, internship, or personal data science projects is a plus.\n **at metlife, we\u2019re leading the global transformation of an industry we\u2019ve long defined. united in purpose, diverse in perspective, we\u2019re dedicated to making a difference in the lives of our customers.**\nlocation expectation: this is a hybrid role **requiring a minimum of 3 days per week** in office.\nthe expected salary range for this position is $65,050 to $83,600\\. this role may also be eligible for annual short\\-term incentive compensation. all incentives and benefits are subject to the applicable plan terms.\n**benefits we offer**  \n\nour u.s. benefits address holistic well\\-being with programs for physical and mental health, financial wellness, and support for families. we offer a comprehensive health plan that includes medical/prescription drug and vision, dental insurance, and no\\-cost short\\- and long\\-term disability. we also provide company\\-paid life insurance and legal services, a retirement pension funded entirely by metlife and 401(k) with employer matching, group discounts on voluntary insurance products including auto and home, pet, critical illness, hospital indemnity, and accident insurance, as well as employee assistance program (eap) and digital mental health programs, parental leave, volunteer time off, tuition assistance and much more!\n**about metlife**  \n\nrecognized on fortune magazine's list of the \"world's most admired companies\", fortune world\u2019s 25 best workplaces\u2122, as well as the fortune 100 best companies to work for\u00ae, metlife, through its subsidiaries and affiliates, is one of the world\u2019s leading financial services companies; providing insurance, annuities, employee benefits and asset management to individual and institutional customers. with operations in more than 40 markets, we hold leading positions in the united states, latin america, asia, europe, and the middle east.\n\n\nour purpose is simple \\- to help our colleagues, customers, communities, and the world at large create a more confident future. united by purpose and guided by our core values \\- win together, do the right thing, deliver impact over activity, and think ahead \\- we\u2019re inspired to transform the next century in financial services. at metlife, it\u2019s \\#alltogetherpossible. join us!\n\n  \n\n  \n\n*metlife is an equal opportunity employer. all employment decisions are made without regards to race, color, national origin, religion, creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, age, disability, marital or domestic/civil partnership status, genetic information, citizenship status (although applicants and employees must be legally authorized to work in the united states), uniformed service member or veteran status, or any other characteristic protected by applicable federal, state, or local law (\u201cprotected characteristics\u201d).* *if you need an accommodation due to a disability, please email us at accommodations@metlife.com. this information will be held in confidence and used only to determine an appropriate accommodation for the application process.*  \n\n  \n\n*metlife maintains a drug\\-free workplace.*\n$65,050 to $83,600",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer - Generative AI",
        "company": "Acuity Insurance",
        "location": "Sheboygan, WI, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "Prompt Engineering",
            "CI/CD",
            "PostgreSQL",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=56a8c0e45b588390",
        "description": "acuity is seeking a senior software engineer to design, build, and maintain production\\-ready generative ai capabilities within existing applications and platforms. this role emphasizes delivering secure, scalable, and user\\-focused ai solutions using managed services and apis, collaborating closely with engineers, architects, and business partners to deliver reliable, enterprise\\-aligned functionality. you will also troubleshoot production issues, refine ai orchestration logic, contribute technical guidance on ai platforms and access approaches, and mentor team members to support best practices, learning, and technical excellence.\n\nessential functions:\n* partner with business analysts, engineers, and stakeholders to understand business challenges and design strong ai solutions, including llm based and agentic systems.\n* demonstrate professionalism and integrity while serving as a role model for engineers and promoting a culture of excellence and curiosity.\n* lead planning and scoping of ai projects, providing input on timelines, model selection, infrastructure, and deliverables.\n* develop and enhance ai applications including rag pipelines, model evaluation frameworks, and agent orchestrations.\n* write clean, scalable, and maintainable code that supports high performing ai systems and meets reliability expectations.\n* demonstrate expert understanding of data ecosystems and how they support ai workflows including data vectorization, retrieval, and context routing.\n* perform testing, debugging, and output validation to ensure ai responses are accurate, safe, and aligned with business logic.\n* produce and maintain clear technical documentation including integration specs, system diagrams, integration guides, and operational runbooks.\n* identify and correct inefficiencies in code, architecture, prompts, data flows, and model parameter tuning.\n* mentor other engineers in ai best practices, model behavior, prompt design, and responsible ai principles.\n* perform code reviews and provide recommendations for accuracy, performance, and adherence to ai safety guidelines.\n* direct and manage ai projects from initiation through delivery including model selection, benchmarking, deployment, and monitoring.\n* identify and develop solutions for highly complex ai integration challenges like prompt chaining, retrieval design, agent coordination, and enterprise scale integrations.\n* understand the architecture of the applications and ai systems you contribute to and ensure new development aligns with strategic patterns.\n* stay current with ai industry trends/tooling and best practices including breakthroughs in llms, agents, and model safety.\n* participate in regular attendance and complete mandatory training programs as assigned by acuity.\n* perform other duties as assigned.\n\n\neducation:\n\n\n\nbachelor\u2019s degree or equivalent combination of education and experience in software engineering, computer science, or related field.\n\n\n\nexperience:\n\n\n\na software engineer with 5 or more years of experience in software engineering, including experience integrating ai capabilities into production applications using external services or apis.\n\n\n\nother qualifications:\n\n\n* expertise across a wide range of tools and technologies, including.\n\t+ programming languages such as java, python, and javascript.\n\t+ relational and non\\-relational databases including db2, postgresql, sql server, and vector databases.\n\t+ familiarity with web ui frameworks such as vue, react, or angular.\n\t+ knowledge of software development workflows, ci/cd pipelines and version control tools.\n\t+ ai and gen ai technologies including llms, retrieval\\-augmented generation (rag), standardized model connection protocols, agent frameworks, and prompt engineering.\n\t+ experience with ai orchestration frameworks, like langchain, to connect llms to data.\n\t+ familiarity with ai observability tools to monitor and debug ai responses.\n\t+ understanding api management, including rate limiting, latency, and token cost optimization.\n\t+ experience with cloud platforms including azure, ibm, gcp, or aws for ai services.\n* ability to analyze user requirements, define technical specifications, estimate work, and deliver high quality solutions.\n* strong analytical thinking with the ability to break down complex problems and apply practical ai driven approaches.\n* strong communication skills and ability to work with cross functional teams.\n* excellence in documentation, testing practices, and operational reliability.\n* ability to stay current with ai advancements, llm research, and evolving best practices.\n* passion for building reliable, safe, and impactful generative ai systems.\n\n\nacuity does not sponsor applicants for u.s. work authorization.\\*\n\n\n*this job is classified as exempt.*\n\n\n*the salary range for this position is $79,500 \\- $130,000 annually. this salary range is an estimate and the actual salary will vary based on applicant\u2019s education, experience, knowledge, skills, and abilities.*\n\n\n*for this role, acuity offers a comprehensive benefits package, including a generous 401(k) contribution, medical, dental, vision, life and disability insurance, paid time off, an employee assistance program, and more. a full description of benefits and eligibility will be provided to candidates during the hiring process.*\n\n\n*we are an equal employment opportunity employer. applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected federal, state/province or local states unrelated to the performance of the work involved.*\n\n\n*if you have a disability and require reasonable accommodations to apply or during the interview process, please contact our talent acquisition team at* *careers@acuity.com**. acuity is dedicated to offering reasonable accommodations during our recruitment process for qualified individuals.*",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist",
        "company": "Pacific Community Ventures",
        "location": "Oakland, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "CI/CD",
            "Git",
            "NoSQL",
            "Tableau",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=0447d34a5a96e2c3",
        "description": "**about us**  \npcv is a nonprofit community development investor that supports small business entrepreneurs and their communities to build economic mobility through the power of inclusive entrepreneurship and a good quality job. we work side\\-by\\-side with small business leaders through our unique integrated model: combining impact\\-first restorative capital and pro bono business advising with our good jobs innovation lab that propels thriving communities with equitable jobs, climate outcomes, uplifts and amplifies their voices and needs through research and policy advocacy.\n\n**the role**  \nare you an experienced data scientist who is passionate about applying your data and technology development skills towards building a more equitable society? if so, please consider joining pacific community ventures as a data scientist. as a mission\\-driven lender, we invest in diverse small businesses across the state of california to help them grow and strengthen their practices. you\u2019ll work to take our renowned data practices to the next level, setting new standards in the community development financial institution (cdfi) ecosystem for machine learning and artificial intelligence innovations to expand access to capital, advising, and good jobs in underestimated communities.\n\n**why you\u2019re needed**  \nwe\u2019re a dynamic nonprofit organization with a growing loan fund that supports more and more small businesses with loans and pro bono mentoring, creating good jobs in low\\-and\\-moderate income communities. with our expansion, we are adding systems, processes, and software to reach entrepreneurs more effectively, and manage the outcomes and impact of our work more efficiently. you will be a member of our data hub team, able to expand pcv\u2019s reputation for data excellence. you bring a strong track record of success in data and analytics, machine learning, artificial intelligence, and insights generation, with strong analytical, technical, writing, and interpersonal skills. you are self\\-directed, able to create clarity out of ambiguity, attentive to detail and data integrity, think critically, and are passionate about investing in underserved communities and creating good jobs.\n\n**position overview**  \nthis position will report to the principal researcher/chief data officer. this is a wonderful opportunity to drive transformative advancements in data infrastructure and innovation by supporting pcv\u2019s data hub initiative and building tools that amplify pcv\u2019s impact across underserved communities.\n\nthe data scientist will advance pcv\u2019s research and innovation capability and build best practices around an ethical and collaborative data culture. this role will sit within the data hub and will serve to strengthen pcv\u2019s reputation as a data leader in the cdfi space.\n\n**key responsibilities**  \n**research with the good jobs innovation lab**  \n\u25cf collaborate closely with the lab to create and facilitate a unique and bold research agenda, based on pcv\u2019s data capabilities, internal knowledge of gaps in the cdfi ecosystem, stakeholder input, and staff capacity.  \n\u25cf mentor lab staff by holding data science office hours to assist in ongoing lab research projects with the aim of increasing staff confidence and knowledge of statistical and machine learning methods.  \n\u25cf design and carry out innovative research projects in collaboration with the lab, from data collection through to delivery of results, in alignment with pcvs research agenda.  \n\u25cf publicly publish and internally peer review research and data stories that advance the use and understanding of data in the cdfi and economic mobility space.\n\n**data culture, education, and governance**  \n\u25cf working with other data staff, conduct a review of data culture at pcv, assessing staff willingness to use, collect and store available data and current practices around data hygiene and governance.  \n\u25cf build pcv\u2019s data governance best practices, taking into account data workers\u2019 daily flow, available industry standards, and pcv\u2019s data infrastructure capabilities.  \n\u25cf create a plan to implement these best practices, including education and regular internal audits around data usage capabilities, and its governance, with the aim of furthering an open and inclusive attitude towards data at pcv.  \n\u25cf regularly touch base with senior management to ensure data governance and data culture continues to be a priority.  \ndata science collaboration  \n\u25cf own data science projects from various stakeholders that require heavy machine learning, ai, or statistical capabilities, such as metric creation, building a chat\\-bot, or a market segmentation model.  \n\u25cf take a data science project through from beginning to end, working closely with stakeholders to understand the problem, design, build and deliver a solution, and monitor results.  \n\u25cf employ and experiment with eda, feature engineering, model evaluation, and other ml techniques to drive iterative scoping of the project, based on stakeholder feedback at various touchpoints.  \n\u25cf write production\\-quality code in python, including testing, logging, and ci/cd, working with data and ml engineers to deploy solutions in a production environment.  \n\u25cf miscellaneous duties and responsibilities can be added at any time.\n\n**qualifications, skills \\& experience:**  \nwe strive hard to be a diverse and inclusive place to work. we value new perspectives, original ideas, and different ways of working. you will bring a background of working with dynamic teams and projects, as well as excellent analytical and communications skills that you are eager to apply to complex social problems. you should demonstrate a deep commitment to creating social and environmental benefit, under\\-invested people, and places.\n\n**education and experience**  \n\u25cf bachelor\u2019s and master\u2019s degrees in data science, statistics, and/or computer science, with a focus on data science and cybersecurity.  \n\u25cf 5\\+ years of experience as a data scientist, in a research\\-oriented role requiring statistical and ml experience.\n\n**technical expertise**  \n\u25cf ability to formulate and carry out a research project from hypothesis and data collection through to testing and delivery of results.  \n\u25cf in\\-depth understanding of statistical frameworks such as probability distributions and bayesian theory, including their limitations.  \n\u25cf hands\\-on experience implementing ml techniques in python, such as sampling, dimension reduction, regression, tree algorithms, and clustering algorithms.  \n\u25cf strong writing skills, experience in writing research briefs and delivering insights and results to stakeholders.  \n\u25cf proficiency in python and machine learning frameworks (e.g., scikit\\-learn, tensorflow, pytorch, pandas).  \n\u25cf familiarity with data policy and governance best practices.  \ncore competencies  \n\u25cf strong problem\\-solving skills with the ability to scope and target key pain points.  \n\u25cf enthusiasm for collaboration and mentorship in order to foster a communal data culture.  \n\u25cf excellent communication skills to deliver complex data in a simple, actionable way.  \n\u25cf commitment to leveraging technology for social good and advancing economic equity.\n\n**other qualifications**  \n\u25cf strongly preferred: experience working in the cdfi ecosystem as demonstrated in comfort with financial services practices and terminology.  \n\u25cf strongly preferred: knowledge of tableau (and other data visualization software).  \n\u25cf strongly preferred: hands\\-on experience deploying models in production using cloud platforms like aws, gcp, or azure.  \n\u25cf strongly preferred: familiarity with ci/cd pipelines, sql/nosql databases, and version control systems like git.  \n\u25cf strongly preferred: experience creating and implementing data governance best practices.\n\n**location/travel**  \nthis position can be remote, but will require at least six hours per day (monday through friday) of overlap with colleagues in the us pacific time zone for virtual collaboration.\n\n**computer software:**  \nmicrosoft office to include word, powerpoint, outlook, and excel  \ntableau and salesforce a plus\n\n**physical and mental requirements:**\n\n* ability to stand for extended periods of time, walk, talk, hear, use hands to finger, grasp, handle or feel, push, pull, reach, crouch, kneel, crawl or bend, and perform repetitive motions of the hands and/or wrists.\n* high mental and visual attention required for planning difficult work methods and sequences to obtain size, shape, or physical qualities of product. and/or extremely close visual attention such as making delicate adjustments to control high speed operations to exercise very precise muscular control.\n\n**to apply**  \ninterested candidates must be legally authorized to work in the united states.\n\npay: $85,000\\.00 \\- $95,000\\.00 per year\n\nbenefits:\n\n* dental insurance\n* health insurance\n* health savings account\n* life insurance\n* paid time off\n* professional development assistance\n* retirement plan\n* vision insurance\n\nwork location: hybrid remote in oakland, ca 94612",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI Engineer Mid-SR",
        "company": "Metova",
        "location": "PR, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "RAG",
            "LLaMA",
            "Pinecone",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=accbd77f01ca5cfb",
        "description": "buscamos un(a) ai engineer con experiencia en construcci\u00f3n y despliegue de soluciones basadas en agentes inteligentes, backend y arquitecturas cognitivas. este rol requiere conocimiento en frameworks de agentes y protocolos (langchain, llama, llms, nlp, a2a, mcp, etc.) y capacidades s\u00f3lidas de software engineering para crear productos con ia embebida. idealmente que tenga conocimiento en contextos financieros, contables, payroll, facturaci\u00f3n o recursos humanos.\n\n**responsabilidades principales**\n\n* experiencia depslegando y programando soluciones basadas en agentes inteligentes y llms, integrando herramientas como langchain, llamaindex, autogen, crewai o frameworks equivalentes.\n* implementar agentes que interact\u00faan con usuarios, apis, sistemas erp o plataformas externas (e.g. whatsapp, sistemas contables, crms).\n* implementar arquitecturas mcp (model context protocol) y a2a (agent\\-to\\-agent) para permitir coordinaci\u00f3n multi\\-agente y flujos aut\u00f3nomos dentro de entornos empresariales.\n* trabajar en ambientes productivos y con interacci\u00f3n directa con equipos de infraestructura, devops y mlops\n* colaborar estrechamente con equipos de producto, ux, datos y backend para tener un delivery exitoso de las funcionalidades planeadas.\n* implementar con buenas pr\u00e1cticas de desarrollo, validaci\u00f3n y monitoreo de agentes, incluyendo tests de integraci\u00f3n, control de versiones de prompts y trazabilidad de decisiones.\n* mantenerse actualizado en frameworks de c\u00f3digo abierto y papers relevantes en el campo de agent frameworks, memory architectures y multi\\-agent systems.\n\n**requirements**\n\n* *4\\-6 a\u00f1os de experiencia en proyectos de inteligencia artificial, con al menos 2\\+ a\u00f1os en implementaci\u00f3n de agentes aut\u00f3nomos o copilts.*\n* *experiencia comprobada utilizando frameworks como langchain, llamaindex, autogen, crewai, semantic kernel o similares.*\n* *conocimiento pr\u00e1ctico de protocolos mcp y a2a, uso de herramientas, gesti\u00f3n de memoria y estado de conversaci\u00f3n.*\n* *s\u00f3lido dominio de python y experiencia con fastapi, asyncio, pydantic y arquitecturas asincr\u00f3nicas.*\n* *experiencia trabajando con vector stores (chroma, weaviate, pinecone) y arquitecturas rag.*\n* *conocimiento en nlp, embeddings, agentes de recuperaci\u00f3n, y funciones de razonamiento y planeaci\u00f3n.*\n* *conocimientos de mlops: ci/cd, docker, kubernetes, monitoreo de agentes, y retraining automatizado.*\n* *ingl\u00e9s t\u00e9cnico fluido para revisar documentaci\u00f3n, papers y colaborar con equipos globales.*\n\n***plus / deseables (nice to have)***\n\n* *experiencia en proyectos multicloud (azure, aws, gcp).*\n* *familiaridad con arquitecturas orientadas a eventos y microservicios, as\u00ed como apis restful y grpc.*\n* *experiencia trabajando con datos empresariales en dominios como contabilidad, finanzas n\u00f3mina, facturaci\u00f3n o erp.*\n* *experiencia trabajando en equipos de desarrollo y producto asi como en equipos de alto rendimiento/startups*\n* *conocimiento pr\u00e1ctico de otros lenguajes como golang, java o c\\# (.net), especialmente en la construcci\u00f3n de componentes de alto rendimiento.*\n* *participaci\u00f3n activa en comunidades de c\u00f3digo abierto de agentes o contribuciones a frameworks emergentes.*\n\n**benefits**\n\n* desarrollo de productos innovadores con enfoque en ia y datos.\n* cultura de buenas pr\u00e1cticas, arquitectura moderna y trabajo colaborativo.\n* posibilidades de crecimiento t\u00e9cnico y formaci\u00f3n continua.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "ML Ops Engineer II",
        "company": "Early Warning Services",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Hadoop",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1bb70d7640bf48cc",
        "description": "at early warning, we\u2019ve powered and protected the u.s. financial system for over thirty years with cutting\\-edge solutions like zelle\u00ae, paze\u2120, and so much more. as a trusted name in payments, we partner with thousands of institutions to increase access to financial services and protect transactions for hundreds of millions of consumers and small businesses.\npositions located in scottsdale, san francisco, chicago, or new york follow a hybrid work model to allow for a more collaborative working environment.\ncandidates responding to this posting must independently possess the eligibility to work in the united states, for any employer, at the date of hire. this position is ineligible for employment visa sponsorship.\nat early warning, we\u2019ve powered and protected the u.s. financial system for over thirty years with cutting\\-edge solutions like zelle\u00ae, paze\u2120, and so much more. as a trusted name in payments, we partner with thousands of institutions to increase access to financial services and protect transactions for hundreds of millions of consumers and small businesses.  \n\n  \n\nbuilding and deploying predictive models is at the heart of what we do. our machine learning operations team enables our data scientists to be able to build and deploy innovative models while developing cutting edge, cloud native capabilities to deliver predictive modeling solutions faster, more accurate, and more efficiently to help keep fraud and bad actors out of the banking system.\noverall purpose\nthis position supports the platforms, tools, and processes that take our models from ideas to production models, serving predictions in real time. the ml ops engineer will partner with our data science, data product management, product engineering, and data platform teams to create and support tools and processes to automate model productionalization.\nessential functions:* designs, builds, and maintains scalable ml infrastructure and pipelines for model training, deployment, and monitoring.\n* optimize orchestration processes to ensure efficient deployment and management of predictive models.\n* monitors and maintains the performance, security, and scalability of the ml infrastructure.\n* collaborates with data scientists and software engineers to streamline the ml lifecycle from development to production.\n* develops and maintains tools for data analysis, experimentation, model versioning, and artifact management. supports data and model governance requirements as needed.\n* creates robust monitoring systems to measure and trend model performance, detect model drift, and ensure optimal performance of models in production.\n* develops automation scripts and tools to improve the efficiency and reliability of mlops processes.\n* optimizes ml workflows for efficiency, scalability, and reliability.\n* provide technical assistance to all team members; troubleshoots moderately complex issues and escalates issues as necessary.\n* supports the company commitment to risk management and protecting the integrity and confidentiality of systems and data.\n* the above job description is not intended to be an all\\-inclusive list of duties and standards of the position. incumbents will follow instructions and perform other related duties as assigned by their supervisor.\n\n\nminimum qualifications* education and experience typically obtained through completion of a bachelor's degree in computer science, engineering, or a related field\n* typically minimum 2 years\u2019 experience or internship in data science, ml engineering or ml ops capacity.\n* intermediate programming skills in python and experience with data science and ml packages and frameworks.\n* demonstrated experience with aws services.\n* intermediate proficiency with containerization technologies (docker, kubernetes) and ci/cd practices\n* demonstrated experience with mlops tools such as mlflow, kubeflow, or similar platforms.\n* understanding and application of data management, distributed computing, and software architecture principles.\n* proven experience delivering real\\-time models in production environments.\n* background and drug screen.\n\n\npreferred qualifications* additional related education and/ or work experience preferred.\n* experience in hybrid (onprem / cloud) environments.\n* hadoop / hive / cloudera experience\n* distributed computing programming skills such as spark\n* experience with scala / java programming languages.\n\n\nphysical requirements\nearly warning works together in a highly collaborative office environment. working conditions consist of a normal office environment. work is primarily sedentary and requires extensive use of a computer and involves sitting for periods of approximately four hours. work may require occasional standing, walking, kneeling, and reaching. must be able to lift 10 pounds occasionally and/or negligible amount of force frequently. requires visual acuity and dexterity to view, prepare, and manipulate documents and office equipment including personal computers. requires the ability to communicate with internal and/or external customers.\nemployee must be able to perform essential functions and physical requirements of position with or without reasonable accommodation.  \n\nthe base pay scale for this position in:  \n\nphoenix, az/ chicago, il in usd per year is: $97,000 \\- $124,000\\.  \n\nsan francisco, ca in usd per year is: $116,000 \\- $149,000\\.  \n\nadditionally, candidates are eligible for a discretionary incentive plan and benefits.  \n\n  \n\nthis pay scale is subject to change and is not necessarily reflective of actual compensation that may be earned, nor a promise of any specific pay for any specific candidate, which is always dependent on legitimate factors considered at the time of job offer. early warning services takes into consideration a variety of factors when determining a competitive salary offer, including, but not limited to, the job scope, market rates and geographic location of a position, candidate\u2019s education, experience, training, and specialized skills or certification(s) in relation to the job requirements and compared with internal equity (peers). the business actively supports and reviews wage equity to ensure that pay decisions are not based on gender, race, national origin, or any other protected classes.\nsome of the ways we prioritize your health and happiness  \n\n* healthcare coverage \u2013 competitive medical (ppo/hdhp), dental, and vision plans as well as company contributions to your health savings account (hsa) or pre\\-tax savings through flexible spending accounts (fsa) for commuting, health \\& dependent care expenses.\n* 401(k) retirement plan \u2013 featuring a 100% company safe harbor match on your first 6% deferral immediately upon eligibility.\n* paid time off \u2013 unlimited time off for exempt (salaried) employees, as well as generous pto for non\\-exempt (hourly) employees, plus 11 paid company holidays and a paid volunteer day.\n* 12 weeks of paid parental leave\n* maven family planning \u2013 provides support through your parenting journey including egg freezing, fertility, adoption, surrogacy, pregnancy, postpartum, early pediatrics, and returning to work.\n\n  \n\nand so much more! we continue to enhance our program, so be sure to check our benefits page here for the latest. our team can share more during the interview process!\npursuant to the san francisco fair chance ordinance, we will consider for employment qualified applicants with arrest and conviction records.*early warning services, llc (\u201cearly warning\u201d) considers for employment, hires, retains and promotes qualified candidates on the basis of ability, potential, and valid qualifications without regard to race, religious creed, religion, color, sex, sexual orientation, genetic information, gender, gender identity, gender expression, age, national origin, ancestry, citizenship, protected veteran or disability status or any factor prohibited by law, and as such affirms in policy and practice to support and promote equal employment opportunity and affirmative action, in accordance with all applicable federal, state, and municipal laws. the company also prohibits discrimination on other bases such as medical condition, marital status or any other factor that is irrelevant to the performance of our employees.*",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Sr Machine Learning Engineer",
        "company": "The Walt Disney Company",
        "location": "Lake Buena Vista, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 13.3,
        "matched_keywords": [
            "AI Engineer",
            "Machine Learning Engineer",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fc65c03a732fa6c9",
        "description": "**job id** 10142996 **location** lake buena vista, florida, united states / burbank, california, united states / seattle, washington, united states / orlando, florida, united states **business** the walt disney company (corporate) **date posted** feb. 23, 2026\n#### **job summary:**\n\n\n**department description:**\n\n\nat disney, we\u2019re storytellers. we make the impossible, possible. the walt disney company is a world\\-class entertainment and technological leader. walt\u2019s passion was to continuously envision new ways to move audiences around the world\u2014a passion that remains our touchstone in an enterprise that stretches from theme parks, resorts and a cruise line to sports, news, movies and a variety of other businesses. uniting each endeavor is a commitment to creating and delivering unforgettable experiences \u2014 and we\u2019re constantly looking for new ways to enhance these exciting experiences.\n\n\nthe enterprise technology mission is to deliver technology solutions that align to business strategies while enabling enterprise efficiency and promoting cross\\-company collaborative innovation. our group drives competitive advantage by enhancing our consumer experiences, enabling business growth, and advancing operational excellence.\n\n**team description:**\n\n\nreporting to the director of automation, tooling, and observability within global network engineering \\& operations (gneo), the machine learning / software engineer plays a critical role in designing, developing, and implementing self\\-healing infrastructure management systems for enterprise\\-wide, production environments. this role combines deep expertise in machine learning, ai technology, software engineering, and devops to create reusable patterns, frameworks, and services to improve reliability across services and platforms. the candidate will serve as a thought leader, identifying opportunities for and applying advanced analytics, predictive modeling, and ai to large\\-scale telemetry, changes, events and incident data to derive actionable insights. the role focuses on building, deploying, and operating machine learning models that proactively detect issues, predict failures, and drive automated, self\\-healing remediation across enterprise systems. the role is intentionally machine learning and ai heavy and is intended to be a strategic driver in that space.\n\n**what you\u2019ll do:**\n\n* work alongside our first\\-class applications, infrastructure \\& operations teams to understand current manual processes and business requirements\n* architect, design, and implement reusable machine learning frameworks, patterns, and services that integrate into the enterprise automation and observability platforms\n* design, train, and deploy machine learning models for anomaly detection, forecasting, predictive analytics, event correlation, pattern recognition, classification, causal analysis, and more in distributed environments that can be used to surface leading indicators of failure\n* build near\\-real\\-time inference pipelines that generate actionable insights from live telemetry, including continuous streams of metrics, logs, traces, and operational events\n* create data abstractions and perform feature engineering on high\\-volume, high\\-cardinality telemetry data\n* evaluate model performance using real production signals and continuously iterate to improve accuracy and reliability\n* build closed\\-loop, event\\-driven systems where model signals trigger automated remediation actions\n* partner with infrastructure and sre teams to identify opportunities and integrate machine learning and ai\\-driven insights into operational tools, workflows, and dashboards\n* analyze incident and historical data to uncover leading indicators and predictive signals\n* own the full machine learning lifecycle: experimentation, validation, deployment, monitoring, and retraining\n* breakdown targeted, manual processes into reusable software modules that leverage machine learning models\n* build emulation and simulation environments (digital twins) of the infrastructure to test ai/ml\\-driven automation under realistic scenarios and allow for faster ideation and iteration for architects and engineers.\n* develop algorithms and frameworks to integrate machine learning and ai technologies into our orchestration platform\n* ensure service reliability, performance, and operational uptime through code\\-driven solutions.\n* conduct root cause analysis, design fault\\-tolerant architectures, and enable self\\-healing automation.\n* implement monitoring dashboards and kpis to provide visibility into automation and tooling performance.\n* collaborate with cross\\-functional teams including network engineers, software developers, machine learning engineers, and operations teams across the enterprise.\n* support the integration of commercial and open\\-source tools while maintaining a vendor\\-agnostic implementation\n\n**required qualifications \\& skills:**\n\n* 7\\+ years of software engineering experience, with expertise in automation, machine learning, and ai technologies\n* proven hands\\-on experience building production\\-grade ml models and inference pipelines; strong proficiency with modern ml frameworks such as pytorch, tensorflow, scikit\\-learn, etc.\n* design, train, and deploy machine learning models for anomaly detection, forecasting, predictive analytics, event correlation, pattern recognition, classification, causal analysis, and more in distributed environments that can be used to surface leading indicators of failure\n* proven hands\\-on experience using software to build frontend, apis and backend functionality; strong proficiency with python, javascript, typescript, go, or rust\n* build emulation and simulation environments (digital twins) of the infrastructure to test ai/ml\\-driven automation under realistic scenarios and allow for faster ideation and iteration for architects and engineers.\n* strong hands\\-on experience building and deploying event\\-driven or streaming data, machine learning models in production\n* solid foundation in statistics, data analysis, and applied machine learning techniques\n* experience working with large\\-scale, real\\-world datasets (noisy, incomplete, non\\-standardized, and evolving)\n* experience operationalizing models in distributed, production environments\n* ability to translate ambiguous operational problems into solvable machine learning use cases\n* experience with modern cloud platforms, container orchestration (kubernetes/docker), identity/auth frameworks, data and workflow orchestration.\n* experience with ai/ml technologies and data engineering concepts. preferred: proven hands\\-on building ai agents.\n* demonstrated success designing and building enterprise\\-scale systems and reusable software frameworks.\n* strong communication, collaboration and leadership skills\n* applies systems thinking to understand how individual components fit into larger, more holistic solutions.\n* capable of quickly shifting between detailed, hands\\-on work and high\\-level strategic thinking.\n\n**preferred qualifications:**\n\n* certifications such as kubernetes (cka/ckad), aws/azure/gcp certifications, ccnp/devnet or nvidia ai engineer.\n* experience developing low\\-code/no\\-code automation platforms or reusable developer toolkits.\n* contributions to open\\-source automation, machine learning, ai, observability, or devops communities.\n* applying unsupervised and semi\\-supervised learning for anomaly detection and signal discovery\n* applying complex event processing and event correlation techniques\n* building time\\-series forecasting models for capacity, latency, and failure prediction\n* experience with feature stores, offline/online feature pipelines, and feature reuse\n* implementing model monitoring for drift, bias, and performance degradation\n* experience with reinforcement learning or decision models for automated remediation and optimization\n* working with real\\-time or near\\-real\\-time inference pipelines\n* experience labeling, curating, and managing training data derived from production telemetry\n* experience mentoring engineers, sharing knowledge, and fostering a learning culture\n* demonstrated curiosity and continuous learning mindset, with a passion for exploring emerging ai/ml, automation, and platform technologies\n\n**required education:**\n\n* bachelor\u2019s degree in computer science, information systems, software, electrical or electronics engineering, or comparable field of study, and/or equivalent work experience\n\n**preferred education:**\n\n* master\u2019s degree in computer science, engineering, or related discipline.\n\n\n\\#disneytech\n\n  \n\nthe hiring range for this position in burbank, ca is $155,700 \\- $208,700 per year and in seattle is $163,100 \\- $218,700 per year. the base pay actually offered will take into account internal equity and also may vary depending on the candidate\u2019s geographic region, job\\-related knowledge, skills, and experience among other factors. a bonus and/or long\\-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Summer 2026 \u2013 Business Value Engineering & Financial Analytics Intern (Master's)",
        "company": "SAS",
        "location": "Cary, NC, US USA",
        "posted_at": "2026-02-22",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Redshift",
            "Git",
            "Databricks",
            "Redshift",
            "Tableau",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=03cc8e157898599f",
        "description": ": **nice to meet you!**\n\n\nwe\u2019re the leader in data and ai. through our software and services, we inspire customers around the world to transform data into intelligence \u2013 and questions into answers.  \n\nif you\u2019re looking for a dynamic, fulfilling career with flexibility and a world\\-class employee experience, you\u2019ll find it here. we\u2019re recognized around the world for our inclusive, meaningful culture and innovative technologies by organizations like fast company, forbes, newsweek and more.  \n\n\n\n**what you\u2019ll do**\n\n\nlooking for \\*that\\* internship? the game\\-changing one that\u2019ll help you learn, grow, and chart your path forward? you\u2019ll find it at sas. our interns aren\u2019t coffee runners \u2013 they do real, meaningful work. our award\\-winning internship program is focused on development, culture, and community. we\u2019ll help you grow professionally, find (or further) your passion, and make memorable connections that last beyond the summer.\n\n  \n\nthe business value engineering team is responsible for providing consultative guidance, including needs discovery, positioning, mapping of software capabilities to business value and vision, leveraging proof of concept results to quantify potential financial impact, and business\\-oriented presentations that highlight the value and uniqueness of sas offerings to prospect organizations. a primary focus of the role will be to apply an understanding of how technology is used to create business and financial value to help customers understand, articulate, and communicate how sas adds business value to a customer\u2019s specific organization or function. as a graduate student intern you will be a significant contributor to a team of people dedicated to ensuring business objectives are well understood and software capabilities are appropriately aligned to successful implementation and adoption.  \n\nas an intern, you might:  \n\n* individually or with another business value engineer, develop financial business cases for customers, calculating return on investment (roi), net present value (npv), payback period, or other key financial metrics which are impacted by sas\u2019 analytics offerings.\n* build out and refine analytics use case frameworks and collaborate with industry consultants to further develop and expand the ongoing collection of customer use cases across major industries.\n* individually or with another business value engineer, strategize with internal stakeholders (inclusive of the sales account executive, data scientists, technical solution architects, and industry consultants) on objectives for customer meetings; work with internal \\& external stakeholders over the course of a sales cycle to synthesize how analytics provides quantitative or qualitative business value for customer\u2019s organization.\n* assist in discovery meetings to collect, analyze, clarify, and document business requirements during the sales cycle to support the formation of a customer\u2019s internal business case.\n* develop custom value propositions or leverage \\& modify existing messaging frameworks which directly align the customer\u2019s vision to the sas analytics solution, value derived from the solution, and how the solution interacts with a customer\u2019s business operations.\n\n  \n\n**required qualifications*** you\u2019re a graduate student enrolled in an accredited university studying business administration, operations or supply chain management, information systems management, analytics, or data science; **not graduating prior to december 2026**.\n* coursework in database management, distributed systems management, managing digital business, operationalizing ai, accounting and finance analytics, and statistics preferred.\n* you\u2019re curious, passionate, authentic, and accountable. these are our values and influence everything we do.\n* strong communication skills \u2013 both written and verbal.\n* leadership abilities. your past experiences demonstrate you'll take initiative and go above and beyond the call of duty.\n* you're interested in the future of analytics and embrace technology\n\n **preferred qualifications*** knowledge of artificial intelligence, machine learning, robotic process automation, business intelligence, software development, data \\& analytics modeling, data visualization, data analysis a plus.\n* previous experience with or knowledge of various software packages/programming languages such as tableau, qlik, aws, amazon redshift, microsoft power platform, javascript, r, python, oracle, sql or databricks also a plus.\n* previous experience with financial analysis and/or business case development a plus.\n* working knowledge of various industries in how they conduct business operations to further translate and convey the impact of data analytic solutions within a specific customer organization or function.\n* previous technology experience in consulting or software pre\\-sales support is considered a plus.\n* finops or cloud economics experience is also a plus.\n* candidate should have a strong interest in data analytics and computer technology and a desire to stay current with the pace of technology and its application across all customers and industries.\n\n **timeline**\n\n* **applications open:** february 2025 on a rolling basis\n* **internship:** may 19, 2026 \u2013 august 7, 2026\\.\n\n **location**\n\n* cary hq/hybrid preferred but open to remote\n\n **perks of the job**\n\n* work with (and learn from) the best. as a sas intern, you\u2019ll get face time with our top executives!\n* free sas programming training and certification.\n* at sas, flexible work is the norm. want to work remotely? that\u2019s cool. prefer a hybrid mix of sweatpants and in\\-person collaboration? that\u2019s great, too.\n* we work hard, but we like to play hard, too. enjoy hackathons, social events and other opportunities to connect \\+ engage.\n* your well\\-being matters, and that\u2019s why we support all dimensions of your well\\-being by offering programs that reduce stress and distractions to help you stay healthy and productive. this includes an on\\-site and remote work/life center staffed by master\u2019s level social workers and an employee assistance program.\n\n **diverse and inclusive**\n\n\nat sas, it\u2019s not about fitting into our culture \u2013 it\u2019s about adding to it. we believe our people make the difference. our diverse workforce brings together unique talents and inspires teams to create amazing software that reflects the diversity of our users and customers. our commitment to diversity is a priority to our leadership, all the way up to the top; and it\u2019s essential to who we are. to put it plainly: you are welcome here.  \n\n\n\n**additional information**\nto qualify, applicants must be legally authorized to work in the united states, and should not require, now or in the future, sponsorship for employment visa status. sas is an equal opportunity/affirmative action employer. all qualified applicants are considered for employment without regard to race, color, religion, gender, sexual orientation, gender identity, age, national origin, disability status, protected veteran status or any other characteristic protected by law. read more: know your rights. also view the pay transparency notice.  \n\nresumes may be considered in the order they are received. sas employees performing certain job functions may require access to technology or software subject to export or import regulations. to comply with these regulations, sas may obtain nationality or citizenship information from applicants for employment. sas collects this information solely for trade law compliance purposes and does not use it to discriminate unfairly in the hiring process. *sas only sends emails from verified \u201csas.com\u201d email addresses and never asks for sensitive, personal information or money. if you have any doubts about the authenticity of any type of communication from, or on behalf of sas, please contact* *recruitingsupport@sas.com.*  \n\npay transparency: *the hourly rate for sas internships is determined by the applicant\u2019s year in school and the position they are hired into. hourly rates range from $15 \u2013 $30 for associate\u2019s/bachelor\u2019s level positions and $29 \\- $49/hour for master\u2019s/ph.d positions. internship roles are not eligible for bonus.*\n\n  \n\nlet's stay in touch! join our talent community to stay up to date on company news, job updates and more.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Mobile Software Engineer (Full Stack)",
        "company": "Sharetec",
        "location": "Nashville, TN, US USA",
        "posted_at": "2026-02-22",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "PostgreSQL",
            "MongoDB",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=927f66c68876634e",
        "description": "sharetec is looking for a **senior mobile software engineer (full stack)** to join our team!\n\n\nat sharetec, we believe in a people\\-first business. our mission is to make millions of people\u2019s lives easier by developing innovative core banking and lending software solutions for credit unions and their members. our customers rely on us for powerful financial technology that enhances operations across accounting, lending, member services, and online banking and they count on us to be a caring, trusted partner in their success.\n\n\nas a **senior mobile software engineer (full stack)** at sharetec, you\u2019ll play a key role in building, evolving, and maintaining production mobile applications used by credit unions across the country. in this role, mobile application development is a core responsibility, not an afterthought. you\u2019ll design and ship android, ios, or cross\\-platform mobile applications while partnering closely with backend and web teams to deliver secure, reliable, and user\\-friendly experiences. you\u2019ll collaborate with product managers, qa, and fellow engineers to design creative solutions and improve system performance. as a senior leader on the team, you\u2019ll be a go\\-to resource\u2014not just for technical architecture, but for mentoring fellow engineers and fostering a culture of continuous learning. this role is ideal for an experienced engineer who enjoys shipping high\\-impact mobile features to production while staying connected to the broader system architecture and helping others grow.\n\n\nthis is a full\\-time, exempt, remote position open to candidates residing in any u.s. state, with the exception of california. the starting salary for this position is $110,000 \\- $140,000 based on experience.\n\n**essential duties and responsibilities**:\n\n**technical leadership**\n\n* architect scalable, secure, and maintainable systems across backend, frontend, and database layers.\n* set the standard for mobile excellence by leading design discussions, code reviews, and engineering best\\-practice initiatives.\n* evaluate new technologies and guide the adoption of tools that improve developer productivity and system performance.\n* define and document architecture patterns, design principles, and shared libraries for reuse across teams.\n* partner with devops to design efficient ci/cd pipelines, deployment automation, and observability tooling.\n\n**software development**\n\n* design, build, and maintain production\\-grade mobile applications for android, ios, or cross\\-platform frameworks.\n* write clean, testable, and efficient code following established coding standards and contributing to their evolution.\n* optimize system performance, reduce technical debt, and improve reliability.\n* collaborate with qa engineers to develop automated testing strategies and ensure high code coverage.\n* participate in production support rotation and proactively drive root cause analysis and prevention.\n\n**collaboration \\& mentorship**\n\n* actively mentor and coach engineers, providing the technical and professional guidance needed to foster career growth and elevate the team\u2019s collective skill set.\n* partner with product managers and stakeholders to translate business needs into technical solutions that balance immediate delivery with long\\-term system health.\n* foster a culture of shared ownership, clear communication, and continuous learning where team members feel supported in taking technical risks.\n\n**qualified candidates should have**:\n\n* **6\\+ years of professional software development experience**, with a proven track record of technical leadership on complex, high\\-stakes projects.\n* **deep mobile expertise:** extensive hands\\-on experience building, deploying, and maintaining production mobile applications (android, ios, or cross\\-platform) that are tightly integrated with backend apis.\n* **full\\-stack proficiency:** expert\\-level knowledge of at least one backend platform (c\\#, .net, java, node.js) and modern frontend frameworks (angular, react, or vue.js).\n* **data \\& api architecture:** strong expertise in database design (progress openedge, postgresql, sql server, or mongodb) and the ability to integrate third\\-party systems and apis securely and efficiently.\n* **legacy modernization:** a proven ability to modernize legacy applications into modular, scalable designs while optimizing for performance.\n* **architectural mastery:** deep understanding of software architecture, design patterns, and performance optimization across the entire stack.\n* **devops \\& cloud:** familiarity with devops tooling, ci/cd pipelines, and cloud services to ensure reliable and automated deployments.\n* **mentorship experience:** proven ability to guide other engineers, provide constructive feedback, and lead by example in code quality and professionalism.\n\n**preferred qualifications**\n\n* **architectural depth:** experience designing distributed systems or microservice architectures that support high\\-availability applications.\n* **modern infrastructure:** hands\\-on experience with containers and orchestration, specifically **docker and kubernetes**, to streamline development and deployment.\n* **security\\-first mindset:** a strong background in implementing secure coding practices and a proactive approach to application security.\n* **fintech expertise:** prior experience in the financial services or fintech industries, with an understanding of the unique challenges of credit union or banking environments.\n* **compliance knowledge:** familiarity with **soc 2** or other regulatory and compliance frameworks relevant to financial data.\n\n**tech stack (what you\u2019ll work with)****:**\n\n**mobile**: react native, ios (swift), and/or android (kotlin) (depending on project needs)\n\n**backend**: .net and restful apis\n\n**frontend**: angular and/or react\n\n**databases**: postgresql, sql server, and progress openedge\n\n**devops \\& tools**: ci/cd pipelines, cloud services, and modern development tooling\n\n**why sharetec:**\n\n\nacquired by evergreen financial technology group (eftg) in late 2020, sharetec is now marching towards rapid growth and expansion into new markets. we are a team of highly focused and dedicated individuals who stop at nothing to achieve success no matter how great or small the challenge; we are also a unique bunch of people that love to work and play together. we do our best to make fun a basic part of every day.\n\n\nsharetec offers a robust benefits package, including competitive salaries, medical, dental, vision, life and disability coverage, paid time off (pto), paid holidays \\- including your birthday off!, $1,000 employee referral program, 401(k) and 401(k) matching. we like to put the fun in the funds with department and company outings like paid food trucks, baseball games, and bowling, along with virtual team\\-building activities such as escape rooms, trivia, and other company\\-wide events.\n\n\nsharetec is an equal opportunity employer.\n\n*keywords: full stack software engineer, senior software engineer, mobile application development, android, ios, react native, core banking software, credit union technology, financial services technology, fintech development, cloud architecture, api integration, ci/cd, devops, software development, microservices, c\\#, .net, angular, react, javascript, sql server, postgresql, mongodb, software engineering jobs, fintech careers, credit union software, banking technology, technical mentor, engineering lead*",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Technical Support Engineer",
        "company": "Sigma Computing",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b94386ac1471ed5e",
        "description": "**about the role:**\n\n\n\nsigma is growing rapidly, and our technical support engineering team is scaling alongside it to meet the needs of an expanding global user base. as a technical support engineer at sigma, you will be part of an award\\-winning team recognized with the 2024 stevie gold award for customer service, helping customers solve technical, business, and data challenges using the sigma platform. you'll work closely with product, engineering, and go\\-to\\-market teams to diagnose complex issues, drive solutions, and contribute to the continuous improvement of our product and support operations.\n\n\n**what you will be doing:**\n\n  \n\n* you will work with sigma's customers and the pre\\-sales team to assist with the diagnosis and resolution of complex technical issues.\n* working closely with the development team, you will develop best practices and tools for diagnosing issues and optimizing the service for performance.\n* collaborate with cross\\-functional groups \\- backend, frontend, devops, design, product, and the go\\-to\\-market teams to create a first\\-class experience for users of our product.\n* participate in quarterly projects, perform periodic on\\-call duties, and other assignments as needed to improve automation and processes.\n\n\n**qualifications we are looking for:**\n\n\n* 2\\+ years of industry experience supporting enterprise products for data analytics.\n* **education:** bachelor's degree in computer science, information systems, or a related field.\n* computer science fundamentals. strong domain expertise in databases and business intelligence\n* sql proficiency \\- very good grasp on joins, partitions, window functions, aggregations, ctes, sub\\-queries etc.\n* sql query performance troubleshooting and plan generation understanding\n* proficient in data modeling concepts\n* ability to properly chart data into logical visualizations\n* a proven track record of building trust with customers and bringing issues to resolution quickly\n* excellent verbal and written communication skills\n* a strong desire to build scalable processes for issue resolution (documenting common patterns for issue resolution, building tooling for diagnosing issues etc)\n* strong collaboration skills and the ability to work with multiple departments and co\\-ordinate issue triaging, diagnosis and resolution\n* desire to be a great teammate and have fun at work\n\n\n**highly desirable skills \\& experiences**\n\n\n* supporting a cloud service in production\n* experience working with snowflake, databricks, redshift, bigquery\n* knowledge of gcp, aws\n* startup experience\n\n\n**additional job details**\n\n\n\nthe base salary range for this position is $90k \\- $125k annually.\n\n\n\ncompensation may vary outside of this range depending on a number of factors, including a candidate's qualifications, skills, competencies and experience. base pay is one part of the total package that is provided to compensate and recognize employees for their work at sigma computing. this role is eligible for an annual bonus, stock options, as well as a comprehensive benefits package.\n\n\n\nif you do not feel that you satisfy all the listed requirements, we encourage you to still apply. we are enthusiastically looking for people that will help us grow our company and sometimes we are imperfect communicators and can't articulate perfectly what experience is required for a role. we are looking for people that are excited to grow and constantly ask how we can do things better. if you are excited about the opportunity, we encourage you to apply even if you don't satisfy 100% of the job requirements.\n\n#### **about us:**\n\n\n\nsigma is the ai apps and analytics platform connected to the cloud data warehouse. using sigma, business and technical teams can build intelligent, production\\-ready ai apps that accelerate and automate operational workflows. sigma provides a spreadsheet interface, sql and python editors, visual builders, and native ai to help teams turn live data into interactive applications, analysis, reports, and embedded experiences.\n\n\n\nsigma announced its $200m in series d financing in may 2024, to continue transforming bi through its innovations in ai infrastructure, data application development, enterprise\\-wide collaboration, and business user adoption. spark capital and avenir growth capital co\\-led the series d funding round, with additional participation from a group of past investors including snowflake ventures and sutter hill ventures.the series d funding, raised at a valuation 60% higher than the company's series c round three years ago, promises to further accelerate sigma's growth.\n\n\ncome join us!\n\n\n#### **benefits for our full\\-time employees:**\n\n\n* equity\n* generous health benefits\n* flexible time off policy. take the time off you need!\n* paid bonding time for all new parents\n* traditional and roth 401k\n* commuter and fsa benefits\n* lunch program\n* dog friendly office\n\n\nsigma computing is an equal opportunity employer. we are committed to building a smart and strong team regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. we look forward to learning how your experience can enable all of us to grow*.*\n\n\n*note: we have an in\\-office work environment in all our offices in sf, nyc, and london.*\n\n\n**our privacy practices**\n\nwhen you submit a job application on this site, sigma processes your personal data for the purposes of evaluating your candidacy for employment at sigma and as otherwise needed throughout the recruitment and hiring process. please review sigma's candidate privacy notice for more details. please note that your personal data may be transferred to a country other than the one in which it was provided (including to usa, the uk, and canada).\n\n\n**sigma's use of ai**\n\nthis hiring process utilizes artificial intelligence tools to assist in candidate screening and assessment. our ai tools are designed to complement, not replace, human decision\\-making.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=382ed68c6be421a9",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=627cfc3847a07be3",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8c3e693503a2f491",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=47fc26ace33f4e57",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=98982339150895a6",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=309f87cd1d128f5b",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=325a8c51c30cf1d5",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d9d29b7e0b3cdd3d",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b35508015ceb239c",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=331f99049a5d1dae",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=59b1ffe4fc565874",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=24051d8ef80e37dd",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=975af84117bd48cb",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=da9723a3cb997006",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2fbdd2a08360c65b",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8da24880aebfb290",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8500db98fb6bca16",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f1d3eb5b6b7312c3",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=03a0717e81c9117f",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Machine Learning Engineer",
        "company": "GEICO",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Kubernetes",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2b05beae17f6dead",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n**overview:** we are seeking an accomplished senior ml engineer to serve as an individual contributor to the generative ai space at geico. in this role, you will be collaborating with a dynamic team of ai and software engineers to design, develop, and deploy systems that drive business value. your passion for scalability, reliability, and usability of generative ai workflows and approaches will allow you to provide hands\\-on guidance, ensure architectural excellence, and lead by example to solve complex, large\\-scale, cross\\-functional problems.\n\n**responsibilities:**\n\n* contribute to the design, development and maintenance of high\\-performance ai solutions that utilize agentic workflows to deliver concrete business value for internal stakeholders. examples include knowledge assistants, voice/text\\-based conversational ai solutions, document/audio processing copilots, voice agents, robotic process automation, etc.\n* collaborate with cross\\-functional teams, including data scientists, ml engineers, software engineers, product managers, designers to gather requirements, help define project scope and prioritize feature backlogs. execute pragmatic technical visions \\& roadmaps that balance business outcomes, product release timelines and engineering excellence.\n* integrate and build solutions using geico ai platform architecture. partner with platform teams to communicate requirements, understand current capabilities and gaps, and contribute to platform development.\n* work on first\\-of\\-its\\-kind solutions within geico, with a deep understanding of business and technical processes, applications, and architecture to guide development.\n* participate in project planning and stakeholder management, ensuring the efficient allocation of resources and timely delivery of solutions.\n* mentor and guide junior engineers via code reviews and design sessions, fostering a collaborative and high\\-performance team culture.\n **basic qualifications**\n\n* 5 years of experience designing and building scalable production aiml applications and systems in cloud environments,\n* proficient in python, java and similar general\\-purpose programming languages.\n* 3 years managing end\\-to\\-end software development life cycle (e.g. cicd pipelines, kubernetes\\-based deployments, testing, monitoring \\& alerting, production support etc.) for backend systems and apis\n* 2 years in training, finetuning, real\\-time/batch inferencing and evaluation systems for aiml models and llms\n* 2 years owning end\\-to\\-end development, monitoring, maintenance, and continuous improvement of scalable, robust aiml applications.\n* bachelor\u2019s degree or above in computer science, engineering, statistics or a related field\n **preferred** **qualifications****:**\n\n* 2 years interfacing directly with internal business stakeholders and/or external stakeholders on aiml initiatives\n* 2 years working with cloud provider solutions such as azure and aws\n* 2 years with tools that power llm\\-based ai agents: eval frameworks, agent tooling, rag pipelines, prompt engineering, etc.\n* strong communication and problem\\-solving skills to excel in dynamic, cross\\-functional and ambiguous decision\\-making environments\n **annual salary**\n\n\n$115,000\\.00 \\- $230,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Machine Learning Engineer",
        "company": "GEICO",
        "location": "Palo Alto, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Kubernetes",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b09166cd6c260952",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n**overview:** we are seeking an accomplished senior ml engineer to serve as an individual contributor to the generative ai space at geico. in this role, you will be collaborating with a dynamic team of ai and software engineers to design, develop, and deploy systems that drive business value. your passion for scalability, reliability, and usability of generative ai workflows and approaches will allow you to provide hands\\-on guidance, ensure architectural excellence, and lead by example to solve complex, large\\-scale, cross\\-functional problems.\n\n**responsibilities:**\n\n* contribute to the design, development and maintenance of high\\-performance ai solutions that utilize agentic workflows to deliver concrete business value for internal stakeholders. examples include knowledge assistants, voice/text\\-based conversational ai solutions, document/audio processing copilots, voice agents, robotic process automation, etc.\n* collaborate with cross\\-functional teams, including data scientists, ml engineers, software engineers, product managers, designers to gather requirements, help define project scope and prioritize feature backlogs. execute pragmatic technical visions \\& roadmaps that balance business outcomes, product release timelines and engineering excellence.\n* integrate and build solutions using geico ai platform architecture. partner with platform teams to communicate requirements, understand current capabilities and gaps, and contribute to platform development.\n* work on first\\-of\\-its\\-kind solutions within geico, with a deep understanding of business and technical processes, applications, and architecture to guide development.\n* participate in project planning and stakeholder management, ensuring the efficient allocation of resources and timely delivery of solutions.\n* mentor and guide junior engineers via code reviews and design sessions, fostering a collaborative and high\\-performance team culture.\n **basic qualifications**\n\n* 5 years of experience designing and building scalable production aiml applications and systems in cloud environments,\n* proficient in python, java and similar general\\-purpose programming languages.\n* 3 years managing end\\-to\\-end software development life cycle (e.g. cicd pipelines, kubernetes\\-based deployments, testing, monitoring \\& alerting, production support etc.) for backend systems and apis\n* 2 years in training, finetuning, real\\-time/batch inferencing and evaluation systems for aiml models and llms\n* 2 years owning end\\-to\\-end development, monitoring, maintenance, and continuous improvement of scalable, robust aiml applications.\n* bachelor\u2019s degree or above in computer science, engineering, statistics or a related field\n **preferred** **qualifications****:**\n\n* 2 years interfacing directly with internal business stakeholders and/or external stakeholders on aiml initiatives\n* 2 years working with cloud provider solutions such as azure and aws\n* 2 years with tools that power llm\\-based ai agents: eval frameworks, agent tooling, rag pipelines, prompt engineering, etc.\n* strong communication and problem\\-solving skills to excel in dynamic, cross\\-functional and ambiguous decision\\-making environments\n **annual salary**\n\n\n$115,000\\.00 \\- $230,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a4c5f2f4766a9526",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior AI Engineer",
        "company": "Medica Services Company LLC",
        "location": "Minnetonka, MN, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "RAG",
            "Copilot",
            "Prompt Engineering",
            "Docker",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a79da7779f52d4b3",
        "description": "medica is a nonprofit health plan with more than a million members that serves communities in minnesota, nebraska, wisconsin, missouri, and beyond. we deliver personalized health care experiences and partner closely with providers to ensure members are genuinely cared for.\n\n\n\nwe're a team that owns our work with accountability, makes data\\-driven decisions, embraces continuous learning, and celebrates collaboration \u2014 because success is a team sport. it's our mission to be there in the moments that matter most for our members and employees. join us in creating a community of connected care, where coordinated, quality service is the norm and every member feels valued.\n\n\n\nmedica is seeking a hands\\-on senior ai platform engineer to help build and scale our next\\-generation ai agents and infrastructure. you\u2019ll work on complex systems that power ai\\-driven healthcare solutions \\- for our members, providers and employees, that improve the healthcare experience end\\-to\\-end. this role is ideal for hands on builders who thrive on ownership, love solving technical and business problems, and are on the cutting edge of software. performs other duties as assigned.\n\n\n**key accountabilities**\n\n\n* design, implement and deploy ai pipelines, systems and platforms \u2013 from evaluation design and benchmarking through product integration\\-ensuring performance, reliability and end\\-to\\-end safety and transparency\n* prototype, build and operate cutting edge product, and consumer facing ai systems, pushing state\\-of\\-the\\-art to solve our core healthcare and technical challenges\n* contribute to technical decision\\-making and architectural discussions for ai and data platforms. upload high engineering standards and share best practices in code review and design discussions\n* stay up to date with the latest research and developments in ai. evaluate new approaches or tools and drive their adoption when they can advance our capabilities. help evangelize to the organization accordingly\n\n\n**required qualifications**\n\n\n* bachelor's degree or equivalent experience in related field\n* 5 years of work experience beyond degree\n\n\n**preferred qualifications**\n\n\n* minimum of 3 years of experience building production grade software with python, go, or equivalent\n* proven ability to create, manage and scale public cloud infrastructure in aws/azure/gcp\n* experience deploying infrastructure via terraform or another declarative tool\n* familiarity with containerization \\& orchestration including docker, k8s, helm\n* modern devops tooling including github actions, prometheus \\+ grafana, open telemetry\n\n\n**while not required, we especially want to talk if you:**\n\n\n* have strong opinions on agent frameworks (e.g., langgraph, autogen, crewai, openai adk)\u2014and can explain when and why to use them (or when/why not to).\n* can explain agent workflows such as orchestrator\\-workers, evaluator\\-optimizer, and prompt chaining\n* have built evaluations for ai systems\u2014for accuracy, bias, security, function calling accuracy, etc\n* can justify decisions between rag, prompt engineering, and fine\\-tuning\n* have experience with modular agent architectures where specialized agents communicate\n* have a stance on when to leverage generative ai\u2014and when not to\n* have worked with healthcare data, hipaa compliance, or in regulated environments\n* love (or hate) claude code, codex, cursor, or github copilot\n* have a favorite mcp server (or 3\\)\n\n\nthis position is an office role, which requires an employee to work onsite at our minnetonka, mn office, on average, 3 days per week.\n\n\n\nthe full salary grade for this position is $102,100 \\- $175,100\\. while the full salary grade is provided, the typical hiring salary range for this role is expected to be between $102,100 \\- $138,605\\. annual salary range placement will depend on a variety of factors including, but not limited to, education, work experience, applicable certifications and/or licensure, the position's scope and responsibility, internal pay equity and external market salary data. in addition to base compensation, this position may be eligible for incentive plan compensation in addition to base salary. medica offers a generous total rewards package that includes competitive medical, dental, vision, pto, holidays, paid volunteer time off, 401k contributions, caregiver services and many other benefits to support our employees.\n\n\n\nthe compensation and benefits information is provided as of the date of this posting. medica\u2019s compensation and benefits are subject to change at any time, with or without notice, subject to applicable law.\n\n\n**eligibility to work in the us:** medica does not offer work visa sponsorship for this role. all candidates must be legally authorized to work in the united states at the time of application. employment is contingent on verification of identity and eligibility to work in the united states.\n\n\n\nwe are an equal opportunity employer, where all qualified candidates receive consideration for employment indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI Platform Engineer",
        "company": "OrderlyMeds",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "Kubernetes",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4bdae7064fd5dc6f",
        "description": "**about orderly wellness**\n\n\norderlymeds is building a better, more supportive healthcare experience\u2014one that\u2019s fast, reliable, and designed with people in mind. as a fully remote company, we\u2019ve grown quickly by combining thoughtful technology with compassionate service, helping tens of thousands of patients get the care and medication they need, without the hassle.\n\n  \n\nwe\u2019re a collaborative, mission\\-driven team working across the u.s. (and beyond!) to improve access to care at scale. with over 100 customer support agents, six pharmacy partners, and a 50\\-state clinical network, we\u2019re scaling fast, and looking for curious, kind, and solutions\\-oriented people to grow with us.\n\n **about the role**\n\n\nwe\u2019re looking for an ai platform engineer to own the infrastructure that powers orderly wellness\u2019 next\\-generation ai systems. you\u2019ll take the foundational platform we\u2019ve built and bring it to production\\-grade scale \\- driving performance, reliability, zero\\-downtime operations, and real\\-time responsiveness across our entire ai stack.\n\n  \n\nthis role sits alongside our ai engineers and ai analysts and focuses on everything under the hood: cloud run optimization, datastore performance tuning, rollout of ci/cd pipelines, batch workflow stability, error detection, scraping/chrome pool orchestration, and ultra\\-fast model serving. if you love solving hard infrastructure problems, squeezing every millisecond out of a system, and making complex systems feel effortless for users \\- this role is for you. this is a hands\\-on ic role with significant ownership and visibility.\n\n **what you will do**\n\n* own reliability \\& uptime for our ai platform \\- ensuring systems are stable, observable, and built for 24/7 availability.\n* optimize performance across our chat, voice, and automation flows, driving latency down from seconds to sub\\-second.\n* architect and maintain scalable infrastructure using google cloud services: cloud run, datastore, cloud storage, pub/sub, secrets manager, and more.\n* design and implement our first ci/cd pipelines, using tools like terraform, cloud build, github actions, or kubernetes\\-based deployments.\n* build and optimize chrome/puppeteer scraping pools and long\\-running batch workflows; create automated recovery and error\\-identification systems.\n* improve workflow run performance, memory usage, resource allocation, concurrency handling, and throughput across services.\n* support our real\\-time voice and chat features by delivering low\\-latency, high\\-throughput serving layers.\n* collaborate with ai engineers to deploy new models, improve inference speed, and maintain high\\-performance api and service layers.\n* establish monitoring, alerting, and logging across the ai platform using tools like cloud logging, cloud monitoring, prometheus, opentelemetry, etc.\n* evaluate and introduce modern open\\-source technologies to keep the platform flexible, scalable, and future\\-proof.\n\n **what you bring**\n\n* 4 \\- 7\\+ years\u2019 experience in devops, site reliability engineering, platform engineering, or cloud infrastructure roles.\n* deep experience with cloud platforms, preferably google cloud (gcp), including cloud run, datastore, storage, iam, and networking fundamentals.\n* proficiency in one or more of: go, python, bash, and experience deploying or supporting microservices at scale.\n* strong foundation in ci/cd, infrastructure as code (terraform, cloud build, github actions), and automated deployment workflows.\n* experience with performance profiling, latency reduction, concurrency, caching, and high\\-throughput system design.\n* experience operating and diagnosing long\\-running jobs, scrapers, browser automation, or message\\-queue\\-driven workflows.\n* strong reliability mindset: observability, metrics, alerts, incident response, logging, chaos testing, and building for fault tolerance.\n* ability to wear multiple hats \\- infrastructure builder, performance tuner, production firefighter, systems architect.\n* a passion for turning prototypes into fast, stable, scalable production systems.\n\n **why you will love it**\n\n* you will own the platform that makes ai possible at orderlymeds \\& other brands.\n* you\u2019ll work directly with the ai team to build the next generation of our infrastructure.\n* you\u2019ll take a platform that works \\- and be the one who makes it fast, resilient, and ready for scale.\n* you\u2019ll introduce the tooling, best practices, and infrastructure patterns that shape the company long\\-term.\n* you\u2019ll get to build with modern, flexible technologies, not inherit legacy systems.\n\n **compensation and benefits**\n\n\norderly wellness offers a comprehensive benefits package designed to support your health, financial well\\-being, and work\\-life balance. benefits include medical, dental, and vision coverage with strong employer contribution, fsa and hsa options (with potential employer contributions), a 401(k) with company match, generous paid time off, paid parental leave, mental health and family\\-planning support, and access to professional development resources.\n\n **the pay range for this role is:**  \n\n90,000 \\- 120,000 usd per year(remote (united states))",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Applied AI Engineer",
        "company": "Propio LS LLC",
        "location": "Overland Park, KS, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "Hugging Face",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "FastAPI",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3055ad914d70d64e",
        "description": "overland park, ks \u2022 data \\& analytics\n\n\n**job type**\n\n\nfull\\-time\n**description**\n\n  \n\n  \n\npropio is on a mission to make communication accessible to everyone. as a leader in real\\-time interpretation and multilingual language services, we connect people with the information they need across language, culture, and modality. we\u2019re committed to building ai\\-powered tools to enhance interpreter workflows, automate multilingual insights, and scale communication quality across industries.  \n\nwe are hiring an **applied ai engineer** to build and deploy practical, high\\-impact ai systems. you will work across speech recognition, large language models, and prompt engineering to ship products like ai\\-generated summaries, interpreter qa tools, and multilingual retrieval systems. you'll operate at the intersection of engineering, research, and product with high ownership and startup\\-level pace.  \n\nthis is a builder role, not a research\\-only position and you will be working closely with our vp of ai to get things live, fast. **key responsibilities:*** prototype, build, and deploy end\\-to\\-end ai applications involving speech, llms, and text generation\n* integrate apis like openai, whisper, deepgram, and open\\-source equivalents for asr and nlp\n* collaborate with engineers to iterate and refine mvps before transitioning to model\\-level optimization\n* develop internal tools and dashboards to test summarization, qa scoring, and multilingual understanding\n* rapidly test ideas and model variations to explore feasibility and impact (build\\-measure\\-learn loop)\n* ensure model pipelines are robust, scalable, and ready for handoff to mlops and production\n* work with the ai pm to align technical outputs with business use cases and feedback loops\n* stay current on applied ai trends in speech and llms, and advise on what to use vs. build\n\n**requirements**\n\n  \n\n  \n\n**qualifications:*** master\u2019s degree in engineering, preferably in computer science, statistics, data science or equivalent work related experience\n* 3\u20135\\+ years of experience working with nlp or speech models in real\\-world applications\n* experience with python, hugging face transformers, openai apis, whisper, langchain, or similar frameworks\n* experience deploying ai models in production or pilot environments (e.g., using fastapi, flask, or streamlit)\n* strong understanding of embeddings, prompt chaining, and pipeline orchestration\n* familiarity with vector databases (e.g., faiss, pinecone, weaviate)\n* comfortable with rapid iteration, mvp mindset, and cross\\-functional collaboration\n* prior exposure to multilingual or low\\-resource language challenges is a plus\n\n **preferred qualifications:*** experience building speech\\-to\\-text pipelines or hybrid asr \\+ llm systems\n* experience with data labeling, annotation, or active learning workflows\n* familiarity with real\\-time audio processing or latency\\-sensitive applications\n* experience in healthcare, legal, or regulated environments (hipaa, phi, section 1557\\)\n\n\n\\#li\\-js1",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Enterprise Data Architect",
        "company": "IvoryCloud",
        "location": "Rockville, MD, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Redshift",
            "Data Lake",
            "CI/CD",
            "Snowflake",
            "Databricks",
            "Redshift",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=180dbcd7c9c036ee",
        "description": "**primary role**\n\nthe enterprise data architect designs and implements scalable, secure, and reliable data architectures across on\u2011premises and cloud environments, ensuring alignment with organizational strategy. the role integrates complex data systems to support analytics, bi, and ai, while enforcing standards for modeling, governance, security, and compliance.  \nthe architect partners with executives, business leaders, and technical teams to modernize legacy platforms, drive cloud and lakehouse adoption, and promote data\u2011driven decision\u2011making.\n\n**key responsibilities**\n\n* lead enterprise\u2011level data architecture strategies ensuring scalability, reliability, performance, and cost efficiency.\n* define and evolve data lake, lakehouse, warehouse patterns to support analytics and ai.\n* design and maintain conceptual, logical, physical data models with modeling standards.\n* oversee integration of complex on\u2011prem systems to support analytics, bi, ai workloads.\n* create shared standards to keep data usage consistent across domains.\n* define and enforce enterprise standards for modeling, data quality, governance, security, privacy, and compliance.\n* establish governance processes for ownership, lineage, metadata, access controls, retention, and regulatory compliance (e.g., pii).\n* coordinate with security to implement encryption, masking, rbac/abac audits.\n* support responsible ai and enterprise data governance practices.\n* partner with data engineers to design batch and streaming pipelines (etl/elt).\n* define standards for ingestion, monitoring, observability, slas, data quality checks, alerting, and incident handling.\n* introduce data observability practices to ensure reliability in enterprise datasets.\n* lead large\u2011scale data migration and modernization initiatives, including cloud adoption and legacy decommissioning.\n* drive adoption of modern data platforms and emerging technologies (lakehouse, streaming, cataloging, governance tools).\n* optimize data storage, retrieval, and lifecycle management for performance, resilience, and cost.\n* partner with platform/devops teams to support ci/cd for data workloads.\n* ensure ai\u2011ready data: curated, documented, high\u2011quality datasets and features for bi, data science, and ai search.\n* support semantic modeling and reusable data products for enterprise analytics.\n* provide technical leadership to engineers, analysts, and project teams on modeling and architecture patterns.\n* collaborate with executives and business stakeholders to align architecture with organizational strategies.\n* communicate complex technical concepts in clear, business\u2011focused language.\n* evaluate and recommend data platform technologies, tools, and patterns based on business needs.\n\n**skills and competencies**\n\n* expert in data architecture and engineering across hybrid and cloud environments.\n* strong knowledge of data lakes, warehouses, lakehouse designs, and streaming systems.\n* strong data modeling skills (relational, dimensional, analytics models).\n* strong sql and advanced programming experience in python or c\\#.\n* hands\u2011on experience with big data/distributed platforms and cloud warehouses (fabric, databricks, snowflake, redshift).\n* experience with etl/elt, streaming ingestion, apis, and messaging patterns.\n* strong knowledge of governance, privacy, security, compliance, and data quality.\n* familiarity with data catalogs, metadata management, and lineage tools.\n* proven ability to lead modernization and cloud migration projects.\n* ability to influence cross\u2011functional teams and manage stakeholders.\n* ability to present architectural options to technical and non\u2011technical audiences.\n* experience defining and measuring slas, slos, and data quality kpis.\n* experience with ci/cd for data workloads and infrastructure\u2011as\u2011code.\n* experience with data observability platforms or practices.\n\n**experience and education**\n\n* 7\\+ years in data roles (e.g., dba, database developer, data engineer, data architect) with at least 3 years in enterprise data architecture or platform design.\n* bachelor\u2019s degree in computer science, data science, information systems, or related degrees.\n* relevant certifications (e.g., azure/aws data or architecture certifications) are highly valued.\n\njob type: full\\-time\n\npay: $125,000\\.00 \\- $140,000\\.00 per year\n\nbenefits:\n\n* 401(k) matching\n* dental insurance\n* health insurance\n* paid time off\n\napplication question(s):\n\n* are you able to come into an office for in\\-person interviews?\n* are you able to come into the office in rockville, md 2 times per week?\n* this position requires us citizenship. are you a us citizen?\n\nwork location: in person",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist, Data Engineer",
        "company": "Nationwide Mutual Insurance Company",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "CI/CD",
            "Jenkins",
            "Git",
            "Snowflake",
            "Databricks",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f15a9458813e7e74",
        "description": "if you\u2019re passionate about being part of a dynamic organization that enables a fortune 100 company with nearly $70 billion in annual sales to drive innovation and adopt new technologies that deliver business results, then nationwide\u2019s technology team could be the place for you! at nationwide\u00ae, \u201con your side\u201d goes beyond just words. our customers are at the center of everything we do and we\u2019re looking for associates who are passionate about delivering extraordinary care.***this role does not qualify for employer\\-sponsored work authorization. nationwide does not participate in the stem opt extension program.***  \n\n  \n\nthis position is within the ids dbo organization, supporting the ha databases (ha db) application area\u2014a portfolio of highly available oracle databases, including inda, cppf, odbs, vldm, explain, and iwdb\u2014that store core annuity, pension, and investment data to support critical back\\-office and digital experiences.\n\n **key responsibilities:**\n\n* **design, build, and maintain data products and pipelines** (batch and streaming) to support reporting, analytics, dashboards, data science, and operational processes.\n* **acquire, transform, and integrate data** from multiple internal and external sources, including structured, semi\u2011structured, and unstructured data.\n* **develop data models and data flows** (e.g., dimensional models, data building blocks) that align to business needs and enterprise standards.\n* **translate business requirements into technical stories**, estimates, and implementation plans for agile delivery teams.\n* **implement and monitor data quality controls** to ensure accuracy, completeness, and reliability of production data; create and run unit and integration tests for data solutions.\n* **build and support apis and data access methods** to expose curated data to downstream consumers and applications.\n* **apply devsecops practices**, including use of ci/cd pipelines, version control, and automated testing, in line with enterprise standards.\n* **partner with data architects, analysts, and business stakeholders** to ensure solutions meet business objectives, data governance expectations, and security/privacy requirements.\n\n **technical skills**\n\n* proficiency in sql and at least one modern programming or scripting language such as python and unix shell scripting.\n* experience with cloud data platforms and infrastructure (e.g., snowflake, databricks, or similar), including security and access management.\n* familiarity with etl/elt tools like informatica, data pipelines, and orchestration frameworks.\n* working knowledge of modern data storage technologies (relational, dimensional, big data, and unstructured data stores).\n\n**job description summary**\n\n\nnationwide\u2019s industry leading workforce is passionate about creating data solutions that are secure, reliable and efficient in support of our mission to provide extraordinary care. nationwide embraces an agile work environment and collaborative culture through the understanding of business processes, relationship entities and requirements using data analysis, quality, visualization, governance, engineering, robotic process automation, and machine learning to produce targeted data solutions. if you have the drive and desire to be part of a future forward data enabled culture, we want to hear from you.  \n\n  \n\nas a data engineer you\u2019ll be responsible for acquiring, curating, and publishing data for analytical or operational uses. data should be in a ready\\-to\\-use form that creates a single version of the truth across all data consumers, including business users, data scientists, and technology. ready\\-to\\-use data can be for both real time and batch data processes and may include unstructured data. successful data engineers have the skills typically required for the full lifecycle software engineering development from translating requirements into design, development, testing, deployment, and production maintenance tasks. you\u2019ll have the opportunity to work with various technologies from big data, relational and sql databases, unstructured data technology, and programming languages.**job description**\n\n**key responsibilities:**\n\n* provides basic to moderate technical consultation on data product projects by analyzing end to end data product requirements and existing business processes to lead in the design, development and implementation of data products.\n* produces data building blocks, data models, and data flows for varying client demands such as dimensional data, standard and ad hoc reporting, data feeds, dashboard reporting, and data science research \\& exploration\n* applies secure software and systems engineering practices throughout the delivery lifecycle to ensure our data and technology solutions are protected from threats and vulnerabilities.\n* translates business data stories into a technical story breakdown structure and work estimate so value and fit for a schedule or sprint is determined.\n* creates simple to moderate business user access methods to structured and unstructured data by such techniques such as mapping data to a common data model, nlp, transforming data as necessary to satisfy business rules, ai, statistical computations and validation of data content.\n* assists the enterprise devsecops team and other internal organizations on ci/cd best practices experience using jira, jenkins, confluence etc.\n* implements production processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\n* develops and maintains scalable data pipelines for both streaming and batch requirements and builds out new api integrations to support continuing increases in data volume and complexity.\n* writes and performs data unit/integration tests for data quality with input from a business requirements/story, creates and executes testing data and scripts to validate that quality and completeness criteria are satisfied. can create automated testing programs and data that are re\\-usable for future code changes.\n* practices code management and integration with engineering git principle and practice repositories.\n\n\nmay perform other responsibilities as assigned.\n\n**reporting relationships:** reports to manager/director data leader.\n\n**typical skills and experiences:**\n\n**education**: undergraduate studies in computer science, management information systems, business, statistics, math, a related field or comparable experience and education strongly preferred. graduate studies in business, statistics, math, computer science or a related field are a plus.\n\n**license/certification/designation**: certifications are not required but encouraged.\n\n**experience**: three to five years of relevant experience with data quality rules, data management organization/standards, practices and software development. experience in data warehousing, statistical analysis, data models, and queries. one to three years\u2019 experience with cloud technology and infrastructure including security and access management. insurance/financial services industry knowledge a plus.\n\n**knowledge, abilities and skills**: data application and practices knowledge. moderate to advanced skills with modern programming and scripting languages (e.g., sql, r, python, spark, unix shell scripting, perl, or ruby). good problem solving, oral and written communication skills.\n\n\nother criteria, including leadership skills, competencies and experiences may take precedence.\n\n\nstaffing exceptions to the above must be approved by the hiring manager\u2019s leader and hr business partner.\n\n**values:** regularly and consistently demonstrates the nationwide values.\n\n**job conditions:**\n\n**overtime eligibility:** exempt (not eligible)\n\n**working conditions**: normal office environment.\n\n**ada**: the above statements cover what are generally believed to be principal and essential functions of this job. specific circumstances may allow or require some people assigned to the job to perform a somewhat different combination of duties.\n\n**benefits**\n\n\nwe have an array of benefits to fit your needs, including: medical/dental/vision, life insurance, short and long term disability coverage, paid time off with newly hired associates receiving a minimum of 18 days paid time off each full calendar year pro\\-rated quarterly based on hire date, nine paid holidays, 8 hours of lifetime paid time off, 8 hours of unity day paid time off, 401(k) with company match, company\\-paid pension plan, business casual attire, and more. to learn more about the benefits we offer.\n\n\nnationwide is an equal opportunity employer. we celebrate diversity and are committed to creating an inclusive culture where everyone feels challenged, appreciated, respected and engaged. nationwide prohibits discrimination and harassment and affords equal employment opportunities to employees and applicants without regard to any characteristic (or classification) protected by applicable law.\n\n **note to employment agencies:**\n\n\nwe value the partnerships we have built with our preferred vendors. nationwide does not accept unsolicited resumes from employment agencies. all resumes submitted by employment agencies directly to any nationwide employee or hiring manager in any form without a signed nationwide client services agreement on file and search engagement for that position will be deemed unsolicited in nature. no fee will be paid in the event the candidate is subsequently hired as a result of the referral or through other means.\n\n\nnationwide pays on a geographic\\-specific salary structure and placement within the actual starting salary range for this position will be determined by a number of factors including the skills, education, training, credentials and experience of the candidate; the scope, complexity and location of the role as well as the cost of labor in the market; and other conditions of employment. if a sales job, sales incentives, based on performance goals are possible in addition to this range. note on compensation for part\\-time roles: please be aware that the salary ranges listed below reflect full\\-time compensation. actual compensation may be prorated based on the number of hours worked relative to a full\\-time schedule.\nthe national salary range for specialist, data engineer : $95,500\\.00\\-$177,500\\.00\nthe expected starting salary range for specialist, data engineer : $95,500\\.00 \\- $143,500\\.00",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist",
        "company": "Indeed",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Hadoop",
            "Tableau",
            "Power BI",
            "Matplotlib",
            "Seaborn",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2ae3389e17a7f4d6",
        "description": "our mission  \n\n\nas the world\u2019s number 1 job site\\*, our mission is to help people get jobs. we strive to cultivate an inclusive and accessible workplace where all people feel comfortable being themselves. we're looking to grow our teams with more people who share our enthusiasm for innovation and creating the best experience for job seekers.  \n\n\n(\\*comscore, total visits, march 2025\\)  \n\n\nday to day  \n\n\ndrive a deep and complete understanding of our users through the application of a mix of machine learning, statistics, and analytics skills. use data to track, understand, and improve user performance and translate that into strategic insights and recommendations. assist the community and identity foundation team with data analysis and aid in making data\\-informed prioritization decisions. work on glassbowl dashboard creation and maintenance: building and maintaining a dashboard to track key metrics and provide insights on user engagement and retention for the glassbowl product. content coverage between fishbowl and glassdoor: conducting an analysis to compare the content coverage between fishbowl and glassdoor and providing recommendations on user cohort to target. maintain key metric report for sr execs for weekly: creating and maintaining a report for senior executives that tracks key metrics such as user engagement and revenue, and providing analysis and insights on a weekly basis. improve user engagement and retention through data\\-driven decision making. increase the efficiency and effectiveness of the community and identity foundation team. identify new opportunities for user acquisition through data analysis.  \n\n\ntelecommute permitted from anywhere in the united states.  \n\n\nresponsibilities  \n\n\nminimum job requirements:  \n\n\nmaster\u2019s degree or foreign degree equivalent in data science, statistics, applied mathematics, computer science, business analytics, business intelligence or related field.  \none (1\\) year of experience in the job offered or in a data scientist\\-related occupation.  \nposition requires one (1\\) year of experience in the following skills:  \n1\\. statistical analysis methods such as experimentation, a/b testing, designing, conducting, interpreting a/b tests for product analytics, statistical analysis techniques, statistical tests or distributions;  \n2\\. programming languages such as: python or r for data manipulation or analysis;  \n3\\. machine learning algorithms such as k\\-nn, naive bayes, svm, or decision forests;  \n4\\. tools for data preprocessing and cleaning;  \n5\\. sql for complex database queries or data extraction;  \n6\\. hadoop, spark, or similar technologies for handling large datasets;  \n7\\. tools like tableau or power bi; and  \n8\\. libraries like matplotlib or seaborn.  \n\n\nmultiple positions available.  \n\n\nsalary range transparency  \n\n\nsalary range: $102,400\\-$153,600 per year.  \n\n\nsalary range disclaimer  \n\n\nthe base salary range represents the low and high end of the indeed salary range for this position in the given work location. actual salaries will vary depending on factors including but not limited to location, experience, and performance. the range(s) listed is just one component of indeed's total compensation package for employees. other rewards may include quarterly bonuses, restricted stock units (rsus), a paid time off policy, and many region\\-specific benefits.  \n\n\nbenefits \\- health, work/life harmony, \\& wellbeing  \n\n\nwe care about what you care about. we have a multitude of benefits to support indeedians, as well as their pets, kids, and partners including medical, dental, vision, disability and life insurance. indeedians are able to enroll in our company\u2019s 401k plan, as well as an equity\\-based incentive program. indeedians will also receive open paid time off, 12 paid holidays a year and up to 26 weeks of paid parental leave. for more information, select your country and learn more about our employee benefits, program, \\& perks at https://www.indeed.com/careers/benefits!  \n\n\nequal opportunities and accommodations statement  \n\n\nindeed is deeply committed to building a workplace and global community where inclusion is not only valued, but prioritized. we\u2019re proud to be an equal employment and affirmative action employer seeking to create a welcoming and inclusive environment. all qualified applicants will receive consideration for employment without regard to disability, status as a protected veteran, or any other non\\-merit based or legally protected grounds.  \n\n\nindeed is dedicated to providing reasonable accommodations to qualified individuals with known disabilities to participate in the employment application process. to learn more about requesting an accommodation, please visit https://www.indeed.com/careers/accommodations. in the request for an accommodation, please inform us of the nature of your request and your contact information. if you are requesting accommodation for an interview, please reach out at least one week in advance of your interview.  \n\n\nfor more information about our commitment to equal opportunity/affirmative action, please visit our esg home page (https://www.indeed.com/esg?co\\=us).  \n\n\ninclusion \\& belonging  \n\n\ninclusion and belonging are fundamental to our hiring practices and company culture, forming an integral part of our vision for a better world of work. at indeed, we\u2019re committed to the wellbeing of our employees and on a mission to make this the best place to work and thrive. we believe that fostering an inclusive environment where every employee feels respected and accepted benefits everyone, fueling innovation and creativity.  \n\n\nwe value diverse experiences, including those who have had prior contact with the criminal legal system. we are committed to providing individuals with criminal records, including formerly incarcerated individuals, a fair chance at employment.  \n\n\nindeed\u2019s employee recruiting privacy policy  \n\n\nlike other employers indeed uses our own technologies to help us find and attract top talent from around the world. in addition to our site\u2019s user and privacy policy found at https://www.indeed.com/legal, we also want to make you aware of our recruitment specific privacy policy found at https://www.indeed.com/legal/indeed\\-jobs.  \n\n\nagency disclaimer  \n\n\nindeed does not pay placement fees for unsolicited resumes or referrals from non\\-candidates, including search firms, staffing agencies, professional recruiters, fee\\-based referral services, and recruiting agencies (each individually, an \"agency\"), subject to local laws. an agency seeking a placement fee must obtain advance written approval from indeed's internal talent acquisition team and execute a fee agreement with indeed for each job opening before making a referral or submitting a resume for that opening.  \n\n\nreference id: 46632  \n\n\nthe deadline to apply to this position is march 31st, 2026\\.  \n\n\n\\#ind123  \n\n\nreference id: 46632",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Applied AI Engineer",
        "company": "propio",
        "location": "Overland Park, KS, US USA",
        "posted_at": "2026-02-23",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "Hugging Face",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "FastAPI",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=223e2d3d1d722fdd",
        "description": "description:\n\npropio is on a mission to make communication accessible to everyone. as a leader in real\\-time interpretation and multilingual language services, we connect people with the information they need across language, culture, and modality. we\u2019re committed to building ai\\-powered tools to enhance interpreter workflows, automate multilingual insights, and scale communication quality across industries.\n\n\nwe are hiring an **applied ai engineer** to build and deploy practical, high\\-impact ai systems. you will work across speech recognition, large language models, and prompt engineering to ship products like ai\\-generated summaries, interpreter qa tools, and multilingual retrieval systems. you'll operate at the intersection of engineering, research, and product with high ownership and startup\\-level pace.\n\n  \n\nthis is a builder role, not a research\\-only position and you will be working closely with our vp of ai to get things live, fast.\n\n **key responsibilities:**\n\n* prototype, build, and deploy end\\-to\\-end ai applications involving speech, llms, and text generation\n* integrate apis like openai, whisper, deepgram, and open\\-source equivalents for asr and nlp\n* collaborate with engineers to iterate and refine mvps before transitioning to model\\-level optimization\n* develop internal tools and dashboards to test summarization, qa scoring, and multilingual understanding\n* rapidly test ideas and model variations to explore feasibility and impact (build\\-measure\\-learn loop)\n* ensure model pipelines are robust, scalable, and ready for handoff to mlops and production\n* work with the ai pm to align technical outputs with business use cases and feedback loops\n* stay current on applied ai trends in speech and llms, and advise on what to use vs. build\n\n\nrequirements:\n**qualifications:**\n\n* master\u2019s degree in engineering, preferably in computer science, statistics, data science or equivalent work related experience\n* 3\u20135\\+ years of experience working with nlp or speech models in real\\-world applications\n* experience with python, hugging face transformers, openai apis, whisper, langchain, or similar frameworks\n* experience deploying ai models in production or pilot environments (e.g., using fastapi, flask, or streamlit)\n* strong understanding of embeddings, prompt chaining, and pipeline orchestration\n* familiarity with vector databases (e.g., faiss, pinecone, weaviate)\n* comfortable with rapid iteration, mvp mindset, and cross\\-functional collaboration\n* prior exposure to multilingual or low\\-resource language challenges is a plus\n\n**preferred qualifications:**\n\n* experience building speech\\-to\\-text pipelines or hybrid asr \\+ llm systems\n* experience with data labeling, annotation, or active learning workflows\n* familiarity with real\\-time audio processing or latency\\-sensitive applications\n* experience in healthcare, legal, or regulated environments (hipaa, phi, section 1557\\)\n\n\n\\#li\\-js1",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Computer Vision Engineer",
        "company": "SportsBiz Group Inc",
        "location": "US USA",
        "posted_at": "2026-02-22",
        "score": 11.1,
        "matched_keywords": [
            "PyTorch",
            "OpenCV",
            "S3",
            "EC2",
            "MLflow",
            "Databricks",
            "PySpark",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a5d516d199f412e9",
        "description": "**help define the future of sports sponsorship technology**\n\n**computer vision engineer (player\\-coach)**\n\nsportsbiz is disrupting the **$100b sports sponsorship ecosystem**. through **deepsport solutions**, our proprietary ai\\-powered analytics platform, we transform chaotic sports broadcasts into structured, actionable intelligence. we uncover hidden inefficiencies in sponsorship investments, empowering global brands to make data\\-driven decisions\u2014instantly.\n\n**are you ready to build the engine that powers the next era of sports marketing?**\n\n**the opportunity**\n\nwe are looking for a **computer vision engineer** who is a \"doer\" at heart. this is a high\\-ownership, high\\-impact role within our data science division. you will not just manage; you will design, implement, and deploy production\\-grade models that operate in the high\\-variance, high\\-stakes world of live sports.\n\n**the tech stack**\n\ncomputer vision is our heartbeat. you will own the detection, tracking, and analysis of branded assets across global broadcasts.\n\n* **core platform:** advanced proficiency in **databricks** (pyspark, delta lake, mlflow).\n* **frameworks:** pytorch, opencv, numpy, pandas, scikit\\-learn.\n* **infrastructure:** aws (ec2, s3, cli) and linux\\-based production environments.\n\n**your mission: from experiment to production**\n\nthis is an execution\\-focused role for a leader who has moved beyond academic experimentation and wants to see their models live in a global saas product.\n\n**applied ml \\& vision pipeline**\n\n* **architect \\& deploy:** design models for object detection and tracking that survive real\\-world broadcast artifacts (lighting changes, camera motion, occlusions).\n* **end\\-to\\-end ownership:** from raw data preparation and feature generation to deployment and performance monitoring.\n* **continuous innovation:** evolve models for ocr, text extraction, and adjacent nlp/speech\\-to\\-text pipelines.\n\n**platform excellence (databricks)**\n\n* build and maintain scalable datasets across **bronze, silver, and gold** layers in delta lake.\n* ensure all pipelines are reproducible, efficient, and production\\-ready in a shared cloud environment.\n\n**the roadmap: growth \\& compensation**\n\nsportsbiz is in a high\\-velocity growth phase. we have successfully raised **$2m** in our seed round and are currently closing a **$5m series a**.\n\n* **the current phase:** as we close our current round, all team members are compensated through **equity \\+ performance\\-based stipends**.\n* **the horizon (anticipated q1/q2\\):** upon round closure, this role transitions to a **full salary \\+ expanded equity \\+ comprehensive benefits**.\n\n**what we\u2019re looking for**\n\n* **the experience:** 3\\+ years of hands\\-on experience in applied computer vision (object detection/tracking is a must).\n* **the skills:** strong python (oop), experience with large\\-scale datasets, and exposure to ocr/nlp systems.\n* **the mindset:** you thrive in scrappy, fast\\-moving environments and value the autonomy of an individual contributor with the influence of an engineering lead.\n\njob type: full\\-time\n\npay: $80,000\\.00 \\- $125,000\\.00 per year\n\nbenefits:\n\n* health insurance\n\nexperience:\n\n* computer vision: 5 years (required)\n\nwork location: remote",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Technical Support Engineer",
        "company": "Sigma Computing",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Redshift",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a01c49c40c088fe3",
        "description": "**about the role:**\n\n\n\nsigma is growing rapidly, and our technical support engineering team is scaling alongside it to meet the needs of an expanding global user base. as a technical support engineer at sigma, you will be part of an award\\-winning team recognized with the 2024 stevie gold award for customer service, helping customers solve technical, business, and data challenges using the sigma platform. you'll work closely with product, engineering, and go\\-to\\-market teams to diagnose complex issues, drive solutions, and contribute to the continuous improvement of our product and support operations.\n\n\n**what you will be doing:**\n\n  \n\n* you will work with sigma's customers and the pre\\-sales team to assist with the diagnosis and resolution of complex technical issues.\n* working closely with the development team, you will develop best practices and tools for diagnosing issues and optimizing the service for performance.\n* collaborate with cross\\-functional groups \\- backend, frontend, devops, design, product, and the go\\-to\\-market teams to create a first\\-class experience for users of our product.\n* participate in quarterly projects, perform periodic on\\-call duties, and other assignments as needed to improve automation and processes.\n\n\n**qualifications we are looking for:**\n\n\n* 2\\+ years of industry experience supporting enterprise products for data analytics.\n* computer science fundamentals. strong domain expertise in databases and business intelligence\n* sql proficiency \\- very good grasp on joins, partitions, window functions, aggregations, ctes, sub\\-queries etc.\n* sql query performance troubleshooting and plan generation understanding\n* proficient in data modeling concepts\n* ability to properly chart data into logical visualizations\n* a proven track record of building trust with customers and bringing issues to resolution quickly\n* excellent verbal and written communication skills\n* a strong desire to build scalable processes for issue resolution (documenting common patterns for issue resolution, building tooling for diagnosing issues etc)\n* strong collaboration skills and the ability to work with multiple departments and co\\-ordinate issue triaging, diagnosis and resolution\n* desire to be a great teammate and have fun at work\n\n\n**highly desirable skills \\& experiences**\n\n\n* supporting a cloud service in production\n* experience working with snowflake, redshift, bigquery\n* knowledge of gcp, aws\n* startup experience\n\n\n**additional job details**\n\n\n\nthe base salary range for this position is $90k \\- $125k annually.\n\n\n\ncompensation may vary outside of this range depending on a number of factors, including a candidate's qualifications, skills, competencies and experience. base pay is one part of the total package that is provided to compensate and recognize employees for their work at sigma computing. this role is eligible for an annual bonus, stock options, as well as a comprehensive benefits package.\n\n\n\nif you do not feel that you satisfy all the listed requirements, we encourage you to still apply. we are enthusiastically looking for people that will help us grow our company and sometimes we are imperfect communicators and can't articulate perfectly what experience is required for a role. we are looking for people that are excited to grow and constantly ask how we can do things better. if you are excited about the opportunity, we encourage you to apply even if you don't satisfy 100% of the job requirements.\n\n#### **about us:**\n\n\n\nsigma is the ai apps and analytics platform connected to the cloud data warehouse. using sigma, business and technical teams can build intelligent, production\\-ready ai apps that accelerate and automate operational workflows. sigma provides a spreadsheet interface, sql and python editors, visual builders, and native ai to help teams turn live data into interactive applications, analysis, reports, and embedded experiences.\n\n\n\nsigma announced its $200m in series d financing in may 2024, to continue transforming bi through its innovations in ai infrastructure, data application development, enterprise\\-wide collaboration, and business user adoption. spark capital and avenir growth capital co\\-led the series d funding round, with additional participation from a group of past investors including snowflake ventures and sutter hill ventures.the series d funding, raised at a valuation 60% higher than the company's series c round three years ago, promises to further accelerate sigma's growth.\n\n\ncome join us!\n\n\n#### **benefits for our full\\-time employees:**\n\n\n* equity\n* generous health benefits\n* flexible time off policy. take the time off you need!\n* paid bonding time for all new parents\n* traditional and roth 401k\n* commuter and fsa benefits\n* lunch program\n* dog friendly office\n\n\nsigma computing is an equal opportunity employer. we are committed to building a smart and strong team regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. we look forward to learning how your experience can enable all of us to grow*.*\n\n\n*note: we have an in\\-office work environment in all our offices in sf, nyc, and london.*\n\n\n**our privacy practices**\n\nwhen you submit a job application on this site, sigma processes your personal data for the purposes of evaluating your candidacy for employment at sigma and as otherwise needed throughout the recruitment and hiring process. please review sigma's candidate privacy notice for more details. please note that your personal data may be transferred to a country other than the one in which it was provided (including to usa, the uk, and canada).\n\n\n**sigma's use of ai**\n\nthis hiring process utilizes artificial intelligence tools to assist in candidate screening and assessment. our ai tools are designed to complement, not replace, human decision\\-making.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Development Engineer - Advertising Technology",
        "company": "Expedia Group",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-22",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Generative AI",
            "Databricks",
            "PySpark",
            "Kafka",
            "R",
            "Java",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5437a1b0e8faf95d",
        "description": "expedia group brands power global travel for everyone, everywhere. we design cutting\\-edge tech to make travel smoother and more memorable, and we create groundbreaking solutions for our partners. our diverse, vibrant, and welcoming community is essential in driving our success.\n\n**why join us?**\n\n\nto shape the future of travel, people must come first. guided by our values and leadership agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\n\n\nwe provide a full benefits package, including exciting travel perks, generous time\\-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees' passion for travel and ensure a rewarding career journey. we\u2019re building a more open world. join us.\n\n**introduction to the team**  \n\nexpedia group advertising builds the platforms and experiences that connect advertisers with millions of travelers worldwide. our mission is to help partners reach the right traveler with the right message at the right time through intuitive advertiser experiences, high\u2011performance ad delivery systems, and intelligent, ai\u2011powered optimization.\n\n\nwe are looking for a **senior software development engineer** to join our **auction and bidding** team, where you will design, build, and optimize **high\u2011scale, high\u2011performance, low\u2011latency systems** that power real\u2011time ad auctions and bidding across our marketplace. you will collaborate closely with **product, data science, and machine learning engineers** to turn ideas into production systems, improve marketplace efficiency and yield, and drive continuous optimization through experimentation. as a senior engineer on the team, you will shape technical direction, champion operational excellence, and consistently raise the technical bar as our platform and business continue to grow.\n\n**in this role, you will:**\n\n* lead the design, implementation, and evolution of backend services that power ad auction and bidding capabilities and to serve relevant ads and increase advertiser reach to travelers across our ad platform\n* drive operational excellence by improving the reliability, scalability, and latency of mission\u2011critical services\n* lead operational reviews that use data and incident learnings to drive systemic improvements in resilience, fault\\-tolerance and operational maturity\n* partner with product, data scientists, and ml engineers on features such as relevance optimization, targeting, and performance modeling, using experimentation to validate impact\n* apply llms and generative ai, where appropriate, to improve ad delivery and automate repetitive tasks at scale\n* mentor and develop engineers across levels through technical guidance, code reviews, and design feedback, consistently raising the team\u2019s technical bar\n\n**minimum qualifications:**\n\n* bachelor\u2019s degree in computer science or a related technical field with **8\\+ years** of experience, or master\u2019s degree with **6\\+ years**, or equivalent related professional experience\n* experience driving the end\u2011to\u2011end technical design and delivery of multiple complex, multi\u2011quarter, cross\u2011team projects\n* experience collaborating with product and business partners to identify problems and deliver scalable technical solutions\n* strong proficiency in at least one programming language (e.g., **java, kotlin**)\n* experience building and operating backend services in the cloud using real\\-time streaming technologies (e.g., **apache** **flink/kafka**) and apis(**grpc/protobuf**)\n* solid understanding of distributed systems, data flows, and operational best practices\n* proven track record of driving **operational excellence** for large\u2011scale production services (e.g., availability, latency, on\u2011call ownership, and incident reviews)\n* experience working with data\u2011driven systems, **generative ai/ml\u2011powered features**, or experimentation platforms\n\n**preferred qualifications:**\n\n* experience with big data tools and frameworks (e.g., pyspark, databricks)\n* experience with advertising technologies (e.g., ad serving, auctions, and bidding)\n\n\nthe total cash range for this position in chicago is $171,500\\.00 to $240,000\\.00\\. employees in this role have the potential to increase their pay up to $274,500\\.00, which is the top of the range, based on ongoing, demonstrated, and sustained performance in the role.\nstarting pay for this role will vary based on multiple factors, including location, available budget, and an individual\u2019s knowledge, skills, and experience. pay ranges may be modified in the future.\n\n\nexpedia group is proud to offer a wide range of benefits to support employees and their families, including medical/dental/vision, paid time off, and an employee assistance program. to fuel each employee\u2019s passion for travel, we offer a wellness \\& travel reimbursement, travel discounts, and an international airlines travel agent (iatan) membership. view our full list of benefits.\n\n**accommodation requests**\n\n\nif you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our recruiting accommodations team through the accommodation request.\n\n\nwe are proud to be named as a best place to work on glassdoor in 2024 and be recognized for award\\-winning culture by organizations like forbes, time, disability:in, and others.\n\n\nexpedia group's family of brands includes: brand expedia\u00ae, hotels.com\u00ae, expedia\u00ae partner solutions, vrbo\u00ae, trivago\u00ae, orbitz\u00ae, travelocity\u00ae, hotwire\u00ae, wotif\u00ae, ebookers\u00ae, cheaptickets\u00ae, expedia group\u2122 media solutions, expedia local expert\u00ae, carrentals.com\u2122, and expedia cruises\u2122. \u00a9 2024 expedia, inc. all rights reserved. trademarks and logos are the property of their respective owners. cst: 2029030\\-50\n\n\nemployment opportunities and job offers at expedia group will always come from expedia group\u2019s talent acquisition and hiring teams. never provide sensitive, personal information to someone unless you\u2019re confident who the recipient is. expedia group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. our email domain is @expediagroup.com. the official website to find and apply for job openings at expedia group is careers.expediagroup.com/jobs.\n\n\nexpedia is committed to creating an inclusive work environment with a diverse workforce. all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. this employer participates in e\\-verify. the employer will provide the social security administration (ssa) and, if necessary, the department of homeland security (dhs) with information from each new employee's i\\-9 to confirm work authorization.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "ML / AI Infrastructure Engineering Intern",
        "company": "Arcline",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "LLaMA",
            "Copilot",
            "Pinecone",
            "FastAPI",
            "Git",
            "Python",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9afaefdf5d1354cd",
        "description": "**about arcline**\n\narcline builds ai\\-powered data tools that help k\\-12 school districts turn fragmented student data into clear, actionable decisions. we work with superintendents and district leaders across alabama, california, kentucky, texas, wisconsin, and more \u2014 replacing months of manual reporting with instant, ai\\-driven answers.\n\nwe're a small, ai\\-native team that ships fast and builds with real users. our interns ship real features to real classrooms.\n\n**the role**\n\nyou'll work on the core intelligence layer that powers arcline's natural language query engine \u2014 the system that lets educators ask questions in plain english and get accurate, cited answers from their district's data.\n\nday to day, that means:\n\n* designing and optimizing rag pipelines using langchain, llamaindex, and vector databases (pgvector, pinecone)\n* preprocessing and normalizing messy education data from multiple district sources for use in ml pipelines\n* evaluating retrieval quality, answer accuracy, and prompt performance across real district datasets\n* experimenting with fine\\-tuning and model adaptation techniques to improve performance on education\\-specific queries\n* architecting agent workflows that route educator questions to the right data sources\n* building evaluation harnesses and benchmarks to systematically improve model output\n* working with the founding engineering team to ship improvements directly to production\n\n**requirements**\n\n* currently pursuing a b.s./b.a. or m.s. in computer science, machine learning, data science, or a related field\n* strong foundations in ml \u2014 you understand embeddings, retrieval, ranking, and evaluation beyond surface level\n* proficiency in python\n* experience with at least one of: langchain, llamaindex, vector databases, or llm apis (openai, anthropic)\n* able to commit to a 10\u201312 week internship beginning in may or june 2026\\. hybrid in san francisco preferred (relocation assistance available); open to remote\n\n**bonus qualifications**\n\n* experience building rag systems or agent\\-based workflows in production or side projects\n* familiarity with evaluation frameworks, prompt optimization, or fine\\-tuning\n* coursework or research in nlp, information retrieval, or knowledge graphs\n* experience with data preprocessing, feature engineering, or working with noisy real\\-world datasets\n* experience with postgres, fastapi, or data pipeline tools (dagster, dbt)\n* ai\\-native development habits \u2014 you use tools like cursor, claude code, github copilot, or codex to ship faster\n\n**compensation**\n\nhourly rate for this position ranges from $45 to $65\\. compensation will vary based on experience level.\n\n**how to apply**\n\nplease submit your application through this posting only. we are unable to review applications sent via email or other channels.\n\nyou don't need to match every listed expectation to apply. we know the best candidates come from diverse backgrounds and experiences \u2014 if this sounds exciting to you, we'd love to hear from you.\n\npay: $45\\.00 \\- $60\\.00 per hour\n\nexpected hours: 40 per week\n\nbenefits:\n\n* relocation assistance\n\nwork location: hybrid remote in san francisco, ca 94114",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Systems Analyst",
        "company": "Houston-Galveston Area Council",
        "location": "Houston, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Copilot",
            "Synapse",
            "Data Lake",
            "Terraform",
            "Git",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a2e29b7af96f5f4f",
        "description": "**about houston\\-galveston area council**\n\n\nthe houston\\-galveston area council is one of the largest regional planning commissions in the country with a diverse service area of 13 counties and more than 7 million people. we are the pulse of our region addressing issues that cross city limits and county lines every single day.\n\n\nwe make decisions that affect our transportation system, ensure the safety and well\\-being of our seniors, connect people to jobs, help families recover from natural disasters, preserve water quality for our children, and so much more. we work to make the region a great place to live, work, and thrive.\n\n  \n\n\n\n**what will i be doing?**\n\n\nwe are seeking an experienced senior systems analyst to join our it team in a critical role that supports and maintains our enterprise infrastructure. this position will be responsible for the administration, optimization, and security of our hybrid cloud and on\\-premises environments. the ideal candidate will serve as a technical expert who can bridge the gap between business requirements and technology solutions while ensuring the stability, security, and performance of our it systems. this role requires strong technical expertise, combined with excellent communication skills, to collaborate effectively with both technical leadership and business stakeholders.\n\n### \u00fc cloud and microsoft 365 administration\n\n\no administer and optimize azure cloud infrastructure and microsoft 365 tenant, including resource management, virtual networks, identity and access management, exchange online, sharepoint, teams, onedrive, and security/compliance features\n\n\no implement and maintain microsoft entra id (azure active directory), conditional access policies, identity governance, and hybrid identity integration with on\\-premises active directory using azure ad connect. configure and manage entra id as an identity provider (idp) for enterprise applications, including sso integration (saml, oauth, oidc), scim provisioning and deprovisioning, application consent policies, and enterprise application lifecycle management\n\n\no support azure infrastructure for data warehousing and analytics platforms, including microsoft fabric, azure synapse analytics, and azure data lake, ensuring appropriate security, performance, and resource optimization for ai/ml and business intelligence workloads\n\n\no monitor cloud resource utilization and optimize costs while maintaining performance standards\n\n### \u00fc virtualization and storage infrastructure\n\n\no administer and maintain virtualization environments (vmware vsphere, microsoft hyper\\-v) and enterprise storage infrastructure (emc vnx, hp alletra), including support for migration initiatives from vmware to alternative platforms\n\n\no monitor and optimize virtual machine and storage performance, manage resource allocation and capacity planning, implement high availability and fault tolerance configurations, and perform vm provisioning and lifecycle management\n\n### \u00fc backup and disaster recovery\n\n\no administer backup infrastructure, including veeam backup and replication, exagrid deduplication appliances, iland cloud backup targets, and azure storage repositories with appropriate retention policies\n\n\no monitor backup job success rates, troubleshoot failures, perform regular disaster recovery testing, maintain dr documentation and runbooks, and ensure compliance with rpo targets, retention policies, and regulatory requirements\n\n### \u00fc active directory and server administration\n\n\no administer on\\-premises active directory domain services including domain controllers, organizational units, users, groups, computer accounts, group policy objects, active directory sites and services, replication, and dns integration\n\n\no implement and maintain server hardening techniques and secure baseline configurations for windows and linux systems following industry best practices (cis, disa stigs, nist), and perform regular patching, updates, and vulnerability remediation\n\n\no monitor and maintain domain controller health, performance, and redundancy\n\n### \u00fc network infrastructure management\n\n\no design, implement, and maintain lan/wan infrastructure, ruckus wireless networking solutions, network segmentation, vlans, routing, and switching infrastructure, with ongoing capacity planning and scalability optimization\n\n### \u00fc security and compliance\n\n\no administer security infrastructure including palo alto security appliances, cloudflare services (ddos protection, waf, cdn), and splunk siem for firewall policies, threat prevention, security monitoring, log analysis, and incident detection\n\n\no implement and maintain security best practices, hardening standards, and compliance across all systems and infrastructure components, and conduct regular security assessments and vulnerability remediation\n\n### \u00fc domain and certificate management\n\n\no manage domain name registrations, dns configurations and infrastructure (internal/external zones, records, dnssec), ssl/tls certificate lifecycle management, and ensure high availability and redundancy of critical dns services\n\n### \u00fc documentation and communication\n\n\no create and maintain comprehensive technical documentation, including system configurations, procedures, sops, architectural decisions, and diagrams (network topology, active directory, virtualization infrastructure, system architecture) using industry\\-standard tools\n\n\no communicate technical information effectively to technical leadership and business stakeholders, and provide regular status updates and reports to management and project teams\n\n### \u00fc technology research and innovation\n\n\no stay current with emerging technologies and industry trends, evaluate new solutions that align with organizational mission and strategic direction, provide recommendations for technology improvements and modernization, and participate in professional development activities\n\n### \u00fc general systems administration\n\n\no provide tier 3 technical support and troubleshooting for complex infrastructure issues\n\n\no collaborate with it team members and business stakeholders on technology initiatives, monitor system performance and capacity planning, and participate in change management processes and maintenance window coordination\n\n\no participate in on\\-call rotation and work after\\-hours and weekends as required for planned maintenance activities, unplanned outages, and support needs\n\n\no perform other duties as assigned to support departmental and organizational objectives\n\n **key qualifications**\n\n\n\u00fc education and experience\n\n\no bachelor's degree in computer science, information technology, or related field, or equivalent work experience\n\n\no minimum 5\\-7 years of experience in systems administration, network administration, or related it infrastructure roles\n\n\no demonstrated experience administering microsoft azure and microsoft 365 environments\n\n\no proven experience with active directory administration in enterprise environments\n\n\no experience with enterprise virtualization platforms (vmware, hyper\\-v, or equivalent)\n\n\no experience with enterprise backup and recovery solutions\n\n  \n\n\u00fc technical skills\n\n\no extensive experience with microsoft cloud and on\\-premises infrastructure, including azure services (compute, storage, virtual networks), microsoft 365 administration (exchange online, sharepoint, teams), active directory domain services, group policy, and hybrid identity solutions (azure ad connect)\n\n\no experience configuring microsoft entra id as an identity provider for third\\-party applications, including sso implementations (saml 2\\.0, oauth 2\\.0, openid connect), automated user provisioning with scim 2\\.0, managing app registrations and service principals, configuring api permissions and consent, and troubleshooting authentication and authorization issues.\n\n\no experience with enterprise virtualization platforms (vmware vsphere, microsoft hyper\\-v) and storage systems (san/nas technologies)\n\n\no hands\\-on experience with enterprise backup and recovery solutions (veeam or equivalent), including disaster recovery planning and business continuity principles\n\n\no experience implementing server hardening techniques, security baseline configurations, and industry security frameworks (cis, disa stigs, nist)\n\n\no solid understanding of networking concepts including tcp/ip, routing, switching, vlans, and subnetting\n\n\no experience with enterprise wireless networking solutions (ruckus or equivalent platforms)\n\n\no hands\\-on experience with enterprise firewall administration (palo alto or similar next\\-generation firewalls)\n\n\no knowledge of dns architecture, configuration, and troubleshooting\n\n\no experience with ssl/tls certificate management and pki infrastructure\n\n\no proficiency in creating technical documentation and network/system diagrams\n\n\no strong analytical and troubleshooting skills with the ability to resolve complex technical issues\n\n\no demonstrated experience using generative ai tools (such as chatgpt, claude, github copilot, or similar platforms) to enhance systems administration work, including script development, troubleshooting assistance, documentation creation, and problem\\-solving\n\n\no ability to effectively craft prompts and leverage ai assistance for technical tasks, including code generation, log analysis, configuration troubleshooting, and technical documentation\n\n\no understanding of generative ai capabilities and limitations in it operations contexts, including awareness of security considerations and data privacy when using ai tools\n\n\n\u00fc professional competencies\n\n\no excellent written and verbal communication skills with the ability to effectively interact with technical teams, leadership, and business stakeholders, and explain technical concepts to non\\-technical audiences\n\n\no proven ability to work independently and manage multiple priorities\n\n\no experience working in a team\\-oriented, collaborative environment\n\n\no commitment to maintaining security and compliance best practices\n\n\no demonstrated commitment to continuous learning, professional development, and adapting to emerging technologies\n\n\no flexibility to work non\\-standard hours, including evenings and weekends when necessary\n\n\no strong attention to detail and organizational skills\n\n **do you have\u2026**\n\n\n\u00fc bachelor's degree in an applicable academic discipline or related field of study\n\n\n\u00fc 5 years of experience with local government, nonprofit programs, schools, or in job\\-related duties\n\n **preferred\u2026**\n\n\n\u00fc certifications\n\n\no microsoft certifications: azure administrator associate, azure solutions architect expert, microsoft 365 enterprise administrator expert, or windows server hybrid administrator associate\n\n\no vmware certified professional (vcp) or microsoft certified: azure virtual desktop specialty\n\n\no veeam certified engineer (vmce) or veeam certified architect (vmca)\n\n\no palo alto networks certified network security administrator (pcnsa) or higher\n\n\no ruckus certified professional or equivalent wireless networking certification\n\n\no security certifications such as comptia security\\+, cissp, or equivalent\n\n\no splunk core certified user or splunk enterprise certified admin\n\n\n\u00fc additional experience and skills\n\n\no experience with backup and disaster recovery technologies, including veeam backup \\& replication, exagrid deduplication appliances, iland cloud services, azure backup, and azure site recovery\n\n\no experience managing enterprise storage arrays (emc vnx, hp alletra) and planning virtualization platform migrations (vmware to hyper\\-v or similar)\n\n\no experience supporting azure data and analytics services such as microsoft fabric, azure synapse analytics, or azure data lake\n\n\no familiarity with azure ai/ml services infrastructure requirements (azure machine learning, cognitive services, azure data lake) and ability to collaborate with data engineering and analytics teams.\n\n\no experience with federated identity management, advanced entra id features including conditional access, identity protection, privileged identity management (pim), and multi\\-factor authentication (mfa) policies\n\n\no knowledge of identity and access management (iam) standards and protocols including saml, oauth 2\\.0, openid connect, scim, and jwt tokens\n\n\no experience with cloudflare services including cdn, waf, and dns management\n\n\no hands\\-on experience with splunk for siem, log management, and analytics\n\n\no experience implementing cis benchmarks, disa stigs, or other hardening frameworks\n\n\no experience with automation and infrastructure as code technologies including powershell, azure cli, terraform, azure resource manager templates, and configuration management tools (desired state configuration, ansible, puppet, chef)\n\n\no proficiency with diagramming tools such as microsoft visio, lucidchart, draw.io, or similar platforms\n\n\no familiarity with itil frameworks and change management processes\n\n\no knowledge of sd\\-wan technologies and implementation\n\n\no knowledge of security and compliance frameworks including zero trust architecture, soc 2, iso 27001, hipaa, and related standards\n\n\no experience with windows server and linux server administration\n\n\no knowledge of privileged access management (pam) solutions\n\n\no previous experience in a leadership or mentoring role\n\n\no project management experience or pmp certification\n\n\no experience presenting technical information to executive leadership and business stakeholders\n\n  \n\n\nall employees of h\\-gac are required to reside within the agency\u2019s region of service to support our commitment to excellence in service of our region.  \n\n  \n\nh\\-gac is an equal opportunity/ada employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, or protected veteran status.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=29e4884190f9d7a3",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Oakland, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=eadabb150b4442f0",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Chevy Chase, MD, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f4da0f08be0e909f",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Fort Worth, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=efaec18d25b07d81",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Washington, DC, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c1cebc1bef750dca",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cde6c28d5dce5f78",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=aaea5358edb7c5ca",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Palo Alto, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a11b62e64d0be8e6",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Richardson, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5ba7b0ffd18b7ca7",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Falls Church, VA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b56c157ed6bcb260",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior .NET Backend Engineer - Billing Platform *Hybrid*",
        "company": "GEICO",
        "location": "Baltimore, MD, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "Kafka",
            "NoSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=93133844d882af32",
        "description": "**at geico, we offer a rewarding career where your ambitions are met with endless possibilities.**\n\n**every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers\u2019 expectations while making a real impact for our company through our shared purpose.**\n\n**when you join our company, we want you to feel valued, supported and proud to work here. that\u2019s why we offer the geico pledge: great company, great culture, great rewards and great careers.**\n\n\nat geico, we are not just an insurance company; we are a technology\\-driven organization that is transforming the insurance landscape. our mission is to leverage cutting\\-edge technology to deliver exceptional experiences for our customers and create innovative solutions that redefine the industry.\n\n**about the team**\n\nthe **billing platform** team at geico oversees the tools, infrastructure, data, reporting, analytics, and services essential for delivering seamless billing experiences to internal users, end customers, and partners. our billing platform functions as the backbone for managing financial transactions and customer interactions, enhancing efficiency, accuracy, and customer satisfaction while supporting strategic growth and ensuring compliance.\n\n**what you will do**\n\nwe are seeking a seasoned software engineer with extensive experience in designing, building, and maintaining large\\-scale applications and distributed systems. you will become an integral part of a team dedicated to managing geico's core billing application. this platform includes a comprehensive array of components such as a core billing engine, invoicing system, commissions management, collections, payment processing, crm integration, subscription management, credit control and dunning management, along with reporting and analytics.\n\n***in this role, you will play a pivotal role in re\\-architecting our platform from the ground up, focusing on enhancing the scalability and efficiency of our systems***.\n\n**responsibilities**\n\n* oversee high\\-level and low\\-level designs of one or more sub\\-systems of the billing application we are building\n* be responsible and accountable for the quality, reliability, usability, and performance of the solutions\n* provide strategic guidance and oversight for multiple billing teams, ensuring alignment with the platform's technical vision and business objectives\n* lead the design and development of complex software systems, ensuring they are scalable, maintainable, and meet high\\-quality standards (this includes evaluating code quality and collaborating with stakeholders to understand and implement project requirements)\n* identify and prioritize technical challenges that may pose risks to business\n* develop solutions to address these issues efficiently, ensuring smooth product development\n* work closely with various departments, including product management and design, to ensure cohesive and successful project delivery\n* facilitate effective communication and collaboration across teams to achieve common goals\n* mentor and guide engineers, fostering a culture of continuous learning and improvement\n* provide technical guidance to help team members overcome challenges and make informed decisions\n\n**who you are**\n\nwe are looking for someone who meets the minimum requirements to be considered for the role. if you meet these requirements, you are encouraged to apply. the preferred qualifications are a bonus, not a requirement.\n\n**minimum requirements**\n\n* 4\\+ years of professional, hands\\-on software development experience\n* strong experience in architecting and designing large\\-scale, complex systems\n* proficient coding skills in **.net and/or golang**, capable of producing high\\-performance, production\\-quality code\n* experience with a wide range of technologies, including **sql and nosql databases**, **kafka, spark, airflow,** or their equivalents\n* proficient in using cloud computing tools throughout the software development lifecycle, with deep expertise in devops, observability, telemetry, and test automation\n* skilled in collaborating across engineering teams and other functions to build alignment, drive decision\\-making, and communicate transparently\n* develop and optimize services using sql server, postgres, rest apis, microservices\n* work with azure, docker, kubernetes, and automation frameworks\n* monitor and debug with app insights, titan, and related tools\n* experience working with xml\\-based data/file systems (xpath, xsd, xml serialization in .net etc.).\n\n**preferred qualifications**:\n\n* experience in the financial technology sector, with a focus on billing, payments, subscription management, and financial reporting\n* advanced .net ecosystem experience (c\\#, asp.net, web api, rest, sql server, visual studio/vs code)\n* proven track record in designing and implementing workflow engines\n* duckcreek product experience\n* continuous delivery / modern deployment practices ( agile )\n* github copilot / ai\\-assisted development experience\n\n**education**:\n\n* bachelor's and/or master's degree, preferably in cs, or equivalent experience\n\n\n\\*this is a hybrid position working out of any of the below offices\\*\n\n\npalo alto, ca\n\n\ndallas, tx\n\n\nchevy chase, md\n\n **annual salary**\n\n\n$100,000\\.00 \\- $215,000\\.00\nthe above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate\u2019s work experience, education and training, the work location as well as market and business considerations.\n\n  \n\n\ngeico will consider sponsoring a new qualified applicant for employment authorization for this position. **the geico pledge:**\n\n**great company:** at geico, we help our customers through life\u2019s twists and turns. our mission is to protect people when they need it most and we\u2019re constantly evolving to stay ahead of their needs.\n\n\nwe\u2019re an iconic brand that thrives on innovation, exceeding our customers\u2019 expectations and enabling our collective success. from day one, you\u2019ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people\u2019s lives.\n\n**great careers:** we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career \u2013 and your potential \u2013 in mind. you\u2019ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels.\n\n**great culture:** we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi\\-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose.\n\n\nas part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers.\n\n**great rewards:** we offer compensation and benefits built to enhance your physical well\\-being, mental and emotional health and financial future.\n\n* comprehensive total rewards program that offers personalized coverage tailor\\-made for you and your family\u2019s overall well\\-being.\n* financial benefits including market\\-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition\\-based incentives; and tuition assistance.\n* access to additional benefits like mental healthcare as well as fertility and adoption assistance.\n* supports flexibility\\- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year.\n\n\nthe equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled.\n\n\ngeico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Kentan Staffing Solutions",
        "location": "Melbourne, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Git",
            "MongoDB",
            "NoSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ba7c1453cd1512f5",
        "description": "**title: senior software engineer**\n\n**location:** melbourne, fl \u2013 must be local and able to go onsite a few times a week\n  \n\n***onsite/hybrid***\n\n**work type:** contract\\-to\\-hire 3\\-6 month contract with full intent to convert to full\\-time\n\n\n**start:** asap\n\n\n**department:** engineering\n  \n\n**reports to:** senior / lead software engineer\n  \n\n**work authorization:** usc or gc holders\n\n\n**industry experience:** healthcare experience is a plus\n\n\n**position overview**\n\nour client is seeking a **senior software engineer** to design, build, and maintain scalable components of a cloud\\-native saas platform operating within a regulated environment. this role is heavily hands\\-on and will contribute across backend services, machine learning integrations, kubernetes deployments, and enterprise\\-grade react applications in an aws\\-based infrastructure.\n\n\nthe ideal candidate is a strong full\\-stack engineer with deep experience in modern javascript ecosystems, distributed systems, and cloud\\-native architectures, and can independently deliver production\\-ready solutions with minimal oversight.\n\n\n**key responsibilities**\n\n**backend development (node.js \\& services)**\n\n* design, develop, and maintain scalable node.js microservices\n* build secure restful and event\\-driven apis\n* integrate ml inference services into backend workflows\n* optimize service performance, reliability, and scalability\n\n**frontend development (react)**\n\n* develop and maintain enterprise\\-scale react applications\n* implement performant state management strategies\n* ensure ui consistency, reliability, and maintainability\n* collaborate closely with product and engineering leadership on feature delivery\n\n**cloud \\& kubernetes operations**\n\n* deploy and manage containerized workloads in kubernetes (eks preferred)\n* implement and support ci/cd pipelines\n* monitor application health, performance, and reliability\n* troubleshoot and resolve production issues in distributed systems\n\n**data \\& nosql systems**\n\n* design and maintain nosql data models (mongodb preferred)\n* optimize query performance and indexing strategies\n* support data pipelines used for machine learning workflows\n\n**engineering standards \\& process**\n\n* follow advanced git workflows and version control best practices\n* participate in code reviews and architectural discussions\n* contribute to agile sprint planning, estimation, and retrospectives\nwrite clean, testable, and well\\-documented code  \n* \n\n**required qualifications**\n\n* 8\\+ years of javascript experience\n* 5\\+ years of node.js experience\n* 5\\+ years of react experience\n* 3\\+ years of kubernetes experience\n* 3\\+ years of aws cloud experience\n* 2\\+ years of nosql database experience (mongodb preferred)\nproven experience integrating machine learning models into production systems  \n* \n\n**technical competencies**\n\n**software architecture**\n\n* strong understanding of microservices and distributed systems\n* api\\-first design mindset\n* experience supporting multi\\-tenant saas platforms\n\n**devops \\& cloud**\n\n* advanced understanding of ci/cd principles\n* containerization and deployment best practices\n* infrastructure awareness (terraform familiarity preferred)\n* observability, logging, and monitoring fundamentals\n\n**mathematics \\& ml awareness**\n\n* strong analytical and mathematical foundation\n* experience working with statistical or ml\\-driven systems\n* understanding of the ml system lifecycle (training, deployment, monitoring)\n\n**version control**\n\n* advanced git usage and branching strategies\n* experience with collaborative code review workflows\n\n**agile delivery**\n\n* strong understanding of agile methodologies\n* experience working in sprint\\-based delivery environments\n* ability to estimate, plan, and break down complex technical features\n\n**preferred / nice\\-to\\-have experience**\n\n* experience in healthcare saas or other regulated industries\n* familiarity with hipaa\\-aligned infrastructure environments\n* experience with edi transactions (837p / 837i)\n* startup or high\\-growth company experience\n* experience with infrastructure\\-as\\-code tools (terraform)",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Junior Software Engineer",
        "company": "Kentan Staffing Solutions",
        "location": "Melbourne, FL, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "MongoDB",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=726fbe9b2b7bd3f4",
        "description": "**title: junior software engineer**\n\n**location:** melbourne, fl\n  \n\nremote with occasional travel (must reside in est)\n\n\n**work type:** contract\\-to\\-hire 3\\-6 month contract with full intent to convert to full\\-time\n\n\n**start:** asap\n\n\n**department:** engineering\n  \n\n**reports to:** senior / lead software engineer\n  \n\n**work authorization:** usc or gc holders\n\n\n**industry experience:** healthcare experience is a plus\n\n\n**position overview**\n\nour client is seeking a **junior software engineer** to support the development of a modern, cloud\\-native saas platform within a highly collaborative engineering organization. this role will contribute across backend services, frontend applications, and emerging machine\\-learning\u2013enabled workflows in a secure aws\\- and kubernetes\\-based environment.\n\n\nthis is an excellent opportunity for an early\\-career engineer with a strong technical foundation who is eager to learn, grow, and be mentored by senior engineers while working on real production systems.\n\n\n**key responsibilities**\n\n**software development**\n\n* implement well\\-defined backend and frontend features under senior guidance\n* develop and maintain node.js services\n* contribute to react\\-based user interfaces\n* write unit and integration tests to ensure code quality\n\n**cloud \\& devops exposure**\n\n* assist with deploying services to kubernetes environments\n* support ci/cd pipeline activities\n* monitor application logs and assist with troubleshooting\n* follow secure coding and deployment best practices\n\n**data \\& machine learning support**\n\n* support integration of ml inference endpoints\n* assist with data validation and preprocessing logic\n* contribute to internal tools used for model evaluation and data workflows\n\n**engineering best practices**\n\n* follow git\\-based workflows and branching standards\n* participate in code reviews and agile ceremonies (standups, sprint planning, retrospectives)\n* write clean, readable, and maintainable code\n* actively seek feedback and mentorship\n\n**required qualifications**\n\n* 1\u20133 years of professional software development experience (or relevant internship/co\\-op experience)\n* strong proficiency in javascript\n* working knowledge of node.js\n* working knowledge of react\n* familiarity with git and version control\n* strong analytical, mathematical, and problem\\-solving skills\n* experience working in agile development environments\n\n**preferred / nice\\-to\\-have experience**\n\n* exposure to kubernetes or containerization tools (docker)\n* exposure to aws cloud services\n* experience with nosql databases (mongodb preferred)\n* basic understanding of restful apis\n* familiarity with python\n* exposure to machine learning concepts or data\\-processing workflows\n* healthcare software experience",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Analytics Engineer",
        "company": "RollKall",
        "location": "Irving, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Dataflow",
            "Git",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=68db2c37ad6b9984",
        "description": "rollkall was founded by a retired police officer with one goal in mind: help public safety professionals\u2014police, firefighters, emts\u2014connect with their communities through something called off\\-duty work. these are extra\\-duty jobs that enhance local safety while putting some extra cash in first responders\u2019 pockets.\n\n\nwe built the platform that makes it easy, efficient, and trusted\u2014for departments, officers, and the organizations that need them.\n\n **position overview**\n\n\nrollkall is seeking an experienced analytics engineer to join our business intelligence team and shape the foundation of our analytics infrastructure. you'll build and maintain high\\-quality data pipelines, establish best practices for data transformation and validation, and drive adoption of ai\\-powered tools \u2014 all while building the dashboards and reports our stakeholders depend on. this role bridges data engineering and business intelligence, with a tech stack that includes domo (cloud data warehouse and visualization), sql server, and azure logic apps for third\\-party integrations.\n\n\nthis position offers the opportunity to shape rollkall's analytics infrastructure \u2014 you won't just be building reports, you'll be driving enhancements to quality standards, documentation practices, and ai strategy that will scale our bi capabilities as we grow. the ideal candidate sees data quality as a strategic advantage, not a compliance checkbox, and is excited about building systems that make reliable data accessible to the entire organization and rollkall's customer base.\n\n **what you\u2019ll be doing**\n\n*data quality \\& governance*\n\n* design and implement data quality frameworks, including validation rules, testing procedures, and monitoring systems to ensure data accuracy and reliability\n* build and maintain comprehensive technical documentation for all data assets, including data dictionaries, transformation logic, business rules, and data lineage\n* enforce analytics engineering best practices and standards across the bi environment\n* proactively identify and resolve data quality issues, conduct root cause analysis, and implement preventive measures\n* champion data governance initiatives and serve as the quality advocate for the bi team\n\n*analytics engineering \\& bi development*\n\n* design, develop, and optimize data transformations using domo's magic etl (both visual dataflows and sql)\n* build scalable, performant data models that serve as the foundation for reporting and analytics\n* create and maintain domo dashboards, cards, and visualizations based on business requirements\n* manage domo platform administration, including user access, dataset schedules, and dataflow performance optimization\n* develop and automate reports and scheduled deliveries to support business operations\n* collaborate with stakeholders to gather requirements, translate business needs into technical solutions, and deliver actionable insights\n* troubleshoot data discrepancies and performance issues across the analytics stack\n\n*ai strategy \\& innovation*\n\n* evaluate domo's ai capabilities and help develop a roadmap for adopting ai\\-powered features that enhance data quality, self\\-service analytics, and business insights\n* leverage existing ai tools (eg, claude code) to accelerate documentation creation and improve data discovery\n* champion the strategic use of ai within the bi environment to drive efficiency and innovation\n* stay current with emerging analytics engineering trends and domo platform developments\n\n### **what we\u2019re looking for**\n\n* demonstrated experience owning data quality initiatives and implementing validation frameworks\n* proven track record of building technical documentation and establishing process standards\n* experience working independently and taking full ownership of projects from requirements through delivery\n* 5\\-7 years of hands\\-on experience in analytics engineering, business intelligence, data analytics, or related roles.\n\n*technical skills*\n\n* bi platform experience: strong experience with at least one modern bi platform (domo strongly preferred; tableau, power bi, or similar platforms acceptable)\n* etl/data transformation: hands\\-on experience building and maintaining etl/elt pipelines and data transformation workflows\n* data modeling: solid understanding of dimensional modeling, data warehousing concepts, and analytics\\-ready data structure\n* version control: experience with github for code repository management\n* sql expertise: advanced sql skills, including query optimization, performance tuning, complex joins, and window function optimization\n\n*business \\& communication skills*\n\n* excellent communication skills with the ability to translate technical concepts for non\\-technical audiences\n* strong analytical and problem\\-solving abilities\n* ability to work effectively with remote team members and collaborate across departments\n* self\\-motivated with the ability to identify and drive process improvements proactively\n* experience balancing multiple priorities and managing stakeholder expectations\n\n*preferred qualifications*\n\n* direct experience with domo platform (magic etl, dataflows, beast modes, domo administration)\n* familiarity with azure cloud services and logic apps\n* experience with ai/ml tools in analytics contexts\n* background in saas, municipal government, or public safety sectors\n* proficiency with object\\-oriented programming (python, .net, or similar)\n* knowledge of json and yaml\n* bachelor\u2019s degree in computer science, engineering, information systems, or related field\n\n **our culture**\n\n\nrollkall is built on four core values: one team (people first, trust, care, diversity, and inclusion), servant leadership (do the right thing, question authority), humbition (humility, ambition, and empathy for our users), and deliver results (bias for action, innovate, embrace change). we're looking for someone who doesn't just fit our culture \u2014 but strengthens it.\n\n**location \\& work schedule**\n\n\nthis position is preferred to work out of our las colinas (dfw) office, though remote candidates will be considered. rollkall employees are required to work in the office two days per week unless otherwise authorized \u2014 our office serves as the hub for innovation, collaboration, and idea sharing across our diverse, global team.\n\n**compensation \\& benefits**\n\n\nalong with a competitive salary and bonus structure, you'll be eligible for healthcare (medical, dental, vision, prescription drugs, fsa/hsa \\& teladoc), employee assistance program, maternity and parental leave, 401(k), and sick, vacation, and paid holiday time.\n\n  \n\nrollkall technologies is an equal opportunity employer committed to a diverse and inclusive workplace. we do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Machine Learning Engineer",
        "company": "Transflo",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "MLflow",
            "CI/CD",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=366c91c1a2a15b5c",
        "description": "role overview:\n\n\nwe are seeking an experienced machine learning engineer specializing in aws bedrock, mlflow, and advanced prompt engineering methodologies to lead the development of state\\-of\\-the\\-art multimodal document identification and extraction solutions. in this role, you will design and fine\\-tune foundation models (fms), implement generative ai (genai) strategies, and leverage advanced prompt engineering techniques for accurate and efficient multimodal document processing.\n\n  \n\n\njob responsibilities:\n\n\n* design, develop, and deploy scalable machine learning models using aws bedrock and sagemaker.\n\n\n* implement and optimize multimodal machine learning pipelines for document identification and extraction.\n\n\n* develop and refine advanced prompt engineering strategies, including hierarchical prompting, context\\-aware prompts, and multi\\-turn dialogue techniques, to enhance the performance of foundation models.\n\n\n* manage the end\\-to\\-end ml lifecycle, including experiment tracking, model versioning, and deployment using mlflow.\n\n\n* ensure robust mlops practices, including ci/cd pipelines, model monitoring, and automated retraining workflows.\n\n\n* optimize model inference performance and cost\\-effectiveness using aws elastic inference and sagemaker optimization techniques.\n\n\n* integrate aws textract and rekognition for enhanced ocr and image processing within ml workflows.\n\n\n* collaborate with cross\\-functional teams, including data scientists, cloud engineers, and business stakeholders, to align ai models with business objectives.\n\n\n* monitor, debug, and enhance machine learning workflows for improved reliability and efficiency.\n\n\n* stay updated on the latest advancements in ai, multimodal machine learning, and aws technologies, and apply them to real\\-world problems.\n\n  \n\n\nqualifications and experience:\n\n\n* extensive experience with aws bedrock for deploying and fine\\-tuning foundation models (fms) for multimodal applications.\n\n\n* proficiency in amazon sagemaker for training complex ml models, hyperparameter tuning, and scalable deployment.\n\n\n* hands\\-on experience with mlflow in aws for experiment tracking, model versioning, and end\\-to\\-end ml lifecycle management.\n\n\n* experience with aws lambda, api gateway, and step functions for building serverless ai pipelines.\n\n\n* familiarity with aws textract and amazon rekognition for document extraction and image recognition tasks.\n\n\n* proficient in the utilization of textual models for image classification or other open source image classification tools.\n\n\n* proficiency in aws deep learning amis for rapid ml environment setup.\n\n\n* experience with amazon elastic inference for cost\\-effective inference acceleration\n\n\n* image\\-text alignment prompts \u2013 creating prompts that effectively link textual and visual data for accurate information extraction.\n\n\n* hierarchical prompting \u2013 designing prompts for complex document structures with nested elements.\n\n\n* context\\-aware prompting \u2013 developing prompts that adapt to the semantic context of documents.\n\n\n* visual layout\\-aware prompting \u2013 crafting prompts that leverage document layout information for precise entity recognition.\n\n\n* few\\-shot and zero\\-shot prompting \u2013 utilizing examples to improve multimodal model performance with minimal labeled data.\n\n\n* multi\\-turn dialogue prompting \u2013 implementing iterative prompts for complex document extraction scenarios.\n\n\n* cross\\-attention prompts \u2013 optimizing attention mechanisms for aligning visual and textual features.\n\n  \n\n\nindividual qualities:\n\n\n* results oriented\n\n\n* independently reliable; performs tasks without close supervision\n\n\n* persistent learner showing a desire to be on the edge of new ai methodologies as it may relate to current business opportunities.\n\n\n* organized; detail\\-oriented, methodical and consistently demonstrates ability to successfully and timely complete assignments.\n\n\n* follows\\-up; consistently performs this in a positive, proactive manner\n\n\n* logical problem\\-solving skills\n\n\n* quality conscious; consistently demonstrates commitment to customers \\& quality\n\n\n* demonstrates timeliness \\& urgency\n\n\n* team work; individual contributor that works well with other team members and consistently promotes a strong team environment work ethic\n\n\n* goal setting; sets/achieves goals and consistently demonstrates a willingness/dedication to process improvement\n\n\n* responsible; takes responsibility for personal actions and consistently demonstrates a willingness to accept greater project responsibilities\n\n\n* professionally candid communications\n\n\n* focused on key success factors\n\n\n* professional attitude; consistently demonstrates ability to accept criticism and manage the conversation appropriately\n\n\n* street smart; can apply knowledge and life experiences in business\n\n\n* positive attitude\n\n\n* flexible \\& adaptable\n\n\n* resourceful",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Machine Learning Engineer 50",
        "company": "Adobe",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Power BI",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=652c1e250e125af4",
        "description": "the opportunity\n  \nadobe journey optimizer b2b is redefining how enterprises engage buying groups through ai\\-powered customer journey orchestration. we're building intelligent systems that understand complex b2b buyer behavior, predict intent signals across accounts, and deliver hyper\\-personalized experiences at every touchpoint\u2014from first awareness through closed revenue.\n  \n  \n\nwe are looking for a machine learning engineer \\& architect to join our ai and agents team, define and own the ml architecture vision for our b2b journey orchestration platform. in this role, you'll shape how thousands of b2b enterprises leverage ai to transform pipeline generation, accelerate deal velocity, and drive measurable revenue impact. your architecture decisions will power billions of personalized interactions annually, directly influencing how marketing and sales teams identify, engage, and convert buying committees.\n  \n  \n\nwhat you'll do with us\n  \n  \n\nyou'll train and finetune ml models that solve business use cases and handle data at scale.\n  \n  \n\nwe'll work together to architect and optimize end\\-to\\-end ml pipelines, ensuring they're scalable, efficient, and robust.\n  \n  \n\nyou'll dive deep into data to recommend the right models, evaluation metrics, and governance approaches.\n  \n  \n\nprovide hands\\-on technical leadership, guiding engineers through architecture, building, implementation, and established guidelines.\n  \n  \n\nwork across organizational boundaries to align priorities and drive projects forward.\n  \n  \n\nthroughout the product lifecycle, you'll engage in architecture, design, deployment, and production operations of ml models and systems\n  \n  \n\n**what will help you thrive:**  \n\n10\\+ years of proven experience in machine learning with successful delivery of ml projects, and 3\\+ years of hands\\-on experience working with generative ai technologies such as llms, evaluations, fine\\-tuning, and more.\n  \n  \n\nyour strong python and deep learning engineering skills, paired with experience in training and inferencing with pytorch or tensorflow/jax, will be essential.\n  \n  \n\nexperience with post\\-training techniques such as fine\\-tuning, alignment or distillation.\n  \n  \n\nknowledge of deployment tools like docker, ml ops, and ml services is valuable, and experience with cloud platforms like azure and aws is a plus.\n  \n  \n\nyour strong verbal and written communication skills and success in multi\\-functional team environments will help us all succeed together.\n  \nways to stand out\n  \n  \n\nyou bring hands\\-on experience with retrieval\\-augmented generation (rag), semantic embeddings, agentic ai workflows, and ml inference systems for personalization or recommendation use cases\n  \n  \n\nyou have published research, contributed to open\\-source ml projects, or hold patents in ai/ml domains\n  \n  \n\nyou have experience with adobe experience platform, marketo engage, or journey optimizer\u2014understanding how ml integrates with enterprise customer data infrastructure\n  \nabout adobe\n  \nadobe empowers everyone to create through innovative platforms and tools that unleash creativity, productivity and personalized customer experiences. adobe\u2019s industry\\-leading offerings including adobe acrobat studio, adobe express, adobe firefly, creative cloud, adobe experience platform, adobe experience manager, and genstudio enable people and businesses to turn ideas into impact, powered by ai and driven by human ingenuity.\n  \n  \n\nour 30,000\\+ employees worldwide are creating the future and raising the bar as we drive the next decade of growth. we\u2019re on a mission to hire the very best and believe in creating a company culture where all employees are empowered to make an impact. at adobe, we believe that great ideas can come from anywhere in the organization. the next big idea could be yours.\n  \n  \n\nlet\u2019s adobe together\n  \nat adobe, we believe in creating a company culture where all employees are empowered to make an impact. learn more about adobe life, including our values and culture , focus on people, purpose and community , adobe for all , comprehensive benefits programs , the stories we tell , the customers we serve, and how you can help us advance our mission of empowering everyone to create.\n  \n  \n\nadobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other protected characteristic. learn more.\n  \n  \n\nadobe aims to make our careers website and recruiting process accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call \\+1 408\\-536\\-3015\\.\n  \n  \n\n**ai use guidelines for interviews:**  \n\nour interviews are designed to reflect your own skills and thinking. the use of ai or recording tools during live interviews is not permitted unless explicitly invited by the interviewer or approved in advance as part of a reasonable accommodation. if these tools are used inappropriately or in a way that misrepresents your work, your application may not move forward in the process.\n  \n  \n\nat adobe, we empower employees to innovate with ai \u2014 and we look for candidates eager to do the same. as part of the hiring experience, we provide clear guidance on where ai is encouraged during the process and where it\u2019s restricted during live interviews. see how we think about ai in the hiring experience .\n  \n  \n\n**expected pay range:** our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position\u202fis $172,500 \\-\\- $306,625 annually. pay\u202fwithin this range varies by work location\u202fand may also depend on job\\-related knowledge, skills,\u202fand experience. your recruiter can share more about the specific salary range for the job location during the hiring process.\n  \n  \n\nin california, the pay range for this position is $211,800 \\- $306,625\n  \n  \n\nat adobe, for sales roles starting salaries are expressed as total target compensation (ttc \\= base \\+ commission), and short\\-term incentives are in the form of sales commission plans. non\\-sales roles starting salaries are expressed as base salary and short\\-term incentives are in the form of the annual incentive plan (aip).\n  \n  \n\nin addition, certain roles may be eligible for long\\-term incentives in the form of a new hire equity award.\n  \n  \n\n**state\\-specific notices:**  \n\n**california :**  \n\nfair chance ordinances\n  \nadobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and \u201cfair chance\u201d ordinances.\n  \n  \n\n**colorado:**  \n\napplication window notice\n  \n  \n\nif this role is open to hiring in colorado (as listed on the job posting), the application window will remain open until at least the date and time stated above in pacific time, in compliance with colorado pay transparency regulations. if this role does not have colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs.\n  \n  \n\n**massachusetts:**  \n\nmassachusetts legal notice\n  \nit is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "AI Developer",
        "company": "Bourns",
        "location": "Carrollton, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "LLaMA",
            "Prompt Engineering",
            "FastAPI",
            "Kubernetes",
            "Git",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6ed224a6ba5ee451",
        "description": "**title:** **ai developer**\n===========================\n\n**career at bourns:**\n\n\nif working for an organization with a long history of technological innovation and high advancement potential in a supportive, collaborative, and positive work environment are top considerations for your next career move, then you need to look at the opportunities at bourns.  \n\n  \n\nbourns was founded 75 years ago with the guiding principles of providing breakthrough technology solutions, high\\-quality products, responsive service, and exceptional value. those principles still hold true today and have instilled a culture that is committed to excellence and ethical operations. as a leading provider of advanced components for power conversion, circuit protection, and motion control sensing solutions, bourns employs more than 9,000 people in 21 countries and operates 17 manufacturing centers and 15 r\\&d facilities. our customers and partners rely on bourns to meet strict design requirements and satisfy international standards in a broad range of industries and applications. we achieve this stature because bourns employees excel in the development of new products and continuous improvement processes that keep the company a global leader in delivering outstanding service.\n\n\n**job description:**\n\n\n**summary of position:**\n\n\n\nworking under limited supervision, the ai developer is responsible for designing, developing, and maintaining ai\\-powered solutions using large language models (llms), retrieval\\-augmented generation (rag), and modern python frameworks within the azure ecosystem. responsibilities include building and optimizing ai workflows, integrating azure openai and related cognitive services, developing scalable python\\-based apis and automations, and ensuring secure, efficient deployment using azure services and containerized environments such as kubernetes. the role includes developing technical documentation, supporting production ai applications, and collaborating with business analysts and cross\\-functional teams to define, refine, and deliver intelligent, reliable, and scalable ai capabilities. the ai developer also contributes to model evaluation, prompt engineering, reusable component design, and participates in system architecture and solution\\-design discussions. this position reports to the senior manager of it development.\n\n  \n\n\n**duties and accountabilities:**\n\n\n\nunder the supervision of senior manager \u2013 development, this role will have the following duties and accountabilities:\n\n\n* analyzes, designs, develops, implements, and supports ai\\-driven solutions and backend components across multiple business units using python, llm frameworks, openai models, and azure services.\n* works with development tools and ai platforms\u2014including openai, vector databases, azure functions, python frameworks, and rag pipelines\u2014to build intelligent automations, conversational agents, and llm\\-powered applications.\n* collaborates with business analysts to gather, clarify, and validate business requirements for ai/ml and automation initiatives.\n* translates business requirements and functional specifications into technical designs, developing scalable, maintainable, and secure ai solutions within the azure ecosystem.\n* develops and proposes new ideas, experiments, and innovative ai capabilities in support of corporate digital transformation goals and strategic objectives.\n* builds, deploys, and maintains python\\-based services, apis, prompt workflows, and ai model integrations, ensuring performance, reliability, and compliance with enterprise architecture standards.\n* troubleshoots complex issues relating to ai models, data pipelines, vector stores, and system integrations, ensuring alignment with business objectives.\n* analyzes, designs, and improves existing ai workflows and systems to enhance accuracy, performance, and scalability.\n* designs, develops, and implements testing procedures, prompt evaluation frameworks, automated validation pipelines, and supporting documentation.\n* diagnoses and resolves ai, python, and azure service\\-related issues using robust debugging techniques, telemetry analysis, and root\\-cause investigation.\n* performs other job\\-related duties as assigned.\n\n  \n\n\n**basic job requirements:**\n\n  \n\n\n**education**:\n\n\n\nbachelor\u2019s degree in computer science, computer engineering, or a related field. a master's degree in computer science or equivalent advanced study is preferred. candidates with a combination of formal education, hands\\-on experience in ai/ml, backend development, cloud platforms, and strong programming skills \\- especially with python, will be considered.\n\n  \n\n\n**training and experience:**\n\nrequires a strong work ethic and solid programming foundation, with practical experience in python and modern development frameworks. experience working with ai/ml concepts, large language models (llms), retrieval augmented generation (rag), or vector databases is highly desirable. familiarity with cloud platforms such as azure, aws, or gcp, along with exposure to tools like langchain, llamaindex, or fastapi is beneficial. sap experience is a plus but not required. must be eager to learn new ai platforms, adopt emerging llm technologies, and continuously expand technical capabilities. should be committed to delivering high quality, reliable, and scalable solutions in a fast paced, results driven environment\n\n  \n\n\n**other skills:**\n\n\n\nstrong analytical skills with both application implementation and operational support experience desired. basic knowledge on handling any database to produce reports and creating applications is desired\n\n  \n\n\n**interpersonal skills:**\n\n\n\nworks well with other members of information technology team and effectively interacts with personnel at all levels within the organization who may have little technical knowledge and are unable to resolve their own issues.\n\n  \n\n\n**oral:**\n\n\n\nclear and concise communication techniques required to effectively present ideas and solutions with both technical and non\\-technical employees at all levels.\n\n  \n\n\n**written:**\n\n\n\nproficient writing skills required to effectively communicate to management project plans and project status. ability to communicate clearly and concisely to both technical and non\\-technical employees at all levels concerning software and/or hardware solutions that will best fit their requirements.\n\n  \n\n\n**scheduling and planning:**\n\n\n\nwill develop and execute small to large scale project plans and provide reports to management.\n\n  \n\n\n**equipment operated:**\n\n\n\npersonal computer and office software such as microsoft office 365 tools including word, excel, powerpoint, sharepoint, and project. other software tools including sap gui, ai/ml with python, sdk toolkit, hana studio or eclipse may be necessary.\n\n  \n\n\n**working environment:**\n\n\n\nprimarily office environment: some travel may also be required\n\n  \n\n\n**hiring range:**\n\n$115760 \u2013 $120000 annually\n\n\nbourns is an equal opportunity employer. at bourns, we are committed to treating all applicants and employees fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law. qualified candidates must be able to perform the essential functions of this position satisfactorily with or without reasonable accommodation. disclaimer: this job post is not necessarily an exhaustive list of all essential responsibilities, skills, tasks, or requirements associated with this position. while this is intended to be an accurate reflection of the position posted, the company reserves the right to modify or change the essential functions of the job based on business necessity.\n\n\nequal opportunity employer: minority/female/disability/veteran\n\n\nif you are results\\-oriented, have the drive to find industry\\-leading ways to meet ongoing technological challenges, and go the extra mile to maximize customer relationships, then we invite you to apply to join the bourns team.\n\n  \n\n\n  \n\n**nearest secondary market:** fort worth",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist Subcontractor",
        "company": "The Hackett Group",
        "location": "FL, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "LangChain",
            "LLaMA",
            "Hugging Face",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=84b99efc2d74d07c",
        "description": "**data scientist**\n\nposition summary:\n\n\nwe are seeking a highly skilled and motivated data scientist to join our dynamic team. the ideal candidate is proficient in python, has extensive experience with cloud platforms (aws, azure, gcp), and possesses a strong background in both theoretical and practical aspects of machine learning techniques.  \n\n\n\n**key responsibilities:**\n\n* python proficiency:\n* utilize python for data analysis, manipulation, and visualization.\ndevelop and implement machine learning models using python\\-based libraries and frameworks.  \n* \n\n* cloud platform knowledge:\n* work with data on various cloud platforms such as aws, azure, and gcp.\ndeploy and manage data science solutions in cloud environments.  \n* \n\n* machine learning techniques:\n* apply classification and regression techniques, including ensemble models, xgboost, svm, and neural networks.\nimplement statistical forecasting models such as arima, s\\-arima, and holt\\-winters.  \n* \n\n* clustering:\nutilize clustering algorithms to uncover patterns and insights from data.  \n\n* \n\n* deep learning frameworks:\n* demonstrate expertise in deep learning frameworks such as tensorflow and pytorch.\napply knowledge in image, text, and sequence processing models.  \n* \n\n* neural networks:\n* implement convolutional neural networks (cnn) for image recognition tasks.\ndevelop recurrent networks (lstm, gru) for sequence\\-based data analysis.  \n* \n\n* generative ai frameworks:\n* work with generative ai frameworks like langchain, hugging face, llama index, lamini, etc.\napply generative models for various applications, including text and image generation.  \n* \n\n* communication and teamwork:\n* effectively communicate findings and insights to both technical and non\\-technical stakeholders.\ncollaborate with cross\\-functional teams to deliver impactful data science solutions.  \n* \n\n* continuous learning:\ndemonstrate a proactive attitude towards learning and staying updated with the latest advancements in data science and ai.  \n\n* \n\n**qualifications:**\n\n* bachelor's degree in computer science, information technology, or a related field (computer science, statistics, data science, etc.).\n* proven experience in applying machine learning and deep learning techniques to real\\-world problems.\n* strong programming skills in python and experience with relevant libraries and frameworks.\n* experience working with cloud platforms (aws, azure, gcp).\n* excellent communication and teamwork skills.\n* a passion for continuous learning and staying current with industry trends.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Solutions Architect",
        "company": "Interworks",
        "location": "Oklahoma City, OK, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Glue",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ae3df7fdbe227203",
        "description": "do you thrive on turning ambiguity into architecture? do you enjoy sitting at the intersection of strategy, technology and business impact? are you comfortable guiding executive stakeholders while also rolling up your sleeves when needed? if so, let's talk.\n\n\n\nwe're interworks, a one\\-of\\-a\\-kind data and it consultancy that believes people come first. we work with the best clients in the world, and we're always on the lookout for thoughtful, strategic and technically sharp individuals who want to make meaningful impact.\n\n\n\nas a solutions architect, you'll lead the design and positioning of modern data and analytics solutions for our clients. you'll partner closely with go\\-to\\-market, sales and delivery teams to architect scalable, sustainable systems that drive long\\-term value. this role blends technical depth, strategic advisory and client leadership.\n\n\n\nwe're intentional about who we bring on board. we're looking for someone who not only understands architecture but can communicate it clearly, build trust quickly and guide clients confidently through complex decisions.\n\n\n\nlocation preferences: we'd love for you to join us from one of our offices in okc, tulsa, stillwater or raleigh\\-durham, nc. remote work may be possible depending on the situation, let's chat.\n\n\n\nsalary range: $110,000\u2013$160,000, depending on experience and qualifications.\n\n\n**what you'll do**\n\n\n* architect scalable, secure and high\\-performing data and analytics solutions across modern cloud platforms\n* translate complex business objectives into clear technical architecture and implementation strategy\n* lead solution design conversations with executive and technical stakeholders\n* evaluate trade\\-offs across cost, performance, scalability and long\\-term maintainability\n* develop high\\-level architecture diagrams, solution proposals and technical positioning materials\n* guide mvp and proof\\-of\\-concept development efforts\n* partner with sales and go\\-to\\-market teams during discovery and pre\\-sales engagements\n* mentor engineers and consultants, providing architectural guidance and best practices\n* identify expansion opportunities within client accounts through trusted technical leadership\n\n\n**what you'll need**\n\n\n\nmust\\-haves\n\n\n* 8\u201310\\+ years of experience in data, analytics, cloud architecture or related technology consulting\n* proven experience designing end\\-to\\-end data and analytics architectures\n* strong understanding of modern cloud platforms and data ecosystems\n* experience leading complex technical discussions with senior client stakeholders\n* ability to clearly communicate architectural concepts to both technical and non\\-technical audiences\n* strong problem\\-solving skills with strategic and systems\\-level thinking\n\n\nwhat we'd like you to have\n\n\n* deep experience with platforms such as snowflake, databricks, google bigquery, redshift or similar\n* experience with orchestration and transformation tools such as dbt, matillion, fivetran or python\n* background in pre\\-sales or solution positioning\n* experience mentoring consultants or leading technical teams\n* familiarity with emerging ai capabilities and how they integrate into modern data platforms\n\n **why interworks**\n\n\n\ninterworks is a people\\-focused tech consultancy that empowers clients with customized, collaborative solutions, and we love pursuing innovation alongside people who inspire us. our approach to work and community is unique and unconventional\u2014just like us\u2014and that's the way we want it. the only thing missing is you. at interworks, we value unique contributions, our people are the glue that holds our business together. we're always looking for the right people, and we could be your perfect fit.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Solutions Architect",
        "company": "Interworks",
        "location": "Stillwater, OK, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Glue",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b2227ed8fd327afa",
        "description": "do you thrive on turning ambiguity into architecture? do you enjoy sitting at the intersection of strategy, technology and business impact? are you comfortable guiding executive stakeholders while also rolling up your sleeves when needed? if so, let's talk.\n\n\n\nwe're interworks, a one\\-of\\-a\\-kind data and it consultancy that believes people come first. we work with the best clients in the world, and we're always on the lookout for thoughtful, strategic and technically sharp individuals who want to make meaningful impact.\n\n\n\nas a solutions architect, you'll lead the design and positioning of modern data and analytics solutions for our clients. you'll partner closely with go\\-to\\-market, sales and delivery teams to architect scalable, sustainable systems that drive long\\-term value. this role blends technical depth, strategic advisory and client leadership.\n\n\n\nwe're intentional about who we bring on board. we're looking for someone who not only understands architecture but can communicate it clearly, build trust quickly and guide clients confidently through complex decisions.\n\n\n\nlocation preferences: we'd love for you to join us from one of our offices in okc, tulsa, stillwater or raleigh\\-durham, nc. remote work may be possible depending on the situation, let's chat.\n\n\n\nsalary range: $110,000\u2013$160,000, depending on experience and qualifications.\n\n\n**what you'll do**\n\n\n* architect scalable, secure and high\\-performing data and analytics solutions across modern cloud platforms\n* translate complex business objectives into clear technical architecture and implementation strategy\n* lead solution design conversations with executive and technical stakeholders\n* evaluate trade\\-offs across cost, performance, scalability and long\\-term maintainability\n* develop high\\-level architecture diagrams, solution proposals and technical positioning materials\n* guide mvp and proof\\-of\\-concept development efforts\n* partner with sales and go\\-to\\-market teams during discovery and pre\\-sales engagements\n* mentor engineers and consultants, providing architectural guidance and best practices\n* identify expansion opportunities within client accounts through trusted technical leadership\n\n\n**what you'll need**\n\n\n\nmust\\-haves\n\n\n* 8\u201310\\+ years of experience in data, analytics, cloud architecture or related technology consulting\n* proven experience designing end\\-to\\-end data and analytics architectures\n* strong understanding of modern cloud platforms and data ecosystems\n* experience leading complex technical discussions with senior client stakeholders\n* ability to clearly communicate architectural concepts to both technical and non\\-technical audiences\n* strong problem\\-solving skills with strategic and systems\\-level thinking\n\n\nwhat we'd like you to have\n\n\n* deep experience with platforms such as snowflake, databricks, google bigquery, redshift or similar\n* experience with orchestration and transformation tools such as dbt, matillion, fivetran or python\n* background in pre\\-sales or solution positioning\n* experience mentoring consultants or leading technical teams\n* familiarity with emerging ai capabilities and how they integrate into modern data platforms\n\n **why interworks**\n\n\n\ninterworks is a people\\-focused tech consultancy that empowers clients with customized, collaborative solutions, and we love pursuing innovation alongside people who inspire us. our approach to work and community is unique and unconventional\u2014just like us\u2014and that's the way we want it. the only thing missing is you. at interworks, we value unique contributions, our people are the glue that holds our business together. we're always looking for the right people, and we could be your perfect fit.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Solutions Architect",
        "company": "Interworks",
        "location": "Raleigh, NC, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Glue",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a40f1918af374ac1",
        "description": "do you thrive on turning ambiguity into architecture? do you enjoy sitting at the intersection of strategy, technology and business impact? are you comfortable guiding executive stakeholders while also rolling up your sleeves when needed? if so, let's talk.\n\n\n\nwe're interworks, a one\\-of\\-a\\-kind data and it consultancy that believes people come first. we work with the best clients in the world, and we're always on the lookout for thoughtful, strategic and technically sharp individuals who want to make meaningful impact.\n\n\n\nas a solutions architect, you'll lead the design and positioning of modern data and analytics solutions for our clients. you'll partner closely with go\\-to\\-market, sales and delivery teams to architect scalable, sustainable systems that drive long\\-term value. this role blends technical depth, strategic advisory and client leadership.\n\n\n\nwe're intentional about who we bring on board. we're looking for someone who not only understands architecture but can communicate it clearly, build trust quickly and guide clients confidently through complex decisions.\n\n\n\nlocation preferences: we'd love for you to join us from one of our offices in okc, tulsa, stillwater or raleigh\\-durham, nc. remote work may be possible depending on the situation, let's chat.\n\n\n\nsalary range: $110,000\u2013$160,000, depending on experience and qualifications.\n\n\n**what you'll do**\n\n\n* architect scalable, secure and high\\-performing data and analytics solutions across modern cloud platforms\n* translate complex business objectives into clear technical architecture and implementation strategy\n* lead solution design conversations with executive and technical stakeholders\n* evaluate trade\\-offs across cost, performance, scalability and long\\-term maintainability\n* develop high\\-level architecture diagrams, solution proposals and technical positioning materials\n* guide mvp and proof\\-of\\-concept development efforts\n* partner with sales and go\\-to\\-market teams during discovery and pre\\-sales engagements\n* mentor engineers and consultants, providing architectural guidance and best practices\n* identify expansion opportunities within client accounts through trusted technical leadership\n\n\n**what you'll need**\n\n\n\nmust\\-haves\n\n\n* 8\u201310\\+ years of experience in data, analytics, cloud architecture or related technology consulting\n* proven experience designing end\\-to\\-end data and analytics architectures\n* strong understanding of modern cloud platforms and data ecosystems\n* experience leading complex technical discussions with senior client stakeholders\n* ability to clearly communicate architectural concepts to both technical and non\\-technical audiences\n* strong problem\\-solving skills with strategic and systems\\-level thinking\n\n\nwhat we'd like you to have\n\n\n* deep experience with platforms such as snowflake, databricks, google bigquery, redshift or similar\n* experience with orchestration and transformation tools such as dbt, matillion, fivetran or python\n* background in pre\\-sales or solution positioning\n* experience mentoring consultants or leading technical teams\n* familiarity with emerging ai capabilities and how they integrate into modern data platforms\n\n **why interworks**\n\n\n\ninterworks is a people\\-focused tech consultancy that empowers clients with customized, collaborative solutions, and we love pursuing innovation alongside people who inspire us. our approach to work and community is unique and unconventional\u2014just like us\u2014and that's the way we want it. the only thing missing is you. at interworks, we value unique contributions, our people are the glue that holds our business together. we're always looking for the right people, and we could be your perfect fit.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Solutions Architect",
        "company": "Interworks",
        "location": "Tulsa, OK, US USA",
        "posted_at": "2026-02-23",
        "score": 11.1,
        "matched_keywords": [
            "Glue",
            "Redshift",
            "BigQuery",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b65ec0cc7ed25eda",
        "description": "do you thrive on turning ambiguity into architecture? do you enjoy sitting at the intersection of strategy, technology and business impact? are you comfortable guiding executive stakeholders while also rolling up your sleeves when needed? if so, let's talk.\n\n\n\nwe're interworks, a one\\-of\\-a\\-kind data and it consultancy that believes people come first. we work with the best clients in the world, and we're always on the lookout for thoughtful, strategic and technically sharp individuals who want to make meaningful impact.\n\n\n\nas a solutions architect, you'll lead the design and positioning of modern data and analytics solutions for our clients. you'll partner closely with go\\-to\\-market, sales and delivery teams to architect scalable, sustainable systems that drive long\\-term value. this role blends technical depth, strategic advisory and client leadership.\n\n\n\nwe're intentional about who we bring on board. we're looking for someone who not only understands architecture but can communicate it clearly, build trust quickly and guide clients confidently through complex decisions.\n\n\n\nlocation preferences: we'd love for you to join us from one of our offices in okc, tulsa, stillwater or raleigh\\-durham, nc. remote work may be possible depending on the situation, let's chat.\n\n\n\nsalary range: $110,000\u2013$160,000, depending on experience and qualifications.\n\n\n**what you'll do**\n\n\n* architect scalable, secure and high\\-performing data and analytics solutions across modern cloud platforms\n* translate complex business objectives into clear technical architecture and implementation strategy\n* lead solution design conversations with executive and technical stakeholders\n* evaluate trade\\-offs across cost, performance, scalability and long\\-term maintainability\n* develop high\\-level architecture diagrams, solution proposals and technical positioning materials\n* guide mvp and proof\\-of\\-concept development efforts\n* partner with sales and go\\-to\\-market teams during discovery and pre\\-sales engagements\n* mentor engineers and consultants, providing architectural guidance and best practices\n* identify expansion opportunities within client accounts through trusted technical leadership\n\n\n**what you'll need**\n\n\n\nmust\\-haves\n\n\n* 8\u201310\\+ years of experience in data, analytics, cloud architecture or related technology consulting\n* proven experience designing end\\-to\\-end data and analytics architectures\n* strong understanding of modern cloud platforms and data ecosystems\n* experience leading complex technical discussions with senior client stakeholders\n* ability to clearly communicate architectural concepts to both technical and non\\-technical audiences\n* strong problem\\-solving skills with strategic and systems\\-level thinking\n\n\nwhat we'd like you to have\n\n\n* deep experience with platforms such as snowflake, databricks, google bigquery, redshift or similar\n* experience with orchestration and transformation tools such as dbt, matillion, fivetran or python\n* background in pre\\-sales or solution positioning\n* experience mentoring consultants or leading technical teams\n* familiarity with emerging ai capabilities and how they integrate into modern data platforms\n\n **why interworks**\n\n\n\ninterworks is a people\\-focused tech consultancy that empowers clients with customized, collaborative solutions, and we love pursuing innovation alongside people who inspire us. our approach to work and community is unique and unconventional\u2014just like us\u2014and that's the way we want it. the only thing missing is you. at interworks, we value unique contributions, our people are the glue that holds our business together. we're always looking for the right people, and we could be your perfect fit.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "SAP iXp Intern - AI Machine Learning",
        "company": "SAP",
        "location": "Palo Alto, CA, US USA",
        "posted_at": "2026-02-22",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "R",
            "Scala",
            "A/B Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=90785d4b0ecee872",
        "description": "**we help the world run better**  \n\nat sap, we keep it simple: you bring your best to us, and we'll bring out the best in you. we're builders touching over 20 industries and 80% of global commerce, and we need your unique talents to help shape what's next. the work is challenging \u2013 but it matters. you'll find a place where you can be yourself, prioritize your wellbeing, and truly belong. what's in it for you? constant learning, skill growth, great benefits, and a team that wants you to grow and succeed. **what you\u2019ll build**\n\n\nthe sap internship experience program is sap\u2019s global, strategic, paid internship program that provides university students with opportunities to find purpose in their careers. this is more than an internship, it\u2019s the foundation for a career built on connection, creativity, and impact.\n\n\nposition title: sap ixp intern \\- ai machine learning\n\n\nlocation: palo alto, ca (in\\-person)\n\n\nexpected start date: june 2026\n\n\ncontract duration: 6 months (extendable)\n\n\nworking hours: 20\\-40 hours/week\n\n  \n\nas an ai/ml intern, you will work alongside senior data scientists and engineers to build, optimize, and deploy intelligent systems. you won\u2019t just be watching from the sidelines; you will be responsible for exploring messy, real\\-world datasets, training models, and helping integrate ai features into our core products.\n\n* data engineering: collect, clean, and preprocess large\\-scale structured and unstructured datasets to ensure they are \"model\\-ready.\"\n* model development: assist in designing and implementing ml algorithms (supervised, unsupervised, and deep learning) using frameworks like pytorch or tensorflow.\n* experimentation: conduct a/b testing, hyperparameter tuning, and error analysis to improve model precision and recall.\n* genai \\& llms: work with large language models (llms), including prompt engineering, fine\\-tuning, and implementing rag (retrieval\\-augmented generation) pipelines.\n* mlops: help maintain and monitor model performance in production; assist in containerizing models using docker for deployment.\n* research \\& documentation: stay current with the latest ai research (e.g., via arxiv) and document your experimental results for the wider engineering team.\n\n **what you\u2019ll bring**\n\n\nwe\u2019re looking for someone who takes initiative, perseveres, and stay curious. you like to work on meaningful innovative projects and are energized by lifelong learning.\n\n* education: currently pursuing a bs, ms, or phd in computer science, data science, mathematics, or a related quantitative field.\n* eligibility: must be currently enrolled, or recently graduated (start date must be within 6 months of graduation date) from a coding academy/bootcamp, apprenticeship, associate, bachelor\u2019s, master\u2019s or jd/phd program\n* problem\\-solving: a proven ability to break down complex problems and think through edge cases.\n* curiosity: a strong desire to learn new libraries and stay updated on the fast\\-moving ai landscape.\n\n **where you belong**\n\n\nbe part of sap next gen, a global community for students, universities, schools and educational partners, who are passionate about innovation and technology.\n\n* culture of collaboration: partner with experienced sap colleagues and expert mentors who will support your growth. grow professionally through personalized mentoring, coaching, and career development support.\n* project\\-driven experience: kickstart your career with hands\\-on learning experience, making an impact from day one by contributing to meaningful projects that help the world run better. you\u2019ll have endless learning resources at your fingertips and gain future\\-ready skills from a variety of virtual, in\\-person, and hybrid learning sessions, cultivated just for you, and aligned with our learning approach.\n* gain visibility: build relationships with leaders and peers across teams and functions. showcase your ideas, skills, and creativity in a global, fast\\-paced environment. open doors for future career opportunities within sap and beyond.\n\n **meet your team \u2013 what do we do?**\n\n\nsap business technology platform (btp), btp fabric \\- business services will be responsible for developing a consistent, scalable, and high\\-quality reusable business capabilities and services. these foundational constructs standardize essential elements \u2014 business entities, workflows, data models, and common reusable apis/services \u2014 so that developers can focus on creating value through innovation and extensions in a highly productive way rather than reinventing basic frameworks. by fostering consistency and offering a shared approach for abap and non\\-abap based application development on top of btp, we will enable ecosystems of developers, partners, and customers a unified foundation to deliver tailored, interoperable solutions that address all business needs.\n\n  \n\nwant to see what life at sap feels like? check out the life at sap youtube channel\n\n\nfollow @lifeatsap on instagram and don't miss anything about our experiences worldwide!\n\n\n\\#lifeatsap \\#sapnextgen\n\n**bring out your best**  \n\nsap innovations help more than four hundred thousand customers worldwide work together more efficiently and use business insight more effectively. originally known for leadership in enterprise resource planning (erp) software, sap has evolved to become a market leader in end\\-to\\-end business application software and related services for database, analytics, intelligent technologies, and experience management. as a cloud company with two hundred million users and more than one hundred thousand employees worldwide, we are purpose\\-driven and future\\-focused, with a highly collaborative team ethic and commitment to personal development. whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. at sap, you can bring out your best.\n  \n\n  \n\n**we win with inclusion**  \n\nsap\u2019s culture of inclusion, focus on health and well\\-being, and flexible working models help ensure that everyone \u2013 regardless of background \u2013 feels included and can run at their best. at sap, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. we ultimately believe in unleashing all talent and creating a better world.  \n\n  \n\nsap is committed to the values of equal employment opportunity and provides accessibility accommodations to applicants with physical and/or mental disabilities. if you are interested in applying for employment with sap and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e\\-mail with your request to recruiting operations team: careers@sap.com.  \n\n  \n\nfor sap employees: only permanent roles are eligible for the sap employee referral program, according to the eligibility rules set in the sap referral policy. specific conditions may apply for roles in vocational training.  \n\nqualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability. **compensation range transparency**: sap believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating sap\u2019s commitment to pay equity. sap provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. the targeted combined range for this position is 15 \\- 62(usd) usd. the actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. any sap variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance. please reference this link for a summary of sap benefits and eligibility requirements: sap north america benefits. **ai usage in the recruitment process**\nfor information on the responsible use of ai in our recruitment process, please refer to our guidelines for ethical usage of ai in the recruiting process.\nplease note that any violation of these guidelines may result in disqualification from the hiring process.  \n\n  \n\nrequisition id: 444150 \\| work area: software\\-design and development \\| expected travel: 0 \\- 10% \\| career status: student \\| employment type: limited full time \\| additional locations: \\#li\\-hybrid",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Kitchen Helper",
        "company": "nan",
        "location": "Daphne, AL, US USA",
        "posted_at": "2026-02-22",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fed55110419e5c18",
        "description": "**job overview**  \nwe are seeking an innovative and highly capable artificial intelligence (ai) specialist to join our dynamic team. in this role, you will leverage cutting\\-edge ai technologies to develop, implement, and optimize intelligent solutions that drive efficiency, enhance user experiences, and solve complex problems. your expertise will contribute to shaping the future of our digital initiatives, empowering our organization with transformative ai capabilities. this position offers an exciting opportunity to work at the forefront of technological innovation in a fast\\-paced environment committed to continuous growth and learning.\n\n**duties**\n\n* design, develop, and refine ai models and algorithms tailored to specific project needs and organizational goals\n* collaborate with cross\\-functional teams including data scientists, engineers, and product managers to integrate ai solutions into existing systems\n* conduct data analysis and feature engineering to improve model accuracy and performance\n* test and validate ai models through rigorous experimentation and evaluation methods\n* monitor ai system performance post\\-deployment, troubleshoot issues, and implement improvements\n* stay current with emerging trends in artificial intelligence, machine learning, deep learning, and related fields to ensure innovative practices\n* document processes, methodologies, and results clearly for transparency and future reference\n\n**skills**\n\n* strong proficiency in programming languages such as python, r, or java used for ai development\n* experience with machine learning frameworks like tensorflow, pytorch, or scikit\\-learn\n* solid understanding of neural networks, natural language processing (nlp), computer vision or reinforcement learning techniques\n* ability to analyze large datasets using sql or similar query languages\n* excellent problem\\-solving skills with a keen eye for detail and accuracy\n* effective communication skills to explain complex technical concepts clearly to non\\-technical stakeholders\n* knowledge of ethical considerations surrounding ai deployment and data privacy standards\n\njoin us as an ai specialist and be part of a forward\\-thinking team dedicated to harnessing the power of artificial intelligence. your expertise will help shape innovative solutions that make a real difference\u2014driving progress today for a smarter tomorrow!\n\njob types: full\\-time, part\\-time, temporary, contract\n\npay: $15\\.00 \\- $18\\.00 per hour\n\nwork location: in person",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Service Delivery Center - Microsoft\u00a0Power Platform & Copilot Engineer - Senior",
        "company": "EY",
        "location": "Alpharetta, GA, US USA",
        "posted_at": "2026-02-22",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "AKS",
            "Git",
            "Power BI",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=dba14496bf2d3822",
        "description": "at ey, we\u2019re all in to shape your future with confidence.\n\n  \n\nwe\u2019ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. join ey and help to build a better working world.\n\n **senior consultant \u2013 power platform \\& copilot engineer**\n\n **about the job:**\n\n\nprocess solutions and automation is a competency within ernst \\& young\u2019s financial and banking services practice providing their clients the world\\-class solutions to tackle their most important challenges and achieve the highest potential and efficiencies in ever changing markets.\n\n  \n\nwe help clients transform their business with process automation across enterprise functions and business units leveraging tools and platforms suitable for client requirements and needs.\n\n  \n\nwe are seeking a highly skilled and motivated copilot and power platform engineer to join our dynamic team. in this role, you will leverage your expertise in microsoft power platform, including power apps, power automate, and power bi, to develop innovative solutions that enhance productivity and drive business transformation. you will also play a key role in integrating ai capabilities into our applications using copilot technologies.\n\n  \n\nthis will be a full\\-time opportunity for a suitable candidate.\n\n\nat ey, you\u2019ll have an opportunity to build a career as unique as you are, with the global scale, support, inclusive culture, and technology to become the best version of you. and we\u2019re counting on your unique voice and perspective to help ey become even better, too. join us and build an exceptional experience for yourself, and a better working world for all.\n\n **key responsibilities:**\n\n* design, develop, and implement agentic ai solutions using copilot studio, power platform components (power apps, power automate, power bi), and azure ai foundry.\n* create and maintain comprehensive documentation for applications, workflows, and processes.\n* engage with senior stakeholders to gather requirements, translate them into technical specifications.\n* collaborate with cross\\-functional teams to ensure alignment on project goals and deliverables.\n* monitor application performance and user feedback to identify areas for improvement.\n* optimize applications for speed, scalability, and user experience.\n* architect and implement the declarative agentic framework, embedding governance, controls, and compliance best practices.\n* develop and validate pilot m365 agents (e.g., for document search, reporting, digital colleague scenarios).\n* guide the team on technical standards, code quality, and solution architecture.\n* conduct code reviews and confirm adherence to best practices in application development.\n* support enablement and training sessions for client teams, promoting safe citizen development.\n* advise on integration patterns (mcp, a2a), security, and data handling.\n* stay current with microsoft copilot, power platform, and ai advancements; recommend adoption of new features.\n* confirm all solutions align with compliance, governance, and risk management requirements and frameworks.\n* manage multiple projects simultaneously, making sure timely delivery and adherence to project timelines and budgets.\n* provide regular updates to management on project status and outcomes.\n\n **to qualify, you must have**\n\n* bachelor's or master\u2019s degree in business administration, information systems, engineering, or a related quantitative field in computer science, information technology, or related field and at least 3 years of relevant work experience.\n* at least 3 years in application development with 4 years hands\\-on power platform and copilot experience.\n* expertise in power apps inclusive of canvas apps, model\\-driven apps, power pages, and their limitations.\n* strong knowledge of power automate (cloud \\& desktop), power bi and copilot studio.\n* experience with ai builder, copilot, and dataverse security and automation concepts.\n* proficiency in environment management, coe toolkit, and tenant\\-level security controls.\n* solid understanding of rdbms (sql, oracle) and integration technologies (rest apis, json, xml).\n* object\\-oriented programming experience (c\\#, java, .net stack).\n* familiarity with agile methodologies, devops tooling.\n* excellent communication skills, ability to mentor junior developers, and deliver maintainable, well\\-documented solutions.\n* certifications: pl\\-900 and mandatory: pl\\-400 or pl\\-600\n\n **ideally, you\u2019ll also have**\n\n* certification ai\\-102\n\n **what we offer you**  \n\nat ey, we\u2019ll develop you with future\\-focused skills and equip you with world\\-class experiences. we\u2019ll empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams. learn more.\n\n* we offer a comprehensive compensation and benefits package where you\u2019ll be rewarded based on your performance and recognized for the value you bring to the business. the base salary range for this job in all geographic locations in the us is $67,000 to $136,800\\. the base salary range for new york city metro area, washington state and california (excluding sacramento) is $80,300 to $155,300\\. individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography. in addition, our total rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options.\n* join us in our team\\-led and leader\\-enabled hybrid model. our expectation is for most people in external, client serving roles to work together in person 40\\-60% of the time over the course of an engagement, project or year.\n* under our flexible vacation policy, you\u2019ll decide how much vacation time you need based on your own personal circumstances. you\u2019ll also be granted time off for designated ey paid holidays, winter/summer breaks, personal/family care, and other leaves of absence when needed to support your physical, financial, and emotional well\\-being.\n\n **are you ready to shape your future with confidence? apply today.**  \n\ney accepts applications for this position on an on\\-going basis.\n\n  \n\nfor those living in california, please click here for additional information.\n\n  \n\ney focuses on high\\-ethical standards and integrity among its employees and expects all candidates to demonstrate these qualities.\n\n **ey \\| building a better working world**\n\n  \n\ney is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.\n\n  \n\nenabled by data, ai and advanced technology, ey teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.\n\n  \n\ney teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. fueled by sector insights, a globally connected, multi\\-disciplinary network and diverse ecosystem partners, ey teams can provide services in more than 150 countries and territories.\n\n  \n\ney provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law.\n\n  \n\ney is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. if you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1\\-800\\-ey\\-help3, select option 2 for candidate related inquiries, then select option 1 for candidate queries and finally select option 2 for candidates with an inquiry which will route you to ey\u2019s talent shared services team (tss) or email the tss at ssc.customersupport@ey.com.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Service Delivery Center - Microsoft Power Platform & Copilot Engineer - Analyst",
        "company": "EY",
        "location": "Alpharetta, GA, US USA",
        "posted_at": "2026-02-22",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "AKS",
            "Git",
            "Power BI",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=964e7d50ab093cb2",
        "description": "at ey, we\u2019re all in to shape your future with confidence.\n\n  \n\nwe\u2019ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. join ey and help to build a better working world.\n\n **service delivery center \u2013 power platform \\& copilot engineer \\- analyst**\n\n **about the job:**\n\n\nprocess solutions and automation is a competency within ernst \\& young\u2019s financial and banking services practice providing their clients the world\\-class solutions to tackle their most important challenges and achieve the highest potential and efficiencies in ever changing markets.\n\n  \n\nwe help clients transform their business with process automation across enterprise functions and business units leveraging tools and platforms suitable for client requirements and needs.\n\n  \n\nwe are seeking a highly skilled and motivated copilot and power platform engineer to join our dynamic team. in this role, you will leverage your expertise in microsoft power platform, including power apps, power automate, and power bi, to develop innovative solutions that enhance productivity and drive business transformation. you will also play a key role in integrating ai capabilities into our applications using copilot technologies.\n\n  \n\nthis will be a full\\-time opportunity for a suitable candidate.\n\n\nat ey, you\u2019ll have an opportunity to build a career as unique as you are, with the global scale, support, inclusive culture, and technology to become the best version of you. and we\u2019re counting on your unique voice and perspective to help ey become even better, too. join us and build an exceptional experience for yourself, and a better working world for all.\n\n **key responsibilities:**\n\n* design, develop, and implement agentic ai solutions using copilot studio, power platform components (power apps, power automate, power bi), and azure ai foundry.\n* create and maintain comprehensive documentation for applications, workflows, and processes.\n* engage with senior stakeholders to gather requirements, translate them into technical specifications.\n* collaborate with cross\\-functional teams to ensure alignment on project goals and deliverables.\n* monitor application performance and user feedback to identify areas for improvement.\n* optimize applications for speed, scalability, and user experience.\n* architect and implement the declarative agentic framework, embedding governance, controls, and compliance best practices.\n* develop and validate pilot m365 agents (e.g., for document search, reporting, digital colleague scenarios).\n* guide the team on technical standards, code quality, and solution architecture.\n* conduct code reviews and confirm adherence to best practices in application development.\n* support enablement and training sessions for client teams, promoting safe citizen development.\n* advise on integration patterns (mcp, a2a), security, and data handling.\n* stay current with microsoft copilot, power platform, and ai advancements; recommend adoption of new features.\n* confirm all solutions align with compliance, governance, and risk management requirements and frameworks.\n* manage multiple projects simultaneously, making sure timely delivery and adherence to project timelines and budgets.\n* provide regular updates to management on project status and outcomes.\n\n **to qualify, you must have**\n\n* bachelor's or master\u2019s degree in business administration, information systems, engineering, or a related quantitative field in computer science, information technology, or related field and at least 1 years of relevant work experience.\n* at least 1 year in application development with 4 years hands\\-on power platform and copilot experience.\n* expertise in power apps inclusive of canvas apps, model\\-driven apps, power pages, and their limitations.\n* strong knowledge of power automate (cloud \\& desktop), power bi and copilot studio.\n* experience with ai builder, copilot, and dataverse security and automation concepts.\n* proficiency in environment management, coe toolkit, and tenant\\-level security controls.\n* solid understanding of rdbms (sql, oracle) and integration technologies (rest apis, json, xml).\n* object\\-oriented programming experience (c\\#, java, .net stack).\n* familiarity with agile methodologies, devops tooling.\n* excellent communication skills, ability to mentor junior developers, and deliver maintainable, well\\-documented solutions.\n* certifications: pl\\-900 and mandatory: pl\\-400 or pl\\-600\n\n **ideally, you\u2019ll also have**\n\n* certification ai\\-102\n\n **what we offer you**  \n\nat ey, we\u2019ll develop you with future\\-focused skills and equip you with world\\-class experiences. we\u2019ll empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams. learn more.\n\n* we offer a comprehensive compensation and benefits package where you\u2019ll be rewarded based on your performance and recognized for the value you bring to the business. the base salary range for this job in all geographic locations in the us is $55,400 to $113,000\\. the base salary range for new york city metro area, washington state and california (excluding sacramento) is $66,600 to $128,500\\. individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography. in addition, our total rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options.\n* join us in our team\\-led and leader\\-enabled hybrid model. our expectation is for most people in external, client serving roles to work together in person 40\\-60% of the time over the course of an engagement, project or year.\n* under our flexible vacation policy, you\u2019ll decide how much vacation time you need based on your own personal circumstances. you\u2019ll also be granted time off for designated ey paid holidays, winter/summer breaks, personal/family care, and other leaves of absence when needed to support your physical, financial, and emotional well\\-being.\n\n **are you ready to shape your future with confidence? apply today.**  \n\ney accepts applications for this position on an on\\-going basis.\n\n  \n\nfor those living in california, please click here for additional information.\n\n  \n\ney focuses on high\\-ethical standards and integrity among its employees and expects all candidates to demonstrate these qualities.\n\n **ey \\| building a better working world**\n\n  \n\ney is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.\n\n  \n\nenabled by data, ai and advanced technology, ey teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.\n\n  \n\ney teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. fueled by sector insights, a globally connected, multi\\-disciplinary network and diverse ecosystem partners, ey teams can provide services in more than 150 countries and territories.\n\n  \n\ney provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law.\n\n  \n\ney is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. if you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1\\-800\\-ey\\-help3, select option 2 for candidate related inquiries, then select option 1 for candidate queries and finally select option 2 for candidates with an inquiry which will route you to ey\u2019s talent shared services team (tss) or email the tss at ssc.customersupport@ey.com.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Agentic AI Engineer-1",
        "company": "Realign",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "LangChain",
            "RAG",
            "Gemini",
            "Copilot",
            "Prompt Engineering",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=28b1ed79dd102956",
        "description": "boston, massachusetts 02108 posted february 23rd, 2026\n looking for more job opportunities? click here!\n\n\n  \njob type: full time\n\n\njob category: it\n\n\njob description\n\n\n**role \\- agentic ai engineer****location \\- boston, ma (onsite)****experience \u2013 5\\+****type \\- full time** **job description****must have technical/functional skills**\nhands\\-on with agent frameworks such as semantic kernel, langgraph, langchain agents or crewai\n\n\nproficiency in python and modern ai/ml frameworks, aws\n\n\nhands\u2011on with bedrock agentcore memory, gateway, and runtime\n\n\nstrong experience with llms (fine\\-tuning, evaluation, prompt engineering)\n\n\n **roles \\& responsibilities**\nfocus on designing, building, and optimizing agentc capabilities using large language models and advanced reasoning frameworks.\n\n\n **key responsibilities**\nstrong technical depth, a passion for building, workstreams in a fast\\-paced, innovation\\-driven environment\n\n\nbuild serverless agentic product architectures and features on aws using bedrock agentcore memory, gateway, identity, interpreter, observability and runtime\n\n\nimplement and operate a2a/mcp servers on aws; integrate with bedrock agents/converse apis\n\n\norchestrate multi\u2011agent plans with strands agents; deploy to eks or lambda\n\n\ninstrument agents with cloudwatch metrics, spans, and traces; enable audits\n\n\ncollaborate with product managers, data engineers, and ux teams to deliver production\\-ready solutions\n\n\nfollow the roadmap for ai initiatives, ensuring they align with organizational goals and show measurable impact\n\n\n **required skills \\& experience**\nstrong problem\\-solving and strategic thinking abilities\n\n\nstrong collaboration skills in research\\-driven environments\n\n\nhands\u2011on with bedrock agentcore memory, gateway, and runtime\n\n\nstrong experience with llms (fine\\-tuning, evaluation, prompt engineering)\n\n\nproficiency in python and modern ai/ml frameworks\n\n\nproficiency in vector databases and embedding models for retrieval tasks\n\n\nexperience with data connectors and api gateways that support seamless communication between systems\n\n\nexperience with generative ai concepts such as retrieval\\-augmented generation (rag), agentic workflows, training llms with structured and unstructured data sets\n\n\nstrong communicator with the ability to present ideas clearly and influence stakeholders \\- with a passion for enabling data\\-driven transformation\n\n\nhands\\-on with agent frameworks such as semantic kernel, langgraph, langchain agents or crewai\n\n\ndeep subject matter expertise in ai technologies, including but not limited to copilot studio, openai, semantic kernels, azure ai foundry, google gemini, microsoft 365, and m365 copilot or anthropic or aws platforms\n\n\n  \n\n\nrequired skills\n\n\ndevops engineer\n\n senior email security engineer",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Perception Engineer - Data",
        "company": "Forterra",
        "location": "Arlington, VA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=24a4bdf67e7eef3b",
        "description": "**about forterra**  \n\n  \n\nforterra is a leading provider of autonomous systems for ground\\-based movement in the working world. amongst some of the earliest innovators in the field of driverless technology, forterra is focused on building systems that protect front\\-line soldiers and enable civilian workers in our industrial base. forterra is the go\\-to provider of ground autonomy solutions for the u.s. department of defense, which harnesses the technology for asymmetric warfare in critical conditions.\n\n **about forterra**  \n\n  \n\nforterra is a leading provider of autonomous systems for ground\\-based movement in the working world. amongst some of the earliest innovators in the field of driverless technology, forterra is focused on building systems that protect front\\-line soldiers and enable civilian workers in our industrial base. forterra is the go\\-to provider of ground autonomy solutions for the u.s. department of defense, which harnesses the technology for asymmetric warfare in critical conditions.\n\n **about the role**\n\n  \n\nforterra is seeking an experienced perception data engineer to join our team in supporting the development of state\\-of\\-the\\-art deep learning algorithms for on\\-road and off\\-road applications. the ideal candidate will have a strong background in managing large quantities of visual data from a variety of sensor types, supporting the tools for developing deep learning algorithms for online real\\-time perception and offline data generation in multiple sensor domains, and running on hardware operating in varied environments under real\\-time constraints.\n\n **what you'll do**\n\n* manage and maintain the perception data processing database\n* design, deploy, and operate metaflow pipelines\n* enhance and support the deep learning training pipeline dashboard\n* own the perception data processing unit test architecture and test asset management\n* develop and validate data upload/download tools for third party data annotation\n* support aws infrastructure and platform reliability\\- build, optimize, and distribute production model debians\n\n **what we're looking for:**\n\n *mission\\-focused technical stewardship*\n\n\nwe\u2019re looking for someone who understands the criticality of reliable data infrastructure, reproducible pipelines, and high\\-integrity tooling within autonomy and perception systems. you recognize that our models, datasets, and platforms must be trustworthy, maintainable, and consistently improving to support downstream mission needs.\n\n *deep ownership of complex systems*\n\n\nyou have a demonstrated ability to take full responsibility for multifaceted environments, from aws resources to kubernetes clusters, metaflow pipelines, dashboards, and ci/cd systems. you proactively identify problems, trace them across layers of the stack, and implement durable fixes rather than temporary workarounds.\n\n *architect\u2019s mindset for data and ml workflows*\n\n\nyou think holistically about data ingestion, annotation, anonymization, model optimization, and deployment pipelines. you understand that data quality, reproducibility, and model lineage are foundational, and design processes that uphold these principles across environments.\n\n *collaborative partner \\& cross\\-functional influencer*\n\n\nyou work effectively with engineering, infrastructure, mlops, it, and security partners, navigating govcloud constraints, modifying setups, coordinating dashboard access, or working with upstream owners of tooling. you can guide others through intricate workflows, mentor teammates, and build trust across organizations.\n\n **minimum qualifications:**\n\n* bs in machine learning, computer vision, or robotics, or with equivalent industrial experience.\n* 4\\+ years of similar academic and/or professional working experience.\n* excellent understanding of standard deep learning algorithms for perception.\n* solid understanding of machine learning training/deployment pipelines and their implementation.\n* experience with pytorch or tensorflow, ros, docker, python\n* excellent core software engineering skills: software design, containerization, unit testing, and debugging.\n* experience with delivering production\\-quality software in a continuously integrated environment using test\\-driven development patterns.\n\n **preferred qualifications:**\n\n* experience with deploying deep learning algorithms into latency\\-constrained on\\-road and off\\-road environments, ideally on nvidia jetson devices\n* excellent programming skills in c\\+\\+\n* experience with cloud infrastructure and aws in particular\n\n  \n\nus salary range  \n\n $150,000\u2014$175,000\n\n  \n\nthe salary range for this role is an estimate and is based on a wide variety of compensation factors. the salary offered to candidates will vary based on a variety of factors including (but not limited to) relevant work experience, education, specialized training, critical expertise, training, and more. equity in forterra is included in most of our full\\-time, high\\-demand roles and is therefore considered part of forterra\u2019s overall compensation package. in addition to base salary and equity, forterra offers competitive benefits for full\\-time employees including:\n\n* premium healthcare benefits: three plan options, including an hsa\\-eligible plan, with forterra covering 80% of the plan premium for you and your dependents.\n* basic life/ad\\&d, short and long\\-term disability insurance plans 100% covered by forterra, plus the option to purchase additional life insurance for you and your dependents.\n* extremely generous company holiday calendar including a winter break in december.\n* competitive paid time off (pto) offering 20 days accrued per year.\n* a minimum of 7 weeks fully paid parental leave for birth/adoption.\n* a $9k annual tuition reimbursement or professional development stipend.\n* fully stocked beverage refrigerators with all the celsius your little heart desires.\n* 401(k) retirement savings plan, including traditional, roth 401(k), and after\\-tax deferral with company match up to 4%.\n\n\nyour recruiter will be able to share more information about our salary and benefits offering during the hiring process.\n\n  \n\nforterra is an equal\\-opportunity employer, providing and promoting equal employment opportunity in accordance with local, state, and federal laws. forterrans are unique, talented individuals who are united through a shared passion to deliver autonomous systems that enable national resilience and a robust supply chain. all qualified applications will receive equal consideration for employment.\n\n **the pay range for this role is:**  \n\n150,000 \\- 175,000 usd per year(ava)",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Sr. Cloud Engineer",
        "company": "Transflo",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "S3",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ce6390023da67737",
        "description": "role overview:\n\n\nthis senior\\-level role designs, builds, and optimizes secure aws environments that underpin key business systems. you will partner with development, security, and operations teams to automate cloud capabilities and improve infrastructure resiliency while ensuring operational excellence.\n\n  \n\n\njob responsibilities:\n\n\n* architect, deploy, develop and optimize aws infrastructure for resiliency and uptime.\n\n\n* implement infrastructure\\-as\\-code (terraform / cloudformation) for repeatable deployments.\n\n\n* build and maintain ci/cd pipelines to optimize software release cycles.\n\n\n* improve observability \u2014 monitoring, logging, alerting, diagnostics.\n\n\n* strengthen cloud security with iam, encryption, vpc segmentation, network controls.\n\n\n* conduct cloud cost optimization and performance improvements.\n\n\n* maintain architecture documentation and operational runbooks.\n\n\n* mentor peers and influence cloud engineering standards across teams.\n\n  \n\n\nqualifications and experience:\n\n\n* 5\\+ years building and operating aws environments.\n\n\n* aws professional certifications (preferred)\n\n\n* deep knowledge of step functions, s3, iam, vpc, lambda, rds/dynamodb, cloudwatch.\n\n\n* strong development skills (python / c\\#).\n\n\n* ci/cd automation experience (github actions, gitlab ci/cd, jenkins).\n\n\n* networking fundamentals: load balancing, routing, segmentation, dns.\n\n\n* experience with dr, backup, and resilience strategies.\n\n\n* strong problem\\-solving and systems analysis.\n\n\n* excellent documentation and communication skills.\n\n  \n\n\nindividual qualities:\n\n\n* results oriented\n\n\n* independently reliable; performs tasks without close supervision\n\n\n* persistent learner showing a desire to be on the edge of new ai methodologies as it may relate to current business opportunities.\n\n\n* organized; detail\\-oriented, methodical and consistently demonstrates ability to successfully and timely complete assignments.\n\n\n* follows\\-up; consistently performs this in a positive, proactive manner\n\n\n* logical problem\\-solving skills\n\n\n* quality conscious; consistently demonstrates commitment to customers \\& quality\n\n\n* demonstrates timeliness \\& urgency\n\n\n* team work; individual contributor that works well with other team members and consistently promotes a strong team environment work ethic\n\n\n* goal setting; sets/achieves goals and consistently demonstrates a willingness/dedication to process improvement\n\n\n* responsible; takes responsibility for personal actions and consistently demonstrates a willingness to accept greater project responsibilities\n\n\n* professionally candid communications\n\n\n* focused on key success factors\n\n\n* professional attitude; consistently demonstrates ability to accept criticism and manage the conversation appropriately\n\n\n* street smart; can apply knowledge and life experiences in business\n\n\n* positive attitude\n\n\n* flexible \\& adaptable\n\n\n* resourceful",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Machine Learning Engineer",
        "company": "Adobe",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "CI/CD",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1c4f153b447e2441",
        "description": "we are seeking a senior ml engineer to join our team of dedicated machine learning and software engineers. this role involves timely engineering, system development, ml model assessment, constructing data pipelines, composing prototypes, evaluation, operationalization.\n  \n  \n\nlooking for individuals skilled in classical and deep learning, able to build and implement ml solutions for production environments. if you thrive on delivering innovative customer features, adopting new technologies, and contributing to high\\-visibility projects, this role may be a good match. the atmosphere is dynamic, fast\\-paced, creative, collaborative, and data\\-centric.\n  \nwhat you\u2019ll do\n  \n  \n\ncontribute to the backend services supporting ml and generative\\-ai functionalities.\n  \n  \n\nbuild, develop, implement, and evaluate machine learning models using classical and deep learning approaches, as well as genai.\n  \n  \n\ncollaborate with cross\\-functional teams to integrate ml solutions into existing products and services.\n  \n  \n\nanalyze and improve efficiency, accuracy, scalability, and stability of ai\\-enabled workflows.\n  \n  \n\nstay current with the latest advancements in ml and ai technologies.\n  \n  \n\nguide and support junior team members to aid in the overall development of the ml team.\n  \nwhat you need to succeed\n  \n  \n\n**required qualifications:**  \n\ncompleted a degree in computer science, machine learning, or a related field.\n  \n  \n\n5\\+ years of experience in machine learning engineering roles.\n  \n  \n\nstrong software engineering skills, including version control, code review, and ci/cd practices.\n  \n  \n\nstrong proficiency in python and experience with ml frameworks such as tensorflow, pytorch, scikit\\-learn, sagemaker.\n  \n  \n\nexperience in developing and deploying ml models in production environments.\n  \n  \n\nfamiliarity with cloud platforms (e.g., aws, gcp, azure) for ml deployment.\n  \n  \n\nstrong understanding of data structures, algorithms, and system development.\n  \n  \n\nstrong leadership and team\\-building skills.\n  \n  \n\nexcellent communication and presentation skills.\n  \n  \n\nfamiliarity with generative ai technologies and applications is advantageous.\n  \n  \n\nfamiliarity with retrieval\\-augmented generation (rag) and other contemporary nlp methods is advantageous.\n  \n  \n\nhaving experience with mlops practices and tools is advantageous.\n  \n  \n\nhaving a background in recommendation systems is advantageous.\n  \nabout adobe\n  \nadobe empowers everyone to create through innovative platforms and tools that unleash creativity, productivity and personalized customer experiences. adobe\u2019s industry\\-leading offerings including adobe acrobat studio, adobe express, adobe firefly, creative cloud, adobe experience platform, adobe experience manager, and genstudio enable people and businesses to turn ideas into impact, powered by ai and driven by human ingenuity.\n  \n  \n\nour 30,000\\+ employees worldwide are creating the future and raising the bar as we drive the next decade of growth. we\u2019re on a mission to hire the very best and believe in creating a company culture where all employees are empowered to make an impact. at adobe, we believe that great ideas can come from anywhere in the organization. the next big idea could be yours.\n  \n  \n\nlet\u2019s adobe together\n  \nat adobe, we believe in creating a company culture where all employees are empowered to make an impact. learn more about adobe life, including our values and culture , focus on people, purpose and community , adobe for all , comprehensive benefits programs , the stories we tell , the customers we serve, and how you can help us advance our mission of empowering everyone to create.\n  \n  \n\nadobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other protected characteristic. learn more.\n  \n  \n\nadobe aims to make our careers website and recruiting process accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call \\+1 408\\-536\\-3015\\.\n  \n  \n\n**ai use guidelines for interviews:**  \n\nour interviews are designed to reflect your own skills and thinking. the use of ai or recording tools during live interviews is not permitted unless explicitly invited by the interviewer or approved in advance as part of a reasonable accommodation. if these tools are used inappropriately or in a way that misrepresents your work, your application may not move forward in the process.\n  \n  \n\nat adobe, we empower employees to innovate with ai \u2014 and we look for candidates eager to do the same. as part of the hiring experience, we provide clear guidance on where ai is encouraged during the process and where it\u2019s restricted during live interviews. see how we think about ai in the hiring experience .\n  \n  \n\n**expected pay range:** our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position\u202fis $139,000 \\-\\- $257,550 annually. pay\u202fwithin this range varies by work location\u202fand may also depend on job\\-related knowledge, skills,\u202fand experience. your recruiter can share more about the specific salary range for the job location during the hiring process.\n  \n  \n\nin california, the pay range for this position is $177,900 \\- $257,550\n  \n  \n\nat adobe, for sales roles starting salaries are expressed as total target compensation (ttc \\= base \\+ commission), and short\\-term incentives are in the form of sales commission plans. non\\-sales roles starting salaries are expressed as base salary and short\\-term incentives are in the form of the annual incentive plan (aip).\n  \n  \n\nin addition, certain roles may be eligible for long\\-term incentives in the form of a new hire equity award.\n  \n  \n\n**state\\-specific notices:**  \n\n**california :**  \n\nfair chance ordinances\n  \nadobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and \u201cfair chance\u201d ordinances.\n  \n  \n\n**colorado:**  \n\napplication window notice\n  \n  \n\nif this role is open to hiring in colorado (as listed on the job posting), the application window will remain open until at least the date and time stated above in pacific time, in compliance with colorado pay transparency regulations. if this role does not have colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs.\n  \n  \n\n**massachusetts:**  \n\nmassachusetts legal notice\n  \nit is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Software Engineer II",
        "company": "Blue Origin",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Git",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3b3a27f0f05bdbda",
        "description": "application close date:\napplications will be accepted on an ongoing basis until the requisition is closed.\nat blue origin, we envision millions of people living and working in space for the benefit of earth. we\u2019re working to develop reusable, safe, and low\\-cost space vehicles and systems within a culture of safety, collaboration, and inclusion. join our team of problem solvers as we add new chapters to the history of spaceflight!\nthis role is part of enterprise technology (et), where we\u2019re developing the digital infrastructure needed to build the road to space, with an emphasis on digital capabilities required to advance blue origin\u2019s mission. enterprise technology is the center of excellence for digital technology at blue origin, providing oversight and governance to align technology and business strategies.\nwe're hiring a motivated software engineer ii who is passionate about technology, eager to learn, and driven by quality. join us in building cutting\\-edge, agentic, workflow\\-driven search and discovery platforms that power enterprise knowledge graphs and ai\\-assisted search experiences.\nas a software engineer, you will contribute to building intuitive search interfaces and robust backend services that enable blue origin teams to discover and access critical data within seconds. as part of our team, you'll apply your skills in python, java, and javascript to develop scalable, intelligent systems that make information more accessible and actionable. you will work closely with experienced engineers and cross\\-functional teams to deliver scalable solutions that support our transition to mass production operations.\nif you're excited about solving complex problems and contributing to a mission\\-driven organization, we\u2019d love to hear from you!  \n\nkey responsibilities:* assist in developing responsive frontend interfaces for search and data discovery applications using modern frameworks like react, angular, or vue\n* contribute to backend api development supporting search functionality, knowledge graph queries, and data integration services\n* support integration with ai/ml systems including rag (retrieval\\-augmented generation) capabilities and enterprise search apis like sinequa\n* work under guidance to deploy applications within aws containers, utilizing services like amazon ecs, eks, or fargate\n* assist in developing data visualization components for search results and knowledge graph exploration\n* learn and apply unit testing frameworks (e.g., junit, pyunit, pytest) to ensure reliability of search apis and applications\n* collaborate with stakeholders to understand search and discovery requirements and translate them into user\\-facing solutions\n* support troubleshooting of search performance, data pipeline integration, and application issues\n* participate in version control, testing, and documentation of search platform applications\n* stay current with search technologies, knowledge graph methodologies, and ai/ml integration patterns\n\n\nrequired qualifications:* bachelor's degree in computer science, software engineering, or a related field\n* 3\\+ years software development experience\n* proficiency with large language models (llms) for code generation, debugging, and development assistance\n* understanding of frontend technologies, particularly javascript frameworks (react preferred)\n* experience with backend development using python, java, or node.js\n* understanding of database design and management (sql, nosql)\n* experience with cloud\\-based environments (aws, azure, or google cloud)\n* good problem\\-solving skills and willingness to learn about complex search and ai systems\n* familiarity with version control (git), testing frameworks, ci/cd pipelines, and agile development methodologies\n* ability to communicate effectively and collaborate with technical and non\\-technical teams\n\n\npreferred qualifications:* experience with search technologies (elasticsearch, solr) or enterprise search platforms\n* familiarity with knowledge graph technologies or graph databases (neo4j, neptune)\n* understanding of ai/ml concepts, particularly rag or information retrieval systems\n* experience with data visualization libraries (d3\\.js, plotly) or 3d rendering (three.js)\n* experience in aerospace or manufacturing industries\n\n  \n\ncompensation range for:\nwa applicants is $164,652\\.00 \\- $230,512\\.80\nother site ranges may differ\nculture statement\ndon\u2019t meet all desired requirements? studies have shown that some people are less likely to apply to jobs unless they meet every single desired qualification. at blue origin, we are dedicated to building an authentic workplace, so if you\u2019re excited about this role but your past experience doesn\u2019t align perfectly with every desired qualification in the job description, we encourage you to apply anyway. you may be just the right candidate for this or other roles.\nexport control regulations\napplicants for employment at blue origin must be a u.s. citizen or national, u.s. permanent resident (i.e. current green card holder), or lawfully admitted into the u.s. as a refugee or granted asylum.\nbackground check* required for all positions: blue\u2019s standard background check\n* required for certain job profiles: defense biometric identification system (dbids) background check if at any time the role requires one to be on a military installation\n* required for certain job profiles: drivers who operate commercial motor vehicles with a gross vehicle weight (gvw), gross vehicle weight rating (gvwr) or combination of power unit and trailer that meets or exceeds 10,001 lbs. and/or transports placardable amounts of hazardous materials by ground in any vehicle on a public road while in commerce, may be subject to additional federal motor carrier safety regulations including: driver qualification files, medical certification (obtained before onboarding), road test, hours of service, drug and alcohol testing (cdl drivers only), vehicle inspection requirements, cdl requirements (if applicable) and hazardous materials transportation/shipping training.\n* required for certain job profiles: ability to obtain and maintain merchant mariner credential, which includes pre\\-employment and random drug testing as well as dot physical\n\n\nbenefits* benefits include: medical, dental, vision, basic and supplemental life insurance, paid parental leave, short and long\\-term disability, 401(k) with a company match of up to 5%, and an education support program.\n* paid time off: up to four (4\\) weeks per year based on weekly scheduled hours, and up to 14 company\\-paid holidays.\n* dependent on role type and job level, employees may be eligible for benefits and bonuses based on the company's intent to reward individual contributions and enable them to share in the company's results, or other factors at the company's sole discretion. bonus amounts and eligibility are not guaranteed and subject to change and cancellation. please check with your recruiter for more details.\n\n\nequal employment opportunity\nblue origin is proud to be an equal opportunity/affirmative action employer and is committed to attracting, retaining, and developing a highly qualified and dedicated work force. blue origin hires and promotes people on the basis of their qualifications, performance, and abilities. we support the establishment and maintenance of a workplace that fosters trust, equality, and teamwork. we provide all qualified applicants for employment and employees with equal opportunities for hire, promotion, and other terms and conditions of employment, regardless of their race, color, religion, sex, sexual orientation, gender identity, national origin/ethnicity, age, physical or mental disability, genetic factors, military/veteran status, or any other status or characteristic protected by federal, state, and/or local law. blue origin will consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state, and local laws, including the washington fair chance act, the california fair chance act, the los angeles fair chance in hiring ordinance, and other applicable laws.\naffirmative action and disability accommodation\napplicants wishing to receive information on blue origin\u2019s affirmative action plans, or applicants requiring a reasonable accommodation in order to participate in the application and/or interview process, please contact us at eeocompliance@blueorigin.com. please note this is a publicly managed inbox. please do not include any personal medical information in your request.\ncalifornia applicant privacy notice\nif you are a california resident, please reference the ca applicant privacy notice here.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Salesforce Tableau Developer - Sales Focused",
        "company": "Ferguson",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Synapse",
            "Git",
            "Databricks",
            "Tableau",
            "Power BI",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e85774318b17a840",
        "description": "**job posting:**\n\nsince 1953, ferguson has been a source of quality supplies for a variety of industries. together we build better infrastructure, better homes and better businesses. we exist to make our customers\u2019 complex projects simple, successful, and sustainable. we proactively solve problems, adapt and grow to continuously serve our customers, communities and each other. ferguson, a fortune 500 company, is proud to provide best\\-in\\-class products, service and capabilities across the following industries: commercial/mechanical, facilities supply, fire and fabrication, hvac, industrial, residential trade, residential building and remodel, waterworks and residential digital commerce. ferguson has approximately 36,000 associates across 1,700 locations. ferguson is a community of proud associates who operate with the shared purpose of building something meaningful. you will build a career that you are proud of, at a company you can believe in.\n\n\njoin ferguson\u2019s decision support center (dsc) and help shape the future of data\\-driven decision\\-making within salesforce. we\u2019re looking for a strategic, analytically minded developer who can pair hands\\-on technical expertise with thought leadership in sales reporting, predictive insights, and pipeline intelligence.\n\n\nthis role is ideal for someone who thrives at the intersection of data engineering, analytics strategy, and sales performance enablement\u2014and who wants to influence how thousands of associates prioritize actions and win more business.\n\n**location:**\n\n\nthis role is open to remote work across the united states or hybrid out of ferguson's corporate offices in newport news, va, according to company policy. the ability to work east coast working hours highly preferred.\n\n**responsibilities:**\n\n**lead sales\\-focused analytics within salesforce**\n\n* design, develop, and deploy advanced tableau dashboards and embedded analytics experiences that directly support sales activities, pipeline management, and opportunity execution.\n* shape the vision for salesforce reporting by transforming business questions into analytical frameworks that help sales teams focus on the right actions at the right time.\n* translate existing crm analytics (crma) content into tableau\\+ while enhancing clarity, usability, and business impact.\n\n**drive forward\\-looking, predictive insights**\n-----------------------------------------------\n\n* apply predictive analytics, statistical modeling, and trend analysis to help the organization anticipate pipeline health, forecast performance, and identify leading indicators of deal success.\n* provide strategic recommendations based on data patterns, customer behavior, conversion signals, and territory dynamics.\n\n**be a thought partner for sales leadership**\n---------------------------------------------\n\n* collaborate with sales, sales enablement, strategy, and other stakeholders to understand performance drivers and deliver insights that guide decision\\-making.\n* communicate analytical findings clearly to both technical and non\\-technical audiences, enabling confident action across the sales organization.\n\n**engineer high\\-quality, scalable solutions**\n----------------------------------------------\n\n* integrate and transform data from salesforce, azure synapse, databricks, and other enterprise systems to power robust analytical models and dashboards.\n* work closely with power bi developers and dsc teammates to ensure platform alignment, governance consistency, and scalable architecture.\n* champion data quality, security, and best practices across all salesforce\\-embedded analytics.\n\n**contribute to team growth and innovation**\n--------------------------------------------\n\n* mentor junior analytics developers and foster a culture of learning, curiosity, and continuous improvement.\n* identify opportunities to streamline processes, automate insights, and reduce analytical friction for sales users.\n\n**qualifications:**\n\n* bachelor\u2019s degree in business, computer science, information systems, or related field; or equivalent experience.\n* **5\\+ years of tableau development**, including embedding analytics in salesforce; **2\\+ years working directly in salesforce environments**.\n* tableau certifications strongly preferred.\n* strong analytical mindset with experience in predictive analytics, forecasting, statistical modeling, or machine learning (preferred).\n* expertise in sql and experience with modern cloud data platforms such as **azure synapse** and **databricks**.\n* familiarity with crm analytics (crma) and experience translating dashboards into tableau\\+ (preferred).\n* ability to think like a consultant\u2014ask the right questions, connect the dots, and guide stakeholders toward impactful decisions.\n* excellent communication skills with a passion for data storytelling.\n\n**about the team**\n\n\nat ferguson, we believe data should empower people to act with confidence. the decision support center builds modern analytics solutions that simplify complexity, elevate performance, and accelerate decision\\-making across the enterprise.\n\n\nyou\u2019ll join a collaborative, forward\\-thinking team that values curiosity, innovation, and practical execution. we solve meaningful business problems and strive to make analytics accessible, intuitive, and actionable.\n\n**why this role matters**\n\n* **shape the future of sales analytics:** become the thought leader guiding how ferguson analyzes pipeline health, account behavior, and opportunity execution.\n* **drive real business outcomes:** your work will directly influence how sales teams prioritize, engage, and win.\n* **grow your skills:** lead embedded analytics in salesforce, expand your predictive analytics expertise, and help build the next generation of sales intelligence.\n* **make an impact:** help transform how ferguson employs data to empower associates and elevate customer outcomes.\n\n\nat ferguson, we care for each other. we value our well\\-being just as much as our hard work. we are committed to a holistic approach towards benefits plans and programs that support the mental, physical and financial well\\-being of our associates. our competitive offering not only includes benefits like health, dental, vision, paid time off, life insurance and a 401(k) with a company match, but our associates also enjoy additional meaningful and inclusive enhancements that are adaptable to their diverse situations and needs, including mental health coverage, gender affirming and family building benefits, paid parental leave, associate discounts, community involvement opportunities and more!\n\n* \n\n**pay range:**\n\n* \n\n*actual pay rate may vary depending upon location. the estimated pay range for this position is below. the specific rate will depend on a candidate\u2019s qualifications and prior experience.*\n\n* \n\n\n$6,000\\.30 \\- $13,016\\.30* \n\n***estimated ranges displayed are monthly for salaried roles*** **or** ***hourly for all other roles.***\n\n* \n\n\nthis role is bonus or incentive plan eligible.\n\n* \n\n\nferguson complies with all wage regulations. the starting wage may be higher in certain locations based on local or state wage requirements.\n\n* \n\n*the company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 cfr 60\\-300\\.5(a), which prohibits discrimination against qualified protected veterans and the requirements of 41 cfr 60\\-741\\.5(a), which prohibits discrimination against qualified individuals on the basis of disability.*\n\n*ferguson enterprises, llc. is an equal employment employer* *f/m/disability/vet/sexual* *orientation/gender* *identity.*\n\n\nequal employment opportunity and reasonable accommodation information",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "LLaMA",
            "Pinecone",
            "Prompt Engineering",
            "Python",
            "R",
            "Java",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8dfb9f688fa2bf88",
        "description": "**role description** **job title: python gen ai developer**  \n\n**work location : new york city, ny**  \n\n**job description:**\n\n\n**responsibilities**\n\n\n* strong data structures any language code comprehension python or java\n* ability to do hands on coding in the interview real time for given scenario with right solution approach\n* proficient in data manipulation and cleansing preprocess large datasets handle missing values perform feature engineering ensure data integrity\n* aiml strong nlp deep learning concepts with transformer architecture understanding\n* llm fine tuning exposure on one or few leading llm like dolly llama2 or any other\n* experience in developing and implementing cutting edge generative ai models to solve complex problems\n* ability to perform prompt engineering and designing right prompts based on functional use cases\n* should have hands on of nlpaiml tool technologies gpt bert or other language models\n* should have experience in building genai applications\n* word embedding and vector db usage with the leading vector db implementation pinecone milvus or pg vector\n* statistical analysis strong proficiency in statistical techniques and methodologies including hypothesis testing regression analysis time series analysis and clustering\n* mandatory certification none\n\n  \n\n**skills** **mandatory skills :** mlops, python\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $95,432\\.00 to $139,600\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "LLaMA",
            "Pinecone",
            "Prompt Engineering",
            "Python",
            "R",
            "Java",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b882e08a5c9eb997",
        "description": "**role description** **job title: python gen ai developer**  \n\n**work location : new york city, ny**  \n\n**job description:**\n\n\n**responsibilities**\n\n\n* strong data structures any language code comprehension python or java\n* ability to do hands on coding in the interview real time for given scenario with right solution approach\n* proficient in data manipulation and cleansing preprocess large datasets handle missing values perform feature engineering ensure data integrity\n* aiml strong nlp deep learning concepts with transformer architecture understanding\n* llm fine tuning exposure on one or few leading llm like dolly llama2 or any other\n* experience in developing and implementing cutting edge generative ai models to solve complex problems\n* ability to perform prompt engineering and designing right prompts based on functional use cases\n* should have hands on of nlpaiml tool technologies gpt bert or other language models\n* should have experience in building genai applications\n* word embedding and vector db usage with the leading vector db implementation pinecone milvus or pg vector\n* statistical analysis strong proficiency in statistical techniques and methodologies including hypothesis testing regression analysis time series analysis and clustering\n* mandatory certification none\n\n  \n\n**skills** **mandatory skills :** mlops, python\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $95,432\\.00 to $139,600\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "LLaMA",
            "Pinecone",
            "Prompt Engineering",
            "Python",
            "R",
            "Java",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e18794724c7fb0b4",
        "description": "**role description** **job title: python gen ai developer**  \n\n**work location : new york city, ny**  \n\n**job description:**\n\n\n**responsibilities**\n\n\n* strong data structures any language code comprehension python or java\n* ability to do hands on coding in the interview real time for given scenario with right solution approach\n* proficient in data manipulation and cleansing preprocess large datasets handle missing values perform feature engineering ensure data integrity\n* aiml strong nlp deep learning concepts with transformer architecture understanding\n* llm fine tuning exposure on one or few leading llm like dolly llama2 or any other\n* experience in developing and implementing cutting edge generative ai models to solve complex problems\n* ability to perform prompt engineering and designing right prompts based on functional use cases\n* should have hands on of nlpaiml tool technologies gpt bert or other language models\n* should have experience in building genai applications\n* word embedding and vector db usage with the leading vector db implementation pinecone milvus or pg vector\n* statistical analysis strong proficiency in statistical techniques and methodologies including hypothesis testing regression analysis time series analysis and clustering\n* mandatory certification none\n\n  \n\n**skills** **mandatory skills :** mlops, python\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $95,432\\.00 to $139,600\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "LLaMA",
            "Pinecone",
            "Prompt Engineering",
            "Python",
            "R",
            "Java",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=baa2a66d4c015d1f",
        "description": "**role description** **need only vi candidates**\n\n\n**job title: python gen ai developer**  \n\n**work location : new york city, ny**  \n\n**job description:**\n\n\n**responsibilities**\n\n\n* strong data structures any language code comprehension python or java\n* ability to do hands on coding in the interview real time for given scenario with right solution approach\n* proficient in data manipulation and cleansing preprocess large datasets handle missing values perform feature engineering ensure data integrity\n* aiml strong nlp deep learning concepts with transformer architecture understanding\n* llm fine tuning exposure on one or few leading llm like dolly llama2 or any other\n* experience in developing and implementing cutting edge generative ai models to solve complex problems\n* ability to perform prompt engineering and designing right prompts based on functional use cases\n* should have hands on of nlpaiml tool technologies gpt bert or other language models\n* should have experience in building genai applications\n* word embedding and vector db usage with the leading vector db implementation pinecone milvus or pg vector\n* statistical analysis strong proficiency in statistical techniques and methodologies including hypothesis testing regression analysis time series analysis and clustering\n* mandatory certification none\n\n  \n\n**skills** **mandatory skills :** mlops, python\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $95,432\\.00 to $139,600\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Specialist - Architecture",
        "company": "LTIMindtree",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "LLaMA",
            "Pinecone",
            "Prompt Engineering",
            "Python",
            "R",
            "Java",
            "Hypothesis Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ba86dc84f904b800",
        "description": "**role description** **job title: python gen ai developer**  \n\n**work location : new york city, ny**  \n\n**job description:**\n\n\n**responsibilities**\n\n\n* strong data structures any language code comprehension python or java\n* ability to do hands on coding in the interview real time for given scenario with right solution approach\n* proficient in data manipulation and cleansing preprocess large datasets handle missing values perform feature engineering ensure data integrity\n* aiml strong nlp deep learning concepts with transformer architecture understanding\n* llm fine tuning exposure on one or few leading llm like dolly llama2 or any other\n* experience in developing and implementing cutting edge generative ai models to solve complex problems\n* ability to perform prompt engineering and designing right prompts based on functional use cases\n* should have hands on of nlpaiml tool technologies gpt bert or other language models\n* should have experience in building genai applications\n* word embedding and vector db usage with the leading vector db implementation pinecone milvus or pg vector\n* statistical analysis strong proficiency in statistical techniques and methodologies including hypothesis testing regression analysis time series analysis and clustering\n* mandatory certification none\n\n  \n\n**skills** **mandatory skills :** mlops, python\n\n **other details**  \n\nbenefits/perks listed below may vary depending on the nature of your employment with ltimindtree (\u201cltim\u201d):\n\n\n\nbenefits and perks:\n\n\n* comprehensive medical plan covering medical, dental, vision\n* short term and long\\-term disability coverage\n* 401(k) plan with company match\n* life insurance\n* vacation time, sick leave, paid holidays\n* paid paternity and maternity leave\n\n\nthe range displayed on each job posting reflects the minimum and maximum salary target for the position across all us locations. within the range, individual pay is determined by work location and job level and additional factors including job\\-related skills, experience, and relevant education or training. depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance\\-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\n\n**disclaimer**: the compensation and benefits information provided herein is accurate as of the date of this posting.\n**ltimindtree** is an equal opportunity employer that is committed to diversity in the workplace. our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family\\-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law. **benefits**  \n\ncompensation range: $95,432\\.00 to $139,600\\.00 per year  \n\n**about ltm**  \n\nltm is an ai\\-centric global technology services company and the business creativity partner to the world\u2019s largest and most disruptive enterprises. we bring human insights and intelligent systems together to help clients create greater value at the intersection of technology and domain expertise. our capabilities span integrated operations, transformation, and business ai \u2014 enabling new ways of working, new productivity paradigms, and new roads to value. together with over 87,000 employees across 40 countries and our global network of partners, ltm \u2014 a larsen \\& toubro company \u2014 owns business outcomes for our clients, helping them not just outperform the market, but to outcreate it. please also note that neither ltm nor any of its authorized recruitment agencies/partners charge any candidate registration fee or any other fees from talent (candidates) towards appearing for an interview or securing employment/internship. candidates shall be solely responsible for verifying the credentials of any agency/consultant that claims to be working with ltm for recruitment. please note that anyone who relies on the representations made by fraudulent employment agencies does so at their own risk, and ltm disclaims any liability in case of loss or damage suffered as a consequence of the same. recruitment fraud alert \\- https://www.ltimindtree.com/recruitment\\-fraud\\-alert/",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Bioinformatics Software Engineer",
        "company": "Harvard University",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "Docker",
            "CI/CD",
            "Git",
            "Python",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8d28ab892406e1df",
        "description": "**february 23, 2026****002365sr**\n  \ncompany description\n\n  \n\nby working at harvard university, you join a vibrant community that advances harvard's world\\-changing mission in meaningful ways, inspires innovation and collaboration, and builds skills and expertise. we are dedicated to creating a diverse and welcoming environment where everyone can thrive.\n\n**why join harvard medical school?**\n\n\nharvard medical school's mission is to nurture a diverse, inclusive community dedicated to alleviating suffering and improving health and well\\-being for all through excellence in teaching and learning, discovery and scholarship, and service and leadership.\n\n\nyou\u2019ll be at the heart of biomedical discovery, education, and innovation, working alongside world\\-renowned faculty and a community dedicated to improving human health. this is more than a job \\- it\u2019s an opportunity to shape the future of medicine.\n\n  \n\njob description\n\n **job summary:**\n\nparticipate in the design of software that supports and enriches research productivity and reliability; implement software solutions. develop software and data services with researchers to ensure that modern standards of reproducible code are kept.\n\n  \n\n**job\\-specific responsibilities:**\n\nwe are looking for a highly skilled bioinformatics software engineer who specializes in designing, developing, deploying, and maintaining scalable bioinformatics pipelines on cloud\\-based infrastructure. the candidate will be responsible for the code base supporting the large\\-scale genomic processing and analysis pipelines at the smaht data analysis center that manages multi\\-omic data (e.g., illumina/pacbio/ont whole genome sequencing (wgs), rna\\-seq). the ideal candidate will have a deep understanding of next\\-generation sequencing (ngs) data analysis, workflow automation, cloud computing, and cloud software engineering best practices. this role will support research and production environments where reproducibility, scalability, and performance are critical.\n\n**job description:**\n\n* design, implement, and maintain bioinformatics pipelines for high\\-throughput sequencing data (e.g., alignment, qc, variant calling from wgs and rna\\-seq) similar to those in existing repositories: **https://github.com/smaht\\-dac/main\\-pipelines**.\n* build reproducible, well\\-tested, and automated workflows using workflow management systems (particularly cwl).\n* architect and manage aws\\-based compute infrastructure to support pipeline execution, including automated deployment, scaling, and monitoring.\n* containerize workflows using docker or similar tools for managed execution and portability.\n* integrate ci/cd tooling to automate testing, deployment, and version control to ensure data integrity and correct execution of the pipeline.\n* develop utility tools for metadata management, file integrity checks or conversion (e.g., vcf, bam to cram), and integration with the smaht data portal.\n* collaborate cross\\-functionally with research scientists, engineers, and it teams to refine requirements and deliver high\\-quality solutions.\n* document code, workflows, and infrastructure configurations clearly.\n\n  \n\nqualifications\n\n **basic qualifications:**\n\n* minimum of five years\u2019 post\\-secondary education or relevant work experience.\n\n  \n\n**additional qualifications and skills:**\n\n* phd in computational biology/bioinformatics/statistics/cs or another quantitative field is strongly preferred.\n* superb programming skills, especially in python and shell scripting, and communication skills are strongly preferred.\n* extensive experience with analysis of high\\-throughput sequencing data and knowledge of bioinformatics tools for sequence alignment, variant calling, sequence data qc, etc.\n* proficiency in docker for creating a reproducible execution environment and workflow description language for orchestrating complex tasks.\n* strong understanding of aws services (ec2, s3\\) or similar cloud platforms for compute and storage.\n* version control \\& ci/cd: git, automated testing, deployment workflows.\n* experience with linux systems, hpc, and distributed computing environments.\n* knowledge of optimizing pipelines for large\\-scale genomic projects.\n\n  \n\nadditional information\n\n* **appointment end date:** this is a one\\-year term position from the date of hire, with the possibility of extension, contingent upon work performance and continued funding to support the position.\n* **standard hours/schedule:** 35 hours per week\n* **visa sponsorship information:** harvard university is unable to provide visa sponsorship for this position.\n* **pre\\-employment screening:** identity\n* **other information:** please note that we are currently conducting a majority of interviews and onboarding remotely and virtually. we appreciate your understanding.\n* **staying informed about your application**: due to the high volume of applications, we may not always be able to reach out right away, but you can track your status anytime through the **careers@harvard** portal.\n\n\n\\#li\\-dk1\n\n**work format details**\n\n\nthis position has been determined by school or unit leaders that some of the duties and responsibilities can be effectively performed at a non\\-harvard location. the work schedule and location will be set by the department at its discretion and based upon operational needs. when not working at a harvard or harvard\\-designated location, employees in hybrid positions must work in a harvard registered state in compliance with the university\u2019s policy on **employment outside of massachusetts**. additional details will be discussed during the interview process. certain visa types and funding sources may limit work location. individuals must meet work location sponsorship requirements prior to employment.\n\n\n**salary grade and ranges**\n\n\nthis position is salary grade level 057\\. please visit **harvard's salary ranges** to view the corresponding salary range and related information.\n\n\n**benefits**\n\n\nharvard offers a comprehensive benefits package that is designed to support a healthy work\\-life balance and your physical, mental and financial wellbeing. because here, you are what matters. our benefits include, but are not limited to:\n\n* generous paid time off including parental leave\n* medical, dental, and vision health insurance coverage starting on day one\n* retirement plans with university contributions\n* wellbeing and mental health resources\n* support for families and caregivers\n* professional development opportunities including tuition assistance and reimbursement\n* commuter benefits, discounts and campus perks\n\n\nlearn more about these and additional benefits on our **benefits \\& wellbeing page**.\n\n\n**eeo/non\\-discrimination commitment statement**\n\n\nharvard university is committed to **equal opportunity** and **non\\-discrimination**. we seek talent from all parts of society and the world, and we strive to ensure everyone at harvard thrives. our differences help our community advance harvard's academic purposes.\n\n\nharvard has an **equal employment opportunity policy** that outlines our commitment to prohibiting discrimination on the basis of race, ethnicity, color, national origin, sex, sexual orientation, gender identity, veteran status, religion, disability, or any other characteristic protected by law or identified in the university's **non\\-discrimination policy**. harvard's **equal employment opportunity policy** and **non\\-discrimination policy** help all community members participate fully in work and campus life free from harassment and discrimination.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Jr. Software Developer",
        "company": "Buyers Edge Platform",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "S3",
            "EC2",
            "CI/CD",
            "Git",
            "MySQL",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=90037e7aa244c1c1",
        "description": "we are seeking a **junior software developer** to join our platform ocr team, supporting the development and evolution of our document processing and data extraction systems. this role is ideal for an early\\-career developer who is excited to work across modern front\\-end and backend technologies while gaining hands\\-on experience with ai\\- and ml\\-driven ocr workflows in a production environment.  \n\n  \n\nyou will work closely with experienced engineers, product managers, and data\\-focused teams to build, enhance, and maintain software that transforms unstructured documents into accurate, structured data used across buyers edge platform.\n\n\n**who we are:**\n\n\n\nbuyers edge platform stands at the forefront of revolutionizing the foodservice industry through technology, purchasing power and partnerships. we are dedicated to empowering stakeholders across the entire foodservice ecosystem (operators, distributors, manufacturers) with efficiency and unprecedented visibility. with a diverse portfolio of over a dozen brands, our mission is clear: to reduce costs, streamline the foodservice supply chain, and propel the industry from manual to automated.\n\n\n\ntoday, we are one of the largest players in foodservice, with over 200k operator locations across north america and over $50 billion of aggregated spend volume. our commitment to foodservice excellence is proven in four distinct areas of value: digital procurement network, fresh solutions, supply chain management, and software. buyers edge platform is not just a provider \u2013 we are a strategic partner on the journey towards a more efficient, connected, and automated future for the foodservice industry.\n\n\n***this is a fully remote role. we are unable to offer work sponsorship for this role.***\n\n\n**your impact:**\n\n\n* contribute to the design, development, and maintenance of backend services supporting ocr workflows using java and spring boot.\n* assist in building and maintaining front\\-end components using react and typescript.\n* work with senior engineers to integrate ai and ml outputs into production ocr pipelines, including classification, extraction, and error\\-correction workflows.\n* support the ingestion, processing, and storage of structured and unstructured data using mysql and aws\\-managed services.\n* participate in code reviews and learn best practices around maintainability, performance, and reliability.\n* help troubleshoot and resolve bugs, performance issues, and data inconsistencies.\n* contribute to ci/cd pipelines and deployment workflows using circleci.\n* learn and apply best practices for logging, monitoring, and cloud\\-based development in aws.\n\n\n**about you:**\n\n\n* 1\u20133 years professional software development experience.\n* strong java and/or typescript experience; comfortable working in both frontend and backend code.\n* solid understanding of restful apis and relational databases.\n* experience writing automated tests (unit, integration).\n* experience with java and oop fundamentals.\n* familiarity with spring or spring boot (academic or professional).\n* exposure to modern front\\-end development using javascript or typescript (react preferred).\n* basic understanding of relational databases such as mysql.\n* comfort working with git and collaborative development workflows.\n* curiosity about ai and machine learning systems, particularly ocr and document processing.\n* strong problem\\-solving skills and willingness to learn.\n* clear written and verbal communication skills\n\n\n**nice to have:**\n\n\n* exposure to ocr, document processing, or unstructured data workflows.\n* familiarity with aws services such as ec2, ecs, rds, or s3\n* experience working in a ci/cd environment.\n* coursework or projects involving machine learning or data processing.\n\n\nnot sure you meet every qualification? studies show that diverse applicants often hesitate to apply unless they check every box. at buyers edge platform, we value authenticity and inclusion\u2014if you're excited about the role, we encourage you to apply. you might be exactly who we're looking for!\n\n\n**what's in this for you:**\n\n* great benefits from day one. we offer medical, dental, vision, fsa, company\\-paid life insurance, and more\u2014plus a 401(k) with company match.\n* grow with us. enjoy strong training, development, and competitive pay.\n* work\\-life balance. our flexible pto policy lets you take time when you need it\u2014no accrual required.\n\n\nwe welcome all.\n\n\n\nwe are committed to creating a diverse environment and are proud to be an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy\\-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Machine Learning Researcher",
        "company": "Rivet Industries",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=756a548f7b63644b",
        "description": "**about rivet**\n\n\nrivet is an american company building integrated task systems \u2014 fusing hardened hardware with software, sensors, ai, and networking \u2014 for industrial workforces and defense personnel. we create capabilities that multiply the effectiveness of every individual and withstand the world\u2019s toughest environments.\n\n  \n\nwe serve the people who build, operate, maintain, and defend our way of life. from technicians and engineers to first responders and service members, they embody the hard work, ingenuity, and meritocratic values that drive western prosperity. yet too often they are forced to rely on outdated tools that fail under modern pressures. rivet exists to reset that priority.\n\n  \n\nat rivet, you\u2019ll join a mission\\-driven team that fuses disciplines to deliver decisive outcomes where they matter most. whether shaping our technology, strengthening our partnerships, or building our culture, every role here contributes to equipping the front lines with the modern systems they deserve.\n\n **who thrives here**\n\n* people with a deep disdain for bureaucracy, empire building, groupthink, dogma, corporate babble, and wasted time\n* teammates who want to work exclusively alongside others at the top of their field\n* experienced, no\\-nonsense professionals who are execution\\-focused and deliver high\\-quality solutions above all else\n\n **role description**\n\n\nwe are seeking a talented ml research engineer to advance our computer vision and sensor fusion capabilities. this role combines cutting\\-edge research with practical implementation of machine learning pipelines for imaging, pose estimation, and model optimization. the ideal candidate will have strong expertise in python, deep learning frameworks, and experience deploying ml models in production environments. you'll explore new ideas, validate them against the state of the art and deliver working prototypes that influence our product and research direction.\n\n **role objectives**\n\n* implement pocs in python/c\\+\\+ to validate ml ideas on embedded hardware\n* conduct research in imaging and video processing pipelines for ar/vr applications\n* document learnings and define clear pathways from prototype to production\n* research and implement model optimization techniques for edge deployment\n* stay current with latest developments in computer vision and machine learning literature\n* prototype novel algorithms and validate performance through experimentation\n* design and implement end\\-to\\-end machine learning pipelines using pytorch and tensorflow lite\n* optimize models for real\\-time performance on mobile and embedded platforms\n* implement mlops best practices for model versioning, monitoring, and continuous integration\n* create scalable data preprocessing and augmentation pipelines\n\n**role requirements**\n\n* bs with 5\\+ years of academic or industry experience in machine learning research or applied ml engineering with shipped or published work (or ms with 2\\+ yrs of the above)\n* proficiency in python with experience in ml frameworks (pytorch, tensorflow)\n* experience with ml pipeline development, model deployment, and production monitoring\n* knowledge of quantization, pruning, and edge deployment techniques\n\n### **research areas (at least one)**\n\n* **imaging/video pipeline:** experience with computational photography, video processing, or camera systems\n* **sensor fusion \\& pose estimation:** research background in multi\\-sensor data fusion, tracking, or slam\n* **model optimization:** experience optimizing ml models for mobile/embedded deployment\n\n### **foundational knowledge (preferred understanding)**\n\n* **camera systems:** intrinsic/extrinsic calibration, pinhole model, distortion correction, fov, color science, exposure control, stereo matching\n* **image processing:** demosaic, denoising, sharpening, color correction, tone mapping, gamma correction, hdr, super resolution, segmentation, white balance\n* **computer vision:** feature detection/matching, optical flow, structure from motion, 3d reconstruction, slam algorithms\n* **imu \\& sensor fusion:** 6dof/3dof tracking, gyroscope/accelerometer/magnetometer integration, sensor calibration, sensor fusion algorithms\n\n### **preferred qualifications**\n\n* phd in computer vision, machine learning, or related field\n* publications in top\\-tier conferences (cvpr, iccv, eccv, neurips, icml)\n* experience with ar/vr or mobile computer vision applications\n* knowledge of cuda programming and gpu optimization\n* experience with cloud platforms (aws, gcp, azure) for ml workloads\n* familiarity with containerization (docker, kubernetes) and ci/cd pipelines\n* experience with distributed training and large\\-scale data processing",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Compliance, Dallas, Associate, Software Engineering",
        "company": "Goldman Sachs",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "MongoDB",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ead7d1da7f34ba0a",
        "description": "**more about this job**\n*please note division and function examples are representative of opportunities common for this skill\\-set. the list is not exhaustive, and availability of open roles is determined based on business need. specific roles will be confirmed through the interview process.*\n\n**responsibilities**\nsoftware engineers primarily focus on software design and development. this is meant to cover most programming positions in engineering, and include positions that were previously considered business software engineers, platform engineers, and quality assurance engineers. combine the best open source software, databases, cloud solutions, and programming languages, to solve problems and provide accurate, complex, scalable applications that help our business and clients gain new insights.\n\n\nas a software engineer, you are the change agents that transform goldman sachs by applying your technical know\\-how.be a part of our embedded engineering teams, that work as a unit with our business partners. collaborate with trading, sales, asset management, banking, finance and others, to build and automate solutions to keep our firm\u2019s position on the cutting edge. or, join our core engineering teams, and elevate all of our businesses by providing reliable, scalable platforms for data engineering, machine learning, networking, developer tooling, collaboration and more.\n\n\ninnovate with ui/ux designers, data scientists, cloud engineers, and more in a collaborative, agile environment where your enthusiasm to take on new problems and learn will have an immediate impact.\n\n\n**basic qualifications**\n\n\n* bachelor\u2019s degree or relevant work experience in computer science, mathematics, electrical engineering or related technical discipline.\n* excellent object oriented or functional analysis and design skills.\n* knowledge of data structures, algorithms, and designing for performance.\n* excellent written and verbal communication skills.\n* ability to solve problems and apply analysis to make data driven decisions.\n* comfortable multi\\-tasking, managing multiple stakeholders and working as part of a global team.\n* can apply an entrepreneurial approach and passion to problem solving and product development.\n* 1\\+ years of software development experience.\n\n\n**expert knowledge in one or more of**\n\n\n* programming in a complied language such as java, or c\\+\\+ or an interpreted language such as python and experience with concurrency and memory management.\n* responsive web development, with professional react/angular/redux experience and advanced javascript proficiency.\n* nosql databases such as mongodb and elastic search.\n\n\n**preferred qualifications**\n\n\n* knowledge or interest in trading technologies in the front\\-office of a trading organization\n* b.s. or m.s. computer science or related field.\n\n\nabout goldman sachs  \n\n  \n\nat goldman sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. founded in 1869, we are a leading global investment banking, securities and investment management firm. headquartered in new york, we maintain offices around the world.  \n\n  \n\nwe believe who you are makes you better at what you do. we're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. about our culture, benefits, and people at gs.com/careers.  \n\n  \n\nwe\u2019re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. : https://www.goldmansachs.com/careers/footer/disability\\-statement.html  \n\n  \n\n  \n\n\u00a9 the goldman sachs group, inc., 2023\\. all rights reserved.\ngoldman sachs is an equal opportunity employer and does not discriminate on the basis of race, color, religion, sex, national origin, age, veterans status, disability, or any other characteristic protected by applicable law.\n**we offer best\\-in\\-class benefits**\nhealthcare \\& medical insurance\nwe offer a wide range of health and welfare programs that vary depending on office location. these generally include medical, dental, short\\-term disability, long\\-term disability, life, accidental death, labor accident and business travel accident insurance.\n\n\nholiday \\& vacation policies\nwe offer competitive vacation policies based on employee level and office location. we promote time off from work to recharge by providing generous vacation entitlements and a minimum of three weeks expected vacation usage each year.\n\n\nfinancial wellness \\& retirement\nwe assist employees in saving and planning for retirement, offer financial support for higher education, and provide a number of benefits to help employees prepare for the unexpected. we offer live financial education and content on a variety of topics to address the spectrum of employees\u2019 priorities.\n\n\nhealth services\nwe offer a medical advocacy service for employees and family members facing critical health situations, and counseling and referral services through the employee assistance program (eap). we provide global medical, security and travel assistance and a workplace ergonomics program. we also offer state\\-of\\-the\\-art on\\-site health centers in certain offices.\n\n\nfitness\nto encourage employees to live a healthy and active lifestyle, some of our offices feature on\\-site fitness centers. for eligible employees we typically reimburse fees paid for a fitness club membership or activity (up to a pre\\-approved amount).\n\n\nchild care \\& family care\nwe offer on\\-site child care centers that provide full\\-time and emergency back\\-up care, as well as mother and baby rooms and homework rooms. in every office, we provide advice and counseling services, expectant parent resources and transitional programs for parents returning from parental leave. adoption, surrogacy, egg donation and egg retrieval stipends are also available.\n\n\nbenefits at goldman sachs\nread more about the full suite of class\\-leading benefits our firm has to offer.\n\n\n  \nopportunity overview\ncorporate title\nassociate\noffice location(s)\ndallas\njob function\nsoftware engineering\ndivision\ncompliance division",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "MLOps Engineer",
        "company": "ValueBase Consulting",
        "location": "Ann Arbor, MI, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Azure ML",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a39342bfa0110d52",
        "description": "**mlops engineer**\n\n\n**ann arbor, michigan**\n\n\n**12\\+ months**\n\n\n**$65/hr on w2**\n\n\n**candidates should be able to work on our w2**\n\n\n**required skills:**\n\n\n* **strong programming skills in languages such as python, java, or scala.**\n* **proven experience as an mlops engineer**, specifically with **azure ml and related azure technologies**.\n* familiarity with containerization technologies such **as docker and orchestration tools like kubernetes**.\n* proficiency in automation tools like **jira, ansible, jenkins, docker compose, artifactory, etc**.\n* knowledge of devops practices and tools for continuous integration, continuous deployment (ci/cd), and infrastructure as code (iac).\n* bachelor\u2019s degree or higher in computer science, engineering, mathematics, or related field.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Risk Adjustment Sr. Data Analyst - Remote",
        "company": "Datavant",
        "location": "Houston, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Snowflake",
            "Databricks",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6d255eff083655ae",
        "description": "datavant is the data collaboration platform trusted for healthcare. guided by our mission to make the world's health data secure, accessible and actionable, we provide critical data solutions for organizations across the healthcare ecosystem \\- including providers, health plans, researchers, and life sciences companies. from fulfilling a single patient's request for their medical records to powering the ai revolution in healthcare, datavanters are building the future of how data is connected and used to improve health.\n  \n  \n\nby joining datavant today, you're stepping onto a driven and highly collaborative team that is passionate about creating transformative change in healthcare.\n  \n  \n\nthe risk adjustment sr data analyst is an experienced analyst possessing a deep understanding of the risk adjustment business domain and is skilled at data modeling and analysis and identifying and communicating insights to stakeholders. you will be joining a team that is focused on ensuring the work we do for our customers is of the highest quality and yields positive outcomes. to achieve this, you will leverage a wide array of data and technology for analytics and partner with various internal and external stakeholders to understand business objectives and deliver results.\n  \n  \n\n**as a sr. data analyst, you will:**  \n\ntake ownership of complex data workflows, build scalable analytics pipelines, and contribute to productizing insights using modern data tools such as sql, dbt, airflow, snowflake, and databricks\n  \nanalyze large datasets to identify trends, patterns, and insights about coding output to enhance productivity and quality, and turn those needs into actionable reporting\n  \nprovide real\\-time data insights to business on\\-demand through ad\\-hoc queries\n  \nuse your technical skills and experience to streamline ad\\-hoc analyses into reusable data products where possible\n  \ncollect, interpret, and aggregate data from multiple data sources for supporting risk adjustment medical record coding and quality processes\n  \ndesign, develop, test, and deploy reporting to support risk adjustment business users' needs\n  \nautomate reporting and analytics when appropriate to make more scalable across customers and deliverable to a broader set of stakeholders\n  \n  \n\nidentify trends in the reporting and work to partner with the teams to improve productivity and quality.\n  \n  \n\nrun various risk adjustment models for medicare advantage, medicaid or aca to forecast patient risk scores and return on investment based on historical data and project variables\n  \nwork closely with cross\\-functional teams, including clients, to understand business needs, and determine the right methodology for analysis and assumptions to provide data\\-driven insights into program performance and partnerships\n  \ncreate clear and concise reports to communicate findings and insights to both technical and non\\-technical stakeholders\n  \nstay abreast of industry trends, new technologies, and methodologies to enhance the team's analytical capabilities\n  \nhelp guide, mentor and train other junior analysts in support of their direct manager\n  \n  \n\n**what you will bring to the table:**  \n\nexperienced (5\\+ years or more) in data analysis, database technologies (oracle/ms sql server, snowflake, databricks), sql queries, ms excel, python, etc.\n  \ndeep understanding of and experience in risk adjustment (ma, aca and md) business analytics with knowledge of different risk models including hcc, rxhcc, hhs\\-hcc and cdps\n  \nintermediate proficiency in sql (advanced analytic queries), python (data manipulation and automation, streamlit, etc.), spreadsheets and bi tools\n  \nexperience analyzing risk adjustment data for trends, disease/diagnosis prevalence and hierarchy\n  \nexperience managing data flows for chart retrieval, ra coding, hedis abstraction and quality\n  \nability to build, architect and deliver robust customer facing and internal reports\n  \nexperience in building queries to collect and interpret raw data from databases to support risk adjustment coding and medical record\n  \nexperience in using business intelligence, data visualization, query, analytic and statistical software to build solutions, perform analysis and interpret data (ssrs, power bi, tableau, looker, sigma)\n  \nstrong problem\\-solving skills with the ability to think critically and provide data\\-driven solutions\n  \nexpertise in the data cleaning, preprocessing, manipulation, integration, processing and interrogation of large datasets\n  \nexceptional initiative and ability to solve problems independently, seek help when needed, and take ownership when navigating ambiguity\n  \nexcellent communication skills\n  \nwell\\-developed time management skills and demonstrable experience of prioritizing work to meet tight deadlines for client deliverables\n  \n  \n\n**bonus points if:**  \n\nan appreciation of the need for effective data privacy and security methods and an awareness of the relevant legislation\n  \nexperience with cloud services for storage and computing\n  \nexperience with machine learning algorithms\n  \nknowledgeable in health plan operations and reporting\n  \n  \n\nwe are committed to building a diverse team of datavanters who are smart, nice, and get things done, where every datavanter is empowered to bring their authentic self to their work. we are all responsible for stewarding a high\\-performance culture in which all datavanters belong and thrive. we are proud to be an equal employment opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status.\n  \n  \n\nour compensation philosophy is to be externally competitive, internally fair, and not win or lose on compensation. salary ranges for this position are developed with the support of benchmarks (competitive san francisco rates for us\\-based roles) and industry best practices. we're building a high\\-growth, high\\-autonomy culture. we rely less on job titles and more on cultivating an environment where anyone can contribute, the best ideas win, and personal growth is driven by expanding impact. this means we default to simple job titles (e.g., software engineer) rather than complex ones (e.g., senior software engineer). the range posted is for a given job title, which can include multiple levels. individual rates for the same job title may differ based on level, responsibilities, skills, and experience for a specific job.\n  \n  \n\nto ensure the safety of patients and staff, many of our clients require post\\-offer health screenings and proof and/or completion of various vaccinations such as the flu shot, tdap, covid\\-19, etc. any requests to be exempted from these requirements will be reviewed by datavant human resources and determined on a case\\-by\\-case basis. depending on the state in which you will be working, exemptions may be available on the basis of disability, medical contraindications to the vaccine or any of its components, pregnancy or pregnancy\\-related medical conditions, and/or religion.\n  \n  \n\nthis job is not eligible for employment sponsorship.\n  \n  \n\ndatavant is committed to a work environment free from job discrimination. we are proud to be an equal employment opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status. to learn more about our commitment, please review our eeo commitment statement here. know your rights, explore the resources available through the eeoc for more information regarding your legal rights and protections. in addition, datavant does not and will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay.\n  \n  \n\nat the end of this application, you will find a set of voluntary demographic questions. if you choose to respond, your answers will be anonymous and will help us identify areas for improvement in our recruitment process. (we can only see aggregate responses, not individual ones. in fact, we aren't even able to see whether you've responded.) responding is entirely optional and will not affect your application or hiring process in any way.\n  \n  \n\ndatavant is committed to working with and providing reasonable accommodations to individuals with physical and mental disabilities. if you need an accommodation while seeking employment, please request it here, by selecting the 'interview accommodation request' category. you will need your requisition id when submitting your request, you can find instructions for locating it here. requests for reasonable accommodations will be reviewed on a case\\-by\\-case basis.\n  \n  \n\nfor more information about how we collect and use your data, please review our privacy policy.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist/AI Trainer",
        "company": "Five9",
        "location": "Remote, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=0d85ec88c21eba2c",
        "description": "join us in bringing joy to customer experience. five9 is a leading provider of cloud contact center software, bringing the power of cloud innovation to customers worldwide.\n\n\nliving our values everyday results in our team\\-first culture and enables us to innovate, grow, and thrive while enjoying the journey together. we celebrate diversity and foster an inclusive environment, empowering our employees to be their authentic selves.\n\n\nwe are seeking a customer\\-facing data scientist/ai trainer to join our professional services team, focusing on advanced conversational ai implementations. the ideal candidate will possess a strong background in natural language processing, python development, and a deep understanding of modern ai technologies. in this role, you will be responsible for designing, building, and optimizing five9's ai\\-based solutions. you will work closely with developers and conversation designers, leveraging your expertise in speech\\-to\\-text (stt), text\\-to\\-speech (tts), and large language model (llm) prompting and evaluations to ensure the highest levels of accuracy and performance in our intelligent virtual agents and agent\\-guided solutions.\n\n\n**responsibilities:**\n\n\n* develop and maintain python code for data analysis, model evaluation, and the automation of ai workflows.\n* design, implement, and evaluate sophisticated prompts for large language models (llms) to optimize performance across various conversational tasks.\n* integrate and fine\\-tune speech\\-to\\-text (stt) and text\\-to\\-speech (tts) models to ensure high\\-quality, natural\\-sounding voice interactions.\n* implement and improve robust methods to quantitatively and qualitatively compare and evaluate llm outputs.\n* collaborate with data scientists and engineers to identify technical requirements, prioritize tasks, and architect scalable ai solutions.\n* create and curate multilingual speech and text datasets to train and benchmark models.\n* evaluate, recommend, and implement machine learning models for tasks such as topic modeling, summarization, and intent classification.\n* develop data visualization tools and dashboards to monitor model performance and provide insights to stakeholders.\n\n\n**minimum qualifications:**\n\n\n* master's degree in human\\-computer interaction, computer science, cognitive science, linguistics, or a similar technical field.\n* proven experience in python programming for data science, machine learning, or software development.\n* solid understanding of conversational ai concepts and the underlying technologies.\n* strong attention to detail and a commitment to maintaining high\\-quality standards in code and model performance.\n* demonstrated ability to work independently and as part of a collaborative, customer\\-facing team.\n* excellent technical communication skills.\n* fluent in english, both written and spoken.\n\n\n**preferred qualifications**\n\n\n* graduate degree or higher in computer science, computational linguistics, or a related field.\n* fluency in spanish and/or portuguese.\n* deep hands\\-on experience with llms, including prompt engineering, fine\\-tuning, and evaluation techniques.\n* expertise in speech\\-to\\-text (stt) and text\\-to\\-speech (tts) technologies and their practical application in conversational ai systems.\n* proficiency with ml libraries and platforms such as scikit\\-learn, tensorflow, pytorch, r, jupyter, and sql.\n* experience in a customer\\-facing role, including technical requirement gathering and presentations.\n* experience with bot frameworks such as ibm watson, amazon lex, rasa, or google dialogflow.\n\n  \n\nfive9 embraces diversity and is committed to building a team that represents a variety of backgrounds, perspectives, and skills. the more inclusive we are, the better we are. five9 is an equal opportunity employer.\n\n\n\nview our privacy policy, including our privacy notice to california residents here: https://www.five9\\.com/pt\\-pt/legal.  \n\n  \n\n\n\nnote: five9 will never request that an applicant send money as a prerequisite for commencing employment with five9\\.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior QA Engineer",
        "company": "Care.com",
        "location": "Salt Lake City, UT, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=22b669fc82caea43",
        "description": "**about care.com**\n------------------\n\n\n\ncare.com is a consumer tech company with heart. we're on a mission to solve a human challenge we all face: finding great care for the ones we love. we're moms and dads and pet parents. we have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. our culture and our products reflect that.\n\n\n\nhere, entrepreneurs, self\\-starters, team players, and big thinkers unite behind a common cause. here, we're applying data analytics, ai, and the latest technologies to solve universal problems and connect people in new ways. if you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, care.com is the place for you.\n\n\n**locations** \u2013 salt lake city, utah \\| austin \\& dallas, texas \\| new york, ny  \n\n**environment** \u2013 hybrid: in office monday, wednesday \\& thursday\n\n\n**overview:**\n-------------\n\n\n\nwe are seeking a highly skilled and motivated **senior qa engineer \u2013 automation (selenium)** to join our technology team. in this role, you will be responsible for designing, developing, and executing automated test strategies for our web applications and services. you will play a key role in ensuring our products meet high\\-quality standards while working within an agile development environment.\n\n\n\nthe ideal candidate has a strong background in web automation, hands\\-on experience with selenium\\-based frameworks, and a passion for building scalable, maintainable automation solutions that enable fast, reliable releases.\n\n\n**specific duties/responsibilities:**\n-------------------------------------\n\n\n* develop, implement, and maintain automated test strategies for web applications\n* design and execute automated end\\-to\\-end, functional, integration, and regression tests to ensure high\\-quality user experiences\n* build and enhance selenium\\-based automation frameworks that are scalable, reusable, and maintainable\n* collaborate closely with product management, development, and qa teams to understand business requirements and technical specifications\n* integrate automated test suites into ci/cd pipelines to support continuous testing and rapid deployment cycles\n* perform api testing and validate backend services to ensure data integrity and system reliability\n* conduct cross\\-browser and cross\\-platform testing to ensure consistent performance across environments\n* identify, diagnose, and report defects proactively, ensuring efficient resolution and root cause analysis\n* contribute to test planning, risk assessments, and quality metrics tracking\n* stay up to date with the latest automation tools, best practices, and emerging technologies in web testing\n\n**skills/experience requirements:**\n-----------------------------------\n\n\n* 5 \\-7 \\+ years of experience in qa, with a strong focus on web application testing\n* 3\\+ years of hands\\-on experience building and maintaining automation frameworks using selenium webdriver\n* strong programming experience in java, python, or javascript for automation development\n* experience designing automation frameworks using page object model (pom) or similar design patterns\n* experience integrating automated tests into ci/cd tools such as jenkins, github actions, or similar platforms\n* solid understanding of restful apis and experience validating api responses using automation tools (e.g., postman, restassured)\n* experience with test management tools and defect tracking systems (e.g., jira, testrail)\n* familiarity with performance testing concepts and tools (e.g., jmeter or similar)\n* experience with version control systems such as git\n* strong analytical, problem\\-solving, and debugging skills\n* excellent written and verbal communication skills, with the ability to clearly document test plans, test cases, and defects\n\n**company overview:**\n---------------------\n\n\n\navailable in more than 20 countries, care.com is the world's leading platform for finding and managing high\\-quality family care. care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families to innovating new ways for caregivers to be paid and obtain professional benefits. since 2007, families have relied on care.com's industry\\-leading products\u2014from child and elder care to pet care and home care. care.com is an iac company (nasdaq: iac).\n\n\n**salary range:** $140,000 to $155,000\n\n\n\nthe base salary range above represents the anticipated low and high end of the national salary range for this position. actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. the range listed is just one component of care.com's total compensation package for employees. other rewards may include annual bonuses and short\\- and long\\-term incentives. in addition, care.com provides a variety of benefits to employees, including health insurance coverage, life and disability insurance, a generous 401k employer matching program, paid holidays, and paid time off (pto).\n\n\n\n\\#li\\-hybrid",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior QA Engineer",
        "company": "Care.com",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4acc2278c1957139",
        "description": "**about care.com**\n------------------\n\n\n\ncare.com is a consumer tech company with heart. we're on a mission to solve a human challenge we all face: finding great care for the ones we love. we're moms and dads and pet parents. we have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. our culture and our products reflect that.\n\n\n\nhere, entrepreneurs, self\\-starters, team players, and big thinkers unite behind a common cause. here, we're applying data analytics, ai, and the latest technologies to solve universal problems and connect people in new ways. if you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, care.com is the place for you.\n\n\n**locations** \u2013 salt lake city, utah \\| austin \\& dallas, texas \\| new york, ny  \n\n**environment** \u2013 hybrid: in office monday, wednesday \\& thursday\n\n\n**overview:**\n-------------\n\n\n\nwe are seeking a highly skilled and motivated **senior qa engineer \u2013 automation (selenium)** to join our technology team. in this role, you will be responsible for designing, developing, and executing automated test strategies for our web applications and services. you will play a key role in ensuring our products meet high\\-quality standards while working within an agile development environment.\n\n\n\nthe ideal candidate has a strong background in web automation, hands\\-on experience with selenium\\-based frameworks, and a passion for building scalable, maintainable automation solutions that enable fast, reliable releases.\n\n\n**specific duties/responsibilities:**\n-------------------------------------\n\n\n* develop, implement, and maintain automated test strategies for web applications\n* design and execute automated end\\-to\\-end, functional, integration, and regression tests to ensure high\\-quality user experiences\n* build and enhance selenium\\-based automation frameworks that are scalable, reusable, and maintainable\n* collaborate closely with product management, development, and qa teams to understand business requirements and technical specifications\n* integrate automated test suites into ci/cd pipelines to support continuous testing and rapid deployment cycles\n* perform api testing and validate backend services to ensure data integrity and system reliability\n* conduct cross\\-browser and cross\\-platform testing to ensure consistent performance across environments\n* identify, diagnose, and report defects proactively, ensuring efficient resolution and root cause analysis\n* contribute to test planning, risk assessments, and quality metrics tracking\n* stay up to date with the latest automation tools, best practices, and emerging technologies in web testing\n\n**skills/experience requirements:**\n-----------------------------------\n\n\n* 5 \\-7 \\+ years of experience in qa, with a strong focus on web application testing\n* 3\\+ years of hands\\-on experience building and maintaining automation frameworks using selenium webdriver\n* strong programming experience in java, python, or javascript for automation development\n* experience designing automation frameworks using page object model (pom) or similar design patterns\n* experience integrating automated tests into ci/cd tools such as jenkins, github actions, or similar platforms\n* solid understanding of restful apis and experience validating api responses using automation tools (e.g., postman, restassured)\n* experience with test management tools and defect tracking systems (e.g., jira, testrail)\n* familiarity with performance testing concepts and tools (e.g., jmeter or similar)\n* experience with version control systems such as git\n* strong analytical, problem\\-solving, and debugging skills\n* excellent written and verbal communication skills, with the ability to clearly document test plans, test cases, and defects\n\n**company overview:**\n---------------------\n\n\n\navailable in more than 20 countries, care.com is the world's leading platform for finding and managing high\\-quality family care. care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families to innovating new ways for caregivers to be paid and obtain professional benefits. since 2007, families have relied on care.com's industry\\-leading products\u2014from child and elder care to pet care and home care. care.com is an iac company (nasdaq: iac).\n\n\n**salary range:** $140,000 to $155,000\n\n\n\nthe base salary range above represents the anticipated low and high end of the national salary range for this position. actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. the range listed is just one component of care.com's total compensation package for employees. other rewards may include annual bonuses and short\\- and long\\-term incentives. in addition, care.com provides a variety of benefits to employees, including health insurance coverage, life and disability insurance, a generous 401k employer matching program, paid holidays, and paid time off (pto).\n\n\n\n\\#li\\-hybrid",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior QA Engineer",
        "company": "Care.com",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=9870ae11a2356e86",
        "description": "**about care.com**\n------------------\n\n\n\ncare.com is a consumer tech company with heart. we're on a mission to solve a human challenge we all face: finding great care for the ones we love. we're moms and dads and pet parents. we have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. our culture and our products reflect that.\n\n\n\nhere, entrepreneurs, self\\-starters, team players, and big thinkers unite behind a common cause. here, we're applying data analytics, ai, and the latest technologies to solve universal problems and connect people in new ways. if you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, care.com is the place for you.\n\n\n**locations** \u2013 salt lake city, utah \\| austin \\& dallas, texas \\| new york, ny  \n\n**environment** \u2013 hybrid: in office monday, wednesday \\& thursday\n\n\n**overview:**\n-------------\n\n\n\nwe are seeking a highly skilled and motivated **senior qa engineer \u2013 automation (selenium)** to join our technology team. in this role, you will be responsible for designing, developing, and executing automated test strategies for our web applications and services. you will play a key role in ensuring our products meet high\\-quality standards while working within an agile development environment.\n\n\n\nthe ideal candidate has a strong background in web automation, hands\\-on experience with selenium\\-based frameworks, and a passion for building scalable, maintainable automation solutions that enable fast, reliable releases.\n\n\n**specific duties/responsibilities:**\n-------------------------------------\n\n\n* develop, implement, and maintain automated test strategies for web applications\n* design and execute automated end\\-to\\-end, functional, integration, and regression tests to ensure high\\-quality user experiences\n* build and enhance selenium\\-based automation frameworks that are scalable, reusable, and maintainable\n* collaborate closely with product management, development, and qa teams to understand business requirements and technical specifications\n* integrate automated test suites into ci/cd pipelines to support continuous testing and rapid deployment cycles\n* perform api testing and validate backend services to ensure data integrity and system reliability\n* conduct cross\\-browser and cross\\-platform testing to ensure consistent performance across environments\n* identify, diagnose, and report defects proactively, ensuring efficient resolution and root cause analysis\n* contribute to test planning, risk assessments, and quality metrics tracking\n* stay up to date with the latest automation tools, best practices, and emerging technologies in web testing\n\n**skills/experience requirements:**\n-----------------------------------\n\n\n* 5 \\-7 \\+ years of experience in qa, with a strong focus on web application testing\n* 3\\+ years of hands\\-on experience building and maintaining automation frameworks using selenium webdriver\n* strong programming experience in java, python, or javascript for automation development\n* experience designing automation frameworks using page object model (pom) or similar design patterns\n* experience integrating automated tests into ci/cd tools such as jenkins, github actions, or similar platforms\n* solid understanding of restful apis and experience validating api responses using automation tools (e.g., postman, restassured)\n* experience with test management tools and defect tracking systems (e.g., jira, testrail)\n* familiarity with performance testing concepts and tools (e.g., jmeter or similar)\n* experience with version control systems such as git\n* strong analytical, problem\\-solving, and debugging skills\n* excellent written and verbal communication skills, with the ability to clearly document test plans, test cases, and defects\n\n**company overview:**\n---------------------\n\n\n\navailable in more than 20 countries, care.com is the world's leading platform for finding and managing high\\-quality family care. care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families to innovating new ways for caregivers to be paid and obtain professional benefits. since 2007, families have relied on care.com's industry\\-leading products\u2014from child and elder care to pet care and home care. care.com is an iac company (nasdaq: iac).\n\n\n**salary range:** $140,000 to $155,000\n\n\n\nthe base salary range above represents the anticipated low and high end of the national salary range for this position. actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. the range listed is just one component of care.com's total compensation package for employees. other rewards may include annual bonuses and short\\- and long\\-term incentives. in addition, care.com provides a variety of benefits to employees, including health insurance coverage, life and disability insurance, a generous 401k employer matching program, paid holidays, and paid time off (pto).\n\n\n\n\\#li\\-hybrid",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Architect, Service & Operational Data",
        "company": "Thomson Reuters",
        "location": "Eagan, MN, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Kinesis",
            "CI/CD",
            "Git",
            "Kafka",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c82df1ac5ad3ee0a",
        "description": "we're hiring an architect, service \\& operational data to design and lead the data architecture that powers service reliability, operational visibility, and informed decision\\-making across technology operations. you'll unify data from monitoring/observability, itsm, incident/problem/change workflows, asset/cmdb, cloud platforms, and application telemetry\u2014turning fragmented signals into trusted, governable, analytics\\-ready data products.\n\n\nthis is a hands\\-on architecture role with strong cross\\-functional leadership: you'll partner with sre, it ops, security, engineering, and analytics to define the operational data strategy and deliver measurable improvements in availability, customer experience, mttr, change risk, and operational efficiency.\n\n\nkey responsibilities\n\n\n**about the role**\n\nas an **architect, service \\& operational data** , you will:\n\n\n* define the operational data strategy and target architecture (current\\-to\\-future state), including domains such as incidents, changes, problems, requests, alerts/events, assets/cmdb, slos/slis, capacity, and cost.\n* design data models and canonical schemas for service and operational datasets (e.g., service topology, event/alert normalization, incident timelines, change\\-to\\-incident linkage).\n* build and govern reliable pipelines that ingest and transform data from tools like itsm, observability platforms, cloud logs/metrics/traces, ci/cd, and infrastructure systems.\n* establish data quality and trust: data contracts, lineage, validation rules, slas/slos for datasets, and operational monitoring for pipelines.\n* create \"operational data products\" (curated datasets, semantic layers, metrics definitions) that enable dashboards, analytics, and automation (aiops, correlation, anomaly detection).\n* partner with stakeholders (sre/it ops/service owners) to identify high\\-value use cases: incident reduction, alert noise reduction, change failure rate, capacity planning, and reliability insights.\n* implement governance and security controls for sensitive operational data (rbac/abac, retention, encryption, auditability, and policy\\-based access).\n* set standards and reference implementations for tooling, integration patterns, and best practices across teams.\n* mentor engineers/analysts and influence roadmap decisions through architecture reviews and technical leadership.\n\n**about you**\n\nyou are a potential fit for the role **architect, service \\& operational data** , if your background includes:\n\n\nrequired qualifications\n\n\n* 8\\+ years in data architecture, platform/analytics engineering, or operational analytics (with 3\\+ years in a lead/architect role).\n* proven experience designing enterprise data architectures (batch \\+ streaming) and delivering scalable data platforms.\n* strong understanding of service operations and reliability concepts: itil/itsm, incident/change/problem workflows, slos/slis, event management, and operational kpis (e.g., mttr, cfr).\n* hands\\-on expertise with modern data stack components: data modeling (dimensional, data vault, domain/event\\-driven patterns), etl/elt and orchestration, data quality/testing frameworks, metadata, lineage, and cataloging, strong sql and at least one programming language (python strongly preferred), experience with cloud data ecosystems (aws/azure/gcp) and security fundamentals, and excellent communication skills and ability to align diverse teams around standards and measurable outcomes.\n\npreferred / nice\\-to\\-have\n\n\n* experience integrating operational tooling such as servicenow, jira service management, pagerduty/opsgenie, dynatrace/new relic/datadog, splunk/elastic, prometheus/grafana, opentelemetry.\n* streaming/event technologies (e.g., kafka/kinesis/pubsub) and real\\-time analytics patterns.\n* knowledge graph / topology modeling for service dependency mapping.\n* experience enabling aiops use cases: correlation, deduplication, anomaly detection, incident prediction.\n* familiarity with finops or capacity/cost analytics tied to service health.\n* certifications (cloud, data, itil) are a plus.\n* what success looks like (first 6\u201312 months)\n* a documented target architecture and prioritized roadmap for service \\& operational data.\n* a canonical operational data model adopted across key domains (incidents, changes, alerts, services/topology).\n* measurable improvements in data reliability (freshness, completeness, accuracy) and stakeholder trust.\n* delivered high\\-impact use cases (e.g., reduced alert noise, improved mttr reporting accuracy, change risk insights).\n* why this role matters\n* operational data is often the most valuable\u2014and most chaotic\u2014data in an organization. this role turns operational signals into a cohesive data foundation that improves reliability, reduces toil, and enables automation at scale.\n\n\\#li\\-mw1\n\n\n**what\u2019s in it for you?**\n\n* **hybrid work model:** we\u2019ve adopted a flexible hybrid working environment (2\\-3 days a week in the office depending on the role) for our office\\-based roles while delivering a seamless experience that is digitally and physically connected.\n* **flexibility \\& work\\-life balance:** flex my way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. this builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work\\-life balance.\n* **career development and growth:** by fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow\u2019s challenges and deliver real\\-world solutions. our grow my way programming and skills\\-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an ai\\-enabled future.\n* **industry competitive benefits:** we offer comprehensive benefit plans to include flexible vacation, two company\\-wide mental health days off, access to the headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\n* **culture:** globally recognized, award\\-winning reputation for inclusion and belonging, flexibility, work\\-life balance, and more. we live by our values: obsess over our customers, compete to win, challenge (y)our thinking, act fast / learn fast, and stronger together.\n* **social impact:** make an impact in your community with our social impact institute. we offer employees two paid volunteer days off annually and opportunities to get involved with pro\\-bono consulting projects and environmental, social, and governance (esg) initiatives.\n* **making a real\\-world impact:** we are one of the few companies globally that helps its customers pursue justice, truth, and transparency. together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\n\nin the united states, thomson reuters offers a comprehensive benefits package to our employees. our benefit package includes market competitive health, dental, vision, disability, and life insurance programs, as well as a competitive 401k plan with company match. in addition, thomson reuters offers market leading work life benefits with competitive vacation, sick and safe paid time off, paid holidays (including two company mental health days off), parental leave, sabbatical leave. these benefits meet or exceeds the requirements of paid time off in accordance with any applicable state or municipal laws. finally, thomson reuters offers the following additional benefits: optional hospital, accident and sickness insurance paid 100% by the employee; optional life and ad\\&d insurance paid 100% by the employee; flexible spending and health savings accounts; fitness reimbursement; access to employee assistance program; group legal identity theft protection benefit paid 100% by employee; access to 529 plan; commuter benefits; adoption \\& surrogacy assistance; tuition reimbursement; and access to employee stock purchase plan.\nthomson reuters complies with local laws that require upfront disclosure of the expected pay range for a position. the base compensation range varies across locations.\\&\\#xa;\\&\\#xa;for any eligible us locations, unless otherwise noted, the base compensation range for this role is $137,100 usd \\- $254,700 usd.\\&\\#xa;\\&\\#xa;base pay is positioned within the range based on several factors including an individual\u2019s knowledge, skills and experience with consideration given to internal equity. base pay is one part of a comprehensive total reward program which also includes flexible and supportive benefits and other wellbeing programs.\\&\\#xa;this role may also be eligible for an annual bonus based on a combination of enterprise and individual performance.\\&\\#xa;\n**about us**\n\nthomson reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. we serve professionals across legal, tax, accounting, compliance, government, and media. our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. reuters, part of thomson reuters, is a world leading provider of trusted journalism and news.\n\n\nwe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. at a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. sound exciting? join us and help shape the industries that move society forward.\n\n\nas a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. to ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. thomson reuters is proud to be an equal employment opportunity employer providing a drug\\-free workplace.\n\n\nthomson reuters makes reasonable accommodations for applicants with disabilities, including veterans with disabilities, and for sincerely held religious beliefs in accordance with applicable law. if you reside in the united states and require an accommodation in the recruiting process, you may contact our human resources department at hr.leave\\-expert@thomsonreuters.com . disability accommodations in the recruiting process may include things like a sign language interpreter, making interview rooms accessible, providing assistive technology, or other relevant accommodations. please note this email is not intended for general recruitment questions and we will promptly respond to inquiries regarding accommodations. more information on requesting an accommodation here.\n\n\nlearn more on how to protect yourself from fraudulent job postings here.\n\n\nmore information about thomson reuters can be found on thomsonreuters.com",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Data Scientist",
        "company": "Ipsos",
        "location": "Culver City, CA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7b2acc4634dfe573",
        "description": "we\u2019re looking for someone with a deep interest in data and analytics and the dedication to apply that passion. we need someone that possesses the technical ability to write production\\-level code and implement data science methods for a diverse set of applications, as well as uncover insights from the models and communicate them concisely and clearly to stakeholders at varying levels of technical sophistication. the role also requires an observant attention to detail, the ability to work well on a small team, and a self\\-starter approach to problem\\-solving and debugging.\n\n\n**as a data scientist, you will:**\n\n* work closely with team members on design and execution of large\\-scale modeling efforts, contributing to analytics libraries and integrating statistical theory\n* collaborate on engineering new data science products by translating needs identified with stakeholders into analytic frameworks that can be built into polished user\\-facing tools\n* build, maintain, and enhance existing codebases\n* synthesize academic research, tech innovations, and cloud scalability, to elevate business value for clients, both current and prospective\n* validate analytics innovations from marketing science and data science teams globally\n* provide consulting for internal teams and clients on data engineering and hygiene, areas for improving process efficiency, and data science algorithms and results\n\n**requirements:**\n\n* high proficiency writing r and/or python, including the ability to produce and maintain reusable and modular codebases\n* bachelor\u2019s degree\n* desire to work in a highly collaborative, fun, consensus\\-oriented environment.\n* experience in analytics, extracting and surfacing value from quantitative data\n* attentive learner with excellent time\\-management skills\n* ability to lucidly communicate data science concepts verbally and in written form\n\n**pluses:**\n\n* professional or academic experience with modern techniques and algorithms in machine learning and statistical computing such as neural networks, genetic algorithms, bayesian modeling, data fusion, drivers analysis, clustering, predictive analytics, network analysis, monte carlo, and agent\\-based models.\n* experience developing data science models in cloud platforms (e.g. gcp, aws)\n* working knowledge of sql and experience with database design and administration.\n* experience with linux server and system administration.\n* experience with collaboration tools (e.g. atlassian suite) and version control systems (e.g. git).\n* experience integrating r and/or python with each other and c\\\\c\\&\\#43;\\&\\#43;.\n* large dataset manipulation. experience in distributed storage and computing.\n* experience with reactjs, typescript/javascript, visualization libraries (e.g. d3\\.js).\n* experience creating and deploying web apps (e.g. electron, web2py).\n* advanced degree (m.s., ph.d.), but not required.\n* experience in the field of market research.\n\n*if you don\u2019t meet 100% of the requirements, we encourage all who feel they might be a fit for the opportunity to apply. we may consider a variety of backgrounds for a particular role and are also committed to considering candidates for available positions throughout our organization, not just the one you\u2019re applying to!*\n\n*in accordance with ny/co/ca/wa law, the estimated base salary range for this role is $80,000 to $85,000\\. your final base salary will be determined based on several non\\-discriminatory factors which may include but are not limited to location, work experience, skills, knowledge, education and/or certifications.*\n\n**what\u2019s in it for you:**\n\n*at ipsos you\u2019ll experience opportunities for career development, an exceptional benefits package (including generous pto, healthcare plans, wellness benefits), a flexible workplace policy, and a strong collaborative culture.*\n\n*to find out more about all the great reasons to work at ipsos, how we\u2019re making an impact around the world, and more about our benefits and employee programs, please visit:*  \n\nwhy work at ipsos \\| us  \n\n\n\n**commitment to diversity**\n\nipsos recognizes the necessity of building an inclusive culture that values each employee\u2019s individuality and diverse perspectives. for more than 40 years, our mission has been to generate and analyze data about society, markets, brands, and behaviors to provide our clients with the insights that elevate their understanding of the world. this could not be fulfilled without ipsos\u2019 diverse employees who compile and analyze this data\u2014they are the essence of who we are and what we do.  \n\n\n\nwe are committed to providing equal opportunity to all employees, creating an environment that promotes inclusion, and enabling employees from all walks of life to flourish. ipsos encourages our employees to act in a respectful and responsible manner, in line with code of best practices concerning diversity and inclusion, human rights, equality, and civility for every individual.  \n\n\n\nipsos is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or any other protected class and will not be discriminated against on the basis of disability.  \n\n\n\n\\#li\\-fm \\#li hybrid onsite",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Big Data Engineer",
        "company": "Highmark Health",
        "location": "PA, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "Git",
            "Kafka",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e566377201b9121f",
        "description": "**company :**\n-------------\n\nhighmark health\n**job description :**\n---------------------\n\n**job summary**\n\n**\\*\\*\\*candidate must be us citizen (due to contractual/access requirements)\\*\\*\\***\n\nthis job is responsible for designing and engineering data solutions for the enterprise and, working closely with business, analytic and it stakeholders, assists with the development and maintenance of these solutions. this includes coding data ingestion pipelines, transformations and delivery programs/logic for people and systems to access data for operational and/or analytic needs. duties include but are not limited to the coding, testing, and implementation of ingestion and extraction pipelines, transformation and cleansing processes, and processes that load and curate data in conformed, fit\\-for\\-purpose data structures. the incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non\\-technical audiences) in their daily work. the incumbent will work with cross\\-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels. the incumbent is expected to utilize technologies such as, but not limited to: **google cloud platform, sql, kafka, python, linux shell scripting, teradata, oracle, and git** .\n\n\n**essential responsibilities**\n\n* in partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well\\-engineered data solutions in a variety of environments, including traditional data warehouses, big data solutions, and cloud\\-oriented platforms. create high performance cloud and big data systems to be used with operational and analytic applications.\n* work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. work with business stakeholders and strategic partners to implement and support operational and analytic platforms. this may include products purchased by the organization that must be ingested or modeled/derived data maintained by enterprise platforms and data consumers.\n* working across multiple, disparate systems and platforms, design, code, test, implement, and maintain scalable and extensible frameworks that support data engineering services.\n* align with security, data governance and data quality programs by driving assigned components of metadata management, data quality management, and the application of business rules. develop and maintain associated data engineering processes and participate in required operating procedures as part of the enterprise\u2019s overall information management activities. includes data cleansing, standardization, technical metadata documentation, and the de\\-identification and/or tokenization of data.\n* develop, optimize and/or maintain machine learning and ai engineering processes (mlops) that are deployed to cloud or big data environments. these may be based on prototypes built by data scientists or capability frameworks implemented to allow data scientists to build efficiently in production environments.\n* develop tasks across multiple projects with limited need for guidance. this includes providing guidance and education to intermediate and junior contributors within team. manage relationships with customers of the function. attend meetings with customers on a stand\\-alone basis or with team as needed.\n* establish standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. keep current with big data technologies in order to recommend best tools in order to perform current and future work\n* other duties as assigned or requested.\n\n**education**\n\n**required**\n\n* bachelor's degree in software engineering, information systems, computer science, data science or related field\n\n**substitutions**\n\n* none\n\n**preferred**\n\n* master's degree in software engineering, information systems, computer science, data science or related field\n\n**experience**\n\n**required**\n\n* 5 years in data platform development, data engineering, software development, or data science\n* 3 years in big data or cloud data platform\n\n**preferred**\n\n* 3 years in healthcare industry\n* 3 years of data warehousing\n* 3 years of database administration\n* dashboarding (rshiny, tableau, powerbi)\n\n**licenses and certifications**\n\n**required**\n\n* none\n\n**preferred**\n\n* cloud certification (gcp, azure, aws)\n\n**skills**\n\n* sql\n* data warehousing\n* problem\\-solving\n* communication skills\n* analytical skills\n* spark or python or related tool\ncloud technologies  \n* \n\n**language (other than english):**\n\nnone\n\n\n**travel requirement:**\n\n0% \\- 25%\n\n\n**physical, mental demands and working conditions**\n\n**position type**\n\noffice\\-based\n\n\nteaches / trains others regularly\n\n\nrarely\n\n\ntravel regularly from the office to various work sites or from site\\-to\\-site\n\n\ndoes not apply\n\n\nworks primarily out\\-of\\-the office selling products/services (sales employees)\n\n\nnever\n\n\nphysical work site required\n\n\nno\n\n\nlifting: up to 10 pounds\n\n\nconstantly\n\n\nlifting: 10 to 25 pounds\n\n\noccasionally\n\n\nlifting: 25 to 50 pounds\n\n\nrarely\n\n\n***disclaimer:*** *the job description has been designed to indicate the general nature and essential duties and responsibilities of work performed by employees within this job title. it may not contain a comprehensive inventory of all duties, responsibilities, and qualifications required of employees to do this job.*  \n\n  \n\n***compliance requirement*** *: this job adheres to the ethical and legal standards and behavioral expectations as set forth in the code of business conduct and company policies.*  \n\n  \n\n*as a component of job responsibilities, employees may have access to covered information, cardholder data, or other confidential customer information that must be protected at all times. in connection with this, all employees must comply with both the health insurance portability accountability act of 1996 (hipaa) as described in the notice of privacy practices and privacy policies and procedures as well as all data security guidelines established within the company\u2019s handbook of privacy policies and practices and information security policy.*  \n\n  \n\n*furthermore, it is every employee\u2019s responsibility to comply with the company\u2019s code of business conduct. this includes but is not limited to adherence to applicable federal and state laws, rules, and regulations as well as company policies and training requirements.*\n\n**pay range minimum:**\n\n$78,900\\.00\n**pay range maximum:**\n\n$147,500\\.00\n*base pay is determined by a variety of factors including a candidate\u2019s qualifications, experience, and expected contributions, as well as internal peer equity, market, and business considerations. the displayed salary range does not reflect any geographic differential highmark may apply for certain locations based upon comparative markets.*\n\nhighmark health and its affiliates prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibit discrimination against all individuals based on any category protected by applicable federal, state, or local law.\n\n\nwe endeavor to make this site accessible to any and all users. if you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact the email below.\n\n\nfor accommodation requests, please contact hr services online at hrservices@highmarkhealth.org\n\n\ncalifornia consumer privacy act employees, contractors, and applicants notice",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Security Software Developer",
        "company": "Vidoori",
        "location": "Hyattsville, MD, US USA",
        "posted_at": "2026-02-23",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=073709b581c90d36",
        "description": "vidoori is a leading provider of digital transformation and technology solutions, empowering organizations across multiple industries to accelerate business value and innovation. as part of our continued growth, we are seeking an experienced, security\\-focused lead security software developer who is analytical, collaborative, and passionate about building secure, resilient software at scale. if you enjoy leading technical delivery, mentoring engineers, and embedding security practices across the development lifecycle, we invite you to join our inclusive and progressive team.\n\n**security engineering and technical leadership \u2013 role overview**\n-----------------------------------------------------------------\n\n\nas the security software developer at vidoori, you will own the design, development, and operational security of critical applications and services. you will lead a small team of developers and security engineers, partner with product and infrastructure teams, and ensure secure architecture and coding practices are embedded in our delivery lifecycle. this role is ideal for a seasoned developer with deep secure\\-coding expertise, practical application security experience, and a strong commitment to mentoring and continuous improvement.\n\n### **key responsibilities \u2013 secure development, architecture, and team leadership**\n\n* lead the design and implementation of secure, scalable software systems across web, api, and cloud\\-native environments.\n* develop and enforce secure coding standards, guidelines, and best practices; perform code reviews focused on security and maintainability.\n* architect and deliver threat modelling, secure design reviews, and security requirements for new features and integrations.\n* integrate application security tools into ci/cd pipelines (sast, dast, sca) and drive automation to reduce risk and developer friction.\n* lead incident response for application\\-level security issues, conduct post\\-incident reviews, and implement corrective measures.\n* mentor and coach developers on secure development practices, vulnerability remediation, and security\\-aware engineering culture.\n* collaborate with product owners, devops, qa, and compliance teams to ensure traceability of security requirements and alignment with regulatory or client obligations.\n* design and deliver security training, run workshops, and contribute to continuous improvement of the security engineering function.\n\n#### **essential skills and experience**\n\n* bachelor\u2019s degree in computer science, software engineering, or a related discipline, or equivalent practical experience.\n* 12\\+ years of professional software development experience with at least 3 years leading or mentoring engineering teams in production environments.\n* strong software development background in one or more modern languages (e.g., java, c\\#, python, go, or node.js) and familiarity with cloud platforms (aws, azure, or gcp).\n* proven application security experience, including secure coding practices, secure design, vulnerability remediation, and threat modelling.\n* hands\\-on experience with security tooling and automation: sast, dast, sca, interactive scanning, and secrets detection integrated into ci/cd.\n* knowledge of authentication and authorization technologies (oauth2, openid connect, jwt, iam), encryption, key management, and secure deployment patterns.\n* experience with containerisation and orchestration security (docker, kubernetes) and cloud\\-native security controls.\n* excellent communication and stakeholder management skills with the ability to translate security risk into business terms.\n* relevant certifications (e.g., cissp, csslp, oscp, ceh, or cloud security certs) are desirable but not mandatory.\n\n##### **benefits and professional growth \u2013 security careers at vidoori**\n\n* competitive salary and comprehensive benefits to support personal and career development.\n* flexible, hybrid and remote working options to support work\u2013life balance and wellbeing.\n* opportunities for technical leadership, mentoring, and access to security training and certification support.\n* an inclusive, innovative company culture that values diversity, collaboration, and continuous learning.\n* clear career pathways into senior security engineering, architecture, or leadership roles as your skills and interests evolve.\n\n###### **application details \u2013 security software developer opportunities at vidoori**\n\n* **location:** hybrid \u2013 maryland/washington dc area\n* **residency:** must be a us citizen\n* **employment type:** full\\-time\n\n\nhelp shape secure digital transformation at vidoori\u2014where your technical leadership, pragmatic security approach, and collaborative mindset will protect our clients and accelerate innovation. apply today to become a key contributor to our security engineering practice.",
        "scrapped_date": "2026-02-23"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b6880377961dd34c",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3f8d81da2675f09a",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Jericho, NY, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=80ecea7854c84d2f",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Salt Lake City, UT, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=eadc8b3152de6edf",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=09f0cc3c3e322d55",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Portland, OR, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c6563c9072cece55",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Nashville, TN, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d7a5eb12bfa24145",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Dayton, OH, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cf0d2d6a3f455ebf",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Houston, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e4d966b7a2dd95df",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Morristown, NJ, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4d11ebfcd28367ee",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Richmond, VA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8bbb1511f618cabf",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2db0377bd2033218",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Cleveland, OH, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=feb233ed89e926ae",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Cincinnati, OH, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5732e090675323b9",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Princeton, NJ, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6d5554c8b262ebca",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Philadelphia, PA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=321d41ed26b61c51",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fc8dbba670528e7c",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "McLean, VA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=bb2fb6f6ffa31a2e",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Rochester, NY, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=87b713e6ebe771c3",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Midland, MI, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=80f0d92cfd23c289",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Omaha, NE, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1ea0bc8d88a85d44",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Grand Rapids, MI, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=227f56442e77932a",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c330d3407d356dad",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Minneapolis, MN, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ac530f7ffd36eae0",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Milwaukee, WI, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=90a50b063d1b2b89",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Memphis, TN, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d9122b799e128dfc",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Fort Worth, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2364dd118caf1a52",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Tempe, AZ, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b6a457603f4b679a",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Raleigh, NC, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=4e3fae432856abbb",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Las Vegas, NV, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=605f88116d6b43b4",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Tulsa, OK, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7dd90302f7439c7e",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Kansas City, MO, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d5076d2c13ddf5d3",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Pittsburgh, PA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=14187c0426ae0fa2",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "St. Louis, MO, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=98da314a69e75f1e",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Boca Raton, FL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=86465adf9c646cb9",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f72ac04a60f4d162",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Hartford, CT, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=141d40bfe20c12e8",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Baltimore, MD, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=94375b5d9e7cae99",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Sacramento, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=97c2cd5621dd40bb",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=589547b629f51343",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Jacksonville, FL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=37caaacce8d7ce07",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Honolulu, HI, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a4a13b839782ca75",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Birmingham, AL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=96620e34867483df",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Tampa, FL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3d543eb6b3aa6442",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Louisville, KY, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8a4e6ad85a494486",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7aca591c0990d287",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Davenport, IA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7e0db06ec1b0cb8a",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Boston, MA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a4a46b57ca5d88d1",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Stamford, CT, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1e2524ec035ff2bf",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Detroit, MI, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2d05c90daafadf3a",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Miami, FL, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=5e5834374e4863cc",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Des Moines, IA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=69224637a7d705fa",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "San Diego, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=92a2e1a31907db35",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Indianapolis, IN, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1fdad3de4918ac39",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Boise, ID, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2dcfff1f54bd57bb",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=678a91e6ce1e590b",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Denver, CO, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=43e82f243cf6d372",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cf997269e2e10bdf",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "New Orleans, LA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c9a413fb586ebb9f",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Consultant - GenAI Full Stack Developer",
        "company": "Deloitte",
        "location": "Costa Mesa, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "Pinecone",
            "Prompt Engineering",
            "TensorFlow",
            "PyTorch",
            "Keras"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=77c3f0c49614639e",
        "description": "**genai full stack developer \\- senior consultant**  \n\n  \n\ndeloitte's audit \\& assurance professionals help organizations navigate business risks and opportunities\\-across financial, operational, information technology (it), business, and regulatory areas\\-to build resilience and accelerate performance. in this role, you'll design and deliver end\\-to\\-end generative ai (genai) solutions \\- including retrieval\\-augmented generation (rag) and agentic ai \\- that are production\\-ready, scalable, and aligned to enterprise risk and governance expectations.  \n\n  \n\nrecruiting for this role ends on  \n\n  \n\n**work you'll do**  \n\n* lead business and technical requirements elicitation with client stakeholders; own end\\-to\\-end gap analysis; translate needs into solution architecture, detailed technical specifications, and delivery\\-ready backlog artifacts.\n* drive design, build, test, and deployment of full\\-stack generative ai (genai) applications (web user interface (ui), backend services, and data/model components); ensure non\\-functional requirements (security, performance, reliability) are met.\n* own end\\-to\\-end retrieval\\-augmented generation (rag) implementations (ingestion, chunking, embedding, indexing, retrieval, orchestration); define prompt engineering standards and evaluation harnesses to measure quality and reduce hallucinations.\n* architect agentic ai workflows (tool\\-using agents, multi\\-step orchestration, multi\\-agent patterns); integrate into enterprise platforms and business processes with appropriate controls, auditability, and human\\-in\\-the\\-loop checkpoints.\n* lead model training, fine\\-tuning, and validation; establish evaluation approaches and key performance indicators (kpis) for quality, robustness, bias/safety, and cost/latency; run benchmarking and iteration cycles to meet acceptance criteria.\n* own api and integration service design; deliver scalable restful interfaces; coordinate integration with downstream/upstream systems, identity and access management (iam), and operational workflows.\n* lead extract, transform, load (etl) and data engineering pipeline delivery to curate governed datasets for genai solutions; partner with data governance and risk teams on lineage, access controls, and data quality standards.\n* operationalize deployments using containerized patterns and cloud services; implement monitoring/observability (performance, cost, drift, quality signals) and drive continuous improvement through incident learnings and release management.\n* advise on emerging genai models, frameworks, and toolkits; prototype and recommend options with explicit tradeoffs across value, delivery effort, risk, compliance, and total cost of ownership (tco).\n* collaborate with cross\\-functional teams (product, engineering, data, risk, and stakeholders) to deliver adoption\\-ready solutions and documentation.\n\n  \n\n**the team**  \n\nour team culture is collaborative and encourages team members to take initiative and seek on\\-the\\-job learning opportunities. audit \\& assurance services are focused on engagements related to independent external audit services, accounting, controls \\& reporting advisory, and specialized assurance \\& sustainability. we bring together the diverse skills and industry experience of our people, leading\\-edge technology, and a global network to deliver high\\-quality audits of financial statements and internal controls over financial reporting, along with assurance reports and valuable advice and insights across the corporate reporting landscape. learn more about deloitte audit \\& assurance.  \n\n  \n\n**qualifications**  \n\n**required:**  \n\n* bachelor's degree (or equivalent) in computer science, engineering, data science, or a related field (advanced degree a plus).\n* 4\\+ years of experience in software engineering, full stack development, and/or ai/ml solution delivery.\n* python programming (production\\-grade) and strong sql.\n* natural language processing (nlp) applied to genai solutions.\n* agentic ai design/implementation, including langchain, langgraph, and llamaindex.\n* hands\\-on experience with rag architectures and implementation.\n* strong prompt engineering (design, iteration, and evaluation).\n* experience with vector databases (e.g., pinecone, chroma, faiss or similar) and embedding\\-based retrieval.\n* experience with genai model build: training, fine\\-tuning, and validation; practical llm evaluation using common metrics.\n* experience with model deployment (serving, monitoring, iteration) and production hardening.\n* experience with containers (e.g., docker) and scalable runtime patterns.\n* experience building etl pipelines and data engineering solutions (data quality, preprocessing, and curation).\n* api development and integration (restful services); backend development using fastapi (or equivalent).\n* full stack web development with javascript/typescript.\n* proficiency with html/css and preprocessors (sass/less).\n* experience with front\\-end frameworks (react, angular, or vue).\n* working knowledge of ui/ux design principles (accessibility, usability, responsive design).\n* experience with cloud ai/ml services across azure, aws, and gcp, including vertex ai.\n* you should reside within a commutable distance of your assigned office with the ability to commute daily, if required\n* you can expect to co\\-locate on average 3 times a week with variations based on types of work/projects and client locations\n* ability to travel up to 50%, on average, based on the work you do and the clients/sectors you serve\n* limited immigration sponsorship may be available.\n\n  \n\n**preferred:**  \n\n* experience with deep learning frameworks (e.g., tensorflow, pytorch, keras).\n* familiarity with ai/genai ethics, governance, and responsible ai implementation practices.\n* cloud certification (aws, azure, or gcp) and/or ai/ml certification.\n\n  \n\nthe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $124,658 to $179,431\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Data Scientist, Upstream Development",
        "company": "ExxonMobil",
        "location": "Houston, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "Copilot",
            "TensorFlow",
            "PyTorch",
            "Azure ML",
            "MLflow",
            "CI/CD",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=18763024525c9788",
        "description": "**about us**\n\n  \n\nat exxonmobil, our vision is to lead in energy innovations that advance modern living and a net\\-zero future. as one of the world\u2019s largest publicly traded energy and chemical companies, we are powered by a unique and diverse workforce fueled by the pride in what we do and what we stand for.\n\n  \n\nthe success of our upstream, product solutions and low carbon solutions businesses is the result of the talent, curiosity and drive of our people. they bring solutions every day to optimize our strategy in energy, chemicals, lubricants and lower\\-emissions technologies.\n\n  \n\nwe invite you to bring your ideas to exxonmobil to help create sustainable solutions that improve quality of life and meet society\u2019s evolving needs. learn more about **our what and our why** and how we canwork **together**.\n\n**what you will do**\n--------------------\n\n  \n\nlead the design, development, and deployment of advanced ai/ml solutions for upstream oil \\& gas subsurface \\& well operations such as physics\\-informed machine learning (ml) and production optimization. you will work end\\-to\\-end from problem framing and experimentation through production and sustainment partnering with engineers and business stakeholders to deliver scalable, reliable, and impactful solutions.\n\n**what role will you play in the team**\n---------------------------------------\n\n  \n\n* lead end\\-to\\-end delivery of ai/ml solutions: scoping, modeling, evaluation, deployment, and monitoring.\n* collaborate with cross\\-functional teams to translate business problems into mathematical frameworks.\n* build solutions in one or more domains listed above, applying best practices for data quality, explainability, and governance.\n* develop genai applications (chatbots, copilots, multi\\-agent workflows) and/or time series, optimization, or commercial analytics models.\n* ensure production readiness through mlops practices (ci/cd, mlflow, monitoring, cost optimization).\n* mentor peers and contribute to internal ai capability building.\n**about you**\n-------------\n\n **desired skills**\n\n  \n\n* 5\\+ years of direct experience delivering production ai/ml solutions.\n* experience applying data science to upstream oil \\& gas workflows, including exploration, drilling and reservoir management; analyzing well logs, seismic data, and production metrics to support operational and strategic decisions\n* strong foundations in statistics, probability, and algorithm design.\n* proficiency in python and ml frameworks (pytorch, tensorflow, scikit\\-learn); experience with databricks/spark.\n* familiarity with genai frameworks (langchain, promptflow) and/or optimization libraries.\n* experience with mlops, model governance, and explainable ai techniques (e.g., shap, lime).\n* excellent communication and collaboration skills.\n\n **preferred knowledge/skills**\n\n  \n\n* cloud platforms (azure ml, azure openai, databricks).\n* knowledge graphs, hybrid search/rag, and semantic technologies.\n* agile development and software engineering best practices.\n\n **educational background recommended**\n\n  \n\n* master\u2019s or phd in data science, computer science, engineering, applied math, or related field.\n**your benefits**\n-----------------\n\n **an exxonmobil career is one designed to last. our commitment to you runs deep: our employees grow personally and professionally, with benefits built on our core categories of health, security, finance, and life.**\n\n **we offer you:**\n\n  \n\n* pension plan: enrollment is automatic and at no cost to you. the basic benefit is a monthly annuity to be paid to you in retirement for the rest of your life.\n* savings plan: you can contribute between 6% and 20% of your pay and are encouraged to enroll right away. if you contribute at least 6% to your savings plan, the company will contribute a 7% match.\n* workplace flexibility: we have several programs such as \u201cflex your day\u201d, providing ad\\-hoc flexibility around when and where you work, as well as longer\\-term programs such as leaves of absence and part\\-time work.\n* comprehensive medical, dental, and vision plans.\n* culture of health: programs and resources to support your wellbeing.\n* employee health advisory program: provides confidential professional counseling for you and your family, including tools and resources promoting mental health and resiliency at no additional cost to you.\n* disability plan: income replacement for when you cannot work due to illness or injury occurring on or off the job. enrollment is automatic and at no cost to you.\n\n  \n\nmore information on our company\u2019s benefits can be found at www.exxonmobilfamily.com.\n\n  \n\nplease note benefits may be changed from time to time without notice, subject to applicable law.\n\n**stay connected with us**\n--------------------------\n\n  \n\nlearn more at our **website**  \n\nfollow us on **linkedin** and **instagram**  \n\nlike us on **facebook**  \n\nsubscribe our channel at **youtube**\n\n**employee equal opportunity**\n------------------------------\n\n **exxonmobil is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity, national origin, citizenship status, protected veteran status, genetic information, or physical or mental disability.**\n\n  \n\nnothing herein is intended to override the corporate separateness of local entities. working relationships discussed herein do not necessarily represent a reporting connection, but may reflect a functional guidance, stewardship, or service relationship.\n\n  \n\nexxon mobil corporation has numerous affiliates, many with names that include exxonmobil, exxon, esso and mobil. for convenience and simplicity, those terms and terms like corporation, company, our, we and its are sometimes used as abbreviated references to specific affiliates or affiliate groups. abbreviated references describing global or regional operational organizations and global or regional business lines are also sometimes used for convenience and simplicity. similarly, exxonmobil has business relationships with thousands of customers, suppliers, governments, and others. for convenience and simplicity, words like venture, joint venture, partnership, co\\-venturer, and partner are used to indicate business relationships involving common activities and interests, and those words may not indicate precise legal relationships.\n\n**job id: 82223**\n\n\n\\#li\\-onsite",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Postdoctoral Research Associate, Agentic Workflows",
        "company": "Oak Ridge National Laboratory",
        "location": "Oak Ridge, TN, US USA",
        "posted_at": "2026-02-21",
        "score": 14.4,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "S3",
            "Dataflow",
            "Git",
            "Kafka",
            "Polars",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ffa230522b70ff07",
        "description": "requisition id 15816\n\n**overview:**\n\n\nthe oak ridge national laboratory (ornl) is seeking a dynamic research associate to focus on innovations in ai\\-integrated workflow architectures that span the edge, cloud, and hpc continuum. this position centers on advancing intelligent workflows that enable seamless processes in autonomous discovery, complex data integration, workflow provenance, and interactive reasoning using cutting\\-edge ai agents and toolkits. your contributions will accelerate cross\\-disciplinary scientific progress across climate modeling, computational chemistry, additive manufacturing, and other domains by designing scalable, transparent, and adaptable workflow systems.\n\n  \n\nthis role offers an opportunity to work with ornl\u2019s leadership\\-class computational resources, experimental facilities, and collaborative platforms like flowcept, crewai, intersect, and the s3m facility api, pushing the boundaries in how scientists interact with real\\-time provenance data and ai\\-assisted experiments. furthermore, these advancements in intelligent ai workflows and agentic systems at ornl may play a significant role in contributing to the 2025 genesis mission, towards its goals of accelerating scientific discovery through cutting\\-edge ai integration.\n\n **focus areas:**\n\n* **modular workflow design**: improve scalability, portability, and reliability of loosely coupled workflows for large\\-scale experiments and computational workloads across diverse infrastructures.\n* **interactive ai agents for scientific workflows**: advance llm\\-powered agents that interpret natural language queries, generate runtime provenance insights, reason over metadata, and steer workflows adaptively based on scientific goals.\n* **cross\\-facility orchestration**: streamline interaction between experimental facilities, hpc systems, and edge/cloud platforms, enabling transparent data movement, multi\\-agent collaboration, and workflow modularization.\n* **dynamic workflow schema development**: design rich, metadata\\-driven dataflow schemas for tracking provenance, enabling efficient multi\\-step reasoning and integration across scientific domains.\n* **agentic ai integration with digital twins:** advance the use of workflow\\-integrated agentic ai systems in conjunction with digital twins to enable autonomous, feedback\\-driven experimentation and adaptive workflow reasoning.\n* **integration of hpc \\+ ai \\+ qc workflows**: develop innovative hybrid workflows that integrate high\\-performance computing (hpc), artificial intelligence (ai), and quantum computing (qc) paradigms to address next\\-generation scientific challenges.\n* **workflow benchmarking and optimization**: design and implement robust benchmarking frameworks to analyze and optimize the performance of ai\\-powered workflows and ensure scalability, traceability, and efficiency across diverse infrastructures.\n\n **major duties and responsibilities:**\n\n* research and prototype llm\\-driven agents capable of autonomous and interactive decision\\-making, anomaly detection, and guided experimentation in distributed scientific workflows.\n* design scalable systems for multi\\-workflow provenance capture, enhancing traceability, reproducibility, and transparency while facilitating intelligent multi\\-agent orchestration.\n* collaborate with domain scientists, computer scientists, engineers, and facility operators to integrate ai seamlessly into experimental and computational pipelines.\n* demonstrate the effectiveness of dynamic workflows in representative use cases such as materials discovery, combustion chemistry, and additive manufacturing.\n* publish research findings in peer\\-reviewed journals, conferences (e.g., sc, neurips, aaai), and open\\-source repositories.\n* mentor graduate students and contribute technical expertise to team projects aligned with ornl\u2019s strategic scientific goals.\n\n **basic qualifications:**\n\n* ph.d. in computer science, data science, computational science, or a relevant domain discipline (completed within the last 5 years or nearing completion).\n* experience with scientific workflows, distributed systems, or ai agent development, particularly integrating llms or autonomous tools within complex pipelines.\n* proficiency in modern ai frameworks and tools (e.g., pytorch, tensorflow, langchain, mcp sdks) and programming languages (python, c\\+\\+).\n* experience with provenance systems (e.g., flowcept, w3c prov) and data streaming tools (kafka, redis, rabbitmq).\n* understanding of hpc workflow orchestration platforms such as argo, crewai, parsl, or radical\\-pilot.\n\n **preferred qualifications:**\n\n* knowledge of tools such as grafana, polars, or pandas for monitoring and analyzing large\\-scale workflow execution and provenance data.\n* familiarity with synthetic workflows, graph\\-based reasoning, or computational chemistry/molecular dynamics workflows.\n* expertise in ai techniques such as retrieval\\-augmented generation (rag), schema\\-driven reasoning, and graph traversal in provenance.\n* background in developing scalable tools for cross\\-domain, edge\\-to\\-hpc workflows using distributed architectures.\n* proven ability to integrate dynamic schema design and metadata enrichment into ai workflow systems.\n\n **why join us:**\n\n\nornl is the nexus for transformative science and cutting\\-edge technology. leveraging leadership computing resources and national facilities, this position provides unmatched opportunities to redefine autonomy and scalability in scientific workflows. you\u2019ll work on real\\-world applications that shape the next generation of ai\\-powered computational research, while driving innovation and advancing reproducibility in multidisciplinary science.\n\n **application process:**\n\n\napplicants should submit their cv, a research statement detailing their expertise and vision for intelligent workflows and ai\\-powered provenance, along with up to three representative publications.\n\n **contact information:**\n\n\noak ridge national laboratory  \n\nemail: research.positions@ornl.gov  \n\naddress: 1 bethel valley road, oak ridge, tn 37830\n\n  \n\nthis position will remain open for a minimum of 5 days after which it will close when a qualified candidate is identified and/or hired.\n\n  \n\nwe accept word (.doc, .docx), adobe (unsecured .pdf), rich text format (.rtf), and html (.htm, .html) up to 5mb in size. resumes from third party vendors will not be accepted; these resumes will be deleted and the candidates submitted will not be considered for employment.\n\n  \n\nornl is an equal opportunity employer. all qualified applicants, including individuals with disabilities and protected veterans, are encouraged to apply. ut\\-battelle is an e\\-verify employer.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Sr Analyst Supply Chain Ops Research",
        "company": "Staples",
        "location": "Framingham, MA, US USA",
        "posted_at": "2026-02-20",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "Snowflake",
            "Hadoop",
            "MySQL",
            "NoSQL",
            "Tableau",
            "Power BI",
            "MicroStrategy",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3757762afd6acc3f",
        "description": "**sr operations research analyst (replenishment)**\n\n**work model: onsite, framingham, ma**\n\n  \n\nour supply chain team is dedicated to meeting our customers\u2019 needs both now and in the future. by pairing innovative technology with dynamic employees, we create smarter, more efficient ways to meet our customers\u2019 needs faster. our network of distribution, fulfillment, fleet, furniture installation and professional teams work together in fun and safe environments to deliver state\\-of\\-the\\-art products, services, and expertise to our customers.\n\n**position summary**\n\n\nthe senior operations research analyst (replenishment) is a highly technical and analytical role responsible for optimizing staples\u2019 replenishment ecosystem through advanced modeling, and system\u2011level improvements. this role bridges data science, supply chain planning, and replenishment system design to enhance inventory performance, increase automation, and improve service levels across the network.\n\n\nthis position uses advanced analytics, sql, python, and ai/ml driven insights to diagnose system behaviors, recommend parameter adjustments, and partner with both business stakeholders (planners, inventory analysts, s\\&op teams) and technical partners (data scientists, it engineering) to drive scalable enhancements.\n\n **primary responsibilities**\n\n* optimize replenishment system performance by analyzing system settings (slgs, safety stock, min/max, order policies) and recommending data driven adjustments to improve instock, inventory turns and forecast accuracy.\n* 1\u20133\\+ years of experience in supply chain with a focus on inventory replenishment, project management, and replenishment systems\n* conduct scenario modeling to evaluate the impacts of parameter changes, network shifts, supplier variability, and policy updates.\n* support discovery, testing, and rollout of replenishment system enhancements including awr upgrades, parameter redesigns, and new analytical capabilities.\n* partner with data scientists to design, test, and deploy advanced forecasting, clustering, and optimization models that enhance automation and decision making.\n* collaborate with supply/demand planners and business teams to translate business needs into system or analytical enhancements, including ai driven improvements\n* work with it and engineering to support system updates, parameter changes, and implementation of new replenishment capabilities or ml enhanced features.\n* develop tools, scripts, and insights that simplify workflows, reduce manual effort, and enable exception\\-based replenishment management.\n* serve as a technical sme in replenishment theory, system design while mentoring junior analysts in analytical best practices.\n* ability to collaborate effectively with both business stakeholders and technical teams.\n* ability to communicate complex quantitative analysis in a clear, precise, and actionable manner with both technical and nontechnical customers\n\n **basic qualifications*** bachelor\u2019s in operations research, industrial engineering, supply chain, mathematics, statistics, computer science, or related analytical discipline.\n* 1\u20133\\+ years of experience in supply chain with a focus on inventory replenishment, project management, and replenishment systems\n* strong proficiency in sql and python.\n* proven analytical and problem\u2011solving skills with the ability to interpret complex data and system behaviors.\n* detailed knowledge of the replenishment, forecasting and allocation systems and process for a variety of systems and solutions\n* experience working with data visualization and bi tools such as tableau, power bi, microstrategy, or qlikview\n* experience building solutions on public cloud as well as on\\-premise platforms (e.g.: azure, sql server, snowflake)\n\n **preferred qualifications**\n\n* master\u2019s in operations research, industrial engineering, supply chain, mathematics, statistics, computer science, or a related analytical discipline, plus 1\\-3 years industry experience.\n* experience working with development toolsets such as hadoop, mysql, teradata, nosql, etc.\n* understanding of multi\u2011echelon inventory strategies, supplier variability, and advanced replenishment concepts.\n* experience creating decision\u2011support tools or automation scripts to improve planner productivity and drive process efficiency.\n* familiarity with automated replenishment systems (e.g., awr/e3 or similar) and core concepts (safety stock, slgs, min/max, order policies, forecast drivers).\n\n**we offer:**\n\n* inclusive culture with associate\\-led business resource groups\n* flexible pto (22 days) and holiday schedule (7 observed paid holidays)\n* online and retail discounts, company match 401(k), physical and mental health wellness programs, and more perks and benefits\n\n\nthe salary range represents the expected compensation for this role at the time of posting. the specific base pay may be influenced by a variety of factors to include the candidate's experience, skill set, education, geography, business considerations, and internal equity. in addition to base pay, this role may be eligible for bonuses, or other forms of variable compensation.\n\n\n it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Software Engineer",
        "company": "Mercury Insurance Company",
        "location": "Remote, US USA",
        "posted_at": "2026-02-21",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "Kafka",
            "MongoDB",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cee995a368e6f967",
        "description": "overview:\n\njoin an amazing team that is consistently recognized for our achievements and culture, including our most recent forbes award of being one of america's best midsize employers for 2025!\n\n **position summary:**\n\n\nas a senior software engineer, you will play a crucial role in designing, developing, and maintaining complex software systems. this position involves leading agile teams or projects, mentoring junior engineers, and ensuring the delivery of high\\-quality software solutions.\n\n  \n\n\ngeo\\-salary information:\n***an in\\-person interview may be required during the hiring process***  \n\nstate specific pay scales for this role are as follows:\n\n\n$94,458 to $179,048 (ca, nj, ny, wa, hi, ak, md, ct, ri, ma)\n\n\n$85,871 to $162,771 (nv, or, az, co, wy, tx, nd, mn, mo, il, wi, fl, ga, mi, oh, va, pa, de, vt, nh, me)\n\n\n$77,283 to $146,464 (ut, id, mt, nm, sd, ne, ks, ok, ia, ar, la, ms, al, tn, ky, in, sc, nc, wv)\n\n\nthe expected base salary for this position will vary depending on a number of factors, including relevant experience, skills and location.\n\n\nresponsibilities:\n**essential job functions:**\n\n* write high\\-quality code for complex scenarios with good test coverage.\n* review and translate product requirements into robust designs to ensure high\\-quality distributed application systems that are modular, configurable, reusable, fast, effective, user\\-friendly, secure, compliant, scalable, and maintainable.\n* lead design and code reviews of multiple applications to ensure best practices, adherence to mercury standards, and high quality.\n* lead the sprint team to deliver new or improved features, addressing any impediments to delivering high\\-quality solutions in the sprint backlog.\n* collaborate with other teams and stakeholders to ensure successful delivery of intended functionality.\n* mentor and provide technical guidance to other engineers on the team, fostering a culture of innovation and excellence within mercury engineering.\n\n\nqualifications:\n**education:**\n\n\nminimum:\n\n* bachelor\u2019s degree in computer science, information systems or other related fields or equivalent combination of education and experience\n\n\npreferred:\n\n* master\u2019s degree in computer science.\n\n**experience:**\n\n\nminimum:\n\n* 5\\+ years software programming experience with 2\\+ years experience in technologies used at mercury\n* 1\\+ years of experience leading and mentoring engineering teams.\n\n\npreferred:\n\n* 5 or more years with java programming language.\n* 5 or more years of the spring framework, including spring boot for building microservices and rest apis.\n* proficiency in designing, implementing, and consuming restful apis.\n* hands\\-on experience with mongodb (nosql database) for storing and retrieving data.\n* ability to effectively utilize ai tools such as claude, chatgpt with in ide like intellij , vs code, etc.\n\n**skills:**\n\n* proficient in programming in one or more industry\\-standard languages (e.g., java, python, go, c\\+\\+, javascript).\n* experienced in leveraging various data structures, algorithms, and design patterns. skilled in relational databases, nosql, and caching solutions.\n* experienced in apis using modern protocols like rest and grpc. proficient in message queuing, data streaming, workflow, and integration frameworks (such as apache kafka, apache nifi, amazon sqs, ms mq, apache camel, etc.).\n* knowledgeable in microservices architecture.\n* experienced in containerization technologies like docker and kubernetes.\n* proficient in leveraging devsecops to improve engineering efficiency.\n* experienced in designing and implementing high\\-quality distributed application systems that are modular, configurable, maximize reuse, and are fast, effective, user\\-friendly, secure, compliant, scalable, and maintainable. skilled in leveraging analytics data and experimentation to design and improve systems for speed, scalability, and effectiveness. strong problem\\-solving skills.\n* excellent communication abilities. strong ability to collaborate and influence cross\\-functional teams and stakeholders to address impediments and deliver desired results. demonstrated leadership skills through mentoring and providing technical guidance to team members.\n* experienced in driving technical strategy and vision for a team.\n* preferred: experience in cloud platforms such as aws, google cloud, or azure.\n\n  \n\n\nabout the company:\n***why choose a career at mercury?***\nat mercury, we have been guided by our purpose to help people reduce risk and overcome unexpected events for more than 60 years. we are one team with a common goal to help others. everyone needs insurance and we can\u2019t imagine a world without it.\nour team will encourage you to grow, make time to have fun, and work together to make great things happen. we embrace the strengths and values of each team member. we believe in having diverse perspectives where everyone is included, to serve customers from all walks of life.\nwe care about our people, and we mean it. we reward our talented professionals with a competitive salary, bonus potential, and a variety of benefits to help our team members reach their health, retirement, and professional goals.  \n\nlearn more about us here: https://www.mercuryinsurance.com/about/careers\nperks and benefits:\n***we offer many great benefits, including:**** competitive compensation\n* flexibility to work from anywhere in the united states for most positions\n* paid time off (vacation time, sick time, 9 paid company holidays, volunteer hours)\n* incentive bonus programs (potential for holiday bonus, referral bonus, and performance\\-based bonus)\n* medical, dental, vision, life, and pet insurance\n* 401 (k) retirement savings plan with company match\n* engaging work environment\n* promotional opportunities\n* education assistance\n* professional and personal development opportunities\n* company recognition program\n* health and wellbeing resources, including free mental wellbeing therapy/coaching sessions, child and eldercare resources, and more\n\n\nmercury insurance is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by federal, state, or local law.\npay range: usd $94,458\\.00 \\- usd $179,048\\.00 /yr.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "IT Architect Advisor - Enterprise Architecture Data and AI",
        "company": "Medtronic",
        "location": "Mounds View, MN, US USA",
        "posted_at": "2026-02-21",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "Prompt Engineering",
            "BigQuery",
            "Synapse",
            "Data Lake",
            "Git",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=48d58844218ed408",
        "description": "we anticipate the application window for this opening will close on \\- 13 mar 2026  \n\n  \n\nat medtronic you can begin a life\\-long career of exploration and innovation, while helping champion healthcare access and equity for all. you\u2019ll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.\n\n\n**a day in the life**\n=====================\n\nthis role will serve as the enterprise architect for data and ai, providing technical, architectural, and thought leadership across the enterprise and at medtronic, we bring bold ideas forward with speed and decisiveness to put patients first in everything we do. in\\-person exchanges are invaluable to our work. we're working onsite 4 days a week as part of our commitment to fostering a culture of professional growth and cross\\-functional collaboration as we work together to engineer the extraordinary. in your role, you may work from the following medtronic sites:\n  \n\n  \n\n\u2022 mounds view, minnesota \u2022 fort worth, texas\n  \n\n\u2022 fridley, minnesota (ohq) \u2022 lafayette, colorado\n  \n\n\u2022 irvine, california (uci) \u2022 boston, massachusetts\n  \n\n\u2022 rice creek, minnesota \u2022 jacksonville, florida  \n\n  \n\n  \n\n  \n\nin this role, you will serve as the enterprise architect for data and ai, providing technical, architectural, and thought leadership across the enterprise and owning the definition of the data and ai vision and strategic direction for medtronic it. you will be responsible for shaping and guiding end\\-to\\-end data and ai architectures that enable scalable analytics, advanced insights, and ai\\-driven business outcomes while remaining aligned with enterprise strategy and long\\-term transformation goals.\n  \n\n  \n\nthe ideal candidate brings extensive hands\\-on experience across application development, enterprise architecture, data and analytics platforms, spanning both on\\-premises and public cloud environments. it requires deep expertise across relational and non\\-relational data stores, including strong experience in otlp environments as well as traditional and modern data warehouse and lakehouse architectures. experience in medtech or other highly regulated industries is a strong advantage.\n  \n\n  \n\nthis role requires a strong, opinionated point of view and the ability to establish end\\-to\\-end data and ai architectures with authority, supported by detailed technical depth and practical implementation experience. the enterprise architect for data and ai will analyze current and emerging enterprise needs and propose optimal architectures that balance business value, scalability, security, and operational sustainability. the role also encompasses architectural leadership for ai and machine learning systems, including integration of ai into enterprise data platforms, analytics workflows, and business processes. a strong understanding of data standards, governance, privacy, regulatory, and security implications is essential, particularly within a highly regulated environment. the successful candidate will ensure that data and ai solutions are designed responsibly, securely, and in compliance with enterprise and regulatory requirements while enabling innovation and measurable business impact.\n  \n\n  \n\nwe believe that when people from different cultures, genders, and points of view come together, innovation is the result \u2014and everyone wins. medtronic walks the walk, creating an inclusive culture where you can thrive. our unwavering commitment to inclusion, diversity, and equity (id\\&e) means zero barriers to opportunity within medtronic and a culture where all employees belong, are respected, and feel valued for who they are and the life experiences they contribute. we know equity starts beyond our workplace, and we must play a role in addressing systemic inequities in our communications to achieve long\\-term sustainable impact. anchored in our mission, we continue to drive id\\&e forward both to enhance the well\\-being of medtronic employees and to accelerate innovation that brings our lifesaving technologies to more people in more places around the world.\n  \n\n  \n\nbring your talents to an industry leader in medical technology and healthcare solutions \u2013 we\u2019re a market leader and growing every day. you can be proud to be a part of technologies that are rooted in our long history of mission\\-driven innovation. you will be empowered to shape your own career. we encourage and support your growth with the training, mentorship, and guidance you need to own your future success. together, we can transform healthcare. join us for a career in it that changes lives. medtronic is committed to fostering a diverse and inclusive culture. check out the accomplishments of our women in it group! http://bit.ly/medtronicwomeninit\n**careers that change lives**\n\n* serve as the enterprise architect for data and ai, acting as the primary architectural authority and trusted advisor for data, analytics, ai, and genai initiatives across the enterprise.\n* define, guide, and influence end\\-to\\-end architectures for data and ai solutions, ensuring alignment with business strategy, enterprise standards, platform roadmaps, and long\\-term transformation objectives.\n* partner closely with enterprise architecture, data platforms, advanced analytics, sap coe, it value streams, and business teams to shape solution designs that are scalable, secure, compliant, and future\\-ready.\n* provide hands\\-on architectural guidance across the full lifecycle of data and ai solutions, including data ingestion, transformation, feature engineering, analytics, model development, deployment, monitoring, and lifecycle management.\n* advise on ai and data platform strategy, including cloud\\-native data platforms, analytics services, ml platforms, genai services, vector databases, orchestration frameworks, and integration patterns.\n* establish and evolve reference architectures, architectural patterns, and design guardrails for data and ai to promote reuse, reduce technical debt, and accelerate delivery.\n* ensure data governance, privacy, security, and responsible ai principles are embedded into solution designs in partnership with security, privacy, and compliance stakeholders, particularly in regulated environments.\n* review and influence architectural decisions to avoid unnecessary customization, minimize parallel or duplicative solutions, and maximize leverage of enterprise\\-standard platforms and native capabilities.\n* support innovation and digital transformation initiatives by identifying pragmatic applications of emerging data and ai technologies and advising where they can deliver tangible business outcomes.\n* act as a technical thought leader and mentor to architects and senior engineers across the organization, helping raise overall architectural maturity in data and ai domains.\n* communicate complex architectural concepts clearly and effectively to a broad audience, including senior it leadership and business stakeholders, translating technical decisions into business impact.\n\n**must have** (minimum qualifications)\n\n\n* bachelor\u2019s degree\n* 15\\+ years of experience with a bachelor\u2019s degree or 13\\+ years of experience with an advanced degree\n\n**nice to have** (preferred qualifications)\n\n\n* strongly preferred:\n\t+ bachelor\u2019s or advanced degree in computer science, information technology, engineering, or relevant discipline; advanced degree strongly preferred\n\t+ experience with sap data and analytics landscapes, including s/4hana, bdc, data sphere, mdg, and sap\\-native analytics and integration patterns.\n\t+ hands\\-on exposure to genai architectures, including large language models, retrieval\\-augmented generation (rag), prompt engineering, and ai\\-enabled workflows.\n\t+ experience defining or applying responsible ai frameworks, ai governance models, or ethical ai practices.\n\t+ prior experience in medical technology, healthcare, or other highly regulated industries.\n\t+ familiarity with enterprise architecture frameworks and tooling, while maintaining a pragmatic, outcome\\-driven mindset.\n* 15\\+ years of it experience with deep, hands\\-on expertise in data engineering, data architecture, and analytics platforms.\n* 7\\+ years of experience designing and influencing enterprise\\-scale data architectures, including data warehouses, data lakes/lakehouses, and real\\-time or event\\-driven data platforms.\n* strong experience with public cloud platforms and cloud\\-native data services such as snowflake, databricks, bigquery, azure synapse, or equivalent technologies.\n* demonstrated experience working with ai, machine learning, and genai solutions, including understanding of model lifecycle management, mlops concepts, and enterprise ai integration patterns.\n* deep understanding of data governance, metadata management, data quality, privacy, and security, especially within highly regulated environments.\n* proven ability to influence without direct authority, working across multiple teams and stakeholders to drive alignment and high\\-quality architectural outcomes.\n* strong written and verbal communication skills, with the ability to translate complex technical concepts into clear, actionable guidance for both technical and non\\-technical audiences.\n* track record of staying current with emerging trends in data, ai, and genai, and applying them thoughtfully within enterprise constraints.\n\n**physical job requirements**\n\nthe above statements are intended to describe the general nature and level of work being performed by employees assigned to this position, but they are not an exhaustive list of all the required responsibilities and skills of this position.\n\n\nthe physical demands described within the responsibilities section of this job description are representative of those that must be met by an employee to successfully perform the essential functions of this job. reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. for office roles: while performing the duties of this job, the employee is regularly required to be independently mobile. the employee is also required to interact with a computer, and communicate with peers and co\\-workers. contact your manager or local hr to understand the work conditions and physical requirements that may be specific to each role.\n\n\n**benefits \\& compensation**\n============================\n\n**medtronic offers a competitive salary and flexible benefits package**  \n\na commitment to our employees lives at the core of our values. we recognize their contributions. they share in the success they help to create. we offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage.  \n\n  \n\n\n\nsalary ranges for u.s (excl. pr) locations (usd):$180,000\\.00 \\- $270,000\\.00  \n\nthis position is eligible for a short\\-term incentive called the medtronic incentive plan (mip).\nthis position is eligible for an annual long\\-term incentive plan.\nthe base salary range is applicable across the united states, excluding puerto rico and specific locations in california. the offered rate complies with federal and local regulations and may vary based on factors such as experience, certification/education, market conditions, and location. compensation and benefits information pertains solely to candidates hired within the united states (local market compensation and benefits will apply for others).\nthe following benefits and additional compensation are available to those regular employees who work 20\\+ hours per week: health, dental and vision insurance , health savings account , healthcare flexible spending account , life insurance, long\\-term disability leave , dependent daycare spending account , tuition assistance/reimbursement , and simple steps (global well\\-being program).  \n\n\n\nthe following benefits and additional compensation are available to all regular employees: incentive plans, 401(k) plan plus employer contribution and match , short\\-term disability , paid time off , paid holidays , employee stock purchase plan , employee assistance program , non\\-qualified retirement plan supplement (subject to irs earning minimums) , and capital accumulation plan (available to vice presidents and above, or subject to irs earning minimums).  \n\n\n\nregular employees are those who are not temporary, such as interns. temporary employees are eligible for paid sick time, as required under applicable state law, and the employee stock purchase plan. please note some of the above benefits may not apply to workers in puerto rico.  \n\n\n\nfurther details are available at the link below:\n\n\n\n\n**about medtronic**\n===================\n\nwe lead global healthcare technology and boldly attack the most challenging health problems facing humanity by searching out and finding solutions.\n  \n\nour mission \u2014 to alleviate pain, restore health, and extend life \u2014 unites a global team of 95,000\\+ passionate people.\n  \n\nwe are engineers at heart\u2014 putting ambitious ideas to work to generate real solutions for real people. from the r\\&d lab, to the factory floor, to the conference room, every one of us experiments, creates, builds, improves and solves. we have the talent, diverse perspectives, and guts to engineer the extraordinary.\n\n  \n\n\n\nit is the policy of medtronic to provide equal employment opportunity (eeo) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. in addition, medtronic will provide reasonable accommodations for qualified individuals with disabilities.\n\n\nif you are applying to perform work for medtronic, inc. (\u201cmedtronic\u201d) in any position which will involve performing at least two (2\\) hours of work on average each week within the unincorporated areas of los angeles county, you can find here a list of all material job duties of the specific job position which medtronic reasonably believes that criminal history may have a direct, adverse and negative relationship potentially resulting in the withdrawal of a conditional offer of employment. medtronic will consider for employment qualified job applicants with arrest or conviction records in accordance with the los angeles county fair chance ordinance for employers and the california fair chance act.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Specialist, Data Engineer",
        "company": "Nationwide Mutual Insurance Company",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-22",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "CI/CD",
            "Jenkins",
            "Git",
            "Snowflake",
            "Databricks",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e118e0038cca00a8",
        "description": "if you\u2019re passionate about being part of a dynamic organization that enables a fortune 100 company with nearly $70 billion in annual sales to drive innovation and adopt new technologies that deliver business results, then nationwide\u2019s technology team could be the place for you! at nationwide\u00ae, \u201con your side\u201d goes beyond just words. our customers are at the center of everything we do and we\u2019re looking for associates who are passionate about delivering extraordinary care.**if you are passionate about the importance of data engineering and delivering clean, automated, integrated, and governed data required to fulfill our company\u2019s most critical strategic objectives, then we have a great opportunity for you. as a member of the enterprise data office, the data engineer works closely with our dev line partners, data architects, business stakeholders, platform owners, and a host of other internal actors in the designing, building, and operationalizing of end\\-to\\-end data delivery solutions.**\n\n**this position combines hands\\-on data engineering, along with understanding both the strategic goals and desired capabilities of the business units we support. it involves the development of data pipelines and application architecture spanning the gamut of data ingestion, harmonization, and curation to enable business value realization across numerous consumer tiers. as new solutions are developed as part of build activities, this role is also critical in working with application teams and other key business and it stakeholders to aid them in the production hand\\-off, adoption, and ongoing management of new technologies, processes, and best practices.**\n\n**a successful candidate should possess the following skills and/or experience:**\n\n* **minimum of 3 years performing complex data pipeline development across numerous platforms (e.g. informatica, databricks, snowflake, native python).**\n* **minimum of 2 years development experience utilizing aws cloud technologies (storage, container, compute, security, and automation).**\n* **fluency in python, sql, and unix. high proficiency with generating complex sql queries is required. ability to understand and explain complex data integration and extraction / transformation / load processes.**\n* **demonstrated proficiency in utilizing ai tools to perform automated code development and testing tasks as part of typical daily work routine.**\n* **actively promotes reusable processes, engineering patterns, and work products. demonstrated success at applying standardized automation, secure data practices, and quality routines to data pipelines and information reporting solutions.**\n* **routine development and deployment of application monitoring and alerting capabilities as part of standard pipeline build activities. extensive experience utilizing devops and ci/cd tools as part of regular development activities.**\n* **demonstrated history of converting innovative ideas into scalable, production ready solutions.**\n* **knowledge of data governance tools, processes and policies, as well as influencing their adoption on projects driven by our it and business partners.**\n* **proven communication skills illustrating ability to effectively interact with people at all levels of business and technology organizations.**\n* **demonstrated skill working directly with stakeholders, business partners, smes, systems peers, and cross\\-functional teams to collaborate and gain agreement on target solution designs.**\n* **experience providing skill coaching to junior resources, resources transitioning into newer skills and technologies, and mentees.**\n* **insurance and financial domain experience is highly desired.**\n* **experience working in an agile environment using lean, kanban and scrum practices.**\n\n**this role will work a hybrid schedule coming into the columbus, ohio office 2 days per week.**\n\n**\\*\\*this role does not qualify for employer\\-sponsored work authorization. nationwide does not participate in the stem opt extension program\\*\\***\n\n\n\\#li\\-az1\n\n**job description summary**\n\n\nnationwide\u2019s industry leading workforce is passionate about creating data solutions that are secure, reliable and efficient in support of our mission to provide extraordinary care. nationwide embraces an agile work environment and collaborative culture through the understanding of business processes, relationship entities and requirements using data analysis, quality, visualization, governance, engineering, robotic process automation, and machine learning to produce targeted data solutions. if you have the drive and desire to be part of a future forward data enabled culture, we want to hear from you.  \n\n  \n\nas a data engineer you\u2019ll be responsible for acquiring, curating, and publishing data for analytical or operational uses. data should be in a ready\\-to\\-use form that creates a single version of the truth across all data consumers, including business users, data scientists, and technology. ready\\-to\\-use data can be for both real time and batch data processes and may include unstructured data. successful data engineers have the skills typically required for the full lifecycle software engineering development from translating requirements into design, development, testing, deployment, and production maintenance tasks. you\u2019ll have the opportunity to work with various technologies from big data, relational and sql databases, unstructured data technology, and programming languages.**job description**\n\n**key responsibilities:**\n\n* provides basic to moderate technical consultation on data product projects by analyzing end to end data product requirements and existing business processes to lead in the design, development and implementation of data products.\n* produces data building blocks, data models, and data flows for varying client demands such as dimensional data, standard and ad hoc reporting, data feeds, dashboard reporting, and data science research \\& exploration\n* applies secure software and systems engineering practices throughout the delivery lifecycle to ensure our data and technology solutions are protected from threats and vulnerabilities.\n* translates business data stories into a technical story breakdown structure and work estimate so value and fit for a schedule or sprint is determined.\n* creates simple to moderate business user access methods to structured and unstructured data by such techniques such as mapping data to a common data model, nlp, transforming data as necessary to satisfy business rules, ai, statistical computations and validation of data content.\n* assists the enterprise devsecops team and other internal organizations on ci/cd best practices experience using jira, jenkins, confluence etc.\n* implements production processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\n* develops and maintains scalable data pipelines for both streaming and batch requirements and builds out new api integrations to support continuing increases in data volume and complexity.\n* writes and performs data unit/integration tests for data quality with input from a business requirements/story, creates and executes testing data and scripts to validate that quality and completeness criteria are satisfied. can create automated testing programs and data that are re\\-usable for future code changes.\n* practices code management and integration with engineering git principle and practice repositories.\n\n\nmay perform other responsibilities as assigned.\n\n**reporting relationships:** reports to manager/director data leader.\n\n**typical skills and experiences:**\n\n**education**: undergraduate studies in computer science, management information systems, business, statistics, math, a related field or comparable experience and education strongly preferred. graduate studies in business, statistics, math, computer science or a related field are a plus.\n\n**license/certification/designation**: certifications are not required but encouraged.\n\n**experience**: three to five years of relevant experience with data quality rules, data management organization/standards, practices and software development. experience in data warehousing, statistical analysis, data models, and queries. one to three years\u2019 experience with cloud technology and infrastructure including security and access management. insurance/financial services industry knowledge a plus.\n\n**knowledge, abilities and skills**: data application and practices knowledge. moderate to advanced skills with modern programming and scripting languages (e.g., sql, r, python, spark, unix shell scripting, perl, or ruby). good problem solving, oral and written communication skills.\n\n\nother criteria, including leadership skills, competencies and experiences may take precedence.\n\n\nstaffing exceptions to the above must be approved by the hiring manager\u2019s leader and hr business partner.\n\n**values:** regularly and consistently demonstrates the nationwide values.\n\n**job conditions:**\n\n**overtime eligibility:** exempt (not eligible)\n\n**working conditions**: normal office environment.\n\n**ada**: the above statements cover what are generally believed to be principal and essential functions of this job. specific circumstances may allow or require some people assigned to the job to perform a somewhat different combination of duties.\n\n**benefits**\n\n\nwe have an array of benefits to fit your needs, including: medical/dental/vision, life insurance, short and long term disability coverage, paid time off with newly hired associates receiving a minimum of 18 days paid time off each full calendar year pro\\-rated quarterly based on hire date, nine paid holidays, 8 hours of lifetime paid time off, 8 hours of unity day paid time off, 401(k) with company match, company\\-paid pension plan, business casual attire, and more. to learn more about the benefits we offer.\n\n\nnationwide is an equal opportunity employer. we celebrate diversity and are committed to creating an inclusive culture where everyone feels challenged, appreciated, respected and engaged. nationwide prohibits discrimination and harassment and affords equal employment opportunities to employees and applicants without regard to any characteristic (or classification) protected by applicable law.\n\n **note to employment agencies:**\n\n\nwe value the partnerships we have built with our preferred vendors. nationwide does not accept unsolicited resumes from employment agencies. all resumes submitted by employment agencies directly to any nationwide employee or hiring manager in any form without a signed nationwide client services agreement on file and search engagement for that position will be deemed unsolicited in nature. no fee will be paid in the event the candidate is subsequently hired as a result of the referral or through other means.\n\n\nnationwide pays on a geographic\\-specific salary structure and placement within the actual starting salary range for this position will be determined by a number of factors including the skills, education, training, credentials and experience of the candidate; the scope, complexity and location of the role as well as the cost of labor in the market; and other conditions of employment. if a sales job, sales incentives, based on performance goals are possible in addition to this range. note on compensation for part\\-time roles: please be aware that the salary ranges listed below reflect full\\-time compensation. actual compensation may be prorated based on the number of hours worked relative to a full\\-time schedule.\nthe national salary range for specialist, data engineer : $95,500\\.00\\-$177,500\\.00\nthe expected starting salary range for specialist, data engineer : $95,500\\.00 \\- $143,500\\.00",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "AI Engineer \u2013 Agentic AI Solutions for Automated Decision Making",
        "company": "KBR",
        "location": "Sioux Falls, SD, US USA",
        "posted_at": "2025-11-26",
        "score": 11.1,
        "matched_keywords": [
            "AI Engineer",
            "Generative AI",
            "RAG",
            "Prompt Engineering",
            "Data Lake",
            "CI/CD",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=483d9c2d7745bdd6",
        "description": "**belong. connect. grow. with kbr!**\n\naround here, we define the future. we are a company of innovators, thinkers, creators, explorers, volunteers, and dreamers. we all share one goal: to improve the world responsibly and safely by supporting the science that informs decision makers and protects earth.\n\n\nabout the role\nwe are seeking a highly experienced ai engineer to join our team in developing an agentic ai solution designed to streamline and automate decision making against data lakes. this is a unique opportunity to work on a mission\\-critical system that leverages generative ai (genai) and large language models (llms) to deliver real\\-world impact in a regulated environment.\n\n\nyou will collaborate closely with architects, product managers, and engineers to design, build, and optimize ai\\-driven workflows that meet stringent compliance and performance requirements.\n\n\nresponsibilities* agentic ai development: architect and implement agent\\-based ai systems to manage complex, multi\\-step permitting workflows.\n* prompt engineering \\& orchestration: design, test, and refine prompts and system instructions for llms to ensure accuracy and reliability.\n* integration \\& automation: build backend services and apis to integrate ai components with federal systems and cloud infrastructure.\n* model evaluation \\& compliance: develop evaluation frameworks to monitor llm performance, ensuring outputs meet regulatory standards.\n* scalable architecture: design solutions that are secure, cost\\-effective, and optimized for aws cloud environments.\n* continuous improvement: iterate based on metrics, user feedback, and compliance audits to enhance system performance.\n\n\nqualifications* 10\\+ years of experience in software engineering or solution architecture.\n* 5\\+ years of hands\\-on experience with python in production environments.\n* \\*\\*\\*must be a u.s. citizen\\*\\*\\*\n* strong expertise in aws cloud architecture and cost optimization strategies.\n* familiarity with llms, generative ai, and agentic ai frameworks \u2013 with emphasis on claude and bedrock.\n* experience with prompt engineering, rag pipelines, and vector databases.\n* familiarity with geospatial data integration with esri products.\n* knowledge of federal compliance requirements and secure system design.\n* excellent communication skills and ability to work in a distributed, remote team.\n\n\npreferred skills* experience with workflow automation and orchestration tools such as salesforce.\n* background in nlp, knowledge graphs, and structured output validation.\n* familiarity with ci/cd pipelines and evaluation tooling for ai systems.\n\n\nwhy join us?* work on cutting\\-edge ai solutions with real\\-world impact.\n* collaborate with a highly skilled, mission\\-driven team.\n* enjoy a remote\\-first culture with flexibility and autonomy.\n* competitive compensation and benefits.\n\n**special requirements**\n\n* three years of continuous u.s. residency for government security credential.\n* ability to obtain and maintain a national agency check and background investigation for government facility access.\n* experience and/or education in lieu of these qualifications may be considered.\n* **\\*\\*\\*must be a u.s. citizen\\*\\*\\***\n\n**benefits**\n\n* competitive benefits: 401k with match, medical, dental, vision, life insurance, ad\\&d, flexible spending, disability, paid time off, flexible schedule.\n* career advancement through professional training and development.\n* benefits may be aligned with partner companies for government contracting.\n* more details: https://careers.kbr.com/us/en/kbr\\-benefits?\n\n**ready to join kbr and develop your career in cloud engineering while contributing to innovative and impactful projects? apply today!**\n\n***kbr partners with several other companies to fulfill its requirements as a government contractor. the selected subcontracting companies align their benefits as closely as possible to those above.***\n\n**kbr benefits**\n\nkbr offers a selection of competitive lifestyle benefits which could include 401k plan with company match, medical, dental, vision, life insurance, ad\\&d, flexible spending account, disability, paid time off, or flexible work schedule. we support career advancement through professional training and development.\n\n\nclick here to learn more: **kbr benefits**\n\n\n\\#li\\-em2\n\n\nbelong, connect and grow at kbr  \n\n  \n\nat kbr, we are passionate about our people and our zero harm culture. these inform all that we do and are at the heart of our commitment to, and ongoing journey toward being a people first company. that commitment is central to our team of team\u2019s philosophy and fosters an environment where everyone can belong, connect and grow. we deliver \u2013 together.\n\n\nkbr is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "2026 Summer Intern - Data Engineering",
        "company": "Western Digital",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-22",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Docker",
            "CI/CD",
            "Jenkins",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=77ed82398fcdee21",
        "description": "**company description**  \n\nat western digital, our vision is to power global innovation and push the boundaries of technology to make what you thought was once impossible, possible.\n\n\nat our core, western digital is a company of problem solvers. people achieve extraordinary things given the right technology. for decades, we\u2019ve been doing just that\u2014our technology helped people put a man on the moon and capture the first\\-ever picture of a black hole.\n\n\nwe offer an expansive portfolio of technologies, hdds, and platforms for business, creative professionals, and consumers alike under our western digital\u00ae, wd\u00ae, and wd\\_black\u2122.\n\n\nwe are a key partner to some of the largest and highest\\-growth organizations in the world. from enabling systems to make cities safer and more connected, to powering the data centers behind many of the world\u2019s biggest companies and hyperscale cloud providers, to meeting the massive and ever\\-growing data storage needs of the ai era, western digital is fueling a brighter, smarter future.\n\n\ntoday\u2019s exceptional challenges require your unique skills. together, we can build the future of data storage.\n\n **job description*** build and maintain dashboards, automated reports, and test scripts, integrating outputs into internal systems.\n* experiment with ai tools to enhance access to reports, documentation, and knowledge bases.\n* contribute production\\-quality code, lead scoped projects across data, automation, backend, or visualization, and learn hands\\-on with mentorship from engineers.\n\n*this position is part of our early career program at wd. our early career program is designed to support individuals beginning their professional career by providing the foundational training through a structured onboarding, mentorship, and development curriculum.*\n\n **qualifications*** currently pursuing a bs/ms in computer science, data science, engineering, or a related field.\n* hands\\-on experience with r, sql, or python through coursework, academic projects, research, or personal projects.\n* strong analytical and problem\\-solving skills, with attention to detail demonstrated in academic work or project experience.\n* ability to communicate clearly and collaborate effectively with teammates, including students, researchers, and mentors.\n* interest in or curiosity about the data storage (hdd) industry, gained through classes, projects, or independent learning.\n* exposure to generative ai, nlp, or other ai/ml tools through coursework, labs, or projects.\n* familiarity with business intelligence tools or cloud platforms (e.g., google cloud, aws) through academic or project\\-based use.\n* basic understanding of version control and ci/cd concepts (e.g., git; exposure to jenkins, docker is a plus).\n* experience with additional programming languages such as c\\+\\+, java, or javascript/node.js, gained through coursework, side projects, or research.\n\n **additional information**  \n\nwestern digital is committed to providing equal opportunities to all applicants and employees and will not discriminate against any applicant or employee based on their race, color, ancestry, religion (including religious dress and grooming standards), sex (including pregnancy, childbirth or related medical conditions, breastfeeding or related medical conditions), gender (including a person\u2019s gender identity, gender expression, and gender\\-related appearance and behavior, whether or not stereotypically associated with the person\u2019s assigned sex at birth), age, national origin, sexual orientation, medical condition, marital status (including domestic partnership status), physical disability, mental disability, medical condition, genetic information, protected medical and family care leave, civil air patrol status, military and veteran status, or other legally protected characteristics. we also prohibit harassment of any individual on any of the characteristics listed above. our non\\-discrimination policy applies to all aspects of employment. we comply with the laws and regulations set forth in the \"know your rights: workplace discrimination is illegal\u201d poster. our pay transparency policy is available here.\n\n\nwestern digital thrives on the power and potential of diversity. as a global company, we believe the most effective way to embrace the diversity of our customers and communities is to mirror it from within. we believe the fusion of various perspectives results in the best outcomes for our employees, our company, our customers, and the world around us. we are committed to an inclusive environment where every individual can thrive through a sense of belonging, respect and contribution.\n\n\nwestern digital is committed to offering opportunities to applicants with disabilities and ensuring all candidates can successfully navigate our careers website and our hiring process. please contact us at jobs.accommodations@wdc.com to advise us of your accommodation request. in your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying.\n\n\nbased on our experience, we anticipate that the application deadline will be 4/7/26, although we reserve the right to close the application process sooner if we hire an applicant for this position before the application deadline. if we are not able to hire someone from this role before the application deadline, we will update this posting with a new anticipated application deadline.\n\n\n\\#li\\-mt\n\n**compensation \\& benefits details**\n\n* an employee\u2019s pay position within the salary range may be based on several factors including but not limited to (1\\) relevant education; qualifications; certifications; and experience; (2\\) skills, ability, knowledge of the job; (3\\) performance, contribution and results; (4\\) geographic location; (5\\) shift; (6\\) internal and external equity; and (7\\) business and organizational needs.\n* the salary range is what we believe to be the range of possible compensation for this role at the time of this posting. we may ultimately pay more or less than the posted range and this range is only applicable for jobs to be performed in california, colorado, new york or remote jobs that can be performed in california, colorado and new york. this range may be modified in the future.\n* if your position is non\\-exempt, you are eligible for overtime pay pursuant to company policy and applicable laws. you may also be eligible for shift differential pay, depending on the shift to which you are assigned.\n* you will be eligible to be considered for bonuses under **either** western digital\u2019s short term incentive plan (\u201csti plan\u201d) or the sales incentive plan (\u201csip\u201d) which provides incentive awards based on company and individual performance, depending on your role and your performance. you may be eligible to participate in our annual long\\-term incentive (lti) program, which consists of restricted stock units (rsus) or cash equivalents, pursuant to the terms of the lti plan. please note that not all roles are eligible to participate in the lti program, and not all roles are eligible for equity under the lti plan. rsu awards are also available to eligible new hires, subject to western digital\u2019s standard terms and conditions for restricted stock unit awards.\n* we offer a comprehensive package of benefits including paid vacation time; paid sick leave; medical/dental/vision insurance; life, accident and disability insurance; tax\\-advantaged flexible spending and health savings accounts; employee assistance program; other voluntary benefit programs such as supplemental life and ad\\&d, legal plan, pet insurance, critical illness, accident and hospital indemnity; tuition reimbursement; transit; the applause program; employee stock purchase plan; and the western digital savings 401(k) plan.\n* **note:** no amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. the amount and availability of any bonus, commission, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the company's sole discretion unless and until paid and may be modified at the company\u2019s sole discretion, consistent with the law.\n\n**notice to candidates:** please be aware that western digital and its subsidiaries will never request payment as a condition for applying for a position or receiving an offer of employment. should you encounter any such requests, please report it immediately to western digital ethics helpline or email compliance@wdc.com.",
        "scrapped_date": "2026-02-22"
    },
    {
        "title": "Senior Data Engineer",
        "company": "Deloitte",
        "location": "San Jose, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 20.0,
        "matched_keywords": [
            "RAG",
            "Glue",
            "Redshift",
            "BigQuery",
            "Synapse",
            "Git",
            "Snowflake",
            "Databricks",
            "BigQuery",
            "Redshift"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8ed48e228bf802be",
        "description": "**role overview:** as a senior data engineer , you will actively engage in your engineering craft, taking a hands\\-on approach to multiple high\\-visibility projects. your expertise will be pivotal in delivering solutions that delight customers and users, while also driving tangible value for deloitte's business investments. you will leverage your extensive engineering craftmanship across multiple programming languages and modern frameworks, consistently demonstrating your strong track record in delivering high\\-quality, outcome\\-focused solutions. the ideal candidate will be a dependable team player, collaborating with cross\\-functional teams to design, develop, and deploy advanced software solutions.  \n\n  \n\n**key responsibilities:**  \n\n* **outcome\\-driven accountability:** embrace and drive a culture of accountability for customer and business outcomes. develop engineering solutions that solve complex problems with valuable outcomes, ensuring high\\-quality, lean designs and implementations.\n* **technical leadership and advocacy:** serve as the technical advocate for products, ensuring code integrity, feasibility, and alignment with business and customer goals. lead requirement analysis, component design, development, unit testing, integrations, and support.\n* **engineering craftsmanship:** maintain accountability for the integrity of code design, implementation, quality, data, and ongoing maintenance and operations. stay hands\\-on, self\\-driven, and continuously learn new approaches, languages, and frameworks. create technical specifications, and write high\\-quality, supportable, scalable code ensuring all quality kpis are met or exceeded. demonstrate collaborative skills to work effectively with diverse teams.\n* **customer\\-centric engineering:** develop lean engineering solutions through rapid, inexpensive experimentation to solve customer needs. engage with customers and product teams before, during, and after delivery to ensure the right solution is delivered at the right time.\n* **incremental and iterative delivery:** adopt a mindset that favors action and evidence over extensive planning. utilize a learning\\-forward approach to navigate complexity and uncertainty, delivering lean, supportable, and maintainable solutions.\n* **cross\\-functional collaboration and integration:** work collaboratively with empowered, cross\\-functional teams including product management, experience, and delivery. integrate diverse perspectives to make well\\-informed decisions that balance feasibility, viability, usability, and value. foster a collaborative environment that enhances team synergy and innovation.\n* **advanced technical proficiency:** possess deep expertise in modern software engineering practices and principles, including agile methodologies and devsecops to deliver daily product deployments using full automation from code check\\-in to production with all quality checks through sdlc lifecycle. strive to be a role model, leveraging these techniques to optimize solutioning and product delivery. demonstrate understanding of the full lifecycle product development, focusing on continuous improvement, and learning.\n* **domain expertise:** quickly acquire domain\\-specific knowledge relevant to the business or product. translate business/user needs, architectures, and data designs into technical specifications and code. be a valuable, flexible, and dedicated team member, supportive of teammates, and focused on quality and tech debt payoff.\n* **effective communication and influence:** exhibit exceptional communication skills, capable of articulating complex technical concepts clearly and compellingly. inspire and influence teammates and product teams through well\\-structured arguments and trade\\-offs supported by evidence. create coherent narratives that align technical solutions with business objectives.\n* **engagement and collaborative co\\-creation:** engage and collaborate with product engineering teams at all organizational levels, including customers as needed. build and maintain constructive relationships, fostering a culture of co\\-creation and shared momentum towards achieving product goals. align diverse perspectives and drive consensus to create feasible solutions.\n\n  \n\n**the team:** us deloitte technology product engineering has modernized software and product delivery, creating a scalable, cost\\-effective model that focuses on value/outcomes that leverages a progressive and responsive talent structure. as deloitte's primary internal development team, product engineering delivers innovative digital solutions to businesses, service lines, and internal operations with proven bottom\\-line results and outcomes. it helps power deloitte's success. it is the engine that drives deloitte, serving many of the world's largest, most respected companies. we develop and deploy cutting\\-edge internal and go\\-to\\-market solutions that help deloitte operate effectively and lead in the market. our reputation is built on a tradition of delivering with excellence.  \n\n  \n\n**the successful candidate will possess:**  \n\n* excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care.\n\n  \n\n**required qualifications:**  \n\n* a bachelor's degree in computer science, software engineering, or a related discipline. experience is the most relevant factor.\n* 5\\+ years of experience with sql databases (e.g., postgresql, azure sql, amazon aurora, google cloud sql, or mysql) and nosql databases (e.g., mongodb, amazon dynamodb, azure documentdb, or redis).\n* 3\\+ years of experience with data etl and elt tools (e.g., aws glue, azure data factory, or azure synapse) and data warehousing tools (e.g., sap hana, snowflake, databricks, amazon redshift, or google cloud bigquery).\n* 3\\+ years of experience with programming languages and frameworks such as .net, nodejs, or python.\n* 1\\+ years of experience leveraging ai/genai/llm technologies in data engineering contexts.\n* prior software engineering experience with an understanding of business context diagrams (bcds), sequence, activity, state, entity relationship, and data flow diagrams, as well as oop/ood, data structures, algorithms, and code instrumentation.\n* prior experience with cloud\\-native engineering, using faas, paas, or micro\\-services on any of the cloud hyperscalers such as azure, aws, or gcp.\n* prior experience using one or more of the methodologies and tools such as xp, lean, devsecops, sre, azure devops, github, sonarqube, etc., to deliver high\\-quality products rapidly.\n\n  \n\n**other:**  \n\n* ability to travel 10%, on average, based on the work you do and products you build.\n* limited immigration sponsorship may be available.\n\n  \n\nfor individuals assigned and/or hired to work in california, cleveland, colorado, hawaii, illinois, new jersey, maryland, massachusetts, minnesota, nevada, new york state, washington state, and washington, dc, deloitte is required by law to include a reasonable estimate of the compensation range for this role. this compensation range is specific to california and takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $103,900 to $173,300\\.  \n\n  \n\nyou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.  \n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html  \n\n  \n\nea\\_exphire  \n\nea\\_its\\_exphire  \n\npxe\\_jobs",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Ai/ML Engineer",
        "company": "Johnson Controls",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 20.0,
        "matched_keywords": [
            "AI Engineer",
            "Data Scientist",
            "Generative AI",
            "LangChain",
            "RAG",
            "LLaMA",
            "FAISS",
            "PyTorch",
            "Azure ML",
            "Docker"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1a93129ac0f94b09",
        "description": "johnson controls international (jci) is looking for a machine learning / platform engineer to join our growing ai and data platform team. this role is pivotal in enabling enterprise\\-scale ml and generative ai capabilities by building secure, scalable, and automated infrastructure on azure using terraform and azure devops.\n\n\nyou\u2019ll work at the intersection of ml, devops, and cloud engineering\u2014building the foundation that supports real\\-time llm inference, retraining, orchestration, and integration across jci\u2019s product and operations landscape.\n\n**how you will do it**\n\n**ml platform engineering \\& mlops (azure\\-focused)**\n\n* build and manage end\\-to\\-end ml/llm pipelines on **azure ml** using **azure devops** for ci/cd, testing, and release automation.\n* operationalize llms and generative ai solutions (e.g., gpt, llama, claude) with a focus on automation, security, and scalability.\n* develop and manage infrastructure as code using **terraform**, including provisioning compute clusters (e.g., azure kubernetes service, azure machine learning compute), storage, and networking.\n* implement robust model lifecycle management (versioning, monitoring, drift detection) with azure\\-native mlops components.\n\n**infrastructure \\& cloud architecture**\n\n* design highly available and performant serving environments for llm inference using **azure kubernetes service (aks)** and **azure functions** or **app services**.\n* build and manage rag pipelines using vector databases (e.g., azure cognitive search, redis, faiss) and orchestrate with tools like **langchain** or **semantic kernel**.\n* ensure security, logging, role\\-based access control (rbac), and audit trails are implemented consistently across environments.\n\n**automation \\& ci/cd pipelines**\n\n* build reusable **azure devops pipelines** for deploying ml assets (data pre\\-processing, model training, evaluation, and inference services).\n* use terraform to automate provisioning of azure resources, ensuring consistent and compliant environments for data science and engineering teams.\n* integrate automated testing, linting, monitoring, and rollback mechanisms into the ml deployment pipeline.\n\n**collaboration \\& enablement**\n\n* work closely with data scientists, cloud engineers, and product teams to deliver production\\-ready ai features.\n* contribute to solution architecture for real\\-time and batch ai use cases, including conversational ai, enterprise search, and summarization tools powered by llms.\n* provide technical guidance on cost optimization, scalability patterns, and high\\-availability ml deployments.\n\n**qualifications \\& skills**\n\n**required experience**\n\n* bachelor\u2019s or master\u2019s in computer science, engineering, or a related field.\n* 5\\+ years of experience in ml engineering, mlops, or platform engineering roles.\n* strong experience deploying machine learning models on **azure** using **azure ml** and **azure devops**.\n* proven experience managing infrastructure as code with **terraform** in production environments.\n\n**technical proficiency**\n\n* proficiency in **python** (pytorch, transformers, langchain) and **terraform**, with scripting experience in bash or powershell.\n* experience with **docker** and **kubernetes**, especially within azure (aks).\n* familiarity with ci/cd principles, model registry, and ml artifact management using **azure ml** and **azure devops pipelines**.\n* working knowledge of vector databases, caching strategies, and scalable inference architectures.\n\n**soft skills \\& mindset**\n\n* systems thinker who can design, implement, and improve robust, automated ml systems.\n* excellent communication and documentation skills\u2014capable of bridging platform and data science teams.\n* strong problem\\-solving mindset with a focus on delivery, reliability, and business impact.\n\n**preferred qualifications**\n\n* experience with **llmops**, prompt orchestration frameworks (langchain, semantic kernel), and open\\-weight model deployment.\n* exposure to **smart buildings, iot**, or edge\\-ai deployments.\n* understanding of governance, privacy, and compliance concerns in enterprise genai use cases.\n* certification in azure (e.g., azure solutions architect, azure ai engineer, terraform associate) is a plus.\n\n\nhiring salary range: $85,000 \\- 107,000 (salary to be determined by the education, experience, knowledge, skills, and abilities of the applicant, internal equity, location and alignment with market data.) this position includes a competitive benefits package. for details, please visit the about us tab on the johnson controls careers site at https://jobs.johnsoncontrols.com/about\\-us",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist / Data Science Specialist",
        "company": "Adidev Technologies Inc",
        "location": "Dallas, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 18.9,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "PyTorch",
            "AWS SageMaker",
            "S3",
            "EC2",
            "MLflow",
            "Jenkins",
            "Git",
            "Databricks"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=91cd87d39ed00187",
        "description": "**adidev technologies inc**\n\n\n**www.adidevtechnologies.com**\n\n\n**urgent hire \\- hiring process \\- 24\\-48 hours!**\n\n\n\nadidev technologies is seeking 2 yrs of relevant experience in data science. a project can last anywhere from 6 months to 18 months. salary varies depending on experience, and we are in search of candidates looking to start as soon as possible. excellent written and oral communication are required as is the ability to work well in a team environment.\n\n\n\nif you are looking for a new challenge and are ready to make an impact on a growing team, then this will be a perfect fit. as a data scientist / data analyst / data science specialist for adidev technologies inc., you will be enhancing and debugging large\\-scale applications for one of our well\\-known clients.\n\n\n\nadidev technologies is a growing software consulting company that is constantly expanding. as we are working with renowned clients and ready to take on new ones, we are seeking brilliant software engineers. not only do we offer a great team to work with, but we also offer you an opportunity to make an immediate impact and get rewarded accordingly\n\n  \n\n\n**job responsibilities**\n\n  \n\n\n* perform exploratory data analysis, data verification, and cleaning, robust \\& reproducible statistical analysis with comprehensive reporting of results.\n* develop novel ways to help business partners achieve objectives through analysis and modeling\n* curate and connect external data sets for broad enterprise\\-wide analytic usage\n* be a storyteller to explain the 'why and how' of your data\\-driven recommendations to cross\\-functional teams\n* utilize machine learning to create repeatable, dynamic, and scalable models\n* have a passion to advocate and educate on the value and importance of data\\-driven decision making and analytical methods\n* responsible for implementing robust testing strategies leading to model optimizations, program effectiveness, and attribution of marketing mix elements\n\n  \n\n\n\n\n**required skills**\n\n  \n\n\n* experience designing, managing, developing, and analyzing multiple large complex data models spanning multiple disparate sources\n* comfortable in basic statistical analysis, modeling, clustering, and data mining techniques to identify trends and insights\n* proficiency in using query languages, such as sql, spark data frame api, etc. \\- an advantage\n* lead the conception, development, and evaluation of new deep learning, natural language processing, and machine learning products.\n* knowledge and experience in statistical and data mining techniques: glm/regression, random forest, boosting, trees, text mining, social network analysis, etc.\n* experience in web technologies and frameworks (tensorflow, pytorch, sci\\-kit learn)\n* familiarity with mlops frameworks like mlflow.\n* familiarity with sql.\n* some familiarity with aws lambda, aws sagemaker, jenkins, and databricks.\n* hands\\-on experience with aws cloud computing services (iam, ec2, lambda, s3, dynamodb, rds, cloudformation) for machine learning applications\n* experience with python and r, comfortable working with dataframes\n* ability to incorporate a variety of data sources in an analysis (hdfs, file, database, json, html, etc)\n* experience with technologies used to build analytics dashboards/applications, such as rshiny, dash, and streamlit\n* experience working with version control systems such as git and svn\n* experience writing complex sql queries\n* understanding data warehousing and databases is critical\n* experience with business intelligence/ visualization tools such as qlik, tableau or powerbi\n\n  \n\n\n\n\n**qualifications**\n\n  \n\n* + degree in data science, computer science, engineering, math, or statistics preferred\n* at least 2 yrs of relevant experience in data science\n* experience with python\n* experience with machine learning, deep learning\n* mathematical and statistical background\n* experience with hadoop, hive, and/or spark\n\n  \n\n\n\n\n**benefits**\n\n\n* competitive salary\n* paid relocation\n* remote support\n* guaranteed regular salary reviews\n* job type: w2 or contract 1099 (full\\-time \\- 40 hours)",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Junior Software Engineer",
        "company": "MetLife",
        "location": "Whippany, NJ, US USA",
        "posted_at": "2026-02-20",
        "score": 17.8,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "Copilot",
            "Hugging Face",
            "Prompt Engineering",
            "TensorFlow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=27a8a2a71007930a",
        "description": "location(s)\n* posting location: whippany, new jersey\n\n  \n\ncountry\nunited states\nworking schedule\nfull\\-time\nwork arrangement\nhybrid\nrelocation assistance available\nno\nposted date\n20\\-feb\\-2026\njob id\n15827\n### **description and requirements**\n\n**the team you will join**\nwhen you join metlife\u2019s global technology team, you\u2019ll be part of a forward\\-thinking group dedicated to shaping the future of digital solutions for customers worldwide. you\u2019ll develop, maintain and support technology applications and delivery, leveraging ai, automation, and contemporary ways of working to enhance experiences and drive business outcomes. your work will simplify complex processes, improve tech resiliency, and ensure high\\-performing, seamless solutions that power life\u2019s most important moments. in this dynamic environment, you\u2019ll collaborate with talented peers across teams and functions, expanding your skills in impactful ways. ready to push boundaries and set new industry standards? join us and help drive the future of technology forward. **the opportunity**\nas an early career technologist, you will contribute to the development and support of software applications and platforms that help people build a more confident future. this is an ideal opportunity for new college graduates and early\\-career technologists passionate about building scalable, secure, and intelligent solutions. you\u2019ll work in a collaborative, agile environment and gain exposure to modern software engineering practices and ai\\-enabling technologies.  \n\nemployees hired for this role are expected to start full\\-time in whippany, nj in mid\\-june 2026\\. **how you\u2019ll help us build a confident future (key responsibilities)*** assist in gathering and refining product requirements and user stories.\n* support the design, development, testing, and deployment of full stack software applications (ui, api, data) while leveraging ai tools.\n* participate in peer reviews of code, solution designs, and configurations.\n* help investigate and resolve production incidents and service requests.\n* collaborate with cross\\-functional teams to identify innovative solutions.\n* contribute to the integration of ai capabilities into applications (e.g., genai apis, ml models).\n\n **what you need to succeed (required qualifications)*** a bachelor's degree in computer science, information systems, engineering, or a closely related discipline must be conferred prior to start date.\n* internship or academic project experience in software development or ai/ml.\n* demonstrated ability to quickly learn and apply new skills to deliver high quality software as evidenced by academic or project experience.\n* proficiency in at least one programming language (e.g., java, python, c\\#, javascript/typescript, cobol, etc.).\n* basic understanding of software design principles and coding best practices.\n* familiarity with database concepts, design, and development.\n* familiarity with cloud\\-native technologies (e.g., azure, aws, gcp).\n* basic knowledge of secure coding practices and automated testing.\n* strong communication, collaboration, and problem\\-solving skills.\n\n **what can give you an edge (preferred qualifications)*** understanding of agile practices, devsecops, and ci/cd pipelines.\n* familiarity with tools like github copilot, azure devops, docker, or kubernetes.\n* exposure to ai/ml and other technology concepts such as:\n\t+ prompt engineering and llm apis (e.g., azure openai)\n\t+ ai sdks and frameworks (e.g., langchain, hugging face, tensorflow, semantic kernel)\n\t+ responsive ui/ux development\n\t+ microservice vs. monolithic architectures\n\t+ containerization\n\t+ relational vs. nosql databases\n\nlocation expectation: this is a hybrid role **requiring a minimum of 3 days per week** in office.\nthe expected salary range for this position is 64100\\-76000\\. this role may also be eligible for annual short\\-term incentive compensation. all incentives and benefits are subject to the applicable plan terms.\n**benefits we offer**  \n\nour u.s. benefits address holistic well\\-being with programs for physical and mental health, financial wellness, and support for families. we offer a comprehensive health plan that includes medical/prescription drug and vision, dental insurance, and no\\-cost short\\- and long\\-term disability. we also provide company\\-paid life insurance and legal services, a retirement pension funded entirely by metlife and 401(k) with employer matching, group discounts on voluntary insurance products including auto and home, pet, critical illness, hospital indemnity, and accident insurance, as well as employee assistance program (eap) and digital mental health programs, parental leave, volunteer time off, tuition assistance and much more!\n**about metlife**  \n\nrecognized on fortune magazine's list of the \"world's most admired companies\", fortune world\u2019s 25 best workplaces\u2122, as well as the fortune 100 best companies to work for\u00ae, metlife, through its subsidiaries and affiliates, is one of the world\u2019s leading financial services companies; providing insurance, annuities, employee benefits and asset management to individual and institutional customers. with operations in more than 40 markets, we hold leading positions in the united states, latin america, asia, europe, and the middle east.\n\n\nour purpose is simple \\- to help our colleagues, customers, communities, and the world at large create a more confident future. united by purpose and guided by our core values \\- win together, do the right thing, deliver impact over activity, and think ahead \\- we\u2019re inspired to transform the next century in financial services. at metlife, it\u2019s \\#alltogetherpossible. join us!\n\n  \n\n  \n\n*metlife is an equal opportunity employer. all employment decisions are made without regards to race, color, national origin, religion, creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, age, disability, marital or domestic/civil partnership status, genetic information, citizenship status (although applicants and employees must be legally authorized to work in the united states), uniformed service member or veteran status, or any other characteristic protected by applicable federal, state, or local law (\u201cprotected characteristics\u201d).* *if you need an accommodation due to a disability, please email us at accommodations@metlife.com. this information will be held in confidence and used only to determine an appropriate accommodation for the application process.*  \n\n  \n\n*metlife maintains a drug\\-free workplace.*\n64100\\-76000",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Software AI Engineer",
        "company": "Southern New Hampshire University",
        "location": "Remote, US USA",
        "posted_at": "2026-02-19",
        "score": 16.7,
        "matched_keywords": [
            "AI Engineer",
            "Copilot",
            "S3",
            "Glue",
            "Athena",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "PostgreSQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=eaf894cd20490ad2",
        "description": "**description**\n---------------\n\n\nsouthern new hampshire university is a team of innovators. world changers. individuals who believe in progress with purpose. since 1932, our people\\-centered strategy has defined us \u2014 and helped us grow a team that now serves over 180,000 learners worldwide.\n\n\nour mission to transform lives is made possible by talented people who bring diverse industry experience, backgrounds and skills to the university. and today, we're ready to expand our reach. all we need is you.\n\n**make an impact \u2014 from near or far**\n\n\nat snhu, you'll have the option to work remotely in the following states: alabama, arizona, arkansas, delaware, florida, georgia, hawaii, idaho, indiana, iowa, kansas, kentucky, louisiana, maine, maryland, massachusetts, michigan, mississippi, missouri, nebraska, new hampshire, new mexico, north carolina, north dakota, ohio, oklahoma, south carolina, south dakota, tennessee, texas, utah, vermont, virginia, west virginia, wisconsin and wyoming.\n\n\nwe ask that our remote employees have access to a reliable internet connection and a dedicated, properly equipped workspace that is free of distractions. employees must reside in, and work from, one of the above approved states.\n\n**the opportunity**\n\n\nsouthern new hampshire university (snhu) is looking for an experienced senior python backend engineer. you will report directly to the vice president of ai engineering and you will engineer the backend systems that power our agentic ai learning platform, empowering human\\-centered learning for the age of ai.\n\n\nas a backend and platform engineer with data engineering skills, you'll build the infrastructure that powers our ai learning platform. you'll also ensure our data science team has the systems they need to measure the effectiveness and guide personalization.\n\n\nyou will work remotely from any of our approved states \\#li\\-remote\n\n**what you'll do:**\n\n* **agentic backend and data platform\\*\\***\n* you will design and implement scalable apis for agent orchestration and learner interaction\n* you will build data pipelines that feed agent evaluation and continuous improvement cycles\n* you will create event streaming infrastructure for real\\-time learner interaction analysis\n* you will ensure data quality and accessibility for training and evaluating learning agents\n* you will improve data infrastructure for cost and performance in close coordination with data and ml teams\n* this is a remote position\n\n**measurement and evaluation**\n\n* design and implement rigorous evaluation frameworks to measure agent performance and improve cycles\n* develop llm evaluation processes and perform error analysis to identify systemic improvements for agentic learning systems\n* support instrumentation that makes evaluation relevant (metrics, traces/logs, and analysis workflows)\n\n**on\\-call and incident response**\n\n* participate in on\\-call rotation and incident response for learning platform reliability\n* contribute to runbooks, postmortems, and reliability improvements as we evolve our operating model\n\n**what we're looking for:**\n\n* 5\\+ years production python experience at scale\n* aws expertise (lambda, dynamodb, s3, glue, and athena)\n* experience with a relational database such as mysql, postgresql, or oracle\n* experience with agentic ai architectures and evaluation frameworks\n* data pipeline development (streaming and batch)\n* infrastructure as code experience (cdk, terraform)\n* production saas experience, including participation in on\\-call / incident response\n* experience in github actions or similar ci/cd platforms\n* experience communicating updates and resolutions to customers and other partners)\n\n**ai\\-augmented development:**\n\n* active user of ai development tools (github copilot, cursor, claude code, codex) with personal projects and evolution over the past 12 months\n* demonstrated examples of ai\\-augmented productivity gains\n* enthusiasm for pushing boundaries of ai\\-assisted engineering\n\n\nwe believe real innovation comes from inclusion \\- where different experiences, perspectives and talents are celebrated. so if you're wondering whether snhu is right for you, take the leap and apply. you might be just the person we're looking for.\n\n**compensation**\n\n\nthe annual pay range for this position is $113,908\\.00 \\- $182,287\\.00\\. actual offer will be based on skills, qualifications, experience and internal equity, in addition to relevant business considerations. we expect this position to be hired in the following target hiring range $125,868\\.00 \\- $170,293\\.00\\.**exceptional benefits (because you\u2019re exceptional)**\n\n\nyou\u2019re the whole package. your benefits should be, too. as a full\\-time employee at snhu, you\u2019ll get:\n\n* high\\-quality, low\\-deductible medical insurance\n* low to no\\-cost dental and vision plans\n* 5 weeks of paid time off (plus almost a dozen paid holidays)\n* employer\\-funded retirement\n* free tuition program\n* parental leave\n* mental health and wellbeing resources",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Software AI Engineer - Fullstack",
        "company": "Southern New Hampshire University",
        "location": "Remote, US USA",
        "posted_at": "2026-02-19",
        "score": 16.7,
        "matched_keywords": [
            "AI Engineer",
            "LangChain",
            "LLaMA",
            "Copilot",
            "S3",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "PostgreSQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=04eec46adb7daf9d",
        "description": "**description**\n---------------\n\n\nsouthern new hampshire university is a team of innovators. world changers. individuals who believe in progress with purpose. since 1932, our people\\-centered strategy has defined us \u2014 and helped us grow a team that now serves over 180,000 learners worldwide.\n\n\nour mission to transform lives is made possible by talented people who bring diverse industry experience, backgrounds and skills to the university. and today, we're ready to expand our reach. all we need is you.\n\n**make an impact \u2014 from near or far**\n\n\nat snhu, you'll have the option to work remotely in the following states: alabama, arizona, arkansas, delaware, florida, georgia, hawaii, idaho, indiana, iowa, kansas, kentucky, louisiana, maine, maryland, massachusetts, michigan, mississippi, missouri, nebraska, new hampshire, new mexico, north carolina, north dakota, ohio, oklahoma, south carolina, south dakota, tennessee, texas, utah, vermont, virginia, west virginia, wisconsin and wyoming.\n\n\nwe ask that our remote employees have access to a reliable internet connection and a dedicated, properly equipped workspace that is free of distractions. employees must reside in, and work from, one of the above approved states.\n\n**the opportunity**\n\n\nsouthern new hampshire university (snhu) is looking for an experienced senior ai engineer, platform (full stack). you will report directly to the vice president of ai engineering. your responsibilities will include engineering the backend systems that power our agentic ai learning platform. this will involve designing agent orchestration, which includes planning, memory, and tools. additionally, you will shape the evaluation strategy for llms and agents, empowering human\\-centered learning for the age of ai.\n\n\nyou're backend\\-first but product\\-minded: when needed, you'll also ship lightweight web (and potentially mobile) changes to deliver end\\-to\\-end slices without being the ui specialist.\n\n\nyou will also help build the foundations for production readiness (observability, safe deployments, performance/cost hygiene) and help us evolve toward sustainable on\\-call operations as usage grows.\n\n\nyou will work remotely from any of our approved states \\#li\\-remote\n\n**what you'll do:**\n\n**agentic backend and platform (primary)**\n\n* you will design and implement scalable apis/services for developing ai agents, including orchestration, memory, tool use, and learner interaction\n* you will implement rigorous evaluation frameworks to improve agent performance (offline and in\\-product), and build the instrumentation to make results actionable improvement cycles\n* you will build data flows/pipelines that allow real\\-time personalization and learning insights\n* you will ensure data quality and accessibility for training and evaluating learning agents\n* you will improve developer velocity with solid api design, testing practices, and realistic architecture choices\n* this is a remote position\n\n**full\\-stack product delivery (meaningful, but not \"frontend lead\")**\n\n* you will ship end\\-to\\-end product slices: implement backend capabilities and the corresponding lightweight web ui (and occasionally mobile) to unblock product progress\n* you will build lightweight internal tools/admin surfaces for evaluation, debugging, content/ops workflows, and product iteration\n* you will collaborate with product/design to deliver high\\-quality user experiences while keeping scope and maintainability sane\n\n**site reliability foundations**\n\n* you will help establish the basics of production readiness: monitoring, dashboards, logging/tracing, and performance/cost hygiene\n* you will contribute to automated testing and deployment pipelines for rapid, safe iteration (ci/cd, rollbacks, environment hygiene)\n* you will participate in incident response when needed and help shape runbooks, postmortems, and an on\\-call model appropriate for our current stage\n\n**what we're looking for:**\n\n* 5\\+ years production python experience at scale (or equivalent)\n* aws expertise (lambda, s3, and cloudwatch)\n* familiarity with compound systems combining llms, ml, and traditional software components\n* experience with agentic ai architectures (e.g., crewai, langgraph/langchain, autogen, llamaindex, mcp, a2a, acp, dspy, or custom frameworks)\n* hands\\-on experience with evaluation methodologies for llms and agent\\-based ai systems\n* infrastructure as code experience (cdk, terraform, or similar)\n* github actions or similar ci/cd platforms\n* experience communicating updates and resolutions to customers and other partners)\n* comfortable working across the stack when needed. read and write typescript, and ship small ui features using react/next.js or similar technologies. additionally, the ability to contribute occasional mobile changes in ios/swift and android/kotlin.\n* experience building production readiness / sre foundations (instrumentation/observability basics, safe deployments, performance/cost hygiene)\n\n**ai\\-augmented development:**\n\n* active user of ai development tools (github copilot, cursor, claude code, codex) with personal projects and evolution over the past 12 months\n* demonstrated examples of ai\\-augmented productivity gains\n* enthusiasm for pushing boundaries of ai\\-assisted engineering\n* experience with a relational database such as mysql, postgresql, or oracle\n\n\nwe believe real innovation comes from inclusion \\- where different experiences, perspectives and talents are celebrated. so if you're wondering whether snhu is right for you, take the leap and apply. you might be just the person we're looking for.\n\n**compensation**\n\n\nthe annual pay range for this position is $113,908\\.00 \\- $182,287\\.00\\. actual offer will be based on skills, qualifications, experience and internal equity, in addition to relevant business considerations. we expect this position to be hired in the following target hiring range $125,868\\.00 \\- $170,293\\.00\\.**exceptional benefits (because you\u2019re exceptional)**\n\n\nyou\u2019re the whole package. your benefits should be, too. as a full\\-time employee at snhu, you\u2019ll get:\n\n* high\\-quality, low\\-deductible medical insurance\n* low to no\\-cost dental and vision plans\n* 5 weeks of paid time off (plus almost a dozen paid holidays)\n* employer\\-funded retirement\n* free tuition program\n* parental leave\n* mental health and wellbeing resources",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI Engineer II - Blue Ring",
        "company": "Blue Origin",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-20",
        "score": 16.7,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "FAISS",
            "Pinecone",
            "TensorFlow",
            "PyTorch",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "CI/CD"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ed2387d63f350d09",
        "description": "application close date:\napplications will be accepted on an ongoing basis until the requisition is closed.\nat blue origin, we envision millions of people living and working in space for the benefit of earth. we\u2019re working to develop reusable, safe, and low\\-cost space vehicles and systems within a culture of safety, collaboration, and inclusion. join our team of problem solvers as we add new chapters to the history of spaceflight!\nthe role is part of the in\\-space systems business unit, which is focused on addressing two of the most compelling challenges in spaceflight today: space infrastructure and increasing mobility on\\-orbit.\nblue ring is blue origin's multi\\-mission space mobility platform, and this team is building the next generation of intelligent ground systems that will redefine how satellite fleets are operated\u2014moving beyond manual monitoring and reactive processes.\nwe are seeking an ai engineer ii to join the blue ring software team. you will contribute to the ai and machine learning capabilities embedded in blue ring's ground system, spanning multi\\-agent monitoring, conversational knowledge retrieval, and ml\\-driven anomaly detection that empower satellite operations at fleet scale. in this role, you will help build agent tooling, maintain rag knowledge pipelines, develop telemetry ml models, and support production ai services that operators rely on daily.\nrole and responsibilities:\nas a member of the blue ring ai/ml engineering team, you will contribute to the ai/ml systems that form the intelligence layer for blue origin's satellite fleet operations. working alongside senior engineers, you will help build, test, and deploy the agents, knowledge pipelines, and ml models that reduce operator cognitive burden and enable fleet\\-scale management. you will be expected to:\nmulti\\-agent system development:* implement and test agent components within the multi\\-agent architecture for telemetry analysis, anomaly detection, and reporting\n* build and maintain integrations that connect agents to ground system services\n* develop agent tooling, prompt templates, and orchestration logic for llm service integrations\n* build and maintain rag pipeline components including document ingestion, embedding generation, and vector database indexing\n* implement retrieval strategies and answer generation workflows for operator queries\n* implement components for human\\-in\\-the\\-loop decision workflows, including investigation packages and remediation options\n* deploy and maintain containerized ai services using docker and aws infrastructure\n* contribute to audit trail, observability systems, and production operations including incident response\n\n\nmodel development (post\\-training and time\\-series analysis):* prepare and transform spacecraft telemetry data for ml model training\n* assist in training and evaluating anomaly detection and time\\-series forecasting models on telemetry data\n* support automated model retraining pipelines that incorporate new flight data and operator feedback\n* develop model evaluation scripts and metrics tracking\n\n\nrequired qualifications:* able to work onsite in one of our kent, wa, renton, wa, or reston, va offices.\n* bachelor's degree in computer science, machine learning, electrical engineering, or a related field\n* 3\\+ years of professional software development experience with exposure to ai/ml applications\n* proficiency in python and familiarity with at least one ai/ml framework (pytorch, tensorflow)\n* experience with building or contributing to llm\\-based applications, rag pipelines, or agent\\-based systems\n* exposure to time\\-series data processing, anomaly detection, or ml model training on sensor or telemetry data\n* experience with cloud platforms (aws preferred), containerization (docker), and basic ci/cd practices\n* solid understanding of cs fundamentals: data structures, algorithms, object\\-oriented design, and version control\n* strong analytical and problem\\-solving skills with attention to detail\n* good written and verbal communication skills for documentation and team collaboration\n\n\npreferred qualifications:* experience with aws bedrock, sagemaker, or equivalent managed ai/ml services\n* experience with databricks or similar platforms for data engineering and ml workflows\n* familiarity with vector databases (e.g., pinecone, pgvector, faiss) and embedding model pipelines\n* experience with time\\-series ml architectures (rnns, lstms, transformers) for forecasting or anomaly detection\n* exposure to multi\\-agent system frameworks or agentic ai orchestration patterns\n* familiarity with spacecraft operations, satellite telemetry, or aerospace engineering concepts\n* experience with infrastructure\\-as\\-code tools (terraform) and kubernetes\n* experience with real\\-time data streaming systems (kafka, kinesis)\n* familiarity with reinforcement learning concepts or graph neural network applications\n* interest or experience in model compression, edge deployment, or resource\\-constrained inference\n\n  \n\ncompensation range for:\nwa applicants is $134,961\\.00 \\- $188,945\\.40\nother site ranges may differ\nculture statement\ndon\u2019t meet all desired requirements? studies have shown that some people are less likely to apply to jobs unless they meet every single desired qualification. at blue origin, we are dedicated to building an authentic workplace, so if you\u2019re excited about this role but your past experience doesn\u2019t align perfectly with every desired qualification in the job description, we encourage you to apply anyway. you may be just the right candidate for this or other roles.\nexport control regulations\napplicants for employment at blue origin must be a u.s. citizen or national, u.s. permanent resident (i.e. current green card holder), or lawfully admitted into the u.s. as a refugee or granted asylum.\nbackground check* required for all positions: blue\u2019s standard background check\n* required for certain job profiles: defense biometric identification system (dbids) background check if at any time the role requires one to be on a military installation\n* required for certain job profiles: drivers who operate commercial motor vehicles with a gross vehicle weight (gvw), gross vehicle weight rating (gvwr) or combination of power unit and trailer that meets or exceeds 10,001 lbs. and/or transports placardable amounts of hazardous materials by ground in any vehicle on a public road while in commerce, may be subject to additional federal motor carrier safety regulations including: driver qualification files, medical certification (obtained before onboarding), road test, hours of service, drug and alcohol testing (cdl drivers only), vehicle inspection requirements, cdl requirements (if applicable) and hazardous materials transportation/shipping training.\n* required for certain job profiles: ability to obtain and maintain merchant mariner credential, which includes pre\\-employment and random drug testing as well as dot physical\n\n\nbenefits* benefits include: medical, dental, vision, basic and supplemental life insurance, paid parental leave, short and long\\-term disability, 401(k) with a company match of up to 5%, and an education support program.\n* paid time off: up to four (4\\) weeks per year based on weekly scheduled hours, and up to 14 company\\-paid holidays.\n* dependent on role type and job level, employees may be eligible for benefits and bonuses based on the company's intent to reward individual contributions and enable them to share in the company's results, or other factors at the company's sole discretion. bonus amounts and eligibility are not guaranteed and subject to change and cancellation. please check with your recruiter for more details.\n\n\nequal employment opportunity\nblue origin is proud to be an equal opportunity/affirmative action employer and is committed to attracting, retaining, and developing a highly qualified and dedicated work force. blue origin hires and promotes people on the basis of their qualifications, performance, and abilities. we support the establishment and maintenance of a workplace that fosters trust, equality, and teamwork. we provide all qualified applicants for employment and employees with equal opportunities for hire, promotion, and other terms and conditions of employment, regardless of their race, color, religion, sex, sexual orientation, gender identity, national origin/ethnicity, age, physical or mental disability, genetic factors, military/veteran status, or any other status or characteristic protected by federal, state, and/or local law. blue origin will consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state, and local laws, including the washington fair chance act, the california fair chance act, the los angeles fair chance in hiring ordinance, and other applicable laws.\naffirmative action and disability accommodation\napplicants wishing to receive information on blue origin\u2019s affirmative action plans, or applicants requiring a reasonable accommodation in order to participate in the application and/or interview process, please contact us at eeocompliance@blueorigin.com. please note this is a publicly managed inbox. please do not include any personal medical information in your request.\ncalifornia applicant privacy notice\nif you are a california resident, please reference the ca applicant privacy notice here.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Machine Learning Engineer, Risk Modeling",
        "company": "Block",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-20",
        "score": 15.6,
        "matched_keywords": [
            "Machine Learning Engineer",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "MLflow",
            "CI/CD",
            "Snowflake",
            "PySpark",
            "MySQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2749ecb9f8a0dc09",
        "description": "block builds technology to increase access to the global economy. across our ecosystem, including square and cash app, we create tools that help businesses run and grow, and individuals move, manage, and grow their money with confidence.\n\n\n\nsquare empowers sellers of all sizes with integrated, omnichannel tools to accept payments, manage operations, access financial services, and reach customers across online and in\\-person channels.\n\n\n\ncash app complements this by providing a fast, accessible financial platform for millions of people to send, spend, save, invest, and borrow, helping redefine how individuals interact with money.operating at massive scale across both ecosystems means trust and safety are foundational. our teams build systems that protect real people and businesses, safeguard financial activity, and ensure our products remain reliable, secure, and easy to use.\n\n\n\nblock is a global, distributed company with a culture rooted in ownership, creativity, and impact. whether supporting sellers on square or customers on cash app, we\u2019re united by a shared mission: to make the global economy more accessible and inclusive.\n\n\n#### **the role**\n\n\n\nwe\u2019re hiring senior machine learning engineers to join block\u2019s risk machine learning organization, where teams apply ml at massive scale to detect, prevent, and reduce fraud and abuse across cash app and square.\n\n\n\nthis opening supports multiple senior\\-level roles, with team placement determined through a collaborative matching process based on your experience, interests, and current business needs. today, we\u2019re growing teams focused on chargeback and fraud prevention as well as trust and safety initiatives supporting cash app families and teen banking.\n\n\n\nacross teams, your work will directly protect our ecosystem, reduce financial loss, and enable safe, seamless financial experiences for millions of customers, sellers, and families.\n\n\n**team focus areas**\n\n\n\ndepending on your background and interests, you may join a team focused on one of the following areas:\n\n\n**fraud \\& chargeback risk**  \n\nbuild machine learning models and systems that detect fraudulent transactions, reduce chargebacks, and protect the integrity of payments across cash app and square.\n\n\n**trust, safety \\& families**  \n\ndevelop machine learning systems that enable safe and trustworthy financial experiences for teens and families by detecting risky or abusive behavior, strengthening account integrity, and supporting long\\-term trust in cash app.\n\n\n\nboth teams value first\\-principles thinking, rapid iteration, and shipping production ml solutions with real\\-world impact.\n\n\n#### **you will**\n\n\n* partner with product, engineering, data science, policy, and operations to design and productionize ml\\-driven risk solutions at scale\n* own end\\-to\\-end machine learning systems, from problem definition and modeling to deployment, monitoring, and iteration\n* lead technical decision\\-making within your workstreams and influence ml strategy and planning\n* build tooling and processes that improve the speed, reliability, and impact of the ml development lifecycle\n* apply state\\-of\\-the\\-art modeling techniques and third\\-party data sources to improve detection and decision\\-making\n* investigate emerging fraud, abuse, and risk patterns to proactively inform product safeguards and policy\n* collaborate closely with ml platform and engineering teams to ensure models operate reliably in real time and at scale\n\n#### **you have**\n\n\n* 2\\+ years of industry experience in machine learning, applied ai, or related fields\n* bachelor\u2019s degree in a quantitative field (computer science, engineering, statistics, physics, applied math); master\u2019s or phd preferred\n* proven experience independently designing, deploying, and maintaining ml solutions in production\n* strong familiarity with techniques such as tree\\-based models, deep learning, transfer learning, or reinforcement learning\n* experience influencing technical direction and collaborating with cross\\-functional partners at scale\n* strong communication skills, sound judgment, and an ownership mindset\n* curiosity and alignment with block\u2019s mission of economic empowerment\n\n#### **technologies we use and teach**\n\n\n* python (numpy, pandas, scikit\\-learn, xgboost, pytorch, tensorflow/keras)\n* pyspark, mlflow, workflow orchestration tools (airflow, prefect)\n* gcp (vertex ai), aws\n* snowflake, mysql, tableau, mode\n* containerization, ci/cd, and production ml best practices\n\n  \n\n\nblock takes a market\\-based approach to pay, and pay may vary depending on your location. u.s. locations are categorized into one of four zones based on a cost of labor index for that geographic area. the successful candidate\u2019s starting pay will be determined based on job\\-related skills, experience, qualifications, work location, and market conditions. these ranges may be modified in the future.\n\n  \n\nzone a:\n$189,000\u2014$283,600 usd\nzone b:\n$179,600\u2014$269,400 usd\nzone c:\n$170,100\u2014$255,100 usd\nzone d:\n$160,700\u2014$241,100 usd**application guidelines**\n\n\n\ncandidates may submit up to 9 active applications within a 60\\-day period. reapplications to the same role are accepted 90 days after a previous application has been reviewed.\n\n\n**use of ai in our hiring process**\n\n\n\nwe may use automated ai tools to evaluate job applications for efficiency and consistency. these tools comply with local regulations, including bias audits, and we handle all personal data in accordance with state and local privacy laws.\n\n\n\ncontact us here with hiring practice or data usage questions.\n\n\n*every benefit we offer is designed with one goal: empowering you to do the best work of your career while building the life you want. remote work, medical insurance, flexible time off, retirement savings plans, and modern family planning are just some of our offering.*\n\n\n*block, inc. (nyse: xyz) builds technology to increase access to the global economy. each of our brands unlocks different aspects of the economy for more people.* ***square*** *makes commerce and financial services accessible to sellers.* ***cash app*** *is the easy way to spend, send, and store money.* ***afterpay*** *is transforming the way customers manage their spending over time.* ***tidal*** *is a music platform that empowers artists to thrive as entrepreneurs.* ***bitkey*** *is a simple self\\-custody wallet built for bitcoin.* ***proto*** *is a suite of bitcoin mining products and services. together, we\u2019re helping build a financial system that is open to everyone.*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Machine Learning Engineer, Risk Modeling",
        "company": "Block",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 15.6,
        "matched_keywords": [
            "Machine Learning Engineer",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "MLflow",
            "CI/CD",
            "Snowflake",
            "PySpark",
            "MySQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=43f57fcb65df08a3",
        "description": "block builds technology to increase access to the global economy. across our ecosystem, including square and cash app, we create tools that help businesses run and grow, and individuals move, manage, and grow their money with confidence.\n\n\n\nsquare empowers sellers of all sizes with integrated, omnichannel tools to accept payments, manage operations, access financial services, and reach customers across online and in\\-person channels.\n\n\n\ncash app complements this by providing a fast, accessible financial platform for millions of people to send, spend, save, invest, and borrow, helping redefine how individuals interact with money.operating at massive scale across both ecosystems means trust and safety are foundational. our teams build systems that protect real people and businesses, safeguard financial activity, and ensure our products remain reliable, secure, and easy to use.\n\n\n\nblock is a global, distributed company with a culture rooted in ownership, creativity, and impact. whether supporting sellers on square or customers on cash app, we\u2019re united by a shared mission: to make the global economy more accessible and inclusive.\n\n\n#### **the role**\n\n\n\nwe\u2019re hiring senior machine learning engineers to join block\u2019s risk machine learning organization, where teams apply ml at massive scale to detect, prevent, and reduce fraud and abuse across cash app and square.\n\n\n\nthis opening supports multiple senior\\-level roles, with team placement determined through a collaborative matching process based on your experience, interests, and current business needs. today, we\u2019re growing teams focused on chargeback and fraud prevention as well as trust and safety initiatives supporting cash app families and teen banking.\n\n\n\nacross teams, your work will directly protect our ecosystem, reduce financial loss, and enable safe, seamless financial experiences for millions of customers, sellers, and families.\n\n\n**team focus areas**\n\n\n\ndepending on your background and interests, you may join a team focused on one of the following areas:\n\n\n**fraud \\& chargeback risk**  \n\nbuild machine learning models and systems that detect fraudulent transactions, reduce chargebacks, and protect the integrity of payments across cash app and square.\n\n\n**trust, safety \\& families**  \n\ndevelop machine learning systems that enable safe and trustworthy financial experiences for teens and families by detecting risky or abusive behavior, strengthening account integrity, and supporting long\\-term trust in cash app.\n\n\n\nboth teams value first\\-principles thinking, rapid iteration, and shipping production ml solutions with real\\-world impact.\n\n\n#### **you will**\n\n\n* partner with product, engineering, data science, policy, and operations to design and productionize ml\\-driven risk solutions at scale\n* own end\\-to\\-end machine learning systems, from problem definition and modeling to deployment, monitoring, and iteration\n* lead technical decision\\-making within your workstreams and influence ml strategy and planning\n* build tooling and processes that improve the speed, reliability, and impact of the ml development lifecycle\n* apply state\\-of\\-the\\-art modeling techniques and third\\-party data sources to improve detection and decision\\-making\n* investigate emerging fraud, abuse, and risk patterns to proactively inform product safeguards and policy\n* collaborate closely with ml platform and engineering teams to ensure models operate reliably in real time and at scale\n\n#### **you have**\n\n\n* 2\\+ years of industry experience in machine learning, applied ai, or related fields\n* bachelor\u2019s degree in a quantitative field (computer science, engineering, statistics, physics, applied math); master\u2019s or phd preferred\n* proven experience independently designing, deploying, and maintaining ml solutions in production\n* strong familiarity with techniques such as tree\\-based models, deep learning, transfer learning, or reinforcement learning\n* experience influencing technical direction and collaborating with cross\\-functional partners at scale\n* strong communication skills, sound judgment, and an ownership mindset\n* curiosity and alignment with block\u2019s mission of economic empowerment\n\n#### **technologies we use and teach**\n\n\n* python (numpy, pandas, scikit\\-learn, xgboost, pytorch, tensorflow/keras)\n* pyspark, mlflow, workflow orchestration tools (airflow, prefect)\n* gcp (vertex ai), aws\n* snowflake, mysql, tableau, mode\n* containerization, ci/cd, and production ml best practices\n\n  \n\n\nblock takes a market\\-based approach to pay, and pay may vary depending on your location. u.s. locations are categorized into one of four zones based on a cost of labor index for that geographic area. the successful candidate\u2019s starting pay will be determined based on job\\-related skills, experience, qualifications, work location, and market conditions. these ranges may be modified in the future.\n\n  \n\nzone a:\n$189,000\u2014$283,600 usd\nzone b:\n$179,600\u2014$269,400 usd\nzone c:\n$170,100\u2014$255,100 usd\nzone d:\n$160,700\u2014$241,100 usd**application guidelines**\n\n\n\ncandidates may submit up to 9 active applications within a 60\\-day period. reapplications to the same role are accepted 90 days after a previous application has been reviewed.\n\n\n**use of ai in our hiring process**\n\n\n\nwe may use automated ai tools to evaluate job applications for efficiency and consistency. these tools comply with local regulations, including bias audits, and we handle all personal data in accordance with state and local privacy laws.\n\n\n\ncontact us here with hiring practice or data usage questions.\n\n\n*every benefit we offer is designed with one goal: empowering you to do the best work of your career while building the life you want. remote work, medical insurance, flexible time off, retirement savings plans, and modern family planning are just some of our offering.*\n\n\n*block, inc. (nyse: xyz) builds technology to increase access to the global economy. each of our brands unlocks different aspects of the economy for more people.* ***square*** *makes commerce and financial services accessible to sellers.* ***cash app*** *is the easy way to spend, send, and store money.* ***afterpay*** *is transforming the way customers manage their spending over time.* ***tidal*** *is a music platform that empowers artists to thrive as entrepreneurs.* ***bitkey*** *is a simple self\\-custody wallet built for bitcoin.* ***proto*** *is a suite of bitcoin mining products and services. together, we\u2019re helping build a financial system that is open to everyone.*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Science",
        "company": "Adidev Technologies Inc",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "spaCy",
            "Kubernetes",
            "PostgreSQL",
            "MongoDB",
            "Tableau"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=6fb5d815a109de6d",
        "description": "**adidev technologies inc**\n\n\n**www.adidevtechnologies.com**\n\n\n**urgent hire \\- hiring process \\- 24\\-48 hours!**\n\n\n\nadidev technologies is seeking 1\\-2 yrs of relevant experience in data science. a project can last anywhere from 6 months to 18 months. salary varies depending on experience, and we are in search of candidates looking to start as soon as possible. excellent written and oral communication are required as is the ability to work well in a team environment.\n\n\n\nif you are looking for a new challenge and are ready to make an impact on a growing team, then this will be a perfect fit. as a data scientist/data science specialist for adidev technologies inc., you will be enhancing and debugging large\\-scale applications for one of our well\\-known clients.\n\n\n\nadidev technologies is a growing software consulting company that is constantly expanding. as we are working with renowned clients and ready to take on new ones, we are seeking brilliant software engineers. not only do we offer a great team to work with, but we also offer you an opportunity to make an immediate impact and get rewarded accordingly\n\n  \n\n\n**job description**\n\n\n* demonstrated experience using machine learning, deep learning, statistical methodology, and simulation/optimization modeling in geospatial, network topography, recommendation systems, environmental systems, and/or agronomic problems.\n* strong foundation in python programming in a cloud environment.\n* strong quantitative abilities, distinctive problem\\-solving, and excellent analysis skills\n* expertise in data wrangling using sql,\n* practical knowledge and experience with cloud\\-computing systems and platforms, including the routine deployment of pipelines through kubernetes\n* fluency in querying/extracting/aggregating data via sql scripting.\n* extract, load and transform data (etl) from structured and unstructured sources\n* apply natural language processing and computer vision to solve business use cases,\n* strong skills in scientific data analyses, modeling, visualization and communication of results.\n* knowledge of python libraries (numpy, pandas, scikit\\-learn, tensorflow, pytorch), spacy, mongodb, postgresql, flask, streamlet and a good knowledge of data pipelines construction\n* ph.d., m.s. or b.s. in computer science, computational physics, operations research, geospatial sciences, remote sensing science, environmental sciences, computational astronomy or related scientific discipline\n\n  \n\n  \n\n**must have**\n\n\n* understanding of various machine learning algorithms (e.g. svm, random forests, gradient boosting, log\\-log regression, xgboost, lasso, ridge, clustering techniques, neural networks and others)\n* regression (e.g. ? linear/logistic/mnl/mixed effects/regularization)\n* classification (k\\-means, hierarchical, latent class, dbscan, svm)\n* dimension reduction techniques (principal component analysis, singular value decomposition etc.)\n* optimization (linear programming, stochastic gradient descent, genetic algorithm etc.)\n* experience with neural network approaches to text classification cnn, rnn, lstm,keras\n* machine learning algorithms? neural networks, na\u00efve bayes, bagging \\& boosting, random forest\n* distributed computing tools and cloud technology (aws)\n\n  \n\n\n**qualifications**\n\n  \n\n\n* degree in data science, computer science, engineering, math, or statistics preferred\n* at least 2 yrs of relevant experience in data science\n  \n\n\n  \n\n**skills**\n\n  \n\n\n* sql, statistical modeling, feature engineering, data visualization, deploying models to production, python programming, aws, domains(healthcare/ manufacturing/ marketing/ financial/ telecommunication), powerbi/tableau, data warehouse\n\n  \n\n\n\n\n**benefits**\n\n\n* competitive salary\n* paid relocation\n* remote support\n* guaranteed regular salary reviews\n* job type: w2 or contract 1099 (full\\-time \\- 40 hours)",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Science Specialist",
        "company": "Adidev Technologies Inc",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "spaCy",
            "Kubernetes",
            "PostgreSQL",
            "MongoDB",
            "Tableau"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7686f3290697986d",
        "description": "**adidev technologies inc**\n\n\n**www.adidevtechnologies.com**\n\n\n**urgent hire \\- hiring process \\- 24\\-48 hours!**\n\n\n\nadidev technologies is seeking 1\\-2 yrs of relevant experience in data science. a project can last anywhere from 6 months to 18 months. salary varies depending on experience, and we are in search of candidates looking to start as soon as possible. excellent written and oral communication are required as is the ability to work well in a team environment.\n\n\n\nif you are looking for a new challenge and are ready to make an impact on a growing team, then this will be a perfect fit. as a data scientist/data science specialist for adidev technologies inc., you will be enhancing and debugging large\\-scale applications for one of our well\\-known clients.\n\n\n\nadidev technologies is a growing software consulting company that is constantly expanding. as we are working with renowned clients and ready to take on new ones, we are seeking brilliant software engineers. not only do we offer a great team to work with, but we also offer you an opportunity to make an immediate impact and get rewarded accordingly\n\n  \n\n\n**job description**\n\n\n* demonstrated experience using machine learning, deep learning, statistical methodology, and simulation/optimization modeling in geospatial, network topography, recommendation systems, environmental systems, and/or agronomic problems.\n* strong foundation in python programming in a cloud environment.\n* strong quantitative abilities, distinctive problem\\-solving, and excellent analysis skills\n* expertise in data wrangling using sql,\n* practical knowledge and experience with cloud\\-computing systems and platforms, including the routine deployment of pipelines through kubernetes\n* fluency in querying/extracting/aggregating data via sql scripting.\n* extract, load and transform data (etl) from structured and unstructured sources\n* apply natural language processing and computer vision to solve business use cases,\n* strong skills in scientific data analyses, modeling, visualization and communication of results.\n* knowledge of python libraries (numpy, pandas, scikit\\-learn, tensorflow, pytorch), spacy, mongodb, postgresql, flask, streamlet and a good knowledge of data pipelines construction\n* ph.d., m.s. or b.s. in computer science, computational physics, operations research, geospatial sciences, remote sensing science, environmental sciences, computational astronomy or related scientific discipline\n\n  \n\n  \n\n**must have**\n\n\n* understanding of various machine learning algorithms (e.g. svm, random forests, gradient boosting, log\\-log regression, xgboost, lasso, ridge, clustering techniques, neural networks and others)\n* regression (e.g. ? linear/logistic/mnl/mixed effects/regularization)\n* classification (k\\-means, hierarchical, latent class, dbscan, svm)\n* dimension reduction techniques (principal component analysis, singular value decomposition etc.)\n* optimization (linear programming, stochastic gradient descent, genetic algorithm etc.)\n* experience with neural network approaches to text classification cnn, rnn, lstm,keras\n* machine learning algorithms? neural networks, na\u00efve bayes, bagging \\& boosting, random forest\n* distributed computing tools and cloud technology (aws)\n\n  \n\n\n**qualifications**\n\n  \n\n\n* degree in data science, computer science, engineering, math, or statistics preferred\n* at least 2 yrs of relevant experience in data science\n  \n\n\n  \n\n**skills**\n\n  \n\n\n* sql, statistical modeling, feature engineering, data visualization, deploying models to production, python programming, aws, domains(healthcare/ manufacturing/ marketing/ financial/ telecommunication), powerbi/tableau, data warehouse\n\n  \n\n\n\n\n**benefits**\n\n\n* competitive salary\n* paid relocation\n* remote support\n* guaranteed regular salary reviews\n* job type: w2 or contract 1099 (full\\-time \\- 40 hours)",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "DATA SCIENTIST",
        "company": "Adidev Technologies Inc",
        "location": "Austin, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "PyTorch",
            "XGBoost",
            "Keras",
            "spaCy",
            "Kubernetes",
            "PostgreSQL",
            "MongoDB",
            "Tableau"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2b438984f6dd9115",
        "description": "**adidev technologies inc**\n\n\n**www.adidevtechnologies.com**\n\n\n**urgent hire \\- hiring process \\- 24\\-48 hours!**\n\n\n\nadidev technologies is seeking 1\\-2 yrs of relevant experience in data science. a project can last anywhere from 6 months to 18 months. salary varies depending on experience, and we are in search of candidates looking to start as soon as possible. excellent written and oral communication are required as is the ability to work well in a team environment.\n\n\n\nif you are looking for a new challenge and are ready to make an impact on a growing team, then this will be a perfect fit. as a data scientist/data science specialist for adidev technologies inc., you will be enhancing and debugging large\\-scale applications for one of our well\\-known clients.\n\n\n\nadidev technologies is a growing software consulting company that is constantly expanding. as we are working with renowned clients and ready to take on new ones, we are seeking brilliant software engineers. not only do we offer a great team to work with, but we also offer you an opportunity to make an immediate impact and get rewarded accordingly\n\n  \n\n\n**job description**\n\n\n* demonstrated experience using machine learning, deep learning, statistical methodology, and simulation/optimization modeling in geospatial, network topography, recommendation systems, environmental systems, and/or agronomic problems.\n* strong foundation in python programming in a cloud environment.\n* strong quantitative abilities, distinctive problem\\-solving, and excellent analysis skills\n* expertise in data wrangling using sql,\n* practical knowledge and experience with cloud\\-computing systems and platforms, including the routine deployment of pipelines through kubernetes\n* fluency in querying/extracting/aggregating data via sql scripting.\n* extract, load and transform data (etl) from structured and unstructured sources\n* apply natural language processing and computer vision to solve business use cases,\n* strong skills in scientific data analyses, modeling, visualization and communication of results.\n* knowledge of python libraries (numpy, pandas, scikit\\-learn, tensorflow, pytorch), spacy, mongodb, postgresql, flask, streamlet and a good knowledge of data pipelines construction\n* ph.d., m.s. or b.s. in computer science, computational physics, operations research, geospatial sciences, remote sensing science, environmental sciences, computational astronomy or related scientific discipline\n\n  \n\n  \n\n**must have**\n\n\n* understanding of various machine learning algorithms (e.g. svm, random forests, gradient boosting, log\\-log regression, xgboost, lasso, ridge, clustering techniques, neural networks and others)\n* regression (e.g. ? linear/logistic/mnl/mixed effects/regularization)\n* classification (k\\-means, hierarchical, latent class, dbscan, svm)\n* dimension reduction techniques (principal component analysis, singular value decomposition etc.)\n* optimization (linear programming, stochastic gradient descent, genetic algorithm etc.)\n* experience with neural network approaches to text classification cnn, rnn, lstm,keras\n* machine learning algorithms? neural networks, na\u00efve bayes, bagging \\& boosting, random forest\n* distributed computing tools and cloud technology (aws)\n\n  \n\n\n**qualifications**\n\n  \n\n\n* degree in data science, computer science, engineering, math, or statistics preferred\n* at least 2 yrs of relevant experience in data science\n  \n\n\n  \n\n**skills**\n\n  \n\n\n* sql, statistical modeling, feature engineering, data visualization, deploying models to production, python programming, aws, domains(healthcare/ manufacturing/ marketing/ financial/ telecommunication), powerbi/tableau, data warehouse\n\n  \n\n\n\n\n**benefits**\n\n\n* competitive salary\n* paid relocation\n* remote support\n* guaranteed regular salary reviews\n* job type: w2 or contract 1099 (full\\-time \\- 40 hours)",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Backend Software Engineer (hybrid)",
        "company": "Johnson Controls",
        "location": "Milwaukee, WI, US USA",
        "posted_at": "2026-02-21",
        "score": 15.6,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "PostgreSQL",
            "MySQL",
            "MongoDB"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=f5227f1e48c9e496",
        "description": "**what you will do**\n\n\nat johnson controls, we are committed to building intelligent, efficient, and high\\-quality backend services that power the next generation of building automation and digital products. as a backend software developer, you will design, build, and maintain backend services, apis, and data pipelines that support mission\\-critical applications used by customers worldwide.\n\n\nin this role, you will work closely with product managers, frontend engineers, qa, and cross\\-functional teams to deliver reliable, scalable, and secure backend systems. you will also directly contribute to improving performance, system design, architectural decisions, and overall technical execution across our software ecosystem. you will operate in a modern agile environment using industry\\-standard tools, cloud services, and continuous integration practices. this is a hybrid position requiring candidates to be in the glendale, wi, facility 3 days per week. candidates must be commuting distance to the facility, or open to relocating. we offer various relocation packages. only us citizens will be considered for this position, as sponsorship is not provided.\n\n\n\\-\n\n**how you will do it**\n\n* design, develop, and maintain backend services, apis, and microservices.\n* build clean, reusable, and scalable code following best engineering practices.\n* work collaboratively with frontend, qa, and product teams to deliver end\\-to\\-end features.\n* profile, monitor, and optimize application performance across services and data layers.\n* troubleshoot production issues, identify root causes, and deploy fixes with minimal disruption.\n* contribute to architecture discussions, technical decision\\-making, and system design.\n* write and maintain technical documentation for backend components and services.\n* participate in code reviews to maintain consistency and quality across the codebase.\n* implement unit, integration, and automated tests to ensure reliability.\n* stay up\\-to\\-date with emerging backend technologies and industry trends.\n* follow all corporate and departmental software development standards and quality guidelines.\n* us citizenship and/or permanent resident.\n\n\n\\-\n\n**what we look for**\n\n**required**\n\n* bachelor\u2019s degree in computer science, engineering, or related technical field.\n* 0\\-3 years of experience with backend languages such as c\\# with some node.js.\n* understanding of data structures, algorithms, and object\\-oriented design principles.\n* experience building restful apis, microservices, or distributed systems.\n* familiarity with sql and/or nosql databases (postgresql, mysql, mongodb, dynamodb).\n* experience using git/github or equivalent version control systems.\n* strong debugging, analytical, and problem\\-solving skills.\n\n**preferred**\n\n* experience with cloud platforms (azure).\n* knowledge of docker, kubernetes, or container\\-based architectures.\n* familiarity with event\\-driven systems or messaging technologies\n* experience with ci/cd pipelines (github actions, azure devops, jenkins).\n* understanding of authentication and authorization frameworks (oauth, jwt).\n* wix, wpf and winform experience\n\n\nhiring salary range: $75,000 \\- $100,000 (salary to be determined by the education, experience, knowledge, skills, and abilities of the applicant, internal equity, and alignment with market data.) this position includes a competitive benefits package. the posted salary range reflects the target compensation for this role. however, we recognize that exceptional candidates may bring unique skills and experiences that exceed the typical profile. if you believe your background warrants consideration beyond the stated range, we encourage you to apply. to support an efficient and fair hiring process, we may use technology assisted tools, including artificial intelligence (ai), to help identify and evaluate candidates. all hiring decisions are ultimately made by human reviewers. for details, please visit the about us tab on the johnson controls careers site at https://jobs.johnsoncontrols.com/about\\-us",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer - Full Stack",
        "company": "NTT DATA",
        "location": "MO, US USA",
        "posted_at": "2026-02-20",
        "score": 14.4,
        "matched_keywords": [
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Jenkins",
            "Git",
            "PostgreSQL",
            "MongoDB",
            "NoSQL",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e06ca5d0cbf6168e",
        "description": "job info:  \n\n  \n\nthis role will be a senior individual contributor and technical leader on a collaborative team building cloud\\-native, healthcare\\-focused applications on google cloud platform (gcp). the staff software engineer will design, build, and operate secure, scalable, high\\-performance end\\-to\\-end solutions using java/spring boot microservices and modern web frameworks such as next.js and reactjs. this role partners closely with product management, ux, analytics, and cross\\-functional engineering teams to deliver impactful solutions that advance our mission of providing compassionate, personalized care through technology.  \n\n  \n\n**reporting**  \n\n  \n\nday\\-to\\-day partner to engineering manager / tech lead.  \n\n  \n\n**team**  \n\n  \n\nworks closely with backend, frontend, devops, product, ux, analytics, and support teams.  \n\n  \n\n**key responsibilities**  \n\n* lead the architecture, design, and development of secure, scalable, and high\\-performing full\\-stack solutions.\n* provide technical leadership and mentorship to senior engineers and team members across backend and frontend domains.\n* design, build, and secure scalable restful apis and graphql endpoints.\n* develop high\\-quality, maintainable backend services using java and spring boot.\n* develop modern, performant web applications using next.js / reactjs, javascript, and typescript.\n* implement test\\-driven development (tdd) and ensure strong unit and integration test coverage.\n* conduct comprehensive code and architectural reviews; ensure compliance with quality, security, and performance standards.\n* drive engineering best practices and coding standards across teams to improve reliability and scalability.\n* participate in agile ceremonies (sprint planning, reviews, retrospectives) and produce clear technical documentation and architecture designs.\n* communicate effectively across teams and departments, influencing technical direction and architectural decisions.\n* independently address and resolve high\\-priority technical issues.\n* partner with product management, ux, analytics, and support to align on roadmap priorities and technical trade\\-offs.\n* lead major refactors, system redesigns, and long\\-term technical improvements while balancing technical debt and business needs.\n* develop reusable frameworks, shared components, and internal tooling.\n* lead postmortems and implement sustainable improvements based on learnings.\n\n\n**required qualifications \\& skills**  \n\n* 7 years of professional software development experience in an agile environment.\n* extensive experience architecting and implementing scalable microservices using java and spring boot.\n* proven experience delivering complex full\\-stack applications using next.js / reactjs.\n* strong experience designing, developing, and consuming restful apis, event\\-driven apis, and graphql.\n* hands\\-on expertise with elk stack (elasticsearch, logstash, kibana) for search use cases.\n* solid understanding of object\\-oriented design, design patterns (gof), data structures, and algorithms.\n* experience with both relational databases (postgresql) and nosql stores (redis).\n* hands\\-on experience with docker and kubernetes, including helm charts.\n* experience with message\\-driven systems (e.g., gcp pub/sub, cloud run, aws sqs).\n* strong knowledge of ci/cd pipelines and modern devops practices.\n* experience with behavior\\-driven development (bdd) and atlassian tools (jira, confluence).\n* deep knowledge of computer science fundamentals in ooad, design patterns (including the gang of four), data structures, and algorithms.\n* strong understanding of both relational (postgres) and nosql databases (redis, mongodb).\n* strong understanding of native mobile app development for ios and android, including platform\\-specific ui/ux patterns and device capabilities.\n* excellent verbal and written communication skills.\n\n\n**required technical / tools**  \n\n* backend: java, spring boot, spring data jpa, spring cloud config, junit, mockito, elasticsearch, logstash, kibana\n* frontend: next.js, reactjs, javascript, typescript, jest\n* databases: postgresql (cloud sql), redis\n* devops: github, jenkins, maven, yarn, ci/cd pipelines\n* cloud: google cloud platform (gke, cloud run, pub/sub, gcs)\n* observability: dynatrace (or similar apm), log explorer, alert policies\n* agile / collaboration: jira, confluence\n\n\n**preferred qualifications**  \n\n* experience building scalable, reusable, and accessible ui components.\n* familiarity with behavior\\-driven development (bdd).\n* understanding of oauth 2\\.0 and modern application security principles.\n* knowledge of hl7 fhir standards (preferred).\n* experience with healthcare systems or scheduling platforms (preferred).\n* understanding of native mobile app development for ios and android.\n\n\n**characteristics we seek**  \n\n* curiosity for technology: continuously explores and applies modern technologies to improve patient and clinician experiences.\n* commitment to learning and growth: embraces continuous improvement through learning and reflection.\n* comfort with change: navigates ambiguity, adapts quickly, and communicates proactively.\n* feedback\\-driven mindset: values feedback through design reviews, code reviews, ci/cd, apm, and analytics.\n* positive problem\\-solver: approaches complex challenges with optimism and resilience.\n* strong collaborator: values teamwork, mentorship, and shared ownership of outcomes.\n* alignment with \\*\\*\\* values: demonstrates service of the poor, reverence, integrity, wisdom, creativity, and dedication.\n\n  \n\nfully remote working est/cst hours  \n\n  \n\ncomments for suppliers: this is a senior individual contributor role with technical leadership responsibilities (not a people\\-manager role).  \n\n  \n\nextension will be evaluated near the end of the initial 6\\-month contract based on delivery needs and performance.  \n\n  \n\nwhere required by law, ntt data provides a reasonable range of compensation for specific roles. the starting hourly range for this remote role is $ **71\\.58 to 76\\.87 w2** this range reflects the minimum and maximum target compensation for the position across all us locations. actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. this position may also be eligible for incentive compensation based on individual and/or company performance.  \n\n  \n\nthis position is eligible for company benefits that will depend on the nature of the role offered. company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and ad\\&d insurance, short\\-and long\\-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits .",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI Engineer III - Blue Ring",
        "company": "Blue Origin",
        "location": "Seattle, WA, US USA",
        "posted_at": "2026-02-20",
        "score": 14.4,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Kinesis",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Databricks",
            "Kafka"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=282fd09ebf907b50",
        "description": "application close date:\napplications will be accepted on an ongoing basis until the requisition is closed.\nat blue origin, we envision millions of people living and working in space for the benefit of earth. we\u2019re working to develop reusable, safe, and low\\-cost space vehicles and systems within a culture of safety, collaboration, and inclusion. join our team of problem solvers as we add new chapters to the history of spaceflight!\nthe role is part of the in\\-space systems business unit, which is focused on addressing two of the most compelling challenges in spaceflight today: space infrastructure and increasing mobility on\\-orbit.\nblue ring is blue origin's multi\\-mission space mobility platform, and this team is building the next generation of intelligent ground systems that will redefine how satellite fleets are operated\u2014moving beyond manual monitoring and reactive processes.\nwe are seeking an ai engineer iii to join the blue ring software team. you will help develop and deliver the ai and machine learning capabilities embedded in blue ring's ground system, spanning multi\\-agent monitoring, predictive anomaly detection, conversational knowledge retrieval, and ai\\-assisted decision\\-making that enable a small operations team to manage an entire satellite fleet. this role requires a hands\\-on engineer with deep expertise across the full ai/ml development lifecycle, from building multi\\-agent systems and retrieval\\-augmented generation pipelines to training and deploying time\\-series ml models on real spacecraft telemetry.\nrole and responsibilities:\nas a key member of the blue ring ai/ml engineering team, you will build, deploy, and iterate on ai/ml systems that serve as the intelligence layer for blue origin's satellite fleet operations. working within the architectural direction set by the team lead, you will translate system designs into production\\-ready implementations and contribute technical recommendations that shape the platform's evolution. you will be expected to:\nmulti\\-agent system development:* implement and optimize multi\\-agent systems that coordinate across telemetry analysis, anomaly detection, flight dynamics, and mission planning domains\n* build and maintain agent orchestration frameworks that integrate with llm services and ground system services for reliable agent execution with observability and memory\n* develop and deploy rag pipelines over flight manuals, operations procedures, design documents, and historical log files\n* manage vector database infrastructure, embedding pipelines, and retrieval strategies to deliver accurate answers to operator queries\n* implement human\\-in\\-the\\-loop decision workflows where agents present investigation plans and remediation options for operator approval\n* develop confidence scoring and validation systems for ai recommendations\n* deploy containerized ai services on aws with infrastructure\\-as\\-code, ci/cd pipelines, observability, and automated testing\n* support on\\-call rotations and drive root\\-cause analysis for production issues\n\n\nmodel development (post\\-training and time\\-series analysis)* develop anomaly detection models that identify telemetry deviations beyond threshold\\-based monitoring\n* train and deploy sequence\\-based models for failure prediction on telemetry data\n* build automated model retraining pipelines that incorporate operator feedback and new flight data\n* explore compressed models for resource\\-constrained deployment scenarios\n\n\nrequired qualifications:* able to work onsite in one of our kent, wa, renton, wa, or reston, va offices.\n* bachelor's or master's degree in computer science, machine learning, electrical engineering, or a related field\n* 5\\+ years of professional software development experience with a focus on ai/ml applications in production environments\n* proficiency in python and at least one ai/ml framework (pytorch, tensorflow)\n* hands\\-on experience building and deploying multi\\-agent systems, agentic ai workflows, or llm\\-based applications in production\n* experience designing and operating rag pipelines, including vector databases, embedding models, and retrieval strategies\n* experience with time\\-series ml models (rnns, lstms, transformers) for anomaly detection or forecasting on sensor/telemetry data\n* experience with cloud platforms (aws preferred), containerization (docker, kubernetes), and ci/cd pipeline implementation\n* knowledge of professional software engineering practices including code reviews, source control, automated testing, and operational excellence\n* strong analytical and problem\\-solving skills with attention to detail\n* excellent written and verbal communication skills for documentation and cross\\-team collaboration\n\n\npreferred qualifications:* experience with aws bedrock, sagemaker, or equivalent managed ai/ml services\n* experience with databricks or similar platforms for large\\-scale data engineering and ml model training\n* familiarity with spacecraft operations, satellite telemetry, or aerospace systems\n* experience with reinforcement learning, including multi\\-agent rl, deep q\\-networks, or policy gradient methods\n* experience with graph neural networks for network optimization or topology\\-aware problems\n* background in model compression, quantization, or edge deployment for resource\\-constrained environments\n* experience with real\\-time data streaming systems (kafka, kinesis) and high\\-frequency data ingestion\n* understanding formal verification, safety\\-critical systems, or graduated autonomy frameworks\n* experience building observability and audit systems for ai decision pipelines in regulated or mission\\-critical domains\n* research publications in machine learning, multi\\-agent systems, or applied ai, and/or open\\-source contributions\n\n  \n\ncompensation range for:\nwa applicants is $164,652\\.00 \\- $230,512\\.80\nother site ranges may differ\nculture statement\ndon\u2019t meet all desired requirements? studies have shown that some people are less likely to apply to jobs unless they meet every single desired qualification. at blue origin, we are dedicated to building an authentic workplace, so if you\u2019re excited about this role but your past experience doesn\u2019t align perfectly with every desired qualification in the job description, we encourage you to apply anyway. you may be just the right candidate for this or other roles.\nexport control regulations\napplicants for employment at blue origin must be a u.s. citizen or national, u.s. permanent resident (i.e. current green card holder), or lawfully admitted into the u.s. as a refugee or granted asylum.\nbackground check* required for all positions: blue\u2019s standard background check\n* required for certain job profiles: defense biometric identification system (dbids) background check if at any time the role requires one to be on a military installation\n* required for certain job profiles: drivers who operate commercial motor vehicles with a gross vehicle weight (gvw), gross vehicle weight rating (gvwr) or combination of power unit and trailer that meets or exceeds 10,001 lbs. and/or transports placardable amounts of hazardous materials by ground in any vehicle on a public road while in commerce, may be subject to additional federal motor carrier safety regulations including: driver qualification files, medical certification (obtained before onboarding), road test, hours of service, drug and alcohol testing (cdl drivers only), vehicle inspection requirements, cdl requirements (if applicable) and hazardous materials transportation/shipping training.\n* required for certain job profiles: ability to obtain and maintain merchant mariner credential, which includes pre\\-employment and random drug testing as well as dot physical\n\n\nbenefits* benefits include: medical, dental, vision, basic and supplemental life insurance, paid parental leave, short and long\\-term disability, 401(k) with a company match of up to 5%, and an education support program.\n* paid time off: up to four (4\\) weeks per year based on weekly scheduled hours, and up to 14 company\\-paid holidays.\n* dependent on role type and job level, employees may be eligible for benefits and bonuses based on the company's intent to reward individual contributions and enable them to share in the company's results, or other factors at the company's sole discretion. bonus amounts and eligibility are not guaranteed and subject to change and cancellation. please check with your recruiter for more details.\n\n\nequal employment opportunity\nblue origin is proud to be an equal opportunity/affirmative action employer and is committed to attracting, retaining, and developing a highly qualified and dedicated work force. blue origin hires and promotes people on the basis of their qualifications, performance, and abilities. we support the establishment and maintenance of a workplace that fosters trust, equality, and teamwork. we provide all qualified applicants for employment and employees with equal opportunities for hire, promotion, and other terms and conditions of employment, regardless of their race, color, religion, sex, sexual orientation, gender identity, national origin/ethnicity, age, physical or mental disability, genetic factors, military/veteran status, or any other status or characteristic protected by federal, state, and/or local law. blue origin will consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state, and local laws, including the washington fair chance act, the california fair chance act, the los angeles fair chance in hiring ordinance, and other applicable laws.\naffirmative action and disability accommodation\napplicants wishing to receive information on blue origin\u2019s affirmative action plans, or applicants requiring a reasonable accommodation in order to participate in the application and/or interview process, please contact us at eeocompliance@blueorigin.com. please note this is a publicly managed inbox. please do not include any personal medical information in your request.\ncalifornia applicant privacy notice\nif you are a california resident, please reference the ca applicant privacy notice here.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI Commercial & ML Ops Engineer",
        "company": "MGM Resorts International",
        "location": "Las Vegas, NV, US USA",
        "posted_at": "2026-02-20",
        "score": 14.4,
        "matched_keywords": [
            "Data Scientist",
            "TensorFlow",
            "PyTorch",
            "MLflow",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Databricks",
            "PySpark"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1d1ac103ab878235",
        "description": "us, nevada\n\n\nthe show comes alive at mgm resorts international\n\n\nhave you ever wondered what it would be like to work in a place full of excitement, diversity, and entertainment? are you enthusiastic about being a team player in one of the most fascinating industries in the world? at mgm resorts, we seek individuals like you to create unique and show\\-stopping experiences for our guests.\n\n**the job:**\n\n\nwe are seeking a senior\\-level artificial intelligence and machine learning operations engineer to design, implement, and optimize scalable machine learning and artificial intelligence deployment pipelines that power real\\-world business impact.\n\n\nin this role, you will partner closely with data science, data engineering, and analytics teams to ensure models are production\\-ready, performant, secure, and scalable across cloud platforms. you will automate and operationalize the full machine learning lifecycle, from data ingestion through retraining, while establishing best practices, governance frameworks, and enterprise standards for artificial intelligence and machine learning operations.\n\n\nthis is a highly visible individual contributor role that combines hands\\-on engineering with strategic influence across various initiatives.\n\n**the day\\-to\\-day:**\n\n* design, build, and operate end\\-to\\-end machine learning and artificial intelligence pipelines supporting batch, streaming, and real\\-time inference use cases\n* automate the full machine learning lifecycle including ingestion, feature engineering, training, validation, deployment, monitoring, and retraining\n* implement ci/cd pipelines for machine learning systems with automated testing, validation gates, and controlled model promotion\n* develop orchestration workflows using tools such as airflow, kubeflow, and mlflow for experiment tracking and governance\n* optimize artificial intelligence workloads for performance, scalability, and cost efficiency using distributed compute and cloud\\-native services\n* establish monitoring and observability frameworks including performance metrics, data quality checks, drift detection, and bias monitoring\n* design automated retraining strategies including trigger\\-based, schedule\\-based, and performance\\-based refresh cycles\n* create repeatable prompting frameworks and artificial intelligence guardrails to support safe and effective ai\\-assisted development\n* implement access controls, secrets management, compliance standards, and security best practices across machine learning and artificial intelligence platforms\n* evaluate and operationalize emerging artificial intelligence technologies and vendor tools, identifying measurable business value\n* mentor engineers and data scientists on machine learning operations best practices and influence enterprise\\-wide architectural standards\n\n**the ideal candidate:**\n\n* 5\\+ years of prior relevant experience in machine learning or artificial intelligence engineering, data science, or analytics engineering\n* bachelor\u2019s degree in computer science, data engineering, or related field required\n* deep experience with machine learning and artificial intelligence pipeline development and full lifecycle automation\n* demonstrated ability to provide technical leadership in machine learning operations strategies and pipeline standardization\n* proven impact on improving reliability, scalability, and efficiency of machine learning and artificial intelligence solutions\n* strong proficiency in python and pyspark\n* experience designing and implementing ci/cd for machine learning workflows, including version control systems such as git and dvc\n* experience with monitoring, logging, drift detection, and automated retraining frameworks\n* experience with cloud platforms such as databricks, aws, gcp, or azure\n* proficiency in containerization and orchestration including docker and kubernetes\n* experience with orchestration and lifecycle management tools such as airflow, kubeflow, mlflow, or similar platforms\n* experience guiding teams on artificial intelligence automation best practices\n* experience supporting marketing, revenue, or operations analytics teams preferred\n* familiarity with tensorflow, pytorch, scikit\\-learn, or similar machine learning frameworks preferred\n\n**the perks \\& benefits:**\n\n* prioritize your wellness, access programs crafted to nurture your mental and physical health.\n* enjoy unbeatable discounts on hotel stays, dining, retail, entertainment, and exclusive partner perks for travel, tech, and beyond!\n* savor delicious meals for free in our employee dining room.\n* park with ease\u2014whether you're on or off shift, it's free!\n* from healthcare to financial support and generous time\\-off options, we\u2019ve got you covered.\n* elevate your career with development programs, connect through networking events, and make a difference with community volunteer opportunities.\n\n**view job description:**\n\n\nhttps://mgmresorts.marketpayjobs.com/showjob.aspx?entityid\\=2\\&jobcode\\=12824\n\n**pay range:**\n\n\nthe pay range for this role is:\n\n\n$130,100\\.00 \\- $173,500\\.00\n\n\nthis range represents a good faith estimate of the salary range that mgm reasonably expects to pay for the position upon hire. the actual salary offer will take into account a wide range of factors, including location.\n\n\nthis position is eligible to participate in the company\u2019s incentive plan.\n\n\nemployees in this position are eligible to participate in medical, dental, vision, life insurance, 401(k) plans, and time off plans. specific program offerings vary by eligibility factors such as geographic location, employment status, and union membership.\n\n\nare you ready to **join the show**? apply today!",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Engineer",
        "company": "Abbott",
        "location": "Pleasanton, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "S3",
            "Glue",
            "Redshift",
            "Databricks",
            "Redshift",
            "PySpark",
            "Kafka",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=44aa0d70d66a42cd",
        "description": "abbott is a global healthcare leader that helps people live more fully at all stages of life. our portfolio of life\\-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. our 114,000 colleagues serve people in more than 160 countries.\n\n**working at abbott**\n\n\nat abbott, you can do work that matters, grow, and learn, care for yourself and family, be your true self and live a full life. you\u2019ll also have access to:\n\n* career development with an international company where you can grow the career you dream of.\n* employees can qualify for free medical coverage in our health investment plan (hip) ppo medical plan in the next calendar year\n* an excellent retirement savings plan with high employer contribution\n* tuition reimbursement, the freedom 2 save student debt program and freeu education benefit \\- an affordable and convenient path to getting a bachelor\u2019s degree.\n* a company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by fortune.\n* a company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.\n\n**the opportunity**\n\n\ninterested in applying your wealth of technical knowledge and experience towards an opportunity in the medical field and improving the lives of people with diabetes? the candidate will be responsible for big data engineering, data wrangling, and data analysis in the cloud. the role will also contribute to defining and implementing big data strategy for the organization along with driving the implementation of it solutions for the business. the candidate will be working with other data engineers, data analysts and data scientists to focus on applying data engineering, data science and machine learning approaches to solve business problems.\n\n\nas a member of the data engineering \\& analytics team, you will be building big data collection and analytics capabilities to uncover customer, product and operational insights. candidate should be able to work on a geographically distributed team to develop data pipelines capable of handling complex data sets quickly and securely as well as operationalize data science solutions. additionally, they will be working in a technology\\-driven environment utilizing the latest tools and techniques such as databricks, redshift, s3, lambda, dynamodb, spark and python.\n\n\nthe candidate should have a passion for software engineering to help shape the direction of the team. highly sought\\-after qualities include versatility and a desire to continuously learn, improve, and empower other team members. candidate will support building scalable, highly available, efficient, and secure software solutions for big data initiatives. \\#software\n\n**what you'll do**\n\n* design and implement data pipelines to be processed and visualized across a variety of projects and initiatives\n* develop and maintain optimal data pipeline architecture by designing and implementing data ingestion solutions on aws using aws native services\n* design and optimize data models on aws cloud using databricks and aws data stores such as redshift, rds, s3\n* integrate and assemble large, complex data sets that meet a broad range of business requirements\n* read, extract, transform, stage and load data to selected tools and frameworks as required and requested\n* customizing and managing integration tools, databases, warehouses, and analytical systems\n* process unstructured data into a form suitable for analysis and assist in analysis of the processed data\n* working directly with the technology and engineering teams to integrate data processing and business objectives\n* monitoring and optimizing data performance, uptime, and scale; maintaining high standards of code quality and thoughtful design\n* create software architecture and design documentation for the supported solutions and overall best practices and patterns\n* support team with technical planning, design, and code reviews including peer code reviews\n* provide architecture and technical knowledge training and support for the solution groups\n* develop good working relations with the other solution teams and groups, such as engineering, marketing, product, test, qa\n* stay current with emerging trends, making recommendations as needed to help the organization innovate\n\n**required qualifications**\n\n* bachelors degree in computer science, information technology or other relevant field\n* at least 1\\-3 years of recent experience in software engineering, data engineering or big data\n* ability to work effectively within a team in a fast\\-paced changing environment\n* knowledge of or direct experience with databricks and/or spark\n* software development experience, ideally in python, pyspark, kafka or go, and a willingness to learn new software development languages to meet goals and objectives\n* knowledge of strategies for processing large amounts of structured and unstructured data, including integrating data from multiple sources\n* knowledge of data cleaning, wrangling, visualization and reporting\n* ability to explore new alternatives or options to solve data mining issues, and utilize a combination of industry best practices, data innovations and experience\n* familiarity of databases, bi applications, data quality and performance tuning\n* excellent written, verbal and listening communication skills\n* comfortable working asynchronously with a distributed team\n\n**preferred qualifications**\n\n* knowledge of or direct experience with the following aws services desired s3, rds, redshift, dynamodb, emr, glue, and lambda\n* experience working in an agile environment\n* practical knowledge of linux\n\n**apply now**\n\n**learn more about our health and wellness benefits, which provide the security to help you and your family live full lives:** www.abbottbenefits.com\n\n\nfollow your career aspirations to abbott for diverse opportunities with a company that can help you build your future and live your best life. abbott is an equal opportunity employer, committed to employee diversity.\n\n\nconnect with us at www.abbott.com, on facebook at www.facebook.com/abbott and on twitter @abbottnews and @abbottglobal.\n\n  \n\nthe base pay for this position is $61,300\\.00 \u2013 $122,700\\.00\\. in specific locations, the pay range may vary from the range posted.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist",
        "company": "LMI",
        "location": "Colorado Springs, CO, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Git",
            "PostgreSQL",
            "MySQL",
            "MongoDB",
            "NoSQL",
            "Tableau",
            "Power BI",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=62eecaeeda0ea761",
        "description": "overview:\n\nlmi is actively seeking a dynamic, mid\\-level data scientist to join lmi\u2019s space market data scientist team in colorado springs, co. this role involves working directly with ussf and us government customers, as well as analytical partners, to drive the development of responsive, high\\-performance user interfaces within lmi\u2019s modeling, simulation, prototyping, wargaming, \\& analysis platform, raptr\u00ae.  \n\nthe ideal candidate is passionate about space and interested in using data science skills to help guide ussf into the future. in this role, you will join the data science team to analyze simulations of ussf portfolios, to include supporting guardian training. you will be an active contributor in the data science team to create robust analysis pipelines. you\u2019ll work with our high\\-tech customers to understand their technology, investment, performance, resilience, and policy questions and use this insight to craft high\\-performance analytical pipelines for use in workstation, server, and cloud deployment environments.  \n\nlmi is a new breed of digital solutions provider dedicated to accelerating government impact with innovation and speed. investing in technology and prototypes ahead of need, lmi brings commercial\\-grade platforms and mission\\-ready ai to federal agencies at commercial speed.  \n\nleveraging our mission\\-ready technology and solutions, proven expertise in federal deployment, and strategic relationships, we enhance outcomes for the government, efficiently and effectively. with a focus on agility and collaboration, lmi serves the defense, space, healthcare, and energy sectors\u2014helping agencies navigate complexity and outpace change.  \n\nheadquartered in tysons, virginia, lmi is committed to delivering impactful results that strengthen missions and drive lasting value.\nresponsibilities:\n* collaborate with an integrated team of data scientists, data analysts, and functional smes to develop analytical requirements, identify key data elements, adhere to data protection guidelines, and interpret and communicate analytic results\n* lead data science related taskings in direct support of customer requirements\n* identify, propose, and lead in the development of future analytics products based on the data pipeline to assist customers in gaining value from the data products.\n* understand and analyze complex datasets\n* define and develop techniques to integrate, consolidate, and structure data for analytical use\n* lead the maturation of data quality\n* transform data and analysis into informative visualizations and interactive dashboards\n* provide regular detailed reporting to management and briefings to ussf customers\n* advise on the interpretation and use of data analysis products, dashboards, and reports to non\\-technical customers\n\n\nqualifications:\n**required:*** bachelor\u2019s degree in science, engineering, mathematics, computer science, economics, data analytics, or other related business or quantitative discipline with 2\\-5 years of experience (1\\-4 years with master\u2019s degree) in data science.\n* 2\\+ years experience with data science/analysis coding languages like sql, python, or r with agile development methodologies\n* proficiency in either sql (e.g., mysql, postgresql) or nosql (e.g., mongodb) databases.\n* proficiency in developing dashboards using tableau, qlik, power bi, rshiny, plotly, or d3\\.js\n* experience with data science methods related to data architecture, data munging, data and feature engineering, and predictive analytics.\n* experience with version control software e.g git\n* self\\-starter with the vision to independently identify opportunities for improvement through data analytics\n* ability to work in teams and independently\n* excellent communication skills, written and oral\n* innovative problem\\-solving skills and ability to thrive in a fast\\-paced environment\n* us citizenship required, eligible for a ts clearance\n\n**desired:*** advanced degree in computer science or related field.\n* experience using python libraries to build machine learning models like regression models and decision trees\n* passionate desire to make the joint warfighter world a better place with cost\\-effective, performant space architectures which enable operations and mission success\n* space domain experience and familiarity with space\\-focused software solutions.\n* possess a natural curiosity and desire to challenge yourself and those around you\n* knowledge of cybersecurity best practices.\n* active ts clearance with sci eligibility or prior experience working on government\\-related projects",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Dev Ops and Cloud Engineer, Associate",
        "company": "BlackRock",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Copilot",
            "Docker",
            "Kubernetes",
            "AKS",
            "CI/CD",
            "GitHub Actions",
            "Terraform",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e73c18f9abaa746c",
        "description": "locations: **atlanta, georgia**\n  \n**job description**\n-------------------\n\n\n**about this role**\n\n\nblackrock is one of the world\u2019s preeminent asset management firms and a premier provider of global investment management, risk management and advisory services to institutional, intermediary, and individual investors around the world. blackrock\u2019s mission is to create a better financial future for our clients. we have a responsibility to be the voice of the investor, and we represent each client fairly and equally. constant communication with a diverse team of partners strengthens us and delivers better results for our clients. continuous innovation helps us bring the best of blackrock to our clients. blackrock offers a range of solutions \u2014 from rigorous fundamental and quantitative active management approaches aimed at maximizing outperformance to highly efficient indexing strategies designed to gain broad exposure to the world\u2019s capital markets. our clients can access our investment solutions through a variety of product structures, including individual and institutional separate accounts, mutual funds and other pooled investment vehicles, and the industry\\-leading ishares\u00ae etfs.\n\n**job summary**\n\n\nare you interested in building innovative technology that shapes financial markets? do you like working at the speed of a startup but want to tackle complex problems? do you want to work with, and learn from, hands\\-on leaders in technology and finance?\n\n\nresearch solutions is looking for a talented, ambitious engineer to grow and expand the platform engineering team. the team was created to help global research teams focus on alternative data as a new channel for insight. we recognize that strength comes from diversity and will embrace your unique skills, curiosity, and passion while giving you the opportunity to grow technically and as an individual. you will be responsible for aspects of the design, development, reliability, scalability, and observability of our platform.\n\n\nwe\u2019re looking for a hands\\-on **cloud / devops engineer** with strong experience designing, building, and operating **cloud infrastructure** using **terraform (iac)**. you\u2019ll strengthen our security posture, improve reliability and observability, and automate repeatable operational workflows. **python** will be used primarily to build **maintainable automations, lightweight services, and tooling** (with good tests) that improve developer productivity. experience using **ai\\-assisted development tools** to accelerate delivery and improve quality is a plus.\n\n**responsibilities:**\n\n* design, build, and maintain **cloud infrastructure** (e.g., **gcp/aws/azure**) using **terraform** (iac).\n* build and maintain **infrastructure automations**, operational tooling, and lightweight services using **python** (and occasional bash).\n* improve and enforce best practices for **security** (iam, secrets, least privilege), **reliability**, **observability**, and **cost optimization**.\n* partner with software engineers, sre/devops, and stakeholders to deliver platform capabilities and support application teams.\n* troubleshoot complex production and infrastructure issues; perform root cause analysis; implement preventative fixes and runbooks.\n* improve engineering workflows via **automation and modern sdlc practices** (ci/cd, testing, linting, code review standards).\n* drive responsible adoption of **ai\\-assisted development** practices (e.g., code generation, test creation, refactoring support, docs automation) with appropriate guardrails.\n\n**required qualifications:**\n\n* **2\u20135 years** of experience in **cloud / devops / platform engineering.**\n* strong hands\\-on experience with **cloud infrastructure** (gcp, aws, and/or azure); **at least some exposure to gcp** preferred.\n* proven experience with **terraform** for infrastructure provisioning and lifecycle management (modules, remote state, environment patterns).\n* experience building **reliable automation/tooling in python** (maintainable code, testing, packaging basics).\n* solid troubleshooting and **problem\\-solving** skills across infrastructure and production systems.\n* strong **collaboration and communication** skills; ability to work with both technical and non\\-technical partners.\n* practical experience using **ai tools** in software development (e.g., github copilot, windsurf, claude code) to improve quality and delivery.\n\n**preferred qualifications (nice to have):**\n\n* ci/cd experience (e.g., azure devops, github actions, gitlab ci) and release automation.\n* kubernetes experience (e.g., gke/eks/aks) and containerization (docker, artifact registries).\n* security and compliance familiarity (least privilege iam, secrets management, policy\\-as\\-code).\n* observability tooling experience (cloud logging/monitoring, alerting, dashboards).\n\n\nfor atlanta, ga only the salary range for this position is usd$125,000\\.00 \\- usd$170,000\\.00 . additionally, employees are eligible for an annual discretionary bonus, and benefits including healthcare, leave benefits, and retirement benefits. blackrock operates a pay\\-for\\-performance compensation philosophy and your total compensation may vary based on role, location, and firm, department and individual performance.\n**our benefits**  \n\n  \n\nto help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and flexible time off (fto) so you can relax, recharge and be there for the people you care about.\n\n**our hybrid work model**\n\n\nblackrock\u2019s hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. some business groups may require more time in the office due to their roles and responsibilities. we remain focused on increasing the impactful moments that arise when we work together in person \u2013 aligned with our commitment to performance and innovation. as a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at blackrock.\n\n**about blackrock**\n\n\nat blackrock, we are all connected by one mission: to help more and more people experience financial well\\-being. our clients, and the people they serve, are saving for retirement, paying for their children\u2019s educations, buying homes and starting businesses. their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\n\nthis mission would not be possible without our smartest investment \u2013 the one we make in our employees. it\u2019s why we\u2019re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\n\nfor additional information on blackrock, please visit @blackrock \\| twitter: @blackrock \\| linkedin: www.linkedin.com/company/blackrock\n\n\nblackrock is proud to be an equal opportunity workplace. we are committed to equal employment opportunity to all applicants and existing employees, and we evaluate qualified applicants without regard to race, creed, color, national origin, sex (including pregnancy and gender identity/expression), sexual orientation, age, ancestry, physical or mental disability, marital status, political affiliation, religion, citizenship status, genetic information, veteran status, or any other basis protected under applicable federal, state, or local law. **view the** **eeoc\u2019s know your rights poster and its supplement** **and the** **pay transparency statement****.**\n\n\nblackrock is committed to full inclusion of all qualified individuals and to providing reasonable accommodations or job modifications for individuals with disabilities. if reasonable accommodation/adjustments are needed throughout the employment process, please email disability.assistance@blackrock.com. all requests are treated in line with our **privacy policy**.\n\n\nblackrock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law.\n**job requisition \\#**\n  \n\nr261023",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Engineer III",
        "company": "Grainger",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Snowflake",
            "Kafka",
            "Python",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ca71348ca74ae1ce",
        "description": "**work location type:** **hybrid**\n\n\n**req number** 327958\n\n**about grainger**\n\n\nw.w. grainger, inc., is a leading broad line distributor with operations primarily in north america, japan and the united kingdom. at grainger, we keep the world working\u00ae by serving more than 4\\.5 million customers worldwide with products and solutions delivered through innovative technology and deep customer relationships. known for its commitment to service and award\\-winning culture, the company had 2024 revenue of $17\\.2 billion across its two business models. in the high\\-touch solutions segment, grainger offers approximately 2 million maintenance, repair and operating (mro) products and services, including technical support and inventory management. in the endless assortment segment, zoro.com offers customers access to more than 14 million products, and monotaro.com offers more than 24 million products. for more information, visit www.grainger.com.\n\n **compensation**\n\n\nthe anticipated base pay compensation range for this position is $112,900\\.00 to $188,100\\.00\\.\n\n **rewards and benefits**\n\n\nwith benefits starting on day one, our programs provide choice and flexibility to meet team members' individual needs, including:\n\n* medical, dental, vision, and life insurance plans with coverage starting on day one of employment and 6 free sessions each year with a licensed therapist to support your emotional wellbeing.\n* 18 paid time off (pto) days annually for full\\-time employees (accrual prorated based on employment start date) and 6 company holidays per year.\n* 6% company contribution to a 401(k) retirement savings plan each pay period, no employee contribution required.\n* employee discounts, tuition reimbursement, student loan refinancing and free access to financial counseling, education, and tools.\n* maternity support programs, nursing benefits, and up to 14 weeks paid leave for birth parents and up to 4 weeks paid leave for non\\-birth parents.\n\n\nfor additional information and details regarding grainger\u2019s benefits, please click on the link below:\n\n\nhttps://experience100\\.ehr.com/grainger/home/tools\\-resources/key\\-resources/new\\-hire\n\n\nthe pay range provided above is not a guarantee of compensation. the range reflects the potential base pay for this role at the time of this posting based on the job grade for this position. individual base pay compensation will depend, in part, on factors such as geographic work location and relevant experience and skills.\n\n\nthe anticipated compensation range described above is subject to change and the compensation ultimately paid may be higher or lower than the range described above.\n\n\ngrainger reserves the right to amend, modify, or terminate its compensation and benefit programs in its sole discretion at any time, consistent with applicable law.\n\n**position details**\ngrainger is looking for a data engineer for our data and analytics team. the data engineer will report to a manager of data engineering. the data and analytics team's primary mission is to develop analytics by centralizing and integrating high\\-quality, trusted corporate data in a performant and scalable cloud platform. the data engineer will design and develop data pipelines and products in a modern\\-day, cloud environment. you will work with smes, architects, analysts, data scientists and others to build solutions that integrate data from many of our enterprise data sources.\n**you will*** pioneer a new way of thinking about data pipelines, orchestration and configuration at grainger.\n* develop our next\\-generation micro\\-services to enhance and mature a data\\-driven culture\n* help develop the analytics and data kubernetes products in aws\n* experience building data products for data science use cases\n* assemble complex sets of data that meet requirements\n* identify and design internal process improvements including re\\-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\n* build required tools for extraction, transformation and loading of data from data sources\n* build product features to allow self\\-service analytics\n* work with partners including data, design, product and executive teams and assisting them with data\\-related technical issues\n **you have*** bachelor's degree or equivalent experience required\n* 3\\+ years hands\\-on experience with modern data engineering projects and practices (airflow, kafka, spark, and python) required\n* successful track record in developing and automating large\\-scale, high\\-performance data engineering systems (batch and streaming).\n* experience with both scripting and system programming languages (python and scala).\n* experience with microservices including defining and testing apis\n* experience architecting, developing, and deploying both offline and online feature stores\n* experience leading data integration efforts of data sources\n* experience partnering with internal departments (supply chain, marketing, finance, and hr) to establish requirements.\n* develop junior team members through modern cloud\\-based development\n* translate requirements into technical requirements and produce required source\\-to\\-target data mappings.\n* bring complex concepts into our organization and mentor others.\n* experience with advanced analytics and machine learning\n* technology experience required: aws, sql, python, docker/kubernetes, ci/cd, git, snowflake, dbt, airflow\n *we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex (including pregnancy), national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, protected veteran status or any other protected characteristic under federal, state, or local law. we are proud to be an equal opportunity workplace.*\n\n *we are committed to fostering an inclusive, accessible work environment that includes both providing reasonable accommodations to individuals with disabilities during the application and hiring process as well as throughout the course of one\u2019s employment, should you need a reasonable accommodation during the application and selection process, including, but not limited to use of our website, any part of the application, interview or hiring process, please advise us so that we can provide appropriate assistance.*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Machine Learning Engineer: ML Infra and Model Optimization",
        "company": "GENIES",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Machine Learning Engineer",
            "RAG",
            "EC2",
            "FastAPI",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Python",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8f207a3617ebf2e2",
        "description": "genies is an avatar technology company powering the next era of interactive digital identity through smart avatars. with the avatar framework and intuitive creation tools, genies enables developers, talent, and creators to generate and deploy game\\-ready smart ai companions. the company\u2019s technology stack supports full customization, ai\\-generated fashion and props, and seamless integration of user\\-generated content (ugc). backed by investors including bob iger, silver lake, bond, and nea, genies\u2019 mission is to become the visual and interactive layer for the llm\\-powered internet.\n\n\ngenies is an avatar technology company powering the next era of interactive digital identity through ai companions. with the avatar framework and intuitive creation tools, genies enables developers, talent, and creators to generate and deploy game\\-ready ai companions. the company\u2019s technology stack supports full customization, ai\\-generated fashion and props, and seamless integration of user\\-generated content (ugc). backed by investors including bob iger, silver lake, bond, and nea, genies\u2019 mission is to become the visual and interactive layer for the llm\\-powered internet.\n\n\ngenies is looking for a **ml infra and model optimization engineer** to join our r\\&d team. based either in our los angeles or san francisco offices (hybrid), you will work closely with a dedicated and talented team of technical artists, engineers and artists. together, you will explore new concepts and technologies to further genies' mission of empowering users to develop their own avatar ecosystems. we're looking for someone who is passionate about creating high\\-quality visuals and has the technical foundation to help us build the next wave of digital identity.\n\n**what you\u2019ll be doing:**\n\n* design, build, and maintain production\\-grade ml infrastructure for image and 3d generative models.\n* develop and own backend services and apis that support model inference at scale (high concurrency, low latency, high reliability).\n* deploy, monitor, and operate ml models on cloud and large\\-scale platforms (e.g., sagemaker, kubernetes, ray serve, custom gpu services).\n* optimize inference pipelines using model acceleration techniques such as:\n\n\n\t+ quantization, pruning, mixed precision\n\t+ onnx / tensorrt / torch.compile\n* partner with ml researchers to productionize diffusion models, transformer\\-based models, and 3d generation systems.\n* implement evaluation, logging, monitoring, and alerting to ensure system stability and performance.\n* improve end\\-to\\-end system efficiency across data loading, inference, post\\-processing, and storage.\n* support rapid experimentation while maintaining production safety and scalability.\n\n**what you should have:**\n\n* strong experience building backend and infrastructure systems in production environments.\n* proficiency in python and experience designing apis/services (e.g., fastapi, flask, grpc).\n* hands\\-on experience deploying and operating ml models at scale, including:\n\n\n\t+ gpu\\-based inference services\n\t+ concurrency handling and request batching\n\t+ latency and throughput optimization\n* experience with cloud platforms and ml deployment stacks, such as:\n\n\n\t+ aws (sagemaker, ec2, eks), gcp, or similar\n\t+ docker, containers, ci/cd pipelines\n* solid understanding of systems performance, debugging, and reliability engineering.\n* experience supporting real user traffic, not just offline research workflows.\n\n**bonus skills (nice\\-to\\-have)**\n\n* experience with generative models, especially:\n\n\n\t+ diffusion models\n\t+ transformer\\-based architectures\n\t+ multimodal image / 3d pipelines\n* familiarity with 3d generation or computer graphics pipelines (e.g., meshes, textures, multi\\-view data).\n* hands\\-on experience with model optimization and acceleration, such as:\n\n\n\t+ quantization, pruning, distillation\n\t+ onnx runtime, tensorrt, fsdp, deepspeed\n* experience with distributed systems or scalable inference frameworks (ray, triton, torchserve).\n* background in machine learning fundamentals (training, evaluation, model behavior), even if not research\\-focused.\n\n**here's why you'll love working at genies:**\n\n* you'll work with a team that you\u2019ll be able to learn from and grow with, including support for your own professional development\n* you'll be at the helm of your own career, shaping it with your own innovative contributions to a nascent team and product\n* you'll enjoy the culture and perks of a startup, with the stability of being well funded\n* comprehensive health insurance for you and your family (anthem \\+ kaiser options available), dental and vision insurance\n* flexible paid time off, sick time, and paid company holidays, in addition to paid parental leave, bereavement leave, and jury duty leave for full\\-time employees\n* health \\& wellness support through programs such as monthly wellness reimbursement\n* working in a brand new, bright, open\\-environment and fun office space \\- there\u2019s even a slide!\n* choice of macbook or windows laptop\n\n\nsalary range: $215k\\-$275k depending on experience\n\n**genies is an equal opportunity employer committed to promoting an inclusive work environment free of discrimination and harassment. we value diversity, inclusion, and aim to provide a sense of belonging for everyone.**\n\n**how genies will support you**\n\n* competitive salary and equity packages\n* comprehensive health, dental, and vision insurance\n* unlimited pto\n* parental leave\n* hybrid work structure (minimum 4 days in office weekly)\n* monthly wellness reimbursement\n\n\ngenies is a well\\-funded, fast\\-growing start\\-up that values innovation, creativity, and ownership. our roles and their responsibilities are created with a breadth of scope that introduces each employee to exciting new challenges and opportunities that a growing start\\-up encounters. the actual base pay is dependent upon a number of factors, including: professional background, training, transferable skills, work experience, education, location, business and product needs, and market demand. the base pay range is subject to change and may be modified in the future.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Data Engineer",
        "company": "American Academy of Orthopaedic Surgeons",
        "location": "Rosemont, IL, US USA",
        "posted_at": "2026-02-21",
        "score": 13.3,
        "matched_keywords": [
            "RAG",
            "Synapse",
            "Dataflow",
            "CI/CD",
            "Terraform",
            "Git",
            "Snowflake",
            "PySpark",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c11ce56e07f4ef5b",
        "description": "**you are eager to design, build, and optimize enterprise\\-grade data solutions, in a hands\\-on environment. you get excited about building modern, scalable, cloud\\-powered data platforms that drive real organizational impact. you're enthusiastic about solving complex data challenges and working with cutting\\-edge microsoft technologies and want to join a high\u2011performing team that\u2019s transforming how data empowers decision\\-making.**\n\n\n\nif this sounds like you, please read on!\n\n\n**build next\u2011generation data platforms**\n\n\n* design, build, and maintain scalable enterprise data environments using microsoft fabric components including *lakehouse, warehouse, onelake,* and *dataflows gen2*.\n\n**engineering \\& data processing**\n\n\n* develop, optimize, and troubleshoot high\\-performance spark workloads using pyspark and sparksql to process large\\-scale structured and semi\\-structured datasets.\n\n**etl/elt architecture**\n\n\n* architect and implement enterprise\\-grade etl/elt solutions using azure data factory (adf), microsoft fabric data pipelines, dataflows gen2, and ssis (including azure\u2011ssis ir and on\u2011prem migration scenarios).\n\n**data modeling \\& storage**\n\n\n* implement modern data modeling frameworks such as medallion architecture (bronze/silver/gold) and dimensional modeling (star/snowflake) with expertise in file types including *parquet, json,* and *xml*.\n\n**governance \\& lineage**\n\n\n* integrate microsoft purview for enterprise cataloging, classification, and automated lineage across adf, fabric, sql, and adls.\n\n**data security \\& compliance**\n\n\n* enforce robust data security practices including rbac, column\\-level security, dynamic data masking, and alignment with enterprise governance policies.\n\n**travel**\n\n\n* up to 5 days per year\n\n**qualifications:**\n\n\n**required**\n\n\n* hands\\-on experience with microsoft fabric (lakehouse, warehouse, onelake, dataflows gen2, data pipelines) and azure data services (adf, adls, azure sql, synapse).\n* strong proficiency in pyspark and sparksql for building optimized, large\\-scale transformation pipelines.\n* proven experience developing enterprise etl/elt solutions using adf, fabric data pipelines, and ssis (including azure\u2011ssis ir and migration from on\u2011prem).\n* expertise in medallion architecture, dimensional modeling, and working with parquet, json, xml.\n\n**desired:**\n\n\n* experience working with healthcare, registry, clinical, or highly regulated data, with strong knowledge of privacy and compliance.\n* background implementing ci/cd pipelines (azure devops/github), iac (bicep, arm, terraform), and automated deployment of data or governance assets.\n* experience supporting mlops pipelines or enabling ai/advanced analytics workloads in azure/fabric ecosystems.\n* demonstrated ability in fabric capacity optimization, spark performance tuning, and cloud cost management.\n\nsalary range: $112,000\\-$120,000, depending on qualifications and experience.\n\n\n**bring your expertise, creativity, and ambition, and help us build the future of data. if you\u2019re driven by innovation, energized by solving complex data challenges, and excited to work with the very best of microsoft\u2019s cloud ecosystem, apply today!**\n\nplease share the following:\n\n\n* clearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.\n* resumes must be accompanied by a cover letter with salary expectations to be considered.\n\nplease note: this position is based in rosemont, illinois and is open to applicants who are able to commute at least twice per week to this office. applicants must already be authorized to work in the united states on a full\\-time basis. we are unable to sponsor or take over sponsorship of work visas.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI/ML Engineer (Mid-Level)",
        "company": "OnPoint Consulting, Inc",
        "location": "Bethesda, MD, US USA",
        "posted_at": "2026-02-20",
        "score": 13.3,
        "matched_keywords": [
            "Data Scientist",
            "Copilot",
            "TensorFlow",
            "PyTorch",
            "Azure ML",
            "Synapse",
            "Data Lake",
            "CI/CD",
            "Git",
            "Python"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=ecd65ee8e23a06be",
        "description": "**role: ai/ml engineer (mid\\-level)**\n\n**reports to:**  **program manager**\n\n**work hours**: 40 hour week\n\n**onpoint**, a wholly\\-owned subsidiary of sapient government services, is a vibrant, energetic, and growing iso\\-certified technology and management consulting partner that provides critical thinking, strategic analysis, and thought leadership in a collaborative environment in an enduring effort to improve performance, lower costs, and achieve results for clients. we are guided by our principles: clarity. action. results\u2014for each client engagement we work to gain clarity, move into action, and achieve results. onpoint specializes in delivering solutions in cybersecurity, enterprise systems \\& implementation management, cloud architecture \\& infrastructure services, and digital communications \\& strategic marketing.\n\n**job description**\n\n**summary/objective**\n\nthe mid\\-level ai/ml engineer will support the design, deployment, and operation of production\\-grade ai systems that power cpsc\u2019s sentinel model \\- protecting more consumers, faster, from more hazards by using analytics to shorten time to intervention. this role supports end to end model lifecycle engineering on azure, advance mlops best practices, and build ai agents (copilot studio \\+ python frameworks) that translate signals into timely, actionable decisions\n\n**essential functions**\n\n**key responsibilities**\n\n* build \\& ship production models\n* implement and productionize ml solutions (supervised/unsupervised, nlp, deep learning) with robust data preprocessing, feature engineering, and evaluation pipelines.\n* support model selection, training, validation, optimization, and calibration, ensuring reliability, fairness, and performance at scale.\n* own the mlops lifecycle (azure)\n* establish mlops workflows (ci/cd for ml, experiment tracking, model registry, reproducible builds and deployments).\n* implement model monitoring (drift, data/feature quality, bias, and business kpis), alerting, and automated rollback to keep systems safe and responsive.\n* data engineering for ml\n* design high\\-quality data pipelines (ingest, transform, validate) across structured and unstructured sources; enforce data contracts and lineage.\n* partner with analytics teams to make datasets discoverable, documented, and performant for iterative model development.\n* ai agents \\& copilot integration\n* build ai agents that operationalize safety analytics (copilot studio, python agents, retrieval pipelines) to accelerate triage and decision flow.\n* integrate agents with apis, event streams, dashboards, and case management systems to reduce cycle time from signal to action.\n* engineering excellence \\& governance\n* champion secure\\-by\\-design practices, reproducibility, and auditability (model cards, data sheets, deployment records).\n* contribute to coding standards, code reviews, and knowledge sharing; mentor engineers and data scientists.\n* agile collaboration \\& impact\n* work in agile teams; drive iterative delivery, joint problem\\-solving, and continuous improvement.\n* translate mission goals into technical roadmaps and measurable outcomes tied to sentinel time\\-to\\-intervention targets.\n\n**required qualifications**\n\n* experience: 3\\+ years hands\\-on developing and deploying ai/ml models in production environments.\n* programming: proficient in python (including packaging, testing, performance optimization).\n* ml expertise: understanding of algorithms, model selection, training/validation/optimization, and evaluation at scale.\n* data skills: proficient in data preprocessing, feature engineering, and data visualization for decision support.\n* deep learning \\& mlops: proficient with pytorch/tensorflow, and modern mlops (deployment, monitoring, scaling, ci/cd, experiment tracking, model registry).\n* cloud: experience with azure for ai/ml workloads (e.g., azure ml, azure synapse, azure data lake).\n* ai agents: experience developing ai agents in copilot studio and via python frameworks (tooling, orchestration, retrieval, connectors).\n* bachelor\u2019s degree, or equivalent experience in computer science, data science, mathematics, statistics, engineering, related field, or equivalent professional experience.\n\n**preferred qualifications**\n\n* experience with streaming/event\\-driven architectures (event hubs), feature stores, and vector databases (for retrieval augmented generation).\n* hands\\-on with responsible ai (fairness, explainability, privacy), model governance (model cards, audits), and security in cloud ml.\n* familiarity with domain\\-specific risk analytics and public sector/regulated environments.\n* certifications in azure ai/ml and/or mlops advantageous.\n\n**other**\n\n\u00b7 federal and/or commercial experience\n\n\u00b7 technical troubleshooting support\n\n\\- hybrid or remote work environment\n\n**competencies**\n\n\u00b7 self\\-motivated\n\n\u00b7 ability to maintain an extreme sense of urgency in all interactions with the customer\n\n\u00b7 exceptional email management and written communication skills\n\n\u00b7 solution\\-oriented\n\n\u00b7 ability to research, analyze, and create viable solutions\n\n\u00b7 strong technical attitude and ability to quickly learn and apply technical knowledge\n\n\u00b7 excellent troubleshooting and problem\\-solving skills\n\n\u00b7 ability to multi\\-task\n\n**supervisory responsibility**  \nthis position has no supervisory responsibilities.\n\n**work environment**  \non\\-site/remote/hybrid work environment.\n\n**physical demands**  \nwhile performing the duties of this job, the employee may be required to move physical it equipment.\n\n**position type/expected hours of work**  \n40 hour work week\n\n**travel**\n\noccasional travel to a client site may be required.\n\n**additional eligibility qualifications**  \nnone\n\n**work authorization**  \nrequired\n\n**other duties**\n\nplease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. duties, responsibilities, and activities may change at any time with or without notice.\n\n**aap/eeo statement**\n\n*as part of our dedication to an inclusive and diverse workforce, onpoint is committed to equal employment opportunity without regard for race, color, national origin, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity, or religion. we are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. if you need assistance or an accommodation due to a disability, you may contact us at peoplesuccess@onpointcorp.com or you may call us at 703\\-841\\-5500\\.*\n\njob type: full\\-time\n\npay: $125,000\\.00 \\- $150,000\\.00 per year\n\nbenefits:\n\n* 401(k)\n* 401(k) matching\n* dental insurance\n* flexible schedule\n* flexible spending account\n* health insurance\n* health savings account\n* life insurance\n* paid time off\n* parental leave\n* tuition reimbursement\n* vision insurance\n\nexperience:\n\n* ai models: 3 years (required)\n\nwork location: hybrid remote in bethesda, md 20814",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer \u2013 Identity Shield & Consumer Fraud - Shield",
        "company": "Ally Financial",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "CI/CD",
            "Terraform",
            "Git",
            "PostgreSQL",
            "NoSQL",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1496bf62962757c0",
        "description": "### **general information**\n\ncareer area\ntechnology\nwork location(s)\n601 s. tryon street, nc\nremote?\nno\nref \\#\n21748\nposted date\n02\\-20\\-26\nworking time\nfull time\n### **ally and your career**\n\nally financial only succeeds when its people do \\- and that\u2019s more than some clich\u00e9 people put on job postings. we live this stuff! we see our people as, well, people \\- with interests, families, friends, dreams, and causes that are all important to them. our focus is on the health and safety of our teammates as well as work\\-life balance and diversity and inclusion. from generous benefits to a variety of employee resource groups, we strive to build paths that encourage employees to stretch themselves professionally. we want to help you grow, develop, and learn new things. you\u2019re constantly evolving, so shouldn\u2019t your opportunities be, too? **work schedule:** ally designates roles as (1\\) fully on\\-site, (2\\) hybrid, or (3\\) fully remote. hybrid roles are generally expected to be in the office a certain number of days per week as indicated by your manager. your hiring manager will discuss this role's specific work requirements with you during the hiring process. all work requirements are subject to change at any time based on leader discretion and/or business need.\n### **the opportunity**\n\nat ally, you get a startup feel, but experience the benefits of a company that has worked out the kinks and is fulfilling its purpose. we are always evolving and see that as a good thing. from owning our work to seeing its impact in the real world, our team is relentless in finding new ways technology can help make experiences better and help people. we are problem solvers, we value diverse thinking, we support one another, and we challenge ourselves to think bigger in the journey to deliver customer\\-obsessed tech solutions. to read more about what our tech team does, be sure to visit our tech blog at ally.tech\n\n\njoin ally's identity shield and consumer fraud team to build authentication and identity infrastructure that protects our digital banking customers and enables intelligent fraud prevention. you'll develop systems that implement fine\\-grained access control, manage authentication and authorization flows across ally's digital banking ecosystem, and generate the behavioral and contextual signals that power adaptive fraud detection.\n\n\nat this time, ally will not sponsor a new applicant for employment authorization for this position.\n\n\n### **the work itself**\n\nas a backend engineer on this team, you'll tackle fascinating challenges implementing identity protocols at scale\u2014building oauth 2\\.0 and openid connect flows, developing authorization engines that evaluate complex policies with millisecond latency, and serving authentication decisions that directly impact our ability to keep customers safe and prevent fraud. you'll work with modern cloud technologies to build resilient, scalable platforms where performance and reliability are non\\-negotiable. this is your opportunity to shape ally's next\\-generation identity and authentication infrastructure, mastering security\\-first design principles, distributed systems, serverless architectures, and ai\\-assisted development workflows.\n\n\nthis role offers the rare combination of high\\-impact work, cutting\\-edge technology, and meaningful outcomes. you'll collaborate across engineering, product, security, and data science teams to solve complex problems that matter. whether you're developing authentication services that adapt to risk signals, implementing passwordless flows using passkeys and biometrics, building integrations with enterprise identity providers, or designing event streams that carry authentication signals to fraud detection systems, your work will be visible and valued.\n\n\non this team, we believe in relentless progress balanced with sustainable pace. we support each other, celebrate wins, learn from setbacks, and foster an environment where everyone can do their best work. our \\#1 goal is team success, and our people are above all else.\n\n  \n\n\n\n**key responsibilities:**\n\n\n* design and develop authentication and authorization services implementing modern identity protocols (oauth 2\\.0, openid connect, saml, jwt)\n* build scalable apis for identity management, user registration, verification, and account recovery\n* implement session management and token lifecycle systems across web and mobile platforms\n* develop fine\\-grained authorization engines and policy evaluation services for access control\n* build integrations with distributed identity providers, enterprise sso platforms, and social login services\n* implement passwordless authentication flows using passkeys, biometrics, and device attestation\n* develop adaptive authentication logic that adjusts requirements based on risk signals and context\n* instrument authentication events with rich contextual metadata that power fraud detection and analytics\n* implement comprehensive observability using opentelemetry across identity services\n* write clean, tested code in node.js, typescript, and python following engineering best practices and team standards\n* collaborate with frontend engineers, security teams, and data scientists to deliver solutions that drive real outcomes\n* participate in architectural decisions, technical design reviews, and code reviews\n* build and maintain infrastructure\\-as\\-code for identity platform components using terraform\n* monitor, troubleshoot, and optimize platform performance, reliability, and security posture\n* contribute to documentation, runbooks, and knowledge sharing across the team\n* experiment with emerging technologies and propose innovations that enhance platform capabilities\n* mentor team members and contribute to a culture of continuous learning and improvement\n* work in an agile environment with sprint planning, story elaboration, and iterative delivery\n### **the skills you bring**\n\n**minimum qualifications:**\n\n* 7\\+ years of relevant experience or equivalent combination of education and experience\n* high school diploma or ged equivalent\n\n**preferred qualifications:**\n\n* 7\\+ years of professional software development experience with strong fundamentals in data structures and algorithms\n* strong experience with restful api design, graphql, and microservices architectures\n* hands\\-on experience with aws serverless technologies (lambda, api gateway, appsync, ecs/fargate)\n* solid understanding of authentication and authorization concepts, patterns, and best practices\n* experience working with both relational databases (postgresql) and nosql databases (dynamodb) in production environments\n* strong grasp of security principles including encryption, token management, secret handling, and threat modeling\n* passion for writing maintainable code, automated tests, and clear documentation\n* collaborative mindset with excellent communication skills across technical and non\\-technical audiences\n* self\\-motivated learner who stays curious about new technologies and approaches\n* bsc/ba in computer science, engineering or a related field, or equivalent practical experience\n* deep knowledge of identity protocols and standards (oauth 2\\.0, openid connect, saml, jwt, pkce, webauthn)\n* experience building authentication or authorization systems from scratch or at scale\n* familiarity with identity providers and platforms (auth0, okta, cognito etc.)\n* understanding of passwordless authentication patterns (passkeys, fido2, webauthn)\n* experience with mobile authentication patterns (biometric authentication, device binding, push notifications)\n* knowledge of zero\\-trust architecture principles and continuous verification patterns\n* experience designing adaptive or risk\\-based authentication systems\n* familiarity with session management patterns and token rotation strategies\n* understanding of cryptographic concepts (signing, encryption, key management, certificate handling)\n* experience with infrastructure as code tools like terraform\n* knowledge of observability tools and distributed tracing (dynatrace, opentelemetry, cloudwatch, x\\-ray)\n* experience designing and deploying systems across multi\\-region architectures\n* familiarity with caching strategies for authentication state (elasticache, redis)\n* understanding of compliance requirements (soc2, pci\\-dss, gdpr, ccpa)\n* experience with ci/cd pipelines and automated security testing (sast, dast, dependency scanning)\n* background or interest in cybersecurity, fraud detection, or identity and access management domains\n* knowledge of event\\-driven architectures and how authentication signals power fraud detection systems\n* exposure to ai\\-assisted development tools and workflows\n\n\\#li\\-hybrid\n### **how we'll have your back**\n\nally's compensation program offers market\\-competitive base pay and pay\\-for\\-performance incentives (bonuses) based on achieving personal and company goals. our total rewards program includes industry\\-leading compensation and benefits plus additional incentives that are designed to meet your needs and those of your family so you can get the most out of your career and your life, including:* **time away:** program starts at 20 paid time off days in addition to 11 paid holidays and 8 hours of volunteer time off yearly (time off days are prorated based on start date and program varies based on full or part\\-time status and management level).\n* **planning for the future:** plan for the near and long term with an industry\\-leading 401k retirement savings plan with matching and company contributions, student loan pay downs and 529 educational save up assistance programs, tuition reimbursement, employee stock purchase plan, and financial learning center and financial coach access.\n* **supporting your health \\& well\\-being:** flexible health and insurance options including medical, dental and vision, employee, spouse and child life insurance, short\\- and long\\-term disability, pre\\-tax health savings account with employer contributions, healthcare fsa, critical illness, accident \\& hospital indemnity insurance, and a total well\\-being program that helps you and your family stay on track physically, socially, emotionally, and financially.\n* **building a family:** adoption, surrogacy and fertility assistance as well as paid parental and caregiver leave, dependent day care fsa back\\-up child and adult/elder care days and childcare discounts.\n* **work\\-life integration:** other benefits including mentally fit employee assistance program, subsidized and discounted weight watchers\u00ae program and other employee discount programs.\n* **other compensations:** depending on the role for which you are considered, you may be eligible for travel allowances, relocation assistance, a signing bonus and/or equity.\n* to view more detailed information about ally\u2019s total rewards, please visit this link: https://www.ally.com/content/dam/pdf/corporate/ally\\-total\\-rewards\\-snapshot.pdf\n\n **who we are:**\n\n  \n\nally financial is a customer\\-centric, leading digital financial services company with passionate customer service and innovative financial solutions. we are relentlessly focused on \"doing it right\" and being a trusted financial\\-services provider to our consumer, commercial, and corporate customers. for more information, visit www.ally.com.\n\n  \n\nally is an equal opportunity employer committed to diversity and inclusion in the workplace. all qualified applicants will receive consideration for employment without regard to age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity or expression, pregnancy status, marital status, military or veteran status, genetic disposition or any other reason protected by law.\n\n  \n\nwe are committed to working with and providing reasonable accommodation to applicants with physical or mental disabilities. for accommodation requests, email us at hrpolicy@ally.com. ally will not discriminate against any qualified individual who is capable of performing the essential functions of the job with or without reasonable accommodation.\n\n\n**base pay range**: $110000 \\- $180000 usd\nan individual's position in the range is determined by the specific role, the scope and responsibilities of the role, work experience, education, certification(s), training, and additional qualifications. we review internal pay, the competitive market, and business environment prior to extending an offer.\nincentive compensation: this position is eligible to participate in our annual incentive plan.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer",
        "company": "McKesson",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Terraform",
            "Git",
            "Kafka",
            "PostgreSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a1ace665b571d0bb",
        "description": "mckesson is an impact\\-driven, fortune 10 company that touches virtually every aspect of healthcare. we are known for delivering insights, products, and services that make quality care more accessible and affordable. here, we focus on the health, happiness, and well\\-being of you and those we serve \u2013 we care.\n\n\nwhat you do at mckesson matters. we foster a culture where you can grow, make an impact, and are empowered to bring new ideas. together, we thrive as we shape the future of health for patients, our communities, and our people. if you want to be part of tomorrow\u2019s health today, we want to hear from you.\n\n\nsoftware engineer (p3\\)\n\n\nas a p3 software engineer you will help redesign and rebuild our next\\-generation platform on a modern cloud\\-native technology stack. you will work on small, cross\\-functional, and highly collaborative teams to deliver secure, scalable services that make our healthcare system work more efficiently and effectively. you will have opportunities to take on new responsibility, grow your technical skills, and contribute to a greenfield initiative that is strategically critical to our business.\n\n**our tech stack**\n------------------\n\n**primary skills**: c\\# / .net, typescript, react, postgres, restful apis / graphql\n\n**secondary skills**: kafka, git/version control, terraform, ci/cd pipelines, unit testing frameworks (xunit, jest)\n\n**nice to have**: azure cloud platform, redis, microservices architecture, mfe architecture\n\n**ai \\& tooling:** experience using ai\\-assisted development, testing, and documentation tools (e.g., code\\-generation, test\\-generation, static analysis, or observability tools).\n\n**what you'll do**\n------------------\n\n* design, develop, and maintain web services, apis, and background jobs for the platform using c\\# and typescript.\n* collaborate with product, architecture, qa, and fellow engineers to refine requirements into well\\-designed, testable solutions.\n* own the quality of the solutions you create through unit tests, code reviews, and observability, and support your services in production.\n* contribute to building event\\-driven and microservice\\-based architectures using kafka and other cloud services.\n* participate in backlog refinement, estimation, sprint planning, and retrospectives as part of an agile delivery team.\n* leverage ai tools and automation to accelerate development, testing, and documentation where appropriate.\n* continuously learn and share knowledge with your team to improve engineering practices, patterns, and standards.\n\n\nminimum qualifications:\\-\n\n* 3\\+ years of experience in back\\-end or full\\-stack software development.\n\n**about you**\n-------------\n\n\ntechnical skills:\n\n* 3\\+ years of experience in back\\-end or full\\-stack software development.\n* hands\\-on experience with at least one modern object\\-oriented language (c\\# preferred) and javascript/typescript.\n* familiarity with restful apis and an interest in graphql\\-based api design.\n* experience with relational databases (postgresql preferred), including schema design and query optimization basics.\n* exposure to event\\-driven or microservice architectures and messaging technologies (kafka preferred).\n* experience writing and maintaining unit tests using common frameworks.\n* comfortable working with git, ci/cd pipelines, and modern devops practices.\n\n\nnice to have:\n\n* experience with react or other modern front\\-end frameworks.\n* exposure to mulesoft or other integration platforms.\n* experience building or consuming graphql apis in production.\n* familiarity with cloud platforms (azure preferred) and infrastructure\\-as\\-code tools such as terraform.\n* experience using ai tools to assist with development, testing, or documentation.\n\n\nnon\\-technical skills:\n\n* strong contributor on an agile team with a bias toward collaboration and shared ownership.\n* able to break down work into small, testable increments and deliver iteratively.\n* comfortable asking questions, giving and receiving feedback, and working through ambiguity.\n* proactively share information so that the right people are informed and aligned.\n\n**education \\& experience**\n---------------------------\n\n\ndegree in computer science, software engineering, or related field, or equivalent experience.\n\n\ntypically requires 3\\+ years of relevant software engineering experience.\n\n\nwe are proud to offer a competitive compensation package at mckesson as part of our total rewards. this is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. the pay range shown below is aligned with mckesson's pay philosophy, and pay will always be compliant with any applicable regulations. in addition to base pay, other compensation, such as an annual bonus or long\\-term incentive opportunities may be offered. for more information regarding benefits at mckesson, please click here.\n\n**our base pay range for this position**\n\n\nmckesson has become aware of online recruiting\\-related scams in which individuals who are not affiliated with or authorized by mckesson are using mckesson\u2019s (or affiliated entities, like covermymeds or rxcrossroads) name in fraudulent emails, job postings or social media messages. in light of these scams, please bear the following in mind:  \n\n  \n\nmckesson talent advisors will never solicit money or credit card information in connection with a mckesson job application.\n\n  \n\nmckesson talent advisors do not communicate with candidates via online chatrooms or using email accounts such as gmail or hotmail. note that mckesson does rely on a virtual assistant (gia) for certain recruiting\\-related communications with candidates.\n\n\nmckesson job postings are posted on our career site: careers.mckesson.com.\n\n**mckesson is an equal opportunity employer**\n\n  \n\nmckesson provides equal employment opportunities to applicants and employees, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, genetic information, or any other legally protected category. for additional information on mckesson\u2019s full equal employment opportunity policies, visit our equal employment opportunity page.\n\n  \n\nmckesson welcomes and encourages applications from people with disabilities. accommodations are available on request for candidates taking part in all aspects of the selection process. if you require accommodation please contact us by sending an email to disability\\_accommodation@mckesson.com.\n\n**join us at mckesson!**",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Sr Software Engineer",
        "company": "McKesson",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Terraform",
            "Git",
            "Kafka",
            "PostgreSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=75d8c3155721fa1b",
        "description": "mckesson is an impact\\-driven, fortune 10 company that touches virtually every aspect of healthcare. we are known for delivering insights, products, and services that make quality care more accessible and affordable. here, we focus on the health, happiness, and well\\-being of you and those we serve \u2013 we care.\n\n\nwhat you do at mckesson matters. we foster a culture where you can grow, make an impact, and are empowered to bring new ideas. together, we thrive as we shape the future of health for patients, our communities, and our people. if you want to be part of tomorrow\u2019s health today, we want to hear from you.\n\n\nas a p4 senior software engineer you will help redesign and rebuild our next\\-generation platform on a modern cloud\\-native technology stack. you will work closely with architects, staff engineers, product, and quality engineering to build secure, scalable, and resilient services. as a senior engineer, you will not only contribute hands\\-on to the codebase but also provide technical leadership for your team, mentor other engineers, and help define the engineering practices that enable us to deliver high\\-quality solutions at velocity.\n\n**our tech stack**\n------------------\n\n**primary skills**: c\\# / .net, typescript, react, postgres, restful apis / graphql\n\n**secondary skills**: kafka, git/version control, terraform, ci/cd pipelines, unit testing frameworks (xunit, jest)\n\n**nice to have**: azure cloud platform, redis, microservices architecture, mfe architecture\n\n**ai \\& tooling:** experience using ai\\-assisted development, testing, and observability tools (e.g., code generation, test generation, static analysis, performance analysis) and a willingness to help the team adopt these tools effectively.\n\n**what you'll do**\n------------------\n\n* design, develop, and maintain complex web services, apis, and background jobs for the platform using c\\# and typescript.\n* lead the technical design for features and services, collaborating with architects and staff engineers to ensure solutions align with target architecture and non\\-functional requirements.\n* own the quality, reliability, and operability of the services you and your team build, including unit tests, integration tests, performance considerations, and production support.\n* guide the team in building event\\-driven and microservice\\-based architectures using kafka and other cloud services.\n* influence and improve engineering practices, coding standards, code review quality, and ci/cd pipelines across your team.\n* use ai\\-assisted tools and automation to improve developer productivity, test coverage, and code quality, and help others on the team do the same.\n* partner with product managers and stakeholders to break down work into small, testable increments and help maintain a healthy, well\\-groomed backlog.\n* mentor and coach p3 and early\\-career engineers, providing feedback and support to help them grow.\n\n\nminimum qualifications: \\-\n\n\ntypically 6\\+ years of experience in back\\-end or full\\-stack software development, including ownership of complex features or services in production.\n\n**about you**\n-------------\n\n**technical skills:**\n\n* typically 6\\+ years of experience in back\\-end or full\\-stack software development, including ownership of complex features or services in production.\n* strong hands\\-on experience with c\\#/.net and javascript/typescript in production environments.\n* deep understanding of restful api design and practical experience with graphql\\-based apis.\n* strong experience with relational databases (postgresql preferred), including schema design, query optimization, and performance troubleshooting.\n* hands\\-on experience building or maintaining microservice or event\\-driven architectures using messaging technologies such as kafka.\n* proven experience designing and maintaining unit, integration, and component tests as part of ci/cd pipelines.\n* experience operating services in production, including monitoring, logging, alerting, and incident response.\n* comfortable working with git, trunk\\-based development or similar branching strategies, and modern ci/cd and devops practices.\n\n**nice to have:**\n\n* experience with react or other modern front\\-end frameworks, and comfort working across the stack when needed.\n* experience with mulesoft or other integration platforms.\n* experience designing and implementing graphql schemas and resolvers for complex domains.\n* hands\\-on experience with azure (preferred) or other cloud platforms, plus infrastructure\\-as\\-code tools such as terraform or arm/bicep.\n* experience integrating security\\-by\\-design practices (authentication, authorization, secrets management, data protection) into application code and pipelines.\n* experience using ai tools to assist with design, development, testing, or production support, and a point of view on when and how to use them responsibly.\n\n**non\\-technical skills:**\n\n* demonstrated leadership on an agile team, including helping to define ways of working and facilitating technical discussions.\n* ability to navigate ambiguity, make tradeoffs, and drive work to completion across complex, multi\\-phase projects.\n* strong communication skills with the ability to explain technical concepts to both technical and non\\-technical audiences.\n* proactively share information, surface risks, and align stakeholders across engineering, product, qa, and operations.\n* mentors others through code reviews, pairing, and structured feedback, and contributes to a culture of learning and continuous improvement.\n\n**education \\& experience**\n---------------------------\n\n\ndegree in computer science, software engineering, or related field, or equivalent experience.\n\n\ntypically requires 6\\+ years of relevant software engineering experience, with evidence of technical leadership and ownership of production systems.\n\n\nwe are proud to offer a competitive compensation package at mckesson as part of our total rewards. this is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. the pay range shown below is aligned with mckesson's pay philosophy, and pay will always be compliant with any applicable regulations. in addition to base pay, other compensation, such as an annual bonus or long\\-term incentive opportunities may be offered. for more information regarding benefits at mckesson, please click here.\n\n**our base pay range for this position**\n\n\n$126,000 \\- $210,000\nmckesson has become aware of online recruiting\\-related scams in which individuals who are not affiliated with or authorized by mckesson are using mckesson\u2019s (or affiliated entities, like covermymeds or rxcrossroads) name in fraudulent emails, job postings or social media messages. in light of these scams, please bear the following in mind:  \n\n  \n\nmckesson talent advisors will never solicit money or credit card information in connection with a mckesson job application.\n\n  \n\nmckesson talent advisors do not communicate with candidates via online chatrooms or using email accounts such as gmail or hotmail. note that mckesson does rely on a virtual assistant (gia) for certain recruiting\\-related communications with candidates.\n\n\nmckesson job postings are posted on our career site: careers.mckesson.com.\n\n**mckesson is an equal opportunity employer**\n\n  \n\nmckesson provides equal employment opportunities to applicants and employees, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, genetic information, or any other legally protected category. for additional information on mckesson\u2019s full equal employment opportunity policies, visit our equal employment opportunity page.\n\n  \n\nmckesson welcomes and encourages applications from people with disabilities. accommodations are available on request for candidates taking part in all aspects of the selection process. if you require accommodation please contact us by sending an email to disability\\_accommodation@mckesson.com.\n\n**join us at mckesson!**",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Platform Engineer II",
        "company": "Best Egg",
        "location": "Wilmington, DE, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "S3",
            "Data Lake",
            "Docker",
            "CI/CD",
            "Git",
            "Snowflake",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=1ff6ad916136713c",
        "description": "best egg is a market\\-leading, tech\\-enabled financial platform helping people build financial confidence through a variety of installment lending solutions and financial health tools. we aim to help customers make smart financial decisions and stay on track, so they can be money confident no matter what life throws at them.\nwe offer top\\-tier benefits and growth opportunities in a culture built on our core values: **put people first** \u2013 we foster an inclusive, flexible, and fun workplace.**create clarity** \u2013 open communication drives trust and results.**get things done** \u2013 we focus, prioritize, and deliver with excellence.**deliver with heart** \u2013 we lead with kindness, humility, and strong teamwork.**listen to our customers** \u2013 their needs drive our innovation.\n\nbarclays has entered into an agreement to acquire best egg with closing expected to take place in q2 2026\\. this acquisition will give us the resources and capital to continue on our mission and drive our strategy forward. with an aligned culture, lower cost of funds, and increased employee growth opportunities across a global brand, we are excited about the future of the best egg brand under the barclays umbrella.\nwe are looking for collaborative, innovative team players who like to solve problems. there will also be immense opportunities for those willing to dive in. if you're inspired by growth and want to make a real difference, best egg is the place for you.  \n\n**we\u2019re proud to be an equal opportunity employer** committed to building a diverse, inclusive team.  \n\nat best egg, we\u2019re a cloud\u2011first organization embracing new ideas, modern tooling, and forward\u2011leaning architecture. we work closely with snowflake, aws \\& other partners to push our data platform forward and unlock new ways to deliver value. snowflake powers our analytics foundation, supported by event\u2011driven patterns, python, postgres, json data, and aws data lake. we deploy through a modern ci/cd and infrastructure\u2011as\u2011code pipeline, running workloads across both managed containers and serverless environments \u2014 always choosing the right tool for the job.\n**job summary:**  \n\nwe are seeking a highly skilled and experienced **data platform engineer ii** to join our team. the ideal candidate brings deep expertise across data platforms, data management, data products, sql, cloud\u2011first engineering, and modern devops practices as a seasoned data engineer with a track record of owning complex data initiatives end\u2011to\u2011end. experience with snowflake and aws is strongly preferred. must be an advisor and confidante on data governance, best practices, and platform maturity. this is not a junior or entry\u2011level role \u2014 we are looking for someone who brings depth, judgment, and leadership.  \n\nin this role, you will lead the design, development, and scaling of our data platform capabilities. you will work across diverse data domains, drive data discovery for new and evolving requirements, and build robust data transfer and migration solutions. you will also play a key role in advancing our data governance and data product maturity, ensuring our platform is reliable, scalable, and aligned with best\u2011in\u2011class engineering standards. you should be equally comfortable acting as an independent contributor, a team player, and a advisor.  \n\nthis is an exciting opportunity for a motivated engineer who is passionate about building, improving, and maturing data ecosystems. if you thrive in cloud\u2011native environments, enjoy solving complex data challenges, and want to influence the direction of our data strategy, we encourage you to apply.\n### **duties and responsibilities:**\n\n* + design, build, and deploy data platform components across snowflake, aws, and the broader technology stack.\n\t+ architect, implement, and support data capabilities including automations, integrations, interfaces, and underlying infrastructure.\n\t+ develop, maintain, and optimize ci/cd pipelines to ensure reliable and repeatable data delivery.\n\t+ implement and manage monitoring, logging, and alerting frameworks to ensure the availability, quality, and performance of data assets.\n\t+ serve as a data leader, data champion and subject\u2011matter expert for engineering, product, and business teams, providing mentorship and guidance to junior engineers.\n\t+ collaborate with cross\u2011functional teams to identify, troubleshoot, and solve complex technical and data\u2011related challenges.\n\t+ ensure adherence to security, privacy, and compliance standards, including soc2 and pci\u2011dss.\n\n### **required skills:**\n\n* + 7\\+ years of experience in data management and etl/elt development, with a strong focus on building high\u2011quality, scalable data products.\n\t+ advanced proficiency in python and sql.\n\t+ hands\u2011on experience with aws services such as lambda, s3, serverless architectures, and containerized workloads (e.g., docker).\n\t+ strong background in devops practices, automation, and modern deployment workflows.\n\t+ experience with configuration management tools\n\t+ proven experience designing, building, and maturing data products and data platform capabilities.\n\t+ experience working across both on\u2011premises data environments and cloud\u2011first architectures.\n\t+ excellent analytical and problem\u2011solving skills.\n\t+ strong communication, collaboration, and leadership abilities.\n\n### **recommended skills:**\n\n* + experience with snowflake\n\t+ experience with monte carlo \u2013 observability tool\n\t+ experience with aws \"serverless\" technologies (e.g. lambda, step functions, fargate)\n\t+ knowledge of github products (actions, workspaces, secrets, etc)\n\n$115,000 \\- $140,000 a year\nthis position is also eligible for an annual incentive bonus based on individual and company performance. yearly incentive bonus target 20% of base salary.\n**employee benefits**\nbest egg offers many additional benefits for our employees, including (but not limited to):* pre\\-tax and post\\-tax retirement savings plans with a competitive company matching\n\n\nprogram* generous paid time\\-off plans including vacation, personal/sick time, paid short\\-\n\nterm and long\\-term disability leaves, paid parental leave, and paid company\nholidays\n* multiple health care plans to choose from, including dental and vision options\n* flexible spending plans for health care, dependent care, and health\n\nreimbursement accounts* company\\-paid benefits such as life insurance, wellness platforms, employee\n\n\nassistance programs, and health advocate programs* other great discounted benefits include identity theft protection, pet insurance,\n\n\nfitness center reimbursements, and many more!  \n\nin compliance with the ccpa, best egg is fully committed to handling the personal information and data of employees and job applications responsibly with respect and due care.\nwe may use artificial intelligence (ai) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. these tools assist our recruitment team but do not replace human judgment. final hiring decisions are ultimately made by humans. if you would like more information about how your data is processed, please contact us.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Backend Engineer Intern (LLM)",
        "company": "GENIES",
        "location": "Los Angeles, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "AI Engineer",
            "RAG",
            "Prompt Engineering",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Git",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=c0fa41739b22a607",
        "description": "genies is an avatar technology company powering the next era of interactive digital identity through smart avatars. with the avatar framework and intuitive creation tools, genies enables developers, talent, and creators to generate and deploy game\\-ready smart ai companions. the company\u2019s technology stack supports full customization, ai\\-generated fashion and props, and seamless integration of user\\-generated content (ugc). backed by investors including bob iger, silver lake, bond, and nea, genies\u2019 mission is to become the visual and interactive layer for the llm\\-powered internet.\n\n\ngenies is an avatar technology company powering the next era of interactive digital identity through ai companions. with the avatar framework and intuitive creation tools, genies enables developers, talent, and creators to generate and deploy game\\-ready ai companions. the company\u2019s technology stack supports full customization, ai\\-generated fashion and props, and seamless integration of user\\-generated content (ugc). backed by investors including bob iger, silver lake, bond, and nea, genies\u2019 mission is to become the visual and interactive layer for the llm\\-powered internet.\n\n### **about the opportunity**\n\n\nwe are looking for a **backend software engineer intern (llm)** to join our ai engineering team based in san francisco, ca or los angeles, ca. the team is responsible for developing the backend infrastructure powering the genies avatar ai framework. you will contribute to the next generation of ai 3d avatar entertainment experience, and be involved with designing, coding, and testing software according to the requirements and system plans. you will be expected to collaborate with senior engineers and other team members to develop software solutions, troubleshoot issues, and maintain the quality of our software. you will also be responsible for documenting their work for future reference and improvement.\n\n\nour internship program has a minimum duration of 12 weeks.\n\n### **key responsibilities**\n\n* develop and deploy llm agent systems within our ai\\-powered avatar framework.\n* design and implement scalable and efficient backend systems to support ai applications\n* collaborate with ai and nlp experts to integrate llm, and llm\\-based systems and algorithms into our avatar ecosystem.\n* work with docker, kubernetes, and aws for ai model deployment and scalability\n* contribute to code reviews, debugging, and testing to ensure high\\-quality deliverables.\n\n### **minimum qualifications**\n\n* currently pursuing or a recent graduate from a master's degree or bachelor's in computer science, engineering, machine learning, or related field.\n* course or internship experience related to the following areas : operating systems, data structures \\& algorithms, machine learning\n* strong programming skills in python, java, or c\\+\\+\n* excellent written and verbal communication skills\n* basic understanding of ai/llm concepts and enthusiasm for learning advanced techniques.\n\n### **preferred qualifications**\n\n* experience in building ml /llm powered software systems.\n* previous computer science/software engineering internship experience\n* solid understanding of llm agents, retrieval\\-augmented generation (rag), and prompt engineering.\n* experience with aws, docker and kubernetes\n* experience with ci/cd pipelines\n* experience with api design, schema design\n\n**here's why you'll love working at genies:**\n\n* salary $40\\-$50 per hour.\n* you'll work with a team that you\u2019ll be able to learn from and grow with, including support for your own professional development\n* you'll be at the helm of your own career, shaping it with your own innovative contributions to a nascent team and product with flexible hours and a hybrid(office\\+home) policy\n* you'll enjoy the culture and perks of a startup, with the stability of being well funded\n* flexible paid time off, sick time, and paid company holidays, in addition to paid parental leave, bereavement leave, and jury duty leave for full\\-time employees\n* health \\& wellness support through programs such as monthly wellness reimbursement\n* choice of macbook or windows laptop\n\n**genies is an equal opportunity employer committed to promoting an inclusive work environment free of discrimination and harassment. we value diversity, inclusion, and aim to provide a sense of belonging for everyone.**\n\n**how genies will support you**\n\n* competitive salary and equity packages\n* comprehensive health, dental, and vision insurance\n* unlimited pto\n* parental leave\n* hybrid work structure (minimum 4 days in office weekly)\n* monthly wellness reimbursement\n\n\ngenies is a well\\-funded, fast\\-growing start\\-up that values innovation, creativity, and ownership. our roles and their responsibilities are created with a breadth of scope that introduces each employee to exciting new challenges and opportunities that a growing start\\-up encounters. the actual base pay is dependent upon a number of factors, including: professional background, training, transferable skills, work experience, education, location, business and product needs, and market demand. the base pay range is subject to change and may be modified in the future.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI Software Engineer (contract)",
        "company": "Hilton",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "Generative AI",
            "LangChain",
            "S3",
            "FastAPI",
            "CI/CD",
            "Jenkins",
            "Git",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=8576dc30077a3e83",
        "description": "please note that this is a contract role providing services to hilton through talent solutions. if you are selected for this role, you will be employed by talent solutions and will not be an employee of hilton. contract workers are not employees of hilton or any of its subsidiaries, nor will they be eligible for hilton benefits.\n\n\n\\*\\*\\*this is a us based remote contract role with a preference for you to be near one of our three us office locations (mclean, va, dallas, tx or memphis, tn) or willing to relocate to one of these locations in the event we convert this position to a full\\-time opportunity\\*\\*\\*  \n\n  \n\n**job summary:**  \n\njoin a technology team that is revolutionizing the hospitality industry through cutting\\-edge, consumer\\-facing technologies. this opportunity allows you to work on transformative projects including ai stay planners, ai agents, and llm integrations. you will develop and implement scalable ai\\-driven solutions that enhance both developer productivity and customer experience. this hybrid position offers the chance to work with modern technologies while contributing to projects that directly impact millions of travelers worldwide.  \n\n  \n\n**responsibilities:**  \n\n* build api, mcp servers, ai agents, and llm interactions using python, fastapi, langchain, or strands\n* develop full stack web applications using react, typescript, tailwind css, vitest, and testinglibrary on the front end\n* design and implement scalable cloud applications using aws services such as s3, eks, elasticache, bedrock, agentcore and guardrails\n* build observability solutions using datadog and splunk\n* propose initial ui/ux concepts and flows and iterate based on feedback from product and ux teams\n* work with security and legal teams to evaluate ai assistants and maintain them\n* collaborate with software engineering teams to adapt tools to company codebases by implementing custom instructions, custom agents and agent skills\n* partner with delivery managers, software engineers and other team members to translate requirements into technical solutions, participating in sprint planning, design reviews, and architecture discussions\n* guide the implementation of ai agents for developer productivity and customer experience use cases, including designing api integrations, building evaluation pipelines, and establishing best practices for agent workflows\n* manage production deployment, monitoring and support for ai and agentic use cases\n\n  \n\nskills:  \n\n* proficiency in python, fastapi, langchain, or strands\n* strong experience with react, typescript, tailwind css, vitest, and testinglibrary\n* expertise in aws services including s3, eks, elasticache, bedrock, agentcore and guardrails\n* knowledge of observability tools such as datadog and splunk\n* experience with generative ai\\-driven solutions\n* strong collaboration and communication skills\n* ability to translate business requirements into technical solutions\n* learning mindset with ability to become proficient in new technologies\n* experience with ci/cd tools such as jenkins and gitlab\n* familiarity with no\\-sql databases such as elastic cache or open search\n\n  \n\nqualifications:  \n\n* three years of professional work experience in technology or related field\n* one year of work experience in generative ai\\-driven solutions\n\n  \n\nopportunity to be at the forefront of ai innovation within the hospitality industry, working on projects that directly impact customer experiences and developer productivity.\n**pay rate range**\n\n\n50 \\- 70 usd hourly\n\n\n**additional notes**\n\n\npay rate will vary based on level of experience",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer - Database Integrations",
        "company": "clickhouse",
        "location": "Remote, US USA",
        "posted_at": "2026-02-21",
        "score": 12.2,
        "matched_keywords": [
            "RAG",
            "BigQuery",
            "Data Lake",
            "Kubernetes",
            "Snowflake",
            "BigQuery",
            "Kafka",
            "MySQL",
            "MongoDB",
            "SQL"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=77453e14526da77d",
        "description": "### **about clickhouse**\n\n\n\nrecognized on the 2025 forbes cloud 100 list, clickhouse is one of the most innovative and fast\\-growing private cloud companies. with more than 3,000 customers and arr that has grown over 250 percent year over year, clickhouse leads the market in real\\-time analytics, data warehousing, observability, and ai workloads.\n\n\n\nthe company\u2019s sustained, accelerating momentum was recently validated by a $400m series d financing round. over the past three months, customers including capital one, lovable, decagon, polymarket, and airwallex have adopted the platform or expanded existing deployments. these customers join an established base of ai innovators and global brands such as meta, cursor, sony, and tesla.\n\n\n\nwe\u2019re on a mission to transform how companies use data. come be a part of our journey!\n\n**note:**. we are hiring for this role remotely in any country clickhouse has a hiring presence.\n\n\n### **about the team**\n\n\n\nthe **clickpipes \\- database integrations** team builds the platform that enables real\\-time data replication from databases into clickhouse at petabyte scale.\n\n\n\nas a member of this team, you will be solving complex database\\-related challenges and distributed systems problems, such as understanding database internals to optimize snapshotting strategy, handling schema evolution during live replication, managing data type compatibility across systems, maintaining low end\\-to\\-end latency under unpredictable loads, and leveraging durable execution frameworks to ensure data consistency over unreliable networks. we work in the open \u2014 our database integrations are built on peerdb, an open\\-source cdc platform we actively maintain and contribute to.\n\n\n\nsee some of our recent work:\n\n\n* clickpipes for postgres now supports failover replication slots\n* mongodb cdc to clickhouse with native json support\n* under the hood: building mysql change data capture in clickpipes\n\n### **what you\u2019ll do:**\n\n\n* **build data\\-intensive systems**\n\n\n\t+ design and develop high\\-throughput integrations with databases (postgres, mysql, mongodb), data lakes (iceberg, delta lake), and data warehouses (bigquery, snowflake).\n\t+ handle edge cases in real\\-world production scenarios: unconventional database setups, internals of data types, database upgrades/failovers, large transactions, etc.\n\t+ design integration solutions to enable users to fully harness clickhouse's performance and throughput.**own end\\-to\\-end reliability**\n\n\n\t+ debug complex issues in production leveraging runtime diagnostics (e.g. pprof, parca) and observability tools (e.g. metrics, logging, tracing).\n\t+ build and improve infrastructure and tools to increase system reliability, reduce incident response time, and simplify/automate operations.\n\t+ write clear documentation, both publicly and internally.\n\t+ participate in on\\-call rotation.**drive product innovation**\n\n\n\t+ work directly with customers to understand integration requirements and discover gaps in existing product.\n\t+ collaborate cross\\-functionally with internal teams to ensure operational efficiency.\n\t+ lead technical discussions and influence product roadmaps.\n\n### **about you:**\n\n\n* 5\\+ years of industry experience building data\\-intensive software solutions.\n* proficient in go, or experienced in systems programming with willingness to ramp up quickly in go.\n* cloud\\-native experience deploying and operating services on at least one major cloud platform (aws/gcp/azure).\n* practical experience with kubernetes.\n* strong problem solver and solid production debugging skills.\n* clear communication in writing (design docs, code review) and verbally (technical discussions, customer calls, incident response).\n\n\n**bonus points**\n\n\n* experience with database replication technologies (cdc, logical replication).\n* experience with durable execution frameworks (temporal).\n* experience with data formats and protocols (avro, parquet, protobuf).\n* experience with modern data processing frameworks (e.g. kafka, spark, flink).\n* experience with maintaining/contributing to open\\-source software.\n\n### **why join us**\n\n\n* work on challenging problems at the intersection of database technologies, distributed systems, and cloud\\-native architecture.\n* work on a team that builds software in the open (peerdb), and that cares about the craft of engineering, documentations, and continuous learning.\n* be part of a high\\-visibility team that delivers meaningful impact to customers in a dynamic, hyper\\-growth environment.\n\n\n\\#li\\-remote\n\n**the typical starting salary for this role in the us is**\n$141,000 \\- $208,000 usd**the typical starting salary for this role in us premium markets is**\n$157,000 \\- $230,000 usd### **compensation**\n\n\n\nfor roles based in the **united states**, the typical starting salary range for this position is listed above. in certain locations, such as the san francisco bay area and the new york city metro area, a premium market range may apply, as listed.\n\n\n\nthese salary ranges reflect what we reasonably and in good faith believe to be the minimum and maximum pay for this role at the time of posting. the actual compensation may be higher or lower than the amounts listed, and the ranges may be subject to future adjustments.\n\n\n\nan individual\u2019s placement within the range will depend on various factors, including (but not limited to) education, qualifications, certifications, experience, skills, location, performance, and the needs of the business or organization.\n\n\n\nif you have any questions or comments about compensation as a candidate, please get in touch with us at paytransparency@clickhouse.com.\n\n\n### **perks**\n\n\n* **flexible work environment** \\- clickhouse is a globally distributed company and remote\\-friendly. we currently operate in 20 countries.\n* **healthcare** \\- employer contributions towards your healthcare.\n* **equity in the company** \\- every new team member who joins our company receives stock options.\n* **time off** \\- flexible time off in the us, generous entitlement in other countries.\n* **a $500 home office setup** if you\u2019re a remote employee.\n* **global gatherings** \u2013 we believe in the power of in\\-person connection and offer opportunities to engage with colleagues at company\\-wide offsites.\n\n\n**culture \\- we all shape it**\n\n\n\nas part of our first 500 employees, you will be instrumental in shaping our culture.\n\n  \n\n\n**equal opportunity \\& privacy**\n\n\n\nclickhouse provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type based on factors such as race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI & Data Consultant",
        "company": "Deloitte",
        "location": "Chicago, IL, US USA",
        "posted_at": "2026-02-20",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "LangChain",
            "RAG",
            "CI/CD",
            "Git",
            "Tableau",
            "Power BI",
            "Python",
            "SQL",
            "R"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a97fe7cbba5b42a5",
        "description": "our deloitte ai \\& engineering team to transform technology platforms, drive innovation, and help make a significant impact on our clients' success. you'll work alongside talented professionals reimagining and reengineering operations and processes that are critical to businesses. your contributions can help clients improve financial performance, accelerate new digital ventures, and fuel growth through innovation.  \n\n  \n\n**the team**  \n\n  \n\nour ai \\& data practice offers comprehensive solutions for designing, developing, and operating advanced data and ai platforms, products, insights, and services. we help clients innovate, enhance, and manage their data, ai, and analytics capabilities, ensuring they can grow and scale effectively.  \n\n  \n\n**work you'll do**  \n\n  \n\nas **ai \\& data consultant**, you'll partner with clients to solve complex business challenges and deliver impactful, technology\\-driven change. you'll guide organizations through their digital transformation journeys, utilizing the latest innovations while supporting a variety of high\\-impact initiatives, including:  \n\n* convert data into actionable insights, using quantitative and qualitative data analytics, that contribute to comprehensive research, statistical analysis, and data management.\n* develop software components, automation pipelines, and integrations leveraging your proficiency in python.\n* contribute to the build of scalable data solutions on leading cloud platforms, enhancing automation within ai and data engineering projects.\n* collaborate across teams\\-product managers, data scientists, engineers, and business stakeholders\\-to ensure solutions align with client objectives.\n* apply proven practices developed in fast\\-paced, start\\-up environments to help deloitte's clients accelerate transformation.\n* continuously expand your technical and domain expertise in ai, data, and cloud technologies.\n* communicate updates, progress, and outcomes effectively to cross\\-functional peers and management.\n\n  \n\n**qualifications**  \n\n  \n\nrequired:  \n\n* minimum of 2 years of experience in python for data analysis and model development.\n* 1\\+ years of experience data visualization and dashboards\n* 1\\+ years of experience with sql\n* 1\\+ years of experience in tableau and/or power bi\n* familiarity with cloud platforms (e.g., aws, azure, gcp), data engineering, and pipeline automation\n* bachelor's degree in computer science, artificial intelligence, a related field, or equivalent work experience\n* ability to travel 50%, on average, based on the work you do and the clients and industries/sectors you serve.\n* must be legally authorized to work in the united states without the need for employer sponsorship, now or at any time in the future\n\n  \n\npreferred:  \n\n* 2\\+ years of experience in ai deployment or applied machine learning.\n* 6 months \\+ experience developing agentic ai systems, including agent orchestration, tool integration, and autonomous decision\\-making workflows (ie: langchain; semantic kernel, autogen, strands; crewai. langgraph)\n* advanced degree (master's or phd) in computer science, artificial intelligence, data science, or related field.\n* experience working in regulated industries with an understanding of security, compliance, and responsible ai practices.\n* knowledge of mlops, ci/cd, and model\n\n  \n\ninformation for applicants with a need for accommodation: https://www2\\.deloitte.com/us/en/pages/careers/articles/join\\-deloitte\\-assistance\\-for\\-disabled\\-applicants.html",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist - Kaggle Grandmaster",
        "company": "YO IT CONSULTING",
        "location": "Remote, US USA",
        "posted_at": "2026-02-21",
        "score": 12.2,
        "matched_keywords": [
            "Data Scientist",
            "BigQuery",
            "Snowflake",
            "BigQuery",
            "Polars",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=a7c00bc5a4960085",
        "description": "**engagement type:** independent contractor  \n\n**work mode:** fully remote  \n\n**hours:** 30\u201340 hours/week or full\\-time (flexible)\n\n\n### **about the role**\n\n\nwe are partnering with a leading ai research lab to hire a highly skilled **data scientist with a kaggle grandmaster profile**.\n\n\nin this role, you will transform complex datasets into actionable insights, high\\-performing models, and scalable analytical workflows. you will collaborate closely with researchers and engineers to design rigorous experiments, build advanced statistical and machine learning models, and develop data\\-driven frameworks that support product and research decisions.\n\n\n### **key responsibilities**\n\n\n* analyze large, complex datasets to uncover patterns and generate actionable insights\n* build predictive models and ml pipelines across:\n\n\n\t+ tabular data\n\t+ time\\-series data\n\t+ nlp\n\t+ multimodal datasets\n* design and implement validation strategies, experimental frameworks, and analytical methodologies\n* develop automated data workflows, feature pipelines, and reproducible research environments\n* conduct exploratory data analysis (eda), hypothesis testing, and model\\-driven investigations\n* translate analytical results into clear recommendations for engineering, product, and leadership teams\n* collaborate with ml engineers to productionize models and ensure reliable data workflows at scale\n* present findings via dashboards, structured reports, and documentation\n\n### **required qualifications**\n\n\n* kaggle competitions grandmaster or comparable achievement (top\\-tier rankings, multiple medals, or exceptional competition performance)\n* 3\u20135\\+ years of experience in data science or applied analytics\n* strong proficiency in python and data tools (pandas, numpy, polars, scikit\\-learn, etc.)\n* experience building ml models end\\-to\\-end (feature engineering, training, evaluation, deployment)\n* strong understanding of statistical methods, experiment design, and causal/quasi\\-experimental analysis\n* familiarity with modern data stacks (sql, distributed datasets, dashboards, experiment tracking tools)\n* excellent communication skills and ability to present analytical insights clearly\n\n### **nice to have**\n\n\n* contributions across multiple kaggle tracks (notebooks, datasets, discussions, code)\n* experience in ai labs, fintech, product analytics, or ml\\-driven organizations\n* knowledge of llms, embeddings, and modern ml techniques for text, image, and multimodal data\n* experience with big data ecosystems (spark, ray, snowflake, bigquery, etc.)\n* familiarity with bayesian methods or probabilistic programming frameworks\n\n### **why join**\n\n\n* work on cutting\\-edge ai research workflows\n* collaborate with world\\-class data scientists and ml engineers\n* solve high\\-impact, real\\-world data science challenges\n* experiment with advanced modeling strategies and competition\\-grade validation techniques\n* flexible engagement options ideal for kaggle\\-level problem solvers",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineering MTS",
        "company": "Salesforce",
        "location": "Indianapolis, IN, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Prompt Engineering",
            "CI/CD",
            "Jenkins",
            "GitHub Actions",
            "Git",
            "R",
            "Java",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=872c45b8ca00b9d0",
        "description": "*to get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.*\n\njob category\n\n\nsoftware engineering\njob details\n\n\n**about salesforce**\n\nsalesforce is the \\#1 ai crm, where humans with agents drive customer success together. here, ambition meets action. tech meets trust. and innovation isn\u2019t a buzzword \u2014 it\u2019s a way of life. the world of work as we know it is changing and we're looking for trailblazers who are passionate about bettering business and the world through ai, driving innovation, and keeping salesforce's core values at the heart of it all.\n\n\nready to level\\-up your career at the company leading workforce transformation in the agentic era? you\u2019re in the right place! agentforce is the future of ai, and you are the future of salesforce.\n\n\n**software engineering mts**  \n\n**office hybrid in indianapolis, in**  \n\n  \n\noverview of the role \\& organization:\n  \n\nas a member of technical staff (mts) on our sales transformation team, you'll be a key technical contributor responsible for designing, building, and maintaining mission\\-critical applications that power our business operations. this is a hands\\-on engineering role where you'll write code daily, solve complex technical challenges, and take ownership of features from initial design through production deployment. you'll work in a collaborative, agile environment alongside product managers, designers, and fellow engineers to deliver solutions that directly impact how our organization operates and serves our customers.\n  \n\n  \n\nyou'll be joining the det salesforce on salesforce technology team, a strategic pillar within salesforce's digital enterprise technology organization. as part of salesforce's \"customer zero\" initiative, our team develops and deploys enterprise\\-scale solutions that power the company's global sales operations, customer success processes, and revenue operations while serving as the primary validation ground for emerging salesforce technologies. this role offers the unique opportunity to build cutting\\-edge solutions using agentforce, ai agents, and next\\-generation platform capabilities while directly impacting thousands of internal users and influencing product development across sales \\& partner operations, customer success, revenue operations, and major m\\&a integrations. you'll be at the forefront of salesforce's internal digital transformation, working on high\\-visibility initiatives that demonstrate the full potential of the salesforce platform to both internal stakeholders and the broader customer ecosystem.\n  \n\n  \n\n**your impact:**  \n\nas a member of technical staff, you'll play a critical role in building and maintaining the salesforce platform that powers our business operations and customer experiences. the applications and integrations you develop will streamline workflows, automate manual processes, and enable teams across the organization to work more efficiently at scale. your work will directly impact how thousands of users interact with our systems daily and how our customers experience our services. by implementing cutting\\-edge capabilities like agentforce and ai\\-powered features, you'll help drive innovation while ensuring platform reliability and performance. beyond writing code, you'll influence technical architecture decisions, elevate team capabilities through code reviews and mentorship, and establish best practices that create a foundation for sustainable growth. this role offers the opportunity to make tangible business impact while growing your expertise in enterprise salesforce development and emerging ai technologies.\n  \n\n  \n\n**responsibilities:*** design and develop scalable applications on the salesforce platform that solve complex business problems and deliver exceptional user experiences\n* create robust integrations between salesforce and 3rd party applications using rest/soap apis, middleware, and modern integration patterns\n* lead features from concept through production\u2014including technical design, development, testing, deployment, and monitoring\n* write clean, maintainable code following best practices; manage source code with version control; and comply with deployment standards and governance\n* develop comprehensive unit and functional test cases; maintain high code coverage standards; perform integration and smoke testing before deployment\n* assess the impact of new requirements across the salesforce org and all upstream/downstream applications, systems, and processes\n* partner with product management, and cross\\-functional teams to evaluate, scope, and deliver new development requests\n* deep dive into existing applications to troubleshoot production issues; identify and implement performance optimizations\n* participate in peer code reviews and share knowledge with team members\nkeep up with salesforce releases, emerging technologies like agentforce, and recommend platform improvements  \n* \n\n  \n\n  \n\n**required qualifications:*** 3\\+ years delivering production\\-grade applications in information technology and software engineering\n* 3\\+ years of force.com development: apex, lightning (aura \\& lwc), and visualforce\n* 2\\+ years with lightning web components, javascript, html, css, and java for apis/integrations\n* strong foundation in object\\-oriented programming principles, design patterns, software architecture, and test\\-driven development\n* expert in salesforce soql, sosl, dml operations, governor limits, sales cloud, and service cloud\n* experience with automated testing frameworks (junit, selenium) and maintaining code coverage standards\n* proficient in rest/soap apis, web services, and integration patterns (event\\-driven, batch, real\\-time)\n* skilled with git, modern ides (vs code, intellij, eclipse), and ci/cd tools (jenkins, github actions, gitlab ci)\n* experienced with agile methodologies (scrum, kanban)\n* strong analytical, problem\\-solving, and communication skills with collaborative team approach\nrelated technical degree required  \n* \n\n  \n\n  \n\n**preferred qualifications:*** experience with salesforce dx, scratch orgs, and cli tools\n* experience with managed packages, unlocked packages, or appexchange development\n* experience building or implementing agentforce agents and actions\n* knowledge of prompt builder and prompt engineering best practices\n\nunleash your potential\n\n\nwhen you join salesforce, you\u2019ll be limitless in all areas of your life. our benefits and resources support you to find balance and *be your best* , and our ai agents accelerate your impact so you can *do your best* . together, we\u2019ll bring the power of agentforce to organizations of all sizes and deliver amazing experiences that customers love. apply today to not only shape the future \u2014 but to redefine what\u2019s possible \u2014 for yourself, for ai, and the world.\n\n\naccommodations\n\n\nif you require assistance due to a disability applying for open positions please submit a request via this accommodations request form .\n\n\nposting statement\n\n\nsalesforce is an equal opportunity employer and maintains a policy of non\\-discrimination with all employees and applicants for employment. what does that mean exactly? it means that at salesforce, we believe in equality for all. and we believe we can lead the path to equality in part by creating a workplace that\u2019s inclusive, and free from discrimination. know your rights: workplace discrimination is illegal. any employee or potential employee will be assessed on the basis of merit, competence and qualifications \u2013 without regard to race, religion, color, national origin, sex, sexual orientation, gender expression or identity, transgender status, age, disability, veteran or marital status, political viewpoint, or other classifications protected by law. this policy applies to current and prospective employees, no matter where they are in their salesforce employment journey. it also applies to recruiting, hiring, job assignment, compensation, promotion, benefits, training, assessment of job performance, discipline, termination, and everything in between. recruiting, hiring, and promotion decisions at salesforce are fair and based on merit. the same goes for compensation, benefits, promotions, transfers, reduction in workforce, recall, training, and education.\n\n\nin the united states, compensation offered will be determined by factors such as location, job level, job\\-related knowledge, skills, and experience. certain roles may be eligible for incentive compensation, equity, and benefits. salesforce offers a variety of benefits to help you live well including: time off programs, medical, dental, vision, mental health support, paid parental leave, life and disability insurance, 401(k), and an employee stock purchasing program. more details about company benefits can be found at the following link: https://www.salesforcebenefits.com.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Sr. Quality Engineer",
        "company": "McKesson",
        "location": "Columbus, OH, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "GitHub Actions",
            "Git",
            "Kafka",
            "PostgreSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=484a7d4e140ae699",
        "description": "mckesson is an impact\\-driven, fortune 10 company that touches virtually every aspect of healthcare. we are known for delivering insights, products, and services that make quality care more accessible and affordable. here, we focus on the health, happiness, and well\\-being of you and those we serve \u2013 we care.\n\n\nwhat you do at mckesson matters. we foster a culture where you can grow, make an impact, and are empowered to bring new ideas. together, we thrive as we shape the future of health for patients, our communities, and our people. if you want to be part of tomorrow\u2019s health today, we want to hear from you.\n\n\nas senior quality engineer you will be a software engineer focused on quality, reliability, and testability. you will design and build automated testing frameworks, tools, and quality gates that enable our teams to deliver secure, scalable, and resilient services on a modern cloud\\-native stack. this role sits within the software engineering job family and is distinct from qa analyst or qa technician roles: you will write code, integrate with ci/cd pipelines, and help define the quality engineering practices that support our platform.\n\n\nour tech \\& tooling stack\n\n**application stack (familiarity preferred):** c\\#, .net, typescript, graphql, kafka, postgresql; react and mulesoft are a plus.\n\n**quality \\& automation:** playwright or similar ui test frameworks; api testing tools such as postman, rest assured, or graphql\\-specific tools; test management frameworks such as tosca.\n\n**devops \\& observability:** ci/cd pipelines (e.g., azure devops, github actions), defect tracking and test management tools (e.g., jira), observability platforms (e.g., datadog, splunk), and performance testing tools (e.g., neoload or similar).\n\n**ai \\& tooling:** experience using or interest in ai\\-assisted testing, including test generation, risk\\-based test selection, and defect triage, and a willingness to help teams adopt these tools effectively.\n\n\nwhat you'll do\n\n* design, develop, and maintain automated test suites for web uis, apis (rest and graphql), and backend services using modern test frameworks such as playwright and rest assured.\n* build and evolve test automation frameworks and shared libraries that can be used across teams to ensure consistency, maintainability, and high test coverage.\n* integrate automated tests into ci/cd pipelines, defining and tuning quality gates that provide fast, reliable feedback to development teams.\n* collaborate with software engineers, architects, and product owners to define test strategies, acceptance criteria, and non\\-functional requirements (e.g., performance, reliability, security).\n* use observability tools (e.g., datadog, splunk) and logs/metrics/traces to inform test scenarios, validate system behavior, and detect regressions.\n* contribute to performance and resilience testing (e.g., neoload or similar), helping teams understand and improve system behavior under load and failure scenarios.\n* leverage ai\\-assisted testing tools to generate test cases, prioritize test execution, and analyze defects, while defining guardrails for responsible use.\n* partner with qa analysts/technicians (where present) to translate exploratory and manual test scenarios into maintainable automated tests.\n* coach p3 engineers and other team members on quality engineering practices, including unit testing, test design, and effective use of test tools.\n\n\nminimum qualifications: \\-\n\n\ntypically 6\\+ years of experience in software engineering or quality engineering roles, with a strong focus on test automation and frameworks.\n\n\nabout you\n\n**technical skills:**\n\n* proficiency in at least one modern programming language used in test automation (c\\#, typescript/javascript, or similar), with the ability to write clean, maintainable test code.\n* hands\\-on experience with ui and api test automation (e.g., playwright, cypress, rest assured, postman, or similar tools).\n* experience automating tests for services built with rest and graphql apis and working with json\\-based payloads and schema validation.\n* experience integrating automated tests into ci/cd pipelines and working with tools like azure devops, github actions, or similar.\n* familiarity with observability concepts (metrics, logs, traces) and how they inform test scenarios and validation.\n\n**nice to have:**\n\n* experience with performance testing tools (e.g., neoload, jmeter, or similar) and interpreting performance test results.\n* experience with chaos testing or resilience testing practices.\n* experience working in environments with strong regulatory or compliance requirements (e.g., healthcare, hipaa).\n* experience using ai tools to assist with test generation, analysis, or risk\\-based prioritization.\n\n**non\\-technical skills:**\n\n* strong collaborator on agile/scrum teams, able to work closely with developers, product owners, and operations to deliver high\\-quality increments.\n* ability to think critically about risk, coverage, and tradeoffs, and to communicate testing strategies and results clearly to technical and non\\-technical stakeholders.\n* proactive in identifying gaps in test coverage or quality processes and proposing pragmatic solutions.\n* comfortable mentoring others and contributing to a culture of quality, continuous improvement, and learning.\n\n\neducation \\& experience\n\n\ndegree in computer science, software engineering, or related field, or equivalent experience.\n\n\n.\n\n\nwe are proud to offer a competitive compensation package at mckesson as part of our total rewards. this is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. the pay range shown below is aligned with mckesson's pay philosophy, and pay will always be compliant with any applicable regulations. in addition to base pay, other compensation, such as an annual bonus or long\\-term incentive opportunities may be offered. for more information regarding benefits at mckesson, please click here.\n\n**our base pay range for this position**\n\n\n$119,100 \\- $198,500\nmckesson has become aware of online recruiting\\-related scams in which individuals who are not affiliated with or authorized by mckesson are using mckesson\u2019s (or affiliated entities, like covermymeds or rxcrossroads) name in fraudulent emails, job postings or social media messages. in light of these scams, please bear the following in mind:  \n\n  \n\nmckesson talent advisors will never solicit money or credit card information in connection with a mckesson job application.\n\n  \n\nmckesson talent advisors do not communicate with candidates via online chatrooms or using email accounts such as gmail or hotmail. note that mckesson does rely on a virtual assistant (gia) for certain recruiting\\-related communications with candidates.\n\n\nmckesson job postings are posted on our career site: careers.mckesson.com.\n\n**mckesson is an equal opportunity employer**\n\n  \n\nmckesson provides equal employment opportunities to applicants and employees, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age, genetic information, or any other legally protected category. for additional information on mckesson\u2019s full equal employment opportunity policies, visit our equal employment opportunity page.\n\n  \n\nmckesson welcomes and encourages applications from people with disabilities. accommodations are available on request for candidates taking part in all aspects of the selection process. if you require accommodation please contact us by sending an email to disability\\_accommodation@mckesson.com.\n\n**join us at mckesson!**",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Software Engineer - AI Research Clusters",
        "company": "NVIDIA",
        "location": "Santa Clara, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Cortex",
            "Docker",
            "Kubernetes",
            "Git",
            "Python",
            "R",
            "Java",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=e62aae990e8e1e0d",
        "description": "nvidia is at the forefront of innovations in artificial intelligence, high\\-performance computing, and visualization. our invention\u2014the gpu\u2014functions as the visual cortex of modern computing and is central to groundbreaking applications from generative ai to autonomous vehicles. we are now looking for a senior software engineer to help accelerate the next era of machine learning innovation.\nin this role, you will propose and implement engineering solutions to ensure delivery of functional, reliable, secure, and performance\\-optimal gpu clusters to internal researchers, enable them to focus on training and development by reducing operational disruption and overhead, empower them for self\\-service continuous improvement on reliability, operational excellence \\& performance. your work will empower scientists and engineers to train, fine\\-tune, and deploy the most advanced ml models on some of the world\u2019s most powerful gpu systems.  \n\nwhat you'll be doing:* in this position, you will work with coworkers across the ai platform organization to understand the pain points of validating, monitoring and operating gpu clusters at scale. then you will design, develop and maintain engineering solutions to solve those pain points systematically.\n* you will also research in traditional aiops and the emerging agentic ai, and leverage it to further reduce the operation toil.\n* you will participate in on\\-call support for systems, platforms built and owned by the team.\n\n\nwhat we need to see:* bs/ms in computer science, engineering, or equivalent experience.\n* 5\\+ years in software/platform engineering, including 3\\+ years in ml infrastructure or distributed systems.\n* experience in software development lifecycle on linux\\-based platforms.\n* strong coding skills in languages such as python, c\\+\\+ or rust.\n* experience with docker, kubernetes, gitlab ci, automated deployments.\n* experience with aiops or agentic ai and apply it successfully in production environment.\n\n\nways to stand out from the crowd:* proficiency with full\\-stack development: relational data modeling, db optimization, rest api semantics, javascript, css, providing api as a service.\n* passion for building developer\\-centric platforms with great ux and strong operational reliability.\n* experience running slurm or custom scheduling frameworks in production ml environments.\n* familiarity with gpu computing, linux systems internals, and performance tuning at scale.\n\nyour base salary will be determined based on your location, experience, and the pay of employees in similar positions. the base salary range is 152,000 usd \\- 241,500 usd for level 3, and 184,000 usd \\- 287,500 usd for level 4\\.\nyou will also be eligible for equity and benefits.\napplications for this job will be accepted at least until february 24, 2026\\.\nthis posting is for an existing vacancy.\nnvidia uses ai tools in its recruiting processes.\nnvidia is committed to fostering a diverse work environment and proud to be an equal opportunity employer. as we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI/ML Engineer SME (CMS)",
        "company": "General Dynamics Information Technology",
        "location": "IN, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "Git",
            "Snowflake",
            "Databricks",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=fdbbee24455a9dd6",
        "description": "your impact\n\n\nown your opportunity to work alongside federal civilian agencies. make an impact by providing services that help the government ensure the well being and support of u.s. citizens.\n\n\njob description\n\n\ngdit is under contract to the centers for medicare \\& medicaid services (cms) to design, develop, engineer, implement, enhance, maintain algorithms to detect and prevent potential fraud, waste or abuse in the medicare, medicaid and marketplace programs. we are currently seeking a sr. data scientist advisor to join our team.\n\n\nresponsibilities:\n\n* design, develop, and deploy ai models such as generative ai solution (e.g., large language models (llms)), agentic ai, or ai powered tools working with large datasets and a variety of data, structured and unstructured, to create analytic insights and streamline business processes\n* evaluate analytic solutions and business processes to recommend optimization through emerging technologies and ai/ml techniques\n* remain current on ai technologies, advancements, solutions, and optimization methods to recommend and apply continuous advancement of ai solutions\n* support multiple programs and evaluate overarching solutions, frameworks, and techniques to maximize the value of emerging technology\n* lead project teams to design, develop and deliver complex solutions that achieve strong business value timely and efficiently\n* extract qualitative and quantitative relationships, patterns, and trends from large amounts of data using various tools such as python, r, sql, etc.\n* perform data analysis, statistical analysis, and machine learning modeling to conduct complex studies, develop sound inferences, and summarize results using a variety of descriptive statistics\n* work with a variety of customers, users, and stakeholders to understand business objectives and design technical solutions to address the business needs\n* collaborate within project teams including other data scientists and non\\-technical subject matter experts to achieve high quality and impactful solutions within tight timelines\n\n\nrequired skills:\n\n* ba in related field or equivalent combination of education and experience \\+ at least 15 yrs related experience, or ma \\+ 13 yrs related experience\n* experience with medicare and/or medicaid data\n* experience with fraud detection\n* experience using data visualization tools such as tableau\n* experience conducting health care related research and analytics\n* experience developing and validating ai solutions such as llms, chatbots, content generators, deep learning, and similar modern ai technology in cloud\\-based platforms (aws, azure, etc.)\n* experience conducting statistical/analytical and ml modeling and analysis using databricks, snowflake or python, etc.\n* experience working with large and disparate datasets to efficiently produce ai/ml solutions to meet business needs\n* experience proactively identifying and working collaboratively with stakeholders to resolve data anomalies, data quality, and compliance issues in administrative data\n* superior skills working with large datasets efficiently and effectively incorporating quality assurance techniques throughout the process\n\ndesired skills:\n\n* superior written and verbal skills. must write succinctly and clearly and explain complex situations in plain english to technical and non\\-technical audiences\n* superior customer service skills working proactively and collaboratively with federal research, compliance, and policy staff and stakeholders to implement and enhance internal and external management reports and public\\-facing data analysis tools and data sets\n* superior multi\\-tasking and organization skills. must manage multiple simultaneous projects and prioritize assignments and tasks, accordingly, remaining flexible to changing priorities and new initiatives\n\n\nwork requirements\n\n\nyears of experience\n15 \\+ years of related experience* may vary based on technical training, certification(s), *or* degree\n\ncertification\ntravel required\nless than 10%\nsalary and benefit information\n\n\nthe likely salary range for this position is $182,750 \\- $247,250\\. this is not, however, a guarantee of compensation or salary. rather, salary will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range.  \n\nview information about benefits and our total rewards program.\nabout our work\n\n\nwe are gdit. a global technology and professional services company that delivers technology and mission services to every major agency across the u.s. government, defense and intelligence community. our 30,000 experts extract the power of technology to create immediate value and deliver solutions at the edge of innovation. we operate across over 50 countries worldwide, offering leading capabilities in digital modernization, ai/ml, cloud, cyber and application development. together with our customers, we strive to create a safer, smarter world by harnessing the power of deep expertise and advanced technology.\njoin our talent community to stay up to date on our career opportunities and events at gdit.com/tc.*equal opportunity employer / individuals with disabilities / protected veterans*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Technical Architect - Databricks",
        "company": "Stitch Consulting Services, Inc.",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "MLflow",
            "CI/CD",
            "Terraform",
            "Git",
            "Databricks",
            "PySpark",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=d2439e6093084e24",
        "description": "stitch is the global leader in helping brands drive crm performance through braze and databricks. we work with fortune 1000 brands to design and execute martech\\-driven solutions and lifecycle marketing strategies that drive meaningful outcomes\u2014producing award\\-winning work for brands like taco bell and e.l.f. cosmetics. our fast\\-growing team spans the us, uk, and canada.\n\n\nwe\u2019re actively building the best customer relationship marketing (crm) consultancy in the world. not the biggest\u2014the best. we\u2019re looking for teammates who are energized by this challenge.\n\n### **who you are**\n\n\nfirst and foremost, you\u2019re motivated by impact. to be the best crm consultancy in the world, we have to be motivated by performance\u2014both in how our work drives outcomes for our customers, and for how our work drives outcomes for stitch.\n\n\nthis means you\u2019re not content with the status quo. you\u2019re always looking towards what\u2019s next and seeking opportunities to grow yourself, grow our customers, and grow stitch. you have relentless standards and uphold them unapologetically. you move nimbly, take ownership, drive things forward, and make the people around you better. if this sounds like you, you will thrive here.\n\n### **the role**\n\n\nas a technical architect \u2014 databricks, you\u2019ll play a pivotal role in scaling stitch\u2019s databricks consulting practice. you\u2019ll own end\\-to\\-end lakehouse architecture, from braze and segment integrations to ai\\-driven customer 360 solutions, combining deep technical expertise with the agility of a high\\-growth consultancy. you\u2019ll be instrumental in shaping how stitch delivers databricks solutions and building something that didn\u2019t exist here before.\n\n\nto thrive in this role, you\u2019ll need to be as comfortable presenting architecture recommendations to senior client stakeholders as you are writing pyspark. you bring a strong point of view, a curiosity\\-first approach to client problems, and the drive to make a real impact.\n\n### **what you\u2019ll do**\n\n* lead and drive client working sessions focused on data architecture and data mapping, bringing clarity and direction to complex discussions with senior stakeholders.\n* design and own end\\-to\\-end databricks lakehouse architecture for marketing data ingestion, transformation, storage, and consumption\u2014leveraging unity catalog, delta lake, delta live tables, and databricks sql warehouse.\n* design integrations between databricks and martech platforms (braze, segment, cdps) to enable customer 360 views, churn prediction, media mix modeling, dynamic pricing, and lifetime value analysis.\n* design and implement robust data models\u2014including medallion architecture, star schema, and data vault\u2014optimized for marketing analytics workloads.\n* enable marketers to query data conversationally and generate actionable insights without sql through databricks genie and ai\\-powered analytics.\n* support the design and implementation of ai agents using agent bricks and the mosaic ai agent framework.\n* review and interpret technical capability maps and frameworks, connecting them to data architecture and design decisions.\n* architect data governance, security, and access control policies using unity catalog, including lineage tracking, audit logging, pii masking, and marketing data privacy compliance.\n* integrate databricks workflows into ci/cd pipelines using terraform, git, and azure devops; implement infrastructure\\-as\\-code and automated deployment for notebooks, jobs, and clusters.\n* partner with clients on roadmaps, pocs, and migrations; support pre\\-sales efforts with technical proposals, whiteboarding sessions, and project estimation.\n* manage your time to meet billable targets of 36\\+ hours per week.\n* travel up to 20% for client meetings, workshops, and onsite engagements.\n\n### **how you consult**\n\n\nat stitch, every role is expected to operate as a trusted advisor whose expertise helps our clients with their business needs. as a technical architect, your credibility comes from deep technical depth combined with the ability to communicate clearly across technical and business audiences.\n\n* lead requirements, validation, and ideation sessions with technical and business stakeholders.\n* advise clients on data architecture and platform tradeoffs. navigate ambiguity and translate between marketers and engineers.\n* build relationships with client technical leads. become the architect they trust and request.\n* bring a strong point of view and recommendations\u2014not just options. anticipate what the client needs before they ask.\n* own outcomes on your technical workstream. lead without being asked.\n* share feedback with peers regularly that makes all of us better.\n\n### **what success looks like**\n\n* clients see you as a trusted data architecture advisor. you\u2019re confident meeting with senior stakeholders and bringing a strong point of view around best practices and creative solutions.\n* you show up to every meeting prepared and engaged. every touchpoint\u2014slack, email, zoom, or onsite\u2014is an opportunity to add value. you\u2019re responsive and work with a sense of urgency.\n* you consistently meet deadlines. we\u2019re a professional services business and our success depends on client satisfaction. hitting your deadlines gives the stitchers depending on your work enough runway to do theirs.\n* you manage your weekly schedule intentionally to meet quarterly billable targets. you\u2019ll be context\\-switching between deep technical work, client meetings, internal collaboration, certifications, and time\\-tracking\u2014oftentimes across multiple clients at once. staying organized is what makes it all feel manageable.\n\n### **who your team will be**\n\n\nstitchers come with diverse backgrounds and experiences\u2014but we are tied together by our common threads. these are the values that define our stitchers:\n\n\nform lasting bonds \u2014 we\u2019re a people business. we consistently show up for our team, our partners, and our clients in ways that build trust and champion others.\n\n\nseek solutions \u2014 we aren\u2019t order takers. we\u2019re solution designers. we solve problems, move fast, deliver excellence, and drive results. we always have a point of view.\n\n\ndon\u2019t settle \u2014 we don\u2019t accept the way things have always been done. we think bigger, pursue growth, and find motivation in the discomfort that scares most people away.\n\n\ntake the lead \u2014 we forge our own path without waiting for permission. we push boundaries, lead our team and customers, and find the answer when one doesn\u2019t exist yet.\n\n### **what you\u2019ll get at stitch**\n\n* the opportunity to work within multiple fortune 1000 brands across the retail, media, gaming, quick\\-service, financial services, travel, and healthcare industries.\n* working alongside the most concentrated group of braze experts in the world\u2014backed by more braze credentials and experience than any other company across the globe.\n* the ability to feel your impact and shape how stitch grows as we continue to scale through hypergrowth.\n* the chance to learn new technologies (like braze, databricks, and ai) and grow faster than you ever have before.\n* of course, we offer competitive compensation, benefits, and growth opportunities, too.\n\n**requirements**\n\n* 8\\+ years of experience in data engineering or data architecture.\n* 3\\+ years of hands\\-on databricks experience, including pyspark, sql, and scala.\n* proven experience integrating databricks with braze or similar martech platforms for marketing analytics (salesforce, hubspot, responsys, adobe).\n* deep expertise in delta lake, unity catalog, delta live tables, structured streaming, and mlflow.\n* strong data modeling skills across medallion architecture, star schema, and data vault.\n* cloud platform experience across aws, azure, or gcp, with proficiency in iac tooling such as terraform and arm templates.\n* track record of leading and mentoring engineering teams in consulting environments.\n* client\\-facing experience within an agency or consultancy required.\n* must be eligible to work in the united states or canada without visa sponsorship, now or in the future.\n\n### **certifications**\n\n* databricks data engineer professional and/or platform architect accreditation (required or in progress).\n* braze certifications are a plus\u2014we develop platform expertise internally.\n\n**benefits**\n\n* medical, dental, vision, and life insurance\n* 401k with company match\n* flexible pto policy\n* monthly tech stipend\n* paid parental leave\n* in\\-person onboarding experience at our hq in indianapolis, indiana\n* mental well\\-being support",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Data Scientist",
        "company": "Huntington Bank",
        "location": "Detroit, MI, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "TensorFlow",
            "AWS SageMaker",
            "NoSQL",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Bayesian"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=05207a6563aa981a",
        "description": "**description**\n===============\n\n**summary:**\n\nthe senior data scientist contributes to building and developing the organization\u2019s data infrastructure and supports the senior leadership with insights, management reports, and analysis for decision\\-making processes.\n\n**duties and responsibilities:**\n\n* performs advanced analytics methods to extract value from business data.\n* performs large\\-scale experimentation and build data\\-driven models to answer business questions.\n* conducts research on cutting\\-edge techniques and tools in machine learning/deep learning/artificial intelligence.\n* determines requirements that will be used to train and evolve deep learning models and algorithms.\n* articulates a vision and roadmap for the exploitation of data as a valued corporate asset.\n* influences product teams through presentation of data\\-based recommendations.\n* evangelizes best practices to analytics and products teams.\n* owns the entire model development process, from identifying the business requirements, data sourcing, model fitting, presenting results, and production scoring.\n* performs other duties as assigned.\n\n**basic qualifications:**\n\n* master\u2019s degree in computer science, statistics, or economics\n* 3\\+ years of work and/or educational experience in machine learning or cloud computing, experience using statistics and machine learning to solve complex business problems, experience conducting statistical analysis with advanced statistical software, experience scripting languages, and packages, experience building and deploying predictive models, experience web scraping, and scalable data pipelines and experience with big data analysis tools and techniques.\n\n**preferred qualifications:**\n\n* up\\-to\\-date knowledge of machine learning and data analytics tools and techniques\n* strong knowledge in predictive modeling methodology\n* experienced at leveraging both structured and unstructured data sources\n* willingness and ability to learn new technologies on the job\n* demonstrated ability to communicate complex results to technical and non\\-technical audiences\n* demonstrated ability to work effectively in teams as well as independently across multiple tasks while meeting aggressive timelines\n* strategic, intellectually curious thinker with focus on outcomes\n* professional image with the ability to form relationships across functions\n* strong experience with r/rstudio, python, sas, sql, nosql\n* strong experience with cloud machine learning technologies (e.g., aws sagemaker)\n* strong experience with machine learning environments (e.g., tensorflow, scikit\\-learn, caret)\n* strong understanding of statistical methods and skills such as bayesian networks inference, linear and non\\-linear regression, hierarchical, mixed models/multi\\-level modeling\n* financial services background\n\n **exempt status: (yes** \\= not eligible for overtime pay) (**no** \\= eligible for overtime pay)\n\n\nyes**workplace type:**\n\n\noffice\nour approach to **office** workplace type\n\n\ncertain positions outside our branch network may be eligible for a flexible work arrangement. we\u2019re combining the best of both worlds: in\\-office and work from home. our approach enables our teams to deepen connections, maintain a strong community, and do their best work. remote roles will also have the opportunity to come together in our offices for moments that matter. specific work arrangements will be provided by the hiring team.\n\n\nhuntington is an equal opportunity employer.\n\n\ntobacco\\-free hiring practice: visit huntington's career web site for more details.\n\n**note to agency recruiters:** huntington bank will not pay a fee for any placement resulting from the receipt of an unsolicited resume. all unsolicited resumes sent to any huntington bank colleagues, directly or indirectly, will be considered huntington bank property. recruiting agencies must have a valid, written and fully executed master service agreement and statement of work for consideration.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI Applications Development Intern",
        "company": "Zoox",
        "location": "Foster City, CA, US USA",
        "posted_at": "2026-02-21",
        "score": 11.1,
        "matched_keywords": [
            "LangChain",
            "RAG",
            "LLaMA",
            "Copilot",
            "Prompt Engineering",
            "Git",
            "Python",
            "SQL",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=2d2402acc3891444",
        "description": "the information technology and applications (ita) department is responsible for managing zoox's technology infrastructure, business applications, and information systems that enable the company's operations across all functions. ita oversees critical systems including it infrastructure, business applications, hris platforms, and information security. as zoox scales its autonomous vehicle operations, ita plays a vital role in ensuring the technology backbone supports both our r\\&d innovation and commercial service delivery.  \n\nwe are looking for a forward\\-thinking ai application developer intern to join our it innovation team. unlike traditional coding roles, this position focuses on the rapid prototyping and deployment of internal business tools by leveraging state\\-of\\-the\\-art ai code generation technology (e.g., claude 3\\.5 sonnet / claude code).  \n\nyour mission is to bridge the gap between complex business needs in finance, hr, and legal and functional ai\\-powered solutions. you will be at the forefront of \"agentic workflows,\" using ai to write, debug, and deploy the very tools that will drive our company\u2019s efficiency.\n### **responsibilities**\n\n* **ai\\-driven development:** utilize claude and other llms to generate high\\-quality code, boilerplate, and complex logic for internal web applications and automation scripts.\n* **cross\\-functional prototyping:** work with the cio and partner with stakeholders in finance, hr, procurement, and legal to identify manual bottlenecks and design ai solutions (e.g., automated contract review, spend analysis dashboards, or hr faq bots).\n* **prompt engineering \\& orchestration:** design sophisticated system prompts and utilize frameworks like langchain or llamaindex to build retrieval\\-augmented generation (rag) tools.\n* **rapid integration:** connect ai outputs with existing enterprise apis and data sources\n\n### **requirements**\n\n* currently working towards a b.s., m.s., ph.d., or advanced degree in a relevant engineering program\n* good academic standing\n* able to commit to a 12\\-week internship beginning in may or june of 2026\\.\n* at least one previous industry internship, co\\-op, or project completed in a relevant area\n* ability to relocate to the bay area, california for the duration of the internship\n* interns at zoox may not use any proprietary information they are working on as part of their thesis, any published work with their university, or to be distributed to anyone outside of zoox\n\n### **technical qualifications \\& language requirements**\n\n* **ai literacy:** proficiency in using llms for code generation (claude, github copilot, etc.). you should know how to \"talk to the code\" to iterate quickly.\n* **foundational programming:** strong grasp of python or javascript/typescript. you must be able to read, audit, and fix the code the ai generates.\n* **web frameworks:** familiarity with modern stacks like react, next.js, or streamlit for building quick front\\-end interfaces.\n* data basics: understanding of sql and how to handle structured/unstructured data (json, csv, pdfs).\n* **curiosity:** a \"builder\" mindset\u2014you enjoy taking a vague business problem and turning it into a working demo in days, not months.\n\n### **bonus qualification**\n\n* interest in the autonomous vehicle industry\n\n**compensation**\nthe monthly salary range for this position is $5,500 to $7,500\\. compensation will vary based on geographic location and level of education. additional benefits may include medical insurance, and a housing stipend (relocation assistance will be offered based on eligibility). **about zoox**\nzoox is developing the first ground\\-up, fully autonomous vehicle fleet and the supporting ecosystem required to bring this technology to market. sitting at the intersection of robotics, machine learning, and design, zoox aims to provide the next generation of mobility\\-as\\-a\\-service in urban environments. we\u2019re looking for top talent that shares our passion and wants to be part of a fast\\-moving and highly execution\\-oriented team.  \n\nfollow us on linkedin **accommodations**\nif you need an accommodation to participate in the application or interview process please reach out to accommodations@zoox.com or your assigned recruiter. ***a final note:****you do not need to match every listed expectation to apply for this position. here at zoox, we know that diverse perspectives foster the innovation we need to be successful, and we are committed to building a team that encompasses a variety of backgrounds, experiences, and skills.*\n\nwe may use artificial intelligence (ai) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. these tools assist our recruitment team but do not replace human judgment. final hiring decisions are ultimately made by humans. if you would like more information about how your data is processed, please contact us.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Midlevel Software Engineer",
        "company": "Liveworld",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Git",
            "PostgreSQL",
            "Python",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7dffdc68dbca47db",
        "description": "join our dynamic team as a software engineer and continues the development of cutting\\-edge ai based web application solutions. the midlevel software engineer with ai expertise, reporting to the vice president of software, will play a critical role in developing full stack ai based web application, participating in all phases of software development lifecycle, integrating with ai within web applications built in **ruby on rails \\& python**. knowledge of machine learning models, utilizing nlp techniques, and ai agent knowledge is important for this role,\n\nyou will work closely with product and engineering teams to deliver scalable, real\\-time ai\\-driven solutions.\n\nthis is an exciting opportunity for a software developer with experience in nlp, **machine learning models**, **api development and aws gen ai** to have a direct impact on the growth and success of a dynamic technology company.\n\n**required skills and traits**\n\n\u00b7 **proven experience**: at least 2\\+ years of experience in web applications integrating **ai/ml**, with hands\\-on experience with nlp to enhance preprocessing of data, and aws gen ai.\n\n\u00b7 proficiency in **python** for ai/ml model development and **ruby on rails** or similar for backend integration.\n\n\u00b7 proficiency with django and celery or similar background job processing frameworks\n\n\u00b7 experience building agents and mcp\n\n\u00b7 experience with front end javascript frameworks like angularjs and react.\n\n\u00b7 **ai/ml frameworks**: experience with popular ai/ml frameworks such as **tensorflow**, **pytorch**, bertopic or similar, with the ability to apply them in production environments.\n\n\u00b7 experience implementing and consuming restful apis. major plus if familiar integrating with social network apis like facebook, twitter, instagram, youtube and linkedin apis\n\n\u00b7 demonstrate interest in incorporating designs from mockups to ui components on the application\n\n\u00b7 solid skills with git. a github profile with open\\-source contributions is a major plus\n\n\u00b7 strong practice of tdd/bdd using rspec, jasmine, and cucumber\n\n\u00b7 familiarity with sql (postgresql or such), elasticsearch, python, aws\n\ndebugging errors through logs is a plus. can work effectively with a remote team of engineers as well as independently.\n\n\u00b7 good analytical and problem\\-solving skills.\n\n\u00b7 excellent verbal and written communication skills.\n\n\u00b7 strong work ethic and can\\-do attitude.\n\n**required education and experience**\n\n\u00b7 bachelors or masters\u2019 degree, preferably in cs or related discipline\n\n\u00b7 minimum 4 years\u2019 experience\n\n* knowledge of ai/ml foundation models: familiarity using pre\\-trained models like gpt, aws bedrock models, others,\n* knowledge of aws bedrock and gen ai solutions.\n* experience with programming languages: python (for bedrock workflows), ruby on rails or similar and javascript framework like angular js, react(for full\\-stack development).\n* business understanding: ability to translate business problems into ml solutions.\n\n**job benefits**\n\n\u00b7 competitive salary\n\n\u00b7 stock options\n\n\u00b7 full benefits package including health, dental, vision, life insurance, and 401k.\n\n\u00b7 health and dependent care fsa/hsa options.\n\n\u00b7 paid time off and 10 paid company holidays.\n\n\u00b7 experience in a fast\\-paced and team\\-oriented work environment.\n\npay: $99,682\\.04 \\- $125,000\\.00 per year\n\nbenefits:\n\n* 401(k)\n* ad\\&d insurance\n* dental insurance\n* flexible spending account\n* health insurance\n* health savings account\n* life insurance\n* paid holidays\n* paid sick time\n* paid time off\n* prescription drug insurance\n* vision insurance\n\npeople with a criminal record are encouraged to apply\n\nlanguage:\n\n* english (required)\n\nwork location: remote",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Application Development Intern - Artificial Intelligence",
        "company": "C1",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "Copilot",
            "TensorFlow",
            "Keras",
            "NLTK",
            "Git",
            "Kafka",
            "MongoDB",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=572965e66b126cf4",
        "description": "c1 company overview:\n**c1: 1 contact, 1 connection, 1 choice**  \n\nc1 is the foremost, single\\-source provider of advanced communications and data technology for business. that means if it's digital, we connect our customers to it \\- from phone systems and hardware to computer networks, application development, managed solutions and more. and we're 100% passionate with designing, implementing, managing and supporting our customers' every need from end to end, so that they can focus on what they do best.  \n\nso, when it comes to joining a team of it and communications technology pros who are empowered to do what they do best, your best choice \\- your \\#1 choice \\- is c1\\.\noverview:\n**summary**\n\n\nthe application development intern will be paired with application development\u2019s principal architect and receive guidance from the director, engineering in the development and programming of convergeone software intellectual property (ip) related to applying the open\\-source artificial intelligence/natural language tool kit to the conversations solution. with an advanced educational background (bs or ms in process) in software development and programming, ad interns will gain experience in a fast\\-paced environment, handling multiple projects simultaneously, and be given an opportunity to apply their academic learnings, showcase their initiative and drive to expand responsibilities. roles will be outlined in more detail as the start date approaches depending on convergeone ip development roadmaps and requirements.\n\n\nresponsibilities:\n**essential functions**\n\n* understands and learns about c1 and c1\u2019s corporate directions\n* learns and supports strategies and activities for c1\u2019s innovation and ip development.\n* gains practical, hands\\-on experience with current/next generation:\n\t+ technologies, e.g., bots and artificial intelligence, internet of things, cloud services, microservices architectures, etc.\n\t+ programming languages, e.g., angularjs, node.js, python, etc.\n\t+ big data technologies, e.g., elasticsearch, mongodb, etc.\n\t+ queueing frameworks, e.g., rabbitmq, kafka, etc.\n\t+ cloud infrastructures, e.g., gcp, amazon web services, azure cloud, etc.\n\t+ development methodologies and devops, e.g., agile/lean\n* develops and enhances (through involvement with the complete software development lifecycle):\n\t+ programming and software development skills\n\t+ on\\-job training using cutting\\-edge technology tools\n\t+ problem solving and decision\\-making skills\n* + creative and critical thinking\n\t+ collaboration, communication, and negotiation\n* implements an ai\\-powered chatbot using dialogflow/llms\n* trains and optimizes the bot to make more natural conversations\n* interpretes business conversation flows and conversion of those flows into conversation configuration\n\n\nqualifications:\n**required qualifications**\n\n* strong verbal, written communication skills\n* in\\-process bs/ms computer science or electronic engineering with a focus on software development\n* programming skills/aptitude and knowledge of python and java.\n* nodejs (node.js), python, natural language processing (nlp), webhook development. knowledge of llm/dialogflow/koreai api is advantageous\n* good troubleshooting \\+ problem\\-solving skills\n* ability to work independently as well as on a team and learn from colleagues\n* high adaptability in a dynamic start\\-up environment.\n* basic understanding of ms office preferably visio, powerpoint, word and/or excel\n\n**desired/preferred qualifications**\n\n* specific artificial intelligence skills using frameworks such as crew.ai, model context protocol, nltk, tensorflow, dialogflow, scikit\\-learn, keras, etc.\n* specific java script programming skills using frameworks such as node.js, angularjs, etc.\n* copilot studio experience\n\n\nadditional information:\n**work environment**  \n\nability to handle multiple priorities and demands in a fast\\-paced environment. this job operates in a professional office environment. this role routinely uses standard office equipment such as computers, phones, photocopiers, and filing cabinets.  \n\n  \n\n**physical environment**  \n\nphysical demands described here are representative of those that must be met by a team member to successfully perform the essential functions of this job. reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this job.  \n\n  \n\n**other duties/changes**  \n\nthis job description is not designed to cover or contain a comprehensive listing of all duties, responsibilities or activities that are required of a team member for this job. duties, responsibilities and activities may change at any time with or without notice. at any point in time, the essential functions and primary duties associated with this position will be the principal, major or most important duties, responsibilities and activities that the employee is expected to perform as determined and directed by c1\\. **eeo statement**  \n\nc1 provides equal employment opportunities (eeo) to all team members and applicants for employment opportunities. all qualified applicants will receive consideration for employment, and all team members will be treated with respect to their employment, without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability or veteran status. for further details, please view the eeo policy statement (eeo policy statement) and/or the current version of the workplace poster (https://www.eeoc.gov/sites/default/files/2023\\-06/22\\-088\\_eeoc\\_knowyourrights6\\.12screenrdr.pdf; https://www.eeoc.gov/poster) **e\\-verify:** e\\-verify **right to work**: right to work poster",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer",
        "company": "Ascension",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "CI/CD",
            "Jenkins",
            "Git",
            "MongoDB",
            "NoSQL",
            "SQL",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=11c257b267f7ca14",
        "description": "we are hiring:\n* **department:** growth marketing digital experience (gmdx)\n* **schedule:** monday \\- friday, 8 hour shift\n* **location:** remote\n* **salary:** $100,218 \\- $139,698 per year\n\n\nwhat you will do:\n\nthe software engineer will join a dynamic team dedicated to developing high\\-quality, cloud\\-native software that adheres to best practices and fulfills both functional and non\\-functional requirements. in this collaborative role, they will contribute to all phases of the software development lifecycle and grow their expertise in cutting\\-edge systems. continuous learning is encouraged, and the company provides ample opportunities for professional growth. on\\-call support is part of the role, offered on a rotational basis, ensuring work\\-life balance. **responsibilities*** design and develop robust software solutions as part of a diverse, lean\\-agile team. focus on creating secure, scalable, and highly performant applications for a global user base.\n* write clean, maintainable code and ensure it meets quality standards through comprehensive unit and integration tests.\n* actively participate in lean\\-agile activities to continuously improve our processes and products.\n* engage in peer code reviews to maintain a high standard of code quality and foster a culture of collaboration and mutual learning.\n* enhance skill set through ongoing education in new technologies and engineering practices. understand the impact of your solutions on the organization and our global community.\n* provide empathetic and effective on\\-call support on a rotational basis, managing critical issues to maintain our service levels\n* provide on\\-call support on a rotational basis, effectively prioritizing work to maintain service levels and project momentum.\n\n**complexity of work*** this role requires an agile mindset, strong problem\\-solving skills, and a commitment to lifelong learning in software development trends and practices. the individual will need to adapt quickly to changing technologies while contributing to multiple projects. able to work with minimal supervision.\n\n**key technical competencies*** core backend: java, spring boot\n* databases: mongodb or other nosql database systems\n* devops: github, jenkins, and modern ci/cd practices\n* elastic stack: deep expertise in elasticsearch, including indexing, advanced data modeling (nested objects, parent\\-child relationships), custom analyzers, and using query dsl and aggregations for both search and analytics.\n* google cloud platform (gcp): hands\\-on experience with services such as pub/sub, gke, cloudrun, and gcs storage.\n* observability: familiarity with apm tools like dynatrace and utilizing log explorer and alert policies for monitoring and troubleshooting.\n\n\nwhat you will need:\n\neducation:* high school diploma/ged with 2 years of experience, or associate's degree, or bachelor's degree required.\n\n\nwork experience:* 1 year of experience required.\n* 4 years of experience preferred.\n* 2 years of leadership or management experience preferred.\n\n\nadditional preferences:\n* bachelor's degree in computer science or related field\n* 3\\+ years of prosessional experience in software development with a proven track record in agile environments\n\n\n\\#gmdx\nwhy join our team:\n\nascension is a leading nonprofit catholic health system with a culture and associate experience grounded in service, growth, care and connection. we empower our 99,000\\+ associates to bring their skills and expertise every day to reimagining healthcare, together. recognized as one of the best 150\\+ places to work in healthcare and a military\\-friendly gold employer, you\u2019ll find an inclusive and supportive environment where your contributions truly matter.\nequal employment opportunity employer:\n\nascension provides equal employment opportunities (eeo) to all associates and applicants for employment without regard to race, color, religion, sex/gender, sexual orientation, gender identity or expression, pregnancy, childbirth, and related medical conditions, lactation, breastfeeding, national origin, citizenship, age, disability, genetic information, veteran status, marital status, all as defined by applicable law, and any other legally protected status or characteristic in accordance with applicable federal, state and local laws. for further information, view the eeo know your rights (english) poster or eeo know your rights (spanish) poster. ***fraud prevention notice***\nprospective applicants should be vigilant against fraudulent job offers and interview requests. scammers may use sophisticated tactics to impersonate ascension employees. to ensure your safety, please remember: ascension will never ask for payment or to provide banking or financial information as part of the job application or hiring process. our legitimate email communications will always come from an @ascension.org email address; do not trust other domains, and an official offer will only be extended to candidates who have completed a job application through our authorized applicant tracking system. ***e\\-verify statement***\nemployer participates in the electronic employment verification program. please click here for more information.\nbenefits:\n\npaid time off (pto)  \n\nvarious health insurance options \\& wellness plans  \n\nretirement benefits including employer match plans  \n\nlong\\-term \\& short\\-term disability  \n\nemployee assistance programs (eap)  \n\nparental leave \\& adoption assistance  \n\ntuition reimbursement  \n\nways to give back to your community *benefit options and eligibility vary by position. compensation varies based on factors including, but not limited to, experience, skills, education, performance, location and salary range at the time of the offer.*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Software Engineer",
        "company": "HCA Healthcare",
        "location": "Nashville, TN, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "AKS",
            "Git",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7055f93be6329f9d",
        "description": "##### **introduction**\n\n\nare you passionate about the patient experience? at hca healthcare, we are committed to caring for patients with purpose and integrity. we care like family! jump\\-start your career as a senior software engineer today with hca healthcare.\n\n##### **benefits**\n\n\nhca healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. the available plans and programs include:\n\n* comprehensive medical coverage that covers many common services at no cost or for a low copay. plans include prescription drug and behavioral health coverage as well as free telemedicine services and free airmed medical transportation.\n* additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long\\-term care coverage, moving assistance, pet insurance and more.\n* free counseling services and resources for emotional, physical and financial wellbeing\n* 401(k) plan with a 100% match on 3% to 9% of pay (based on years of service)\n* employee stock purchase plan with 10% off hca healthcare stock\n* family support through fertility and family building benefits with progyny and adoption assistance.\n* referral services for child, elder and pet care, home and auto repair, event planning and more\n* consumer discounts through abenity and consumer discounts\n* retirement readiness, rollover assistance services and preferred banking partnerships\n* education assistance (tuition, student loan, certification support, dependent scholarships)\n* colleague recognition program\n* time away from work program (paid time off, paid family leave, long\\- and short\\-term disability coverage and leaves of absence)\n* employee health assistance fund that offers free employee\\-only coverage to full\\-time and part\\-time colleagues based on income.\n\n  \n\nlearn more about employee benefits\n\n***note: eligibility for benefits may vary by location.***\n\n  \n\ncome join our team as a senior software engineer. we care for our community! just last year, hca healthcare and our colleagues donated $13\\.8 million dollars to charitable organizations. apply today!\n\n##### **job summary and qualifications**\n\n\nposition summary\n\n\nthis position is responsible for developing, testing, and deploying innovative technology and data products within the itg accelerated technologies organization and dt\\&i. they will help us build platforms and services that enable our physicians and clinicians to provide world\\-class care for patients in our hospitals. along with technical expertise, a candidate for this position should be comfortable designing and reasoning about complex distributed systems and working with ai for software development.\n\n\nmajor responsibilities:\n\n\nthis engineer can quickly learn, maintain and pragmatically improve existing solutions as well as design new ones. they will provide key problem resolutions for production systems as needed. they have an in depth understanding of the services provided by itg accelerated technologies and dt\\&i and can develop relationships throughout the organization to assist in accomplishing its goals for the company. this engineer strategically designs, constructs, and implements software in a software development environment. this engineer is a highly motivated self\\-starter and is committed to delivering high quality solutions within agreed upon timelines. they are committed to engineering excellence and comfortable working in a fast paced complex environment. strong application development skills are required using reactjs, typescript, and python. gcp experience is preferred. experience with ai assisted tooling such as github copilot, lovable.dev, v0\\.dev, bolt.new, or other vibe coding tools is required. experience with healthcare data and technology is preferred.\n\n\nmajor responsibilities\n\n* build life\\-changing healthcare technology\n* act as a technical expert within at and project integrations, including requirements gathering, design, development, and testing\n* provide valuable insights in requirements validation and feasibility analysis with respect to at\n* design scalable distributed software\n* clearly communicate software architecture in accordance with c4 model\n* estimate work effort required in delivering features keeping at capabilities in mind\n* produce high quality, modular, reusable code that incorporates coding best practices and serves as an example for less experienced developers\n* design and execute devops strategies and processes, driving the change management which accompanies these types of transformative solutions\n* design, build and maintain automated deployment frameworks (continuous integration, continuous delivery)\n* escalate product issues and risks appropriately and collaborates on solutions\n* possess deep knowledge and extensive experience in software design patterns.\n* produce and review enterprise\\-level system design documentation, including: use cases, software architecture documentation, service mapping (i.e., map service schema to backend source systems), consumer guide (i.e., end user documentation), and transition documentation to support the team.\n* create service level agreements (states the agreed upon availability\\-uptime/downtime, maintenance windows, etc. for a service) and supplementary specifications (i.e., non\\-functional specifications). ensure implementations are up to current standards for coding, naming, security, and versioning.\n* consult on and guide design of software solutions\n* possess deep knowledge and experience with a variety of testing methodologies and drives the adoption of best practices\n* possess excellent communication skills to interface with various stakeholders from business consumer to technical staff.\n* research and become the subject matter expert (sme) on the interaction of the service with source systems as well as interaction with the consumers (business users).\n* lead troubleshooting activities\n* provide after hours/on\\-call support as needed\n* mentor other engineers\n* direct the performance of programming assignments within the department\n* develop software development standards and frameworks\n* work with enterprise architects on technology evaluation / product selection\n* partner with managers in contract negotiation / vendor relations\n* interview / hiring / performance evaluation\n* perform other duties as assigned\n\n\npractice and adhere to the \u201ccode of conduct\u201d philosophy and \u201cmission and value statement.\u201d\n\n\neducation \\& experience:\n\n* bachelor degree required\n\n* masters degree preferred\n* work experience in lieu of degree considered\n\n\n5\\+ years relevant work experience required\n\n\nor equivalent combination of education and/or experience\n\n\nknowledge, skills, abilities, behaviors:\n\n* technology experience: 5\\+ years of experience in most of the following:\n\nwe are looking for experts in these areas. if you don\u2019t have experience in some of these, you are able to work collaboratively on a cross\\-functional team that builds data science signal delivery, data pipeline, and devops infrastructure.\n\n* experience with cloud development technologies, especially google cloud platform\n* strong proficiency in multiple programming languages especially python and sql required.\n* data engineering experience including architecture for ingesting, transforming, and feature engineering data elements used for data science and machine learning.\n* deep understanding of steaming and event driven architectures\n* strong understanding of best practices in the design of rest\\-based apis\n* nuanced understanding of distributed version control\n* experience in data acquisition, data cleansing and parsing required.\n* detailed understanding of devops practices and extensive experience with associated tools.\n* extensive experience and deep understanding of container\\-based platforms such as docker, kubernetes, openshift, and cloud run\n* extensive experience with container monitoring applications such as monitoring solutions such as cloud monitoring, sysdig, data dog, appdynamics, new relic, nagios, and zabbix\n* understanding of data science concepts.\n* sql experience / database interrogation techniques\n* linux command line skills\n* scrum, agile, lean product development, domain driven design\n* excellent communication skills, both written and verbal\n* experience and deep knowledge of service oriented architecture (soa)\n* healthcare experience, preferable\n\n\nadaptability\n\n* treats change and new situations as opportunities for learning or growth.\n* focuses on the beneficial aspects of change and speaks positively about the change to others.\n* seeks to understand changes in work tasks, situations, and environment as well as the logic or basis for change.\n* demonstrates the ability to help others adapt to change and to personally adapt to various work environments.\n\n\nstrategic working relationship:\n\n* builds relationships within the department and across multi\\-disciplinary teams to assist in facilitating discussions regarding data outputs and feedback on usage activities to improve at impact.\n\n\nadaptability \\- expected level of competency\n\n* ability to network with corporate and field contacts.\n* frequent inter\\-departmental contact and presentations.\n* independently works with customers on a 1:1 basis, as well as in teams.\n\n\nworking standards\n\n* continues to refine development and analytical skills.\n* applies a consistent development approach to provide creative solutions to problems.\n* leads the design, construction and implementation of software and analytics solutions in the data science engineering environment.\n* expands the organizational technical knowledge by providing mentoring to internal team and attending professional development opportunities.\n\n\nfreedom to act/ independent judgment\n\n* works independently with receiving minimal guidance. acts as a team lead for multi\\-disciplinary projects.\n* acts as a mentor and provides direction to colleagues with less experience.\n* work is evaluated to ensure objectives are met.\n* exercises judgement in selecting and adapting methods and techniques for obtaining solutions.\n\n* contributes to designing standard practices and procedures required\n\n\nhca healthcare has been recognized as one of the world's most ethical companies\u00ae by the ethisphere institute more than ten times. in recent years, hca healthcare spent an estimated $3\\.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.\n\n  \n\n\"the great hospitals will always put the patient and the patient's family first, and the really great institutions will provide care with warmth, compassion, and dignity for the individual.\"\\- dr. thomas frist, sr.  \n\nhca healthcare co\\-founder\n\n\nif you are looking for an opportunity that provides satisfaction and personal growth, we encourage you to apply for our senior software engineer opening. we promptly review all applications. highly qualified candidates will be contacted for interviews. **unlock the possibilities and apply today!**\n\n*we are an equal opportunity employer. we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.*",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Analytics Developer 3 - HEDIS Quality",
        "company": "Baylor Scott & White Health",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "Cortex",
            "Snowflake",
            "Databricks",
            "Power BI",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=49c56062f945cbd6",
        "description": "#### **about us**\n\n\nhere at baylor scott \\& white health we promote the well\\-being of all individuals, families, and communities. baylor scott and white is the largest not\\-for\\-profit healthcare system in texas that empowers you to live well.\n\n\nour core values are:\n\n* we serve faithfully by doing what's right with a joyful heart.\n* we never settle by constantly striving for better.\n* we are in it together by supporting one another and those we serve.\n* we make an impact by taking initiative and delivering exceptional experience.\n\n#### \n\n#### **benefits**\n\n\nour benefits are designed to help you live well no matter where you are on your journey. for full details on coverage and eligibility, visit the baylor scott \\& white benefits hub to explore our offerings, which may include:\n\n* immediate eligibility for health and welfare benefits\n* 401(k) savings plan with dollar\\-for\\-dollar match up to 5%\n* tuition reimbursement\n* pto accrual beginning day 1\n\n*note: benefits may vary based upon position type and/or level.*\n\n#### **job summary**\n\n\nthe vbc analytics team is a unified, independent organization supporting the analytical needs of both the bsw health plan and quality alliance (aco). our structure enables close alignment with stakeholders and subject matter experts (smes) across operational domains. we support over 1\\.2 million lives across diverse lines of business, delivering industry\\-leading insights to drive strategic decision\\-making.\n\n\nwe are seeking a self\\-motivated, customer facing analytics developer who can own the full product lifecycle\u2014from requirements gathering to design, development, governance, and delivery. the ideal candidate will drive insights into hedis/stars performance in relation to medical trends, utilization patterns, and strategic initiatives.\n\n***the pay range for this position is $40\\.35/hour (entry level qualifications) \\- $62\\.52/hour (highly experienced). the specific rate will depend upon the successful candidate's specific qualifications and prior experience.*****essential functions of the role**\n\n* data integration \\& analysis (20%): work with snowflake sql, python to analyze medical and pharmacy claims and ehr data.\n* architecture \\& modeling (25%): design scalable data architecture and dimensional models using best practices in analytics engineering.\n* product \\& process management (25%): support and enhance existing analytics products, enforce standards, automate workflows, and implement governance policies.\n* ad hoc analysis (20%): deliver exploratory insights, create metrics, analyze trends, and support performance monitoring.\n* professional development (5%): dedicate time weekly for continuous learning and skill advancement.\n* team contributions (5%): collaborate cross\\-functionally and mentor junior developers through code reviews and knowledge sharing.\n* develop and maintain scalable data models, and support advanced analytics.\n* collaborate with stakeholders, data engineers, and product managers to translate business needs into technical solutions.\n* write and optimize complex sql queries or python in snowflake, for efficient handling of large datasets.\n* build and maintain power bi dashboards and reports that drive business outcomes with advanced visuals.\n* integrate and operate ai\\-powered analytics using platforms like snowflake cortex analyst and databricks genie.\n* collaborate with data scientists and domain experts to build and refine semantic models that improve the accuracy of ai\\-generated sql queries and visualizations.\n* lead and contribute to architectural decisions and technical roadmap planning.\n* ensure data integrity, performance, and security via strong governance practices and quality controls\n\n**key success factors**\n\n* bachelor's degree in computer science, information technology, or a related field or equivalent experience required.\n* experience analyzing hedis quality measures preferred\n* 5\\+ years of experience analyzing medical trends and healthcare data preferred.\n* 5\\+ experience with snowflake or equivalent data platforms preferred.\n* 5\\+ years of experience building power bi dashboards with advanced visuals, dax, and power query preferred.\n* strong background in data analysis, modelling, architecture design, and analytics engineering.\n* excellent communication skills with both technical and business audiences.\n* proficiency in python for scripting, automation, and data manipulation.\n* familiarity with agile/scrum methodologies and sdlc best practices.\n* experience with epic clarity and snowflake sql for clinical, operational, or financial analytics.\n* in\\-depth understanding of the healthcare ecosystem, including payer\\-provider dynamics.\n* exposure to power platform tools like power automate and power apps.\n* exposure to data science methodologies and analytical frameworks.\n\n **qualifications**\n\n* education \\- bachelor's or 4 years of work experience above the minimum qualification\n* experience \\- 5 years of experience",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Software Engineer",
        "company": "Forvia",
        "location": "Auburn Hills, MI, US USA",
        "posted_at": "2026-02-19",
        "score": 11.1,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "Docker",
            "Kubernetes",
            "CI/CD",
            "Terraform",
            "Python",
            "R",
            "Java",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=711075e72263a26e",
        "description": "**forvia, a sustainable mobility technology leader**\n----------------------------------------------------\n\n\n\nwe pioneer technology for mobility experience that matter to people.\n\n**your mission, roles and responsibilities**\n--------------------------------------------\n\n\n\nrole summary  \n\nthe software engineer will play a critical role in our development team, working on a variety of projects that span the spectrum from computer vision applications to generative ai models. this position requires a strong understanding of both frontend and backend development practices, as well as proficiency in multiple programming languages. the ideal candidate will be a versatile engineer who can navigate complex software development landscapes, delivering robust solutions that meet our high standards of quality and innovation.  \n\nkey responsibilities  \n\n* design, develop, and maintain software solutions that support various automotive technology projects, including but not limited to computer vision and generative ai applications.\n* collaborate with cross\\-functional teams to define software requirements, system architecture, and integration strategies.\n* write clean, efficient, and maintainable code across the full stack, ensuring high performance and responsiveness of applications, ensuring high levels of quality and reliability\n* develop and implement frontend interfaces using javascript and related frameworks, ensuring an intuitive and user\\-friendly experience.\n* build and maintain backend services and apis in python, java, node, or similar languages, ensuring scalability and security.\n* leverage azure cloud services for deploying and managing applications, optimizing for performance and cost.\n* implement devops strategies, including continuous integration and continuous deployment (ci/cd), to streamline development workflows and enhance product quality.\n* stay up\\-to\\-date with emerging trends and technologies in software development, continuously improving skills and adopting best practices.\n**your profile and competencies to succeed**\n--------------------------------------------\n\n\n\nrequirements  \n\n* bachelor\u2019s or master\u2019s degree in computer science, engineering, or a related field.\n* strong proficiency in multiple programming languages, including .net, python, javascript, and node.js. c\\+\\+ and or rust would also be a plus.\n* experience with frontend development using modern javascript frameworks (e.g., next js) and backend development practices.\n* proven experience with azure cloud services, including application deployment, configuration, and management.\n* solid understanding of devops principles and experience with ci/cd pipelines, automation tools (such as terraform, ansible or azure resource manager), and containerization technologies (e.g., docker, kubernetes).\n* knowledge of software development methodologies and best practices, with a strong focus on code quality, security, and performance.\n* excellent problem\\-solving skills, with the ability to think critically and develop innovative solutions to complex technical challenges.\n* strong communication and collaboration skills, with the ability to work effectively in a team\\-oriented environment.\n* knowledge in micro\\-services software architecture and principles.\n* familiarity with design patterns.\n* familiarity with agile methodologies.\n**what we can do for you**\n--------------------------\n\n\n* at forvia, you will find an engaging and dynamic environment where you can contribute to the development of sustainable mobility leading technologies.\n* we are the seventh\\-largest global automotive supplier, employing more than 157,000 people in more than 40 countries which makes a lot of opportunity for career development.\n* we welcome energetic and agile people who can thrive in a fast\\-changing environment. people who share our strong values. team players with a collaborative mindset and a passion to deliver high standards for our clients. lifelong learners. high performers. globally minded people who aspire to work in a transforming industry, where excellence, speed, and quality count.\n* we cultivate a learning environment, dedicating tools and resources to ensure we remain at the forefront of mobility. our people enjoy an average of more than 22 hours of online and in\\-person training within forvia university (five campuses around the world)\n* we offer a multicultural environment that values diversity and international collaboration. we believe that diversity is a strength. to create an inclusive culture where all forms of diversity create real value for the company, we have adopted gender diversity targets and inclusion action plans.\n* achieving co2 net zero as a pioneer of the automotive industry is a priority: in june 2022, forvia became the first global automotive group to be certified with the new sbti net\\-zero standard (the most ambitious standard of sbti), aligned with the ambition of the 2015 paris agreement of limiting global warming to 1\\.5\u00b0c. three principles guide our action: use less, use better and use longer, with a focus on recyclability and circular economy.\n**why join us**\n---------------\n\n\n\nforvia is an automotive technology group at the heart of smarter and more sustainable mobility. we bring together expertise in electronics, clean mobility, lighting, interiors, seating, and lifecycle solutions to drive change in the automotive industry.\n\n\nwith a history stretching back more than a century, we are the 7th largest global automotive supplier, employing more than 157,000 people in 43 countries. you'll find our technology in around 1 out of 2 vehicles produced anywhere in the world.\n\n\nin june 2022, we became the 1st global automotive group to be certified with the sbti net\\-zero standard. we have committed to reach co2 net zero by no later than 2045\\.\n\n\nas technological innovation and the need for sustainability transform the automotive industry, we are ideally positioned to deliver solutions that will enhance the lives of road\\-users everywhere.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist",
        "company": "Forvia",
        "location": "Auburn Hills, MI, US USA",
        "posted_at": "2026-02-19",
        "score": 11.1,
        "matched_keywords": [
            "Data Scientist",
            "Machine Learning Engineer",
            "Generative AI",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=0761c4262d5d60ff",
        "description": "**forvia, a sustainable mobility technology leader**\n----------------------------------------------------\n\n\n\nwe pioneer technology for mobility experience that matter to people.\n\n**your mission, roles and responsibilities**\n--------------------------------------------\n\n\n\nrole summary  \n\nas a data scientist / machine learning engineer, you will be an integral part of our team, playing a pivotal role in leveraging data\\-driven insights to enhance our products and services and in developing and implementing cutting\\-edge ml and deep learning solutions across a variety of use cases. your responsibilities will span a wide range of use cases, including computer vision for object detection and recognition, predictive maintenance, anomaly detection, to pioneering generative ai projects and design optimization. you will work closely with cross\\-functional teams to design, develop, and deploy ai models that improve our efficiency and innovation. your contributions will directly impact our success in the automotive sector.  \n\nkey responsibilities  \n\n* design, develop, and deploy ai models, including but not limited to computer vision and generative ai, to solve complex problems in the automotive industry.\n* collaborate with cross\\-functional teams to understand business challenges and identify opportunities for leveraging machine learning technologies.\n* utilize generative ai models to innovate in design, r\\&d and manufacturing processes.\n* optimize existing ml models for performance and scalability, leveraging cuda advanced gpu architectures and parallel computing frameworks for efficient computation.\n* design and implement machine learning pipelines to preprocess data, train models, and evaluate performance metrics.\n* utilize python programming language and deep learning frameworks such as pytorch and tensorflow to implement machine learning solutions.\n* apply statistical techniques and machine learning algorithms to solve complex problems in the automotive sector.\n* stay abreast of the latest developments in data science, machine learning, deep learning, and ai technologies, and apply this knowledge to drive continuous innovation within the company.\n* develop and maintain documentation for data analysis methodologies, model development, and deployment processes, ensuring reproducibility and compliance with industry standards.\n**your profile and competencies to succeed**\n--------------------------------------------\n\n\n* phd or master\u2019s degree in computer science, artificial intelligence, machine learning, or a related field.\n* proven experience in machine learning, deep learning, and ai.\n* excellent understanding of python programming language, cuda, and gpu architectures for parallel computing and model optimization.\n* strong knowledge of machine learning architectures and techniques, with a focus on computer vision and generative ai.\n* proficiency in deep learning frameworks such as pytorch and tensorflow.\n* experience with parallel computing and optimization for high\\-performance computing environments.\n* ability to work collaboratively in a team and communicate complex concepts clearly to non\\-technical stakeholders.\n* innovative thinking with a passion for tackling challenging problems and pushing technological boundaries.\n* solid foundation in machine learning architectures, techniques, and frameworks, with a particular emphasis on applications in computer vision and generative ai.\n* excellent analytical skills, with the ability to interpret complex data and translate findings into strategic insights and practical solutions.\n* strong communication skills, capable of conveying technical concepts to non\\-technical stakeholders.\n* minimum of 5 years\u2019 experience in a machine learning engineering role, preferably within the automotive industry or a related sector.\n* proven experience in developing and deploying machine learning models for real\\-world applications.\n* self\\-motivated with a passion for continuous learning and professional development.\n* experience working in agile and on a cloud provider (such as azure) would be a plus\n**what we can do for you**\n--------------------------\n\n\n* at forvia, you will find an engaging and dynamic environment where you can contribute to the development of sustainable mobility leading technologies.\n* we are the seventh\\-largest global automotive supplier, employing more than 157,000 people in more than 40 countries which makes a lot of opportunity for career development.\n* we welcome energetic and agile people who can thrive in a fast\\-changing environment. people who share our strong values. team players with a collaborative mindset and a passion to deliver high standards for our clients. lifelong learners. high performers. globally minded people who aspire to work in a transforming industry, where excellence, speed, and quality count.\n* we cultivate a learning environment, dedicating tools and resources to ensure we remain at the forefront of mobility. our people enjoy an average of more than 22 hours of online and in\\-person training within forvia university (five campuses around the world)\n* we offer a multicultural environment that values diversity and international collaboration. we believe that diversity is a strength. to create an inclusive culture where all forms of diversity create real value for the company, we have adopted gender diversity targets and inclusion action plans.\n* achieving co2 net zero as a pioneer of the automotive industry is a priority: in june 2022, forvia became the first global automotive group to be certified with the new sbti net\\-zero standard (the most ambitious standard of sbti), aligned with the ambition of the 2015 paris agreement of limiting global warming to 1\\.5\u00b0c. three principles guide our action: use less, use better and use longer, with a focus on recyclability and circular economy.\n**why join us**\n---------------\n\n\n\nforvia is an automotive technology group at the heart of smarter and more sustainable mobility. we bring together expertise in electronics, clean mobility, lighting, interiors, seating, and lifecycle solutions to drive change in the automotive industry.\n\n\nwith a history stretching back more than a century, we are the 7th largest global automotive supplier, employing more than 157,000 people in 43 countries. you'll find our technology in around 1 out of 2 vehicles produced anywhere in the world.\n\n\nin june 2022, we became the 1st global automotive group to be certified with the sbti net\\-zero standard. we have committed to reach co2 net zero by no later than 2045\\.\n\n\nas technological innovation and the need for sustainability transform the automotive industry, we are ideally positioned to deliver solutions that will enhance the lives of road\\-users everywhere.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "AI and ML HPC Cluster Engineer",
        "company": "NVIDIA",
        "location": "Santa Clara, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 10.0,
        "matched_keywords": [
            "Generative AI",
            "RAG",
            "TensorFlow",
            "PyTorch",
            "Docker",
            "Python",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=608e4534825362a1",
        "description": "nvidia is a pioneer in accelerated computing, known for inventing the gpu and driving breakthroughs in gaming, computer graphics, high\\-performance computing, and artificial intelligence. our technology powers everything from generative ai to autonomous systems, and we continue to shape the future of computing through innovation and collaboration. within this mission, our team, managed ai superclusters (mars) builds and scales the infrastructure, platforms, and tools that enable researchers and engineers to develop the next generation of ai/ml systems. by joining us, you\u2019ll help design solutions that power some of the world\u2019s most advanced computing workloads.\nnvidia is looking for an ai/ml hpc cluster engineer to join our mars team. you will provide technical engagement and problem solving on the management of large\\-scale hpc systems including the deployment of compute, networking, and storage. you will be working with a team of passionate and skilled engineers across nvidia that are continuously working to provide better tools to build and manage this infrastructure. ideal candidate is strong in linux administration, networking, storage, job schedulers, driving improvements, and has the ability to understand researcher computing needs.\nwhat you'll be doing:* support day\\-to\\-day operations of production on\\-premises and multi\\-cloud ai/hpc clusters, ensuring system health, user satisfaction, and efficient resource utilization.\n* directly administer internal research clusters, conduct upgrades, incident response, and reliability improvements.\n* develop and improve our ecosystem around gpu\\-accelerated computing including developing scalable automation solutions.\n* maintain heterogeneous ai/ml clusters on\\-premises and in the cloud.\n* support our researchers to run their workloads including performance analysis and optimizations\n* analyze and optimize cluster efficiency, job fragmentation, and gpu waste to meet internal sla targets.\n* support root cause analysis and suggest corrective action. proactively find and fix issues before they occur.\n* triage and support postmortems for reliability incidents affecting users or infrastructure.\n* participate in a shared on\\-call rotation supported by strong automation, clear paths for responding to critical issues, and well\\-defined incident workflows.\n\n\nwhat we need to see:* bachelor\u2019s degree in computer science, electrical engineering or related field or equivalent experience\n* minimum 2\\+ years of experience administering multi\\-node compute infrastructure\n* background in managing ai/hpc job schedulers like slurm, k8s, pbs, rtda, bcm (formerly known as bright), or lsf\n* proficient in administering centos/rhel and/or ubuntu linux distributions\n* proven understanding of cluster configuration management tools (ansible, puppet, salt, etc.), container technologies (docker, singularity, podman, shifter, charliecloud), python programming, and bash scripting.\n* passion for continual learning and staying ahead of emerging technologies and effective approaches in the hpc and ai/ml infrastructure fields.\n\n\nways to stand out from the crowd:* background with nvidia gpus, cuda programming, nccl and mlperf benchmarking\n* experience with ai/ml concepts, algorithms, models, and frameworks (pytorch, tensorflow)\n* experience with infiniband with ibop and rdma\n* understanding of fast, distributed storage systems such as lustre and gpfs for ai/hpc workloads\n* applied knowledge in ai/hpc workflows that involve mpi\n\n\nyour base salary will be determined based on your location, experience, and the pay of employees in similar positions. the base salary range is 124,000 usd \\- 195,500 usd.\nyou will also be eligible for equity and benefits.\napplications for this job will be accepted at least until february 24, 2026\\.\nthis posting is for an existing vacancy.\nnvidia uses ai tools in its recruiting processes.\nnvidia is committed to fostering a diverse work environment and proud to be an equal opportunity employer. as we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Senior Software Engineer",
        "company": "eCapital",
        "location": "US USA",
        "posted_at": "2026-02-21",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Copilot",
            "Docker",
            "Kubernetes",
            "Git",
            "PostgreSQL",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=98e272ab19ecdfca",
        "description": "**about us**\n**at ecapital, we help businesses across north america and the u.k. access the capital they need to grow. a steady cash flow can change the course of a business, and through our commercial finance, freight factoring, and abl divisions, we provide funding solutions to 80\\+ industries. we were named to the inc. 5000 fastest growing company list in 2024, recognized as a 'great place to work' by the secured finance network, and celebrated as a 'most innovative company' by abf journal.**\n**the opportunity**\n**we're looking for a senior software engineer to join our global engineering team of 50\\+ engineers. you'll serve as a tech lead, taking ownership of complex systems that power our financial services platform using go, .net, microservices on kubernetes, and postgresql/mssql. beyond building, you'll design system architectures, mentor other engineers, and help shape how we deliver software. this is a role for someone who wants to drive impact, not just contribute.**\n**what\u2019s in it for you**\n**ai\\-native engineering.** **we're building an ai\\-native engineering culture where ai is embedded across the entire development lifecycle, from discovery through deployment. you'll work with spec\\-driven workflows, ai agents as collaborators, and tools like github copilot and claude code. this is a chance to be at the forefront of how software engineering is evolving.**\n**ownership and influence.** **you'll own significant parts of our platform and have real influence over technical direction. our teams are collaborative and self\\-directed, and senior engineers are expected to shape how we approach problems, not just solve them.**\n**room to grow.** **we're a fast\\-growing fintech company with clear paths to staff engineer and engineering leadership. you'll work alongside experienced architects while building your career in an environment that rewards initiative and impact.**\n**what you will do**\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n* design system and software architectures for full\\-stack fintech solutions\n* lead technical design decisions and drive architectural improvements across the platform\n* develop large\\-scale solutions for data processing and storage\n* mentor junior and intermediate engineers, helping them grow technically and professionally\n* own complex projects from conception through delivery\n* work within ai\\-native workflows: define problems clearly, provide context to ai agents, steer solutions, and verify outcomes\n* strategize on emerging technologies and communicate recommendations to leadership\n* collaborate with product, data, and infrastructure teams to deliver reliable solutions\n\n**what you bring*** demonstrated expertise in software engineering with 7\\+ years of professional experience\n* track record of owning and delivering complex projects\n* experience mentoring other engineers\n* strong full\\-stack development skills\n* bachelor's degree in computer science or a related field\n* strong communication skills\n\n**nice to have*** master's degree in computer science or a related field\n* experience with go, .net, or a comparable backend stack\n* experience with front\\-end frameworks such as angular, vue.js, or react\n* familiarity with microservices architecture and distributed systems\n* familiarity with relational databases\n* experience with ai\\-assisted development tools\n* experience with containerization and orchestration (docker, kubernetes)\n* experience with cloud platforms (aws, azure, or gcp)\n* experience in financial services, saas, or high\\-transaction\\-volume environments\n\n**we bring*** competitive compensation\n* annual bonus incentives\n* pto\n* comprehensive health, dental, and life benefits\n* 401(k) matching\n* professional development opportunities at a growing fintech company\n\n**ecapital culture**  \n\nat ecapital, we're not just a funding provider\u2014we're a strategic partner built for what's next. our culture is defined by innovation, scalability, and personalized service. we value:* **agility**: we adapt quickly to changing market conditions and customer needs.\n* **relationships**: we put our clients' needs at the center of everything we do, and we believe the best results come from diverse teams working together.\n* **accountability:** we hold ourselves to the highest standards in all aspects of our work.\n* **innovation**: we constantly push boundaries to create better solutions for our clients.\n\n**we offer a dynamic work environment where you'll have the opportunity to make a significant impact on our business and the smbs we serve. join us in revolutionizing how businesses access and manage capital in the digital age.**\n\n**ai statement**\n**ecapital uses ai\\-enabled tools within our applicant tracking system to support resume screening by comparing qualifications to job requirements. final hiring decisions and resume reviews are always conducted by our recruiters.**\n\n**vacancy type:** **addition**\n\n**how to apply**\n **submit your resume and a brief note about why you're interested in joining ecapital. our process includes an initial recruiter screen, a technical conversation, and a team fit discussion, typically completed within two weeks.**\n***ecapital values diverse experiences and backgrounds. we encourage all qualified candidates to apply, regardless of race, color, religion, gender, sexual orientation, national origin, genetics, disability, age, or veteran status.***\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n2aa1ilnn1a",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Engineer",
        "company": "Driven Brands",
        "location": "Charlotte, NC, US USA",
        "posted_at": "2026-02-21",
        "score": 10.0,
        "matched_keywords": [
            "BigQuery",
            "CI/CD",
            "Snowflake",
            "BigQuery",
            "MySQL",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=039063e25cce7591",
        "description": "company:driven brands\ndriven brands is north america's largest automotive services company with a portfolio of iconic brands including take 5 oil change\u00ae, meineke car care centers\u00ae, maaco\u00ae, 1\\-800\\-radiator \\& a/c\u00ae, auto glass now\u00ae, and carstar\u00ae. our vision is to fuel the pursuit with the simplest, most convenient, and most reliable car care experience.\n\n\nheadquartered in charlotte, nc, driven brands is more than a workplace. we're a launchpad \u2014 for careers, for dreams, and for people driven to do great things.\n\n\nevery day, we fuel the pursuit \u2014 for our customers chasing life's moments, for our franchisees building lasting legacies, and for each other as we grow, lead, and succeed together.\n\n\nperformance matters. we take pride in it. we own it. we show up for one another and for our communities.\n\n\nbecause at driven brands, we're not just fixing cars. we're building futures, unlocking potential, and fueling what's possible \u2014 together.\n\n**job description:**\n\n**data engineer**\n\n\nremote (located in eastern or central time)\n\n*we unfortunately cannot provide visa sponsorship now or in the future.*\n\n\nwe are seeking a skilled and associate\\-level data engineer to join our growing data team. in this role, you will be responsible for designing, building, and maintaining dimensional data models that support our analytics, ai, and business intelligence initiatives. you will work closely with stakeholders to ensure data is accessible, reliable, and optimized for performance.  \n\n  \n\n**how you will own it:**\n\n* **design \\& development:** create and maintain logical, physical, and conceptual data models in google big query using dataform.\n* **requirements gathering:** collaborate with stakeholders to understand business needs and deliver data products in support of those needs\n* **documentation:** develop and manage data dictionaries, metadata, and data lineage.\n* **optimization:** refine and update models for performance, scalability, and cost efficiencies.\n* **self\\-****motivated:** be responsive and take pride in delivering results quickly and accurately\n* **quality counts:** ensure data quality, integrity, and security across all data platforms.\n* **big picture mindset:** with multiple brands to support, we endeavor to look across the entire enterprise to find synergies and deliver consistency across our solutions.\n* **details matter:** we worry about the details so that our customers don\u2019t have to.\n\n**what you\u2019ll bring:*** bachelor\u2019s or master\u2019s degree in computer science, engineering, or a related field.\n* 3 years of experience in data engineering or a similar role.\n* strong experience with sql and relational databases (sql server, mysql, bigquery).\n* experience with cloud platforms (e.g., gcp, aws) and data services (e.g., bigquery, snowflake, dataform).\n* solid understanding of data modeling, etl/elt processes, and data warehousing concepts.\n* must be able to effectively communicate and collaborate with stakeholders.\n* experience within the google big query environment.\n* knowledge of ci/cd pipelines and devops practices.\n* excellent problem\\-solving skills and attention to detail.\n\n\napplicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.\n\n\n\\#li\\-as1\n\n\n\\#dbcorp\n\n\n\\#remote\n\n\nposition location:\n\n\nnorth carolina\ncompensation range:\n\n\n$61,700\\.00 \\- $110,300\\.00\ncompensation frequency:\n\n\nannual\nbase pay offered may vary depending on actual location, job\\-related knowledge, skills, and experience. supplemental pay types may include commissions or bonus incentives, depending on the role. driven brands offers a variety of health and wellness benefits including paid time off and holiday pay. details regarding our benefits can be found here: https://www.drivenbrandsbenefits.com\n\n\nget early access to 50% of your earned wages at any time through our myflexpay program.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist - Quantitative Trading",
        "company": "TotalEnergies",
        "location": "Houston, TX, US USA",
        "posted_at": "2026-02-21",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "Docker",
            "CI/CD",
            "MongoDB",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=51838a211f043506",
        "description": "### **contexte et environnement**\n\nin line with totalenergies\u2019 objective of achieving carbon neutrality by 2050, the company is rapidly expanding its global presence across the entire power value chain, with a particular focus on the fast\u2011growing sectors of renewables and flexible generation assets. to fully capture the value of these power assets in spot markets, totalenergies gas \\& power aims to participate at every stage of the value chain\u2014from curve trading to day\u2011ahead optimization, intraday activity, and ancillary services. short\u2011term power optimization is built on a u.s. framework that aggregates generation assets (ccgt, wind, solar), flexibilities (dsr, batteries), and the power\u2011supply portfolio to maximize value creation for the company.\n\n\nthe job holder is part of the algorithmic trading team based in houston and collaborates closely with sister teams in geneva. the team operates within an uncertain and highly volatile energy\u2011commodity trading environment influenced by global economic conditions, energy\u2011resource development, political and legislative changes, climate\u2011related factors, and evolving energy demand. it is a competitive landscape with significant pressure, time\u2011critical activities, and a high\u2011stress working environment.\n\n\noperations run automatically seven days a week with daily submissions. onsite presence is required five days per week, with periodic short\u2011term remote oversight on weekends and holidays.\n\n\n### **activit\u00e9s**\n\nthe role leads the team\u2019s quantitative and modeling efforts at the forefront of data science in the energy sector. you will be expected to apply advanced modeling techniques to support strategic decision\u2011making and contribute directly to p\\&l generation. you will also provide data\u2011science expertise across the team, offering support on a wide range of modeling and optimization topics.\n\n* **purpose:** the position sits within the modelling and analysis for trading strategy (mats) department, which leads the end\u2011to\u2011end design and development of trading strategies, quantitative models, advanced analytical and ai tools, and market research that support optimized decision\u2011making across all tgp trading desks. positioned at the strategic intersection of trading, risk management, and it, the team ensures agility, robustness, and continuous innovation in the algorithms that drive tgp\u2019s market strategy.\n* **objectives:** contribute to the growth of the algo\u2011trading activity by developing forecasting models, decision\u2011making algorithms, and positioning optimizations. build virtual da\u2011rt strategies and intraday proprietary trading strategies within the established risk\u2011policy framework and compliance rules for u.s. nodal power markets.\n* **impact:** the role directly influences tgp\u2019s trading performance by supporting long\u2011term p\\&l growth through scalable trading strategies and durable analytical infrastructure. it requires strong technical skills, sound risk awareness, critical thinking, and a collaborative mindset. this is a strategic position that strengthens totalenergies gas \\& power\u2019s presence in u.s. short\u2011term power markets.\n* **location: based in houston.**\n\n**key tasks:**\n\n* develop trading models using the mats libraries, covering the full lifecycle from data ingestion to live result delivery, including feature engineering, data optimization, model tuning, risk and positioning models, performance monitoring, and probability meta\u2011optimization.\n* create new indicators and features to enhance machine\u2011learning and quantitative models used in algorithmic trading.\n* monitor model performance in both live and paper\u2011trading environments.\n* oversee production deployment of models in collaboration with squad members and cross\u2011functional mats teams.\n* improve algorithmic\u2011trading strategies through research on u.s. nodal power markets.\n* analyze large datasets to generate meaningful insights that support strategy development.\n* coordinate with team members to ensure high\u2011quality back\u2011testing, validation, and methodological robustness.\n\n\n### **profil du candidat**\n\n* talented and highly motivated individual with a strong technical or engineering background, holding a beng, msc, or phd in a quantitative field such as mathematics, physics, machine learning, optimization, or computer science.\n* one to three years of experience deploying machine\u2011learning models in production (delivering live forecasts) and/or formulating and solving bet\u2011sizing and positioning problems as stochastic optimization under uncertainty.\n* proficiency in python, mongodb, and shell scripting, with a solid understanding of software architecture and sql.\n* familiarity with docker and devops/mlops practices, including ci/cd workflows.\n* in\u2011depth knowledge of machine\u2011learning techniques, including supervised and unsupervised modeling.\n* strong understanding of quantitative bet\u2011sizing methods such as utility\u2011based approaches, kelly\u2011fraction techniques, and risk\u2011constrained optimization.\n* practical experience with time\u2011series analysis, particularly in the context of energy markets and trading signals.\n* ability to analyze complex systems and identify indirect correlations across diverse signals.\n* strong project\u2011management capabilities with clear communication and effective reporting skills.\n\n**additional and advantageous qualifications:**\n\n* broad understanding of energy and commodities markets, including supply\u2011side dynamics.\n* experience with project\u2011management tools such as wrike, jira, or linear.\n\n\n### **informations suppl\u00e9mentaires**\n\n**totalenergies valorise la diversit\u00e9, promeut le d\u00e9veloppement individuel et offre des opportunit\u00e9s d'emploi \u00e9gales \u00e0 tous les candidats.**\n  \n**what we offer**\nat totalenergies we know that you're more than what's on your cv. if this opportunity excites you, but your profile doesn't exactly match the description above...give it a try \\& apply. diversity of perspectives \\& experiences make us stronger.  \n\ntotalenergies celebrates diversity and is committed to equal employment opportunity:\nall qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, sexual orientation, or gender identity), national origin, age, protected veteran status, disability status or any other category protected under applicable federal, state or local law. we are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures.  \n\nall aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs.  \n\ntotalenergies believes that diversity and inclusion among our teammates is critical to our success as a global company, and we seek to recruit, develop, and retain the most talented people from a diverse candidate pool. **about the company:**\ntotalenergies is a multi\\-energy company that produces and markets energies on a global scale: oil and biofuels, natural gas and green gases, renewables and electricity. our 105,000 employees are committed to energy that is ever more affordable, clean, reliable and accessible to as many people as possible. active in more than 130 countries, totalenergies puts sustainable development in all its dimensions at the heart of its projects and operations to contribute to the well\\-being of people. **totalenergies gas and power (tgp)** is the trading arm of totalenergies in the field of low carbon energies (mainly gas, lng and power). as such it operates in fast\\-evolving market dynamics influenced by internal and external factors that require constant adaptation and evolution. uncertainties specific to the trading environment (volatility of prices, supply \\& demand mismatches) are coupled with those coming from the broader energy sector (climate change policies, changes in the energy mix, developments of new energy sources, etc). in such context tgp helps to ensure growth and profitability to a key segment of the business in order to reach the objective of carbon neutrality by 2050\\. **our culture:**\nwe are committed to meet the energy needs of a growing population while fulfilling our ambition to be a major player in the energy transition. the volatility of the energy trading markets requires excellence in risk management and a culture of innovation.  \n\nour people flourish in an environment that promotes expertise, entrepreneurship spirit, agility and a purpose\\-driven culture, generating boundless opportunities to learn, grow and achieve collaborative success while ensuring safety in all our operations.  \n\nwe look for passion, ambition and open\\-mindedness. while we evolve in a demanding and ever\\-changing industry, we cultivate a friendly workplace where team spirit and respect guide our daily routine, and where the diversity of our people and their skills create a nourishing experience for all of us. **please note:** totalenergies is unable to sponsor employment visas or consider candidates on time\\-limited visa status for this position. **ready to power the future of energy trading?**\napply now and be part of a team that\u2019s shaping the future of low\\-carbon energy.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Sr. Business Intelligence Engineer - Digital Experiences & Capabilities",
        "company": "Visa",
        "location": "San Francisco, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 10.0,
        "matched_keywords": [
            "RAG",
            "Git",
            "Hadoop",
            "Tableau",
            "Power BI",
            "R",
            "Scala",
            "Optimization",
            "A/B Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=3df7a7234ca3a31b",
        "description": "**company description**  \n\nvisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose \u2013 to uplift everyone, everywhere by being the best way to pay and be paid.\n\n\nmake an impact with a purpose\\-driven industry leader. join us today and experience life at visa.\n\n **job description**  \n\njob summary\n\n\nwe are seeking a hybrid data specialist with strong data engineering expertise and data analytics skills to play a critical role in building and optimizing the data foundation for visa marketing 360, visa\u2019s global marketing automation and personalization platform. in this role, you will partner with technology teams to design, build, and optimize the data foundation that powers visa marketing 360, define and maintain a flexible, scalable data model that adapts to evolving business and platform needs, and ensure that internal and external data sources are seamlessly integrated, cleansed, enriched, and ready for use in marketing campaigns.\n\n\nmain responsibilities\n\n**data engineering \\& integration**\n\n* partner with technology to prepare and maintain the data foundation for visa marketing 360, integrating multiple internal and external data sources and embed ai\\-driven data processing workflows to improve speed, accuracy, and scalability.\n* perform data cleansing, transformation, and enrichment to create high\\-quality datasets for marketing strategies.\n* integrate internal predictive models and segmentation engine into platform workflows.\n* develop processes to monitor and maintain data quality, proactively identifying and resolve issues.\n* partner with technology to build and maintain data pipelines that ensure reliable, real\\-time, and batch data availability for marketing campaigns\n\n**data model definition \\& governance**\n\n* design, implement, and maintain a flexible, scalable data model that supports evolving marketing campaign requirements, advanced analytics, and ai\\-driven personalization strategies.\n* ensure data model accommodates new data sources, changing business rules, and regional variations without disrupting existing operations.\n* collaborate with product and marketing teams to translate business requirements into robust data structures and relationships.\n* establish data governance standards to maintain consistency, accuracy, and compliance across the platform\u2019s datasets.\n\n**data analytics**\n\n* develop and maintain dashboards that incorporate ai\\-generated insights and predictive kpis to guide marketing strategy.\n* analyze campaign and customer data to identify trends, patterns, and opportunities for optimization.\n* partner with marketing teams to translate data into actionable insights for targeting, personalization, and messaging strategies\n* contribute to a/b testing design and campaign measurement strategies\n\n**ai \\& automation**\n\n* leverage ai to automate data preparation, segmentation, and campaign optimization workflows, reducing time\\-to\\-market and operational effort.\n* evaluate and deploy cutting\\-edge ai analytics tools to enhance decision\\-making speed, accuracy, and innovation in marketing execution.\n\n**collaboration \\& enablement**\n\n* work closely with the sr. product manager to align data capabilities with platform strategy.\n* partner with marketing operations lead to ensure data readiness for campaign execution.\n* provide training and guidance to marketing and operations teams on data tools, dashboards, and best practices.\n\n**key attributes for success**\n\n* **engineering\\-first mindset:** builds scalable, efficient, and reliable data systems.\n* **data model architect:** designs structures that adapt to evolving business needs\n* **analytics\\-**driven: uses data to uncover insights and improve decision\\-making\n* **ai innovator:** seeks out opportunities to embed ai into every stage of the data lifecycle for better speed, accuracy, and impact.\n\n\nthis is a hybrid position. expectation of days in office will be confirmed by your hiring manager.\n\n**this position is not eligible for sponsorship**\n\n **qualifications**  \n\nbasic qualifications\n\n* 8 or more years of relevant work experience with a bachelor degree or at least 5 years of experience with an advanced degree (e.g. masters, mba, jd, md) or 2 years of work experience with a phd\n\n  \n\npreferred qualifications\n\n* 9 or more years of relevant work experience with a bachelor degree or 7 or more relevant years of experience with an advanced degree (e.g. masters, mba, jd, md) or 3 or more years of experience with a phd\n* proven track record in designing and implementing scalable, flexible data models that support evolving business and analytical needs.\n* experience integrating multiple internal and external data sources (e.g., transactional, behavioral, marketing engagement).\n* hands\\-on experience working with large datasets (e.g. hadoop ecosystem)\n* demonstrated ability to clean, manipulate, analyze, visualize, and model large complex datasets.\n* experience transforming technical data outputs into actionable business insights for non\\-technical stakeholders\n* track record in automation of data collection, processing, segmentation, and quality monitoring.\n* proven ability to apply ai/ml concepts to marketing, segmentation, and personalization use cases.\n* experience with dashboarding and visualization tools (tableau, power bi, looker) to support analytics and kpi monitoring.\n* proficiency in data governance and quality frameworks.\n* strong cross\\-functional collaboration and project management skills, with the ability to coordinate deliverables across global teams.\n* excellent communication skills with the ability to translate complex technical topics into clear business language.\n* master\u2019s degree preferred in data science, big data engineering, statistics, or a related technical discipline.\n* strong knowledge of emerging ai trends in data engineering, analytics, and martech, with the ability to translate trends into practical solutions for visa marketing 360\\.\n* strong knowledge of data privacy and compliance (gdpr, ccpa) in marketing contexts.\n* experience in marketing data strategy and integration into martech platforms\n\n  \n\n**additional information** **work hours:** varies upon the needs of the department.\n\n**travel requirements:** this position requires travel 5\\-10% of the time.\n\n**mental/physical requirements:** this position will be performed in an office setting. the position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.\n\n\nvisa is an eeo employer. qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. visa will also consider for employment qualified applicants with criminal histories in a manner consistent with eeoc guidelines and applicable local law.\n\n\nvisa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of article 49 of the san francisco police code.\n\n**u.s. applicants only: the estimated salary range for a new hire into this position is 180,600\\.00 to 262,150\\.00 usd per year, which may include potential sales incentive payments (if applicable). salary may vary depending on job\\-related factors which may include knowledge, skills, experience, and location. in addition, this position may be eligible for bonus and equity. visa has a comprehensive benefits package for which this position may be eligible that includes medical, dental, vision, 401 (k), fsa/hsa, life insurance, paid time off, and wellness program.**",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Application Development Intern - Artificial Intelligence",
        "company": "C1",
        "location": "Remote, US USA",
        "posted_at": "2026-02-20",
        "score": 10.0,
        "matched_keywords": [
            "TensorFlow",
            "Keras",
            "NLTK",
            "Git",
            "Kafka",
            "MongoDB",
            "Python",
            "R",
            "Java"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=b3168e2727d0f09d",
        "description": "c1 company overview:\n**c1: 1 contact, 1 connection, 1 choice**  \n\nc1 is the foremost, single\\-source provider of advanced communications and data technology for business. that means if it's digital, we connect our customers to it \\- from phone systems and hardware to computer networks, application development, managed solutions and more. and we're 100% passionate with designing, implementing, managing and supporting our customers' every need from end to end, so that they can focus on what they do best.  \n\nso, when it comes to joining a team of it and communications technology pros who are empowered to do what they do best, your best choice \\- your \\#1 choice \\- is c1\\.\noverview:\n**summary**\n\n\nthe application development intern will be paired with application development\u2019s principal architect and receive guidance from the director, engineering in the development and programming of convergeone software intellectual property (ip) related to applying the open\\-source artificial intelligence/natural language tool kit to the conversations solution. with an advanced educational background (bs or ms in process) in software development and programming, ad interns will gain experience in a fast\\-paced environment, handling multiple projects simultaneously, and be given an opportunity to apply their academic learnings, showcase their initiative and drive to expand responsibilities. roles will be outlined in more detail as the start date approaches depending on convergeone ip development roadmaps and requirements.\n\n***\\#li\\-jm1***\n\n\nresponsibilities:\n**essential functions**\n\n* understands and learns about c1 and c1\u2019s corporate directions\n* learns and supports strategies and activities for c1\u2019s innovation and ip development.\n* gains practical, hands\\-on experience with current/next generation:\n\t+ technologies, e.g., bots and artificial intelligence, internet of things, cloud services, microservices architectures, etc.\n\t+ programming languages, e.g., angularjs, node.js, python, etc.\n\t+ big data technologies, e.g., elasticsearch, mongodb, etc.\n\t+ queueing frameworks, e.g., rabbitmq, kafka, etc.\n\t+ cloud infrastructures, e.g., gcp, amazon web services, azure cloud, etc.\n\t+ development methodologies and devops, e.g., agile/lean\n* develops and enhances (through involvement with the complete software development lifecycle):\n\t+ programming and software development skills\n\t+ on\\-job training using cutting\\-edge technology tools\n\t+ problem solving and decision\\-making skills\n* + creative and critical thinking\n\t+ collaboration, communication, and negotiation\n* implements an ai\\-powered chatbot using dialogflow/llms\n* trains and optimizes the bot to make more natural conversations\n* interpretes business conversation flows and conversion of those flows into conversation configuration\n\n\nqualifications:\n**required qualifications**\n\n* strong verbal, written communication skills\n* in\\-process bs/ms computer science or electronic engineering with a focus on software development\n* programming skills/aptitude and knowledge of python and java.\n* nodejs (node.js), python, natural language processing (nlp), webhook development. knowledge of llm/dialogflow/koreai api is advantageous\n* good troubleshooting \\+ problem\\-solving skills\n* ability to work independently as well as on a team and learn from colleagues\n* high adaptability in a dynamic start\\-up environment.\n* basic understanding of ms office preferably visio, powerpoint, word and/or excel\n\n**desired/preferred qualifications**\n\n* specific artificial intelligence skills using frameworks such as crew.ai, model context protocol, nltk, tensorflow, dialogflow, scikit\\-learn, keras, etc.\n* specific java script programming skills using frameworks such as node.js, angularjs, etc.\n\n\nadditional information:\n**work environment**  \n\nability to handle multiple priorities and demands in a fast\\-paced environment. this job operates in a professional office environment. this role routinely uses standard office equipment such as computers, phones, photocopiers, and filing cabinets.  \n\n  \n\n**physical environment**  \n\nphysical demands described here are representative of those that must be met by a team member to successfully perform the essential functions of this job. reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this job.  \n\n  \n\n**other duties/changes**  \n\nthis job description is not designed to cover or contain a comprehensive listing of all duties, responsibilities or activities that are required of a team member for this job. duties, responsibilities and activities may change at any time with or without notice. at any point in time, the essential functions and primary duties associated with this position will be the principal, major or most important duties, responsibilities and activities that the employee is expected to perform as determined and directed by c1\\. **eeo statement**  \n\nc1 provides equal employment opportunities (eeo) to all team members and applicants for employment opportunities. all qualified applicants will receive consideration for employment, and all team members will be treated with respect to their employment, without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability or veteran status. for further details, please view the eeo policy statement (eeo policy statement) and/or the current version of the workplace poster (https://www.eeoc.gov/sites/default/files/2023\\-06/22\\-088\\_eeoc\\_knowyourrights6\\.12screenrdr.pdf; https://www.eeoc.gov/poster) **e\\-verify:** e\\-verify **right to work**: right to work poster",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Data Scientist Associate - Payments Data & Analytics",
        "company": "JPMorganChase",
        "location": "New York, NY, US USA",
        "posted_at": "2026-02-21",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "Generative AI",
            "RAG",
            "Databricks",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Scala"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=88e65ed63acca9c3",
        "description": "**job description**  \n\njoin the jpm payments data \\& analytics' treasure service team at jpmorgan chase, where we leverage data science, research, and business acumen to promote client\\-focused innovation. apply your technical skills to projects involving payments analytics and cutting\\-edge gen\\-ai applications, creating impactful solutions for chase treasury services clients.\n\n\nas a data scientist associate in our treasury services team, you will work on projects that integrate agentic workflows, business intelligence, data engineering, and data visualization to grow our payments business. you will play a pivotal role in developing our generative ai use cases and approach.\n\n\nyour work will involve leveraging your technical skills and business acumen to collaboratively design and implement end\\-to\\-end solutions with tangible commercial impact for our clients and customers. our team supports both internal and external payments intelligence applications, as well as custom client analytics requests on an ad hoc basis.\n\n\nutilizing tools such as databricks, python, tableau and sigma, you will develop data pipelines and dashboards, prototype new generative ai capabilities and features, and ensure the quality and integrity of our intelligent solutions. your contributions will help streamline operations, enhance decision\\-making, and foster growth through innovative ai applications.\n\n\n**job responsibilities**\n\n\n* learn our business and how jpmorgan delivers value for our clients globally\n* leverage your technical skillset and curiosity to identify opportunities to help grow our ts business\n* build and rigorously test new data and ai driven insights\n* develop scalable frameworks for seamless ai model integration across business applications. ensure solutions can adapt and expand as needed.\n* build and test ai agents. iterate designs to enhance functionality and user experience. conduct rigorous testing for reliability and effectiveness.\n* use tools like databricks, python, tableau and sigma to create data pipelines and dashboards. support ai\\-driven insights and decision\\-making.\n* monitor ai model performance. identify areas for enhancement. implement updates to maintain quality and relevance.\n* work on agile teams to support data\\-driven decision\\-making and client relationship management.\n**required qualifications, capabilities, and skills**\n\n\n* bachelor's degree in science, technology, engineering, mathematics (stem) or related field.\n* proficiency in python and sql for data processing and analysis.\n* strong problem\\-solving and critical thinking skills.\n* experience in leveraging large language models (llms) and agentic ai techniques.\n* ability to automate repeatable tasks to enhance business process efficiency.\n**preferred qualifications, capabilities, and skills**\n\n\n* experience in data analysis, data science, or related fields.\n* familiarity with aws, databricks for data management and analysis.\n* understanding of quality assurance practices and the importance of data integrity.\n* knowledge of machine learning/data science theory, techniques, and tools.\n* awareness of big data technologies (e.g., spark) and distributed computing concepts like mapreduce.\n**about us**  \n\n\njpmorganchase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world\u2019s most prominent corporate, institutional and government clients under the j.p. morgan and chase brands. our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\nwe offer a competitive total rewards package including base salary determined based on the role, experience, skill set and location. those in eligible roles may receive commission\\-based pay and/or discretionary incentive compensation, paid in the form of cash and/or forfeitable equity, awarded in recognition of individual achievements and contributions. we also offer a range of benefits and programs to meet employee needs, based on eligibility. these benefits include comprehensive health care coverage, on\\-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. additional details about total compensation and benefits will be provided during the hiring process.\n\n\nwe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. we are an equal opportunity employer and place a high value on diversity and inclusion at our company. we do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. we also make reasonable accommodations for applicants\u2019 and employees\u2019 religious practices and beliefs, as well as mental health or physical disability needs. visit our faqs for more information about requesting an accommodation.\n\n\njpmorgan chase \\& co. is an equal opportunity employer, including disability/veterans\n\n  \n\n  \n\n  \n\n**about the team**  \n\n  \n\nj.p. morgan\u2019s commercial \\& investment bank is a global leader across banking, markets, securities services and payments. corporations, governments and institutions throughout the world entrust us with their business in more than 100 countries. the commercial \\& investment bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world.",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "DATA SCIENTIST - SUPPLY CHAIN",
        "company": "The Home Depot",
        "location": "Atlanta, GA, US USA",
        "posted_at": "2026-02-21",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "BigQuery",
            "BigQuery",
            "Tableau",
            "Python",
            "SQL",
            "R",
            "Optimization"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=cef5a66a07b4ad4f",
        "description": "**position purpose:**\nthe data scientist is responsible for supporting data science initiatives that drive business profitability, increased efficiencies and improved customer experience. this role applies industry\\-leading analytical methodologies for working with large datasets to extract meaningful business insight and creatively solve business problems. data scientists are also responsible for ensuring that developed codes are documented into a library of reusable algorithms. based on the specific data science team, this role would need to be knowledgeable in one or more data science specializations, such as optimization, computer vision, recommendation, search or nlp.\n  \n\nas a data scientist, you will apply advanced analytics methods and algorithms for identifying trends and providing business solutions. this role is expected to present insights and recommendations to non\\-technical audiences and explain the benefits and impacts of the recommended solutions. in addition, data scientists collaborate with business partners and cross\\-functional teams, requiring effective communication skills, building relationships, and focus on understanding the overall business area being supported.\n\n **key responsibilities:*** 55% solution development \\- design and develop algorithms and models to use against large datasets to create business insights; participates in large data analytics project teams by serving as a technical lead for analytics projects; may lead small projects and work independently on solution development; execute tasks with high levels of efficiency and quality; make appropriate selection, utilization and interpretation of advanced analytical methodologies\n* 20% communicating results \\- effectively communicate insights and recommendations to both technical and non\\-technical leaders and business customers/partners; present recommendations in a confident manner in order to influence execution of recommendation; prepare reports, updates and/or presentations related to progress made on a project or solution; clearly communicate impacts of recommendations to drive alignment and appropriate implementation\n* 10% business collaboration \\- incorporate business knowledge into solution approach; effectively develop trust and collaboration with internal customers and cross\\-functional teams; work with project teams and business partners to determine project goals\n* 15% technical exploration \\& development \\- seek further knowledge on key developments within data science, technical skill sets, and additional data sources; participate in the continuous improvement of data science and analytics by developing replicable solutions (for example, codified data products, project documentation, process flowcharts) to ensure solutions are leveraged for future projects; build and maintain library of reusable algorithms for future use, ensuring developed codes are documented\n\n  \n\n**direct manager/direct reports:*** this position typically reports to manager or above\n* this position has 0 direct reports\n\n  \n\n**travel requirements:*** typically requires overnight travel less than 10% of the time.\n\n  \n\n**physical requirements:*** most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. on rare occasions there may be a need to move or lift light articles.\n\n  \n\n**working conditions:*** located in a comfortable indoor area. any unpleasant conditions would be infrequent and not objectionable.\n\n  \n\n**minimum qualifications:*** must be eighteen years of age or older.\n* must be legally permitted to work in the united states.\n\n  \n\n**preferred qualifications:*** masters in a quantitative field (computer science, math, statistics, etc.) or equivalent work experience\n* 4\\+ years of experience in business intelligence and analytics\n* working knowledge of microsoft excel and power point\n* experience in a modern scripting language (preferably python)\n* proficient running queries against data (preferably with google bigquery or sql)\n* proficient with data visualization software (preferably tableau)\n* proficient utilizing statistical techniques to identify key insights that help solve business problems\n* knowledgeable in prescriptive modeling like optimization, computer vision, recommendation, search or nlp\n* demonstrated experience in predictive modeling, data mining and data analysis\n\n  \n\n**minimum education:*** the knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.\n\n  \n\n**preferred education:*** no additional education\n\n  \n\n**minimum years of work experience:*** 3\n\n  \n\n**preferred years of work experience:*** no additional years of experience\n\n  \n\n**minimum leadership experience:*** none\n\n  \n\n**preferred leadership experience:*** none\n\n  \n\n**certifications:*** none\n\n  \n\n**competencies:*** action oriented: taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm\n* business insight: applying knowledge of the business and the marketplace to advance the organization's goals\n* collaborates: building partnerships and working collaboratively with others to meet shared objectives\n* communicates effectively: developing and delivering multi\\-mode communications that convey a clear understanding of the unique needs of different audiences\n* customer focus: building strong customer relationships and delivering customer\\-centric solutions\n* drives results: consistently achieving results, even under tough circumstances\n* nimble learning: actively learning through experimentation when tackling new problems, using both successes and failures as learning fodder\n* optimizes work processes: knowing the most efficient and effective processes to get things done, with a focus on continuous improvement\n* plans and aligns: planning and prioritizing work to meet commitments aligned with organizational goals\n* self\\-development: actively seeking new ways to grow and be challenged using both formal and informal development channels",
        "scrapped_date": "2026-02-21"
    },
    {
        "title": "Sr Data Scientist (Remote)",
        "company": "First American",
        "location": "Santa Rosa, CA, US USA",
        "posted_at": "2026-02-20",
        "score": 10.0,
        "matched_keywords": [
            "Data Scientist",
            "RAG",
            "MLflow",
            "Python",
            "SQL",
            "R",
            "Scala",
            "Optimization",
            "A/B Testing"
        ],
        "apply_link": "https://www.indeed.com/viewjob?jk=7152380fec2fea5b",
        "description": "**who we are**\n--------------\n\n\njoin a team that puts its people first! as a member of first american\u2019s family of companies, first american home warranty offers a wide range of home warranty products and services to home sellers, buyers and agents. since 1889, first american (nyse: faf) has held an unwavering belief in its people. they are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. our inclusive, people\\-first culture has earned our company numerous accolades, including being named to the fortune 100 best companies to work for\u00ae list for ten consecutive years. we have also earned awards as a best place to work for women, diversity and lgbtq\\+ employees, and have been included on more than 50 regional best places to work lists. first american will always strive to be a great place to work, for all. for more information, please visit www.careers.firstam.com.**what we do**\n--------------\n\n\nidentifies business trends and problems through complex data analysis. interprets results from multiple sources using a variety of techniques, ranging from simple data aggregation via statistical analysis to advanced machine learning and data mining methodologies. designs, develops, and implements high\\-impact, scalable business solutions that drive measurable value across the organization. partners cross\\-functionally to translate business needs into analytical frameworks and actionable insights.**how you\u2019ll contribute**\n\n* leverage expertise in handling large, complex datasets to perform exploratory data analysis, feature engineering, and statistically sound sample design\n* build, validate, deploy, and enhance complex predictive and machine learning models using data from multiple structured and unstructured sources to materially improve business outcomes\n* design, build, deploy, and monitor machine learning models in production environments, ensuring scalability, reliability, and performance\n* implement model lifecycle management best practices including versioning, experiment tracking, monitoring, retraining strategies, and performance optimization (e.g., mlflow or similar frameworks)\n* automate feedback loops for algorithms and models in production while creating scalable, repeatable data science processes and production\\-grade data products\n* design and execute experiments (a/b testing, causal inference frameworks) to evaluate model performance and business impact\n* act as a technical subject matter expert, recommending and developing innovative analytical approaches aligned with strategic priorities\n* translate complex analytical findings into clear business insights and present results to executive and non\\-technical audiences\n* mentor junior data scientists and contribute to establishing enterprise best practices in modeling, governance, and documentation\n* collaborate with data engineering teams to ensure robust pipelines, high data quality, and scalable model deployment\n* influence cross\\-functional teams and drive adoption of data\\-driven decision\\-making\n* other duties as assigned\n\n**what you\u2019ll bring**\n\n**required education, experience, certification/licensure**\n\n* bachelor\u2019s degree or equivalent experience in a quantitative field such as mathematics, statistics, computer science, engineering, economics, or related discipline\n* master\u2019s or phd preferred\n* 5\u20137\\+ years of professional experience building, validating, deploying, and monitoring predictive and machine learning models in production environments\n* strong programming expertise in **python** with hands\\-on experience building end\\-to\\-end ml solutions\n* experience with **mlflow** (or similar model lifecycle tools) for experiment tracking, model versioning, and deployment management\n* experience working with **sql** and large\\-scale data processing frameworks such as **apache spark**\n* experience designing scalable data solutions in modern data platforms; experience with **microsoft fabric** is a plus\n* proven experience applying machine learning techniques (e.g., regression, classification, clustering, ensemble methods, nlp, etc.)\n* strong communication skills with the ability to explain complex analytical concepts to non\\-technical stakeholders\n\n**\\*\\* note that the following statements only apply to candidates who will be working from an unincorporated area within los angeles county. \\*\\***\nfirst american will consider for employment all qualified applicants, including those with arrest or conviction records, in a manner consistent with the requirements of applicable state and local laws (e.g., the los angeles county fair chance ordinance for employers and the california fair chance act).\nfirst american intends to conduct a review of an applicant\u2019s criminal history in connection with a conditional offer. first american reasonably believes that a criminal history may have a direct, adverse and negative relationship with the following material job duties for this position potentially resulting in the withdrawal of the conditional offer of employment: handling of confidential, proprietary or trade secret information belonging to first american or its customers, administrating or facilitating financial transactions, and the ability to meet customer\\-imposed criminal history requirements.\n**what we offer**\n-----------------\n\n\nby choice, we don\u2019t simply accept individuality \u2013 we embrace it, we support it, and we thrive on it! our people first culture celebrates diversity, equity and inclusion not simply because it\u2019s the right thing to do, but also because it\u2019s the key to our success. we are proud to foster an authentic and inclusive workplace for all. you are free and encouraged to bring your entire, unique self to work. first american is an equal opportunity employer in every sense of the term.\nbased on eligibility, first american offers a comprehensive benefits package including medical, dental, vision, 401k, pto/paid sick leave and other great benefits like an employee stock purchase plan.",
        "scrapped_date": "2026-02-21"
    }
]